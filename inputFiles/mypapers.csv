source_id,year,title,abstract,full_text
5220,1997,Learning Generative Models with the Up Propagation Algorithm,"Up-propagation is an algorithm for inverting and learning neural network
generative models Sensory input is processed by inverting a model that
generates patterns from hidden variables using topdown connections
The inversion process is iterative utilizing a negative feedback loop that
depends on an error signal propagated by bottomup connections The
error signal is also used to learn the generative model from examples
The algorithm is benchmarked against principal component analysis in
experiments on images of handwritten digits.","Learning Generative Models with the

Up(cid:2)Propagation Algorithm

Jong(cid:2)Hoon Oh and H(cid:3) Sebastian Seung

Bell Labs(cid:2) Lucent Technologies

Murray Hill(cid:2) NJ 	

fjhoh(cid:2)seungg(cid:3)bell(cid:4)labs(cid:5)com

Abstract

Up(cid:2)propagation is an algorithm for inverting and learning neural network
generative models(cid:3) Sensory input is processed by inverting a model that
generates patterns from hidden variables using top(cid:2)down connections(cid:3)
The inversion process is iterative(cid:4) utilizing a negative feedback loop that
depends on an error signal propagated by bottom(cid:2)up connections(cid:3) The
error signal is also used to learn the generative model from examples(cid:3)
The algorithm is benchmarked against principal component analysis in
experiments on images of handwritten digits(cid:3)

In his doctrine of unconscious inference(cid:2) Helmholtz argued that perceptions are
formed by the interaction of bottom(cid:7)up sensory data with top(cid:7)down expectations(cid:8)
According to one interpretation of this doctrine(cid:2) perception is a procedure of sequen(cid:7)
tial hypothesis testing(cid:8) We propose a new algorithm(cid:2) called up(cid:7)propagation(cid:2) that
realizes this interpretation in layered neural networks(cid:8) It uses top(cid:7)down connections
to generate hypotheses(cid:2) and bottom(cid:7)up connections to revise them(cid:8)

It is important to understand the di(cid:9)erence between up(cid:7)propagation and its an(cid:7)
cestor(cid:2) the backpropagation algorithm(cid:10)(cid:12)(cid:8) Backpropagation is a learning algorithm
for recognition models(cid:8) As shown in Figure a(cid:2) bottom(cid:7)up connections recognize
patterns(cid:2) while top(cid:7)down connections propagate an error signal that is used to learn
the recognition model(cid:8)

In contrast(cid:2) up(cid:7)propagation is an algorithm for inverting and learning generative
models(cid:2) as shown in Figure b(cid:8) Top(cid:7)down connections generate patterns from a
set of hidden variables(cid:8) Sensory input is processed by inverting the generative
model(cid:2) recovering hidden variables that could have generated the sensory data(cid:8)
This operation is called either pattern recognition or pattern analysis(cid:2) depending
on the meaning of the hidden variables(cid:8) Inversion of the generative model is done
iteratively(cid:2) through a negative feedback loop driven by an error signal from the
bottom(cid:7)up connections(cid:8) The error signal is also used for learning the connections

error

recognition

generation

error

(a)

(b)

Figure (cid:13) Bottom(cid:7)up and top(cid:7)down processing in neural networks(cid:8) (cid:14)a(cid:15) Backprop
network (cid:14)b(cid:15) Up(cid:7)prop network

in the generative model(cid:8)

Up(cid:7)propagation can be regarded as a generalization of principal component analysis
(cid:14)PCA(cid:15) and its variants like Conic(cid:10)(cid:12) to nonlinear(cid:2) multilayer generative models(cid:8) Our
experiments with images of handwritten digits demonstrate that up(cid:7)propagation
learns a global(cid:2) nonlinear model of a pattern manifold(cid:8) With its global parametriza(cid:7)
tion(cid:2) this model is distinct from locally linear models of pattern manifolds(cid:10)(cid:12)(cid:8)



INVERTING THE GENERATIVE MODEL

The generative model is a network of L (cid:18)  layers of neurons(cid:2) with layer  at the
bottom and layer L at the top(cid:8) The vectors xt(cid:2) t (cid:19)  (cid:2) (cid:2) (cid:2) L(cid:2) are the activations of
the layers(cid:8) The pattern x is generated from the hidden variables xL by a top(cid:7)down
pass through the network(cid:2)

xt(cid:0) (cid:19) f (cid:14)Wtxt(cid:15)(cid:3)

t (cid:19) L(cid:3) (cid:2) (cid:2) (cid:2) (cid:3)  (cid:2)

(cid:14)(cid:15)

The nonlinear function f acts on vectors component by component(cid:8) The matrix
Wt contains the synaptic connections from the neurons in layer t to the neurons in
layer t (cid:2) (cid:8) A bias term bt(cid:0) can be added to the argument of f (cid:2) but is omitted
here(cid:8) It is convenient to de(cid:20)ne auxiliary variables (cid:21)xt by xt (cid:19) f (cid:14)(cid:21)xt(cid:15)(cid:8) In terms of
these auxiliary variables(cid:2) the top(cid:7)down pass is written as

(cid:21)xt(cid:0) (cid:19) Wtf (cid:14)(cid:21)xt(cid:15)

(cid:14)(cid:15)

Given a sensory input d(cid:2) the top(cid:7)down generative model can be inverted by (cid:20)nding
hidden variables xL that generate a pattern x matching d(cid:8)
If some of the hid(cid:7)
den variables represent the identity of the pattern(cid:2) the inversion operation is called
recognition(cid:8) Alternatively(cid:2) the hidden variables may just be a more compact repre(cid:7)
sentation of the pattern(cid:2) in which case the operation is called analysis or encoding(cid:8)
The inversion is done iteratively(cid:2) as described below(cid:8)

In the following(cid:2) the operator (cid:3) denotes elementwise multiplication of two vectors(cid:2)
so that z (cid:19) x (cid:3) y means zi (cid:19) xiyi for all i(cid:8) The bottom(cid:7)up pass starts with the
mismatch between the sensory data d and the generated pattern x(cid:2)

(cid:4) (cid:19) f (cid:14)(cid:21)x(cid:15) (cid:3) (cid:14)d (cid:2) x(cid:15) (cid:3)

which is propagated upwards by

(cid:4)t (cid:19) f (cid:14)(cid:21)xt(cid:15) (cid:3) (cid:14)W T

t (cid:4)t(cid:0)(cid:15) (cid:2)

(cid:14)(cid:15)

(cid:14)(cid:15)

When the error signal reaches the top of the network(cid:2) it is used to update the hidden
variables xL(cid:2)

(cid:22)xL (cid:4) W T

L (cid:4)L(cid:0) (cid:2)

(cid:14)(cid:15)

This update closes the negative feedback loop(cid:8) Then a new pattern x is generated
by a top(cid:7)down pass (cid:14)(cid:15)(cid:2) and the process starts over again(cid:8)

This iterative inversion process performs gradient descent on the cost function 
 jd(cid:2)
xj(cid:2) subject to the constraints (cid:14)(cid:15)(cid:8) This can be proved using the chain rule(cid:2) as in
the traditional derivation of the backprop algorithm(cid:8) Another method of proof is
to add the equations (cid:14)(cid:15) as constraints(cid:2) using Lagrange multipliers(cid:2)




jd (cid:2) f (cid:14)(cid:21)x(cid:15)j (cid:18)

L

X

t(cid:4)

(cid:4)T
t(cid:0)(cid:10)(cid:21)xt(cid:0) (cid:2) Wtf (cid:14)(cid:21)xt(cid:15)(cid:12) (cid:2)

(cid:14)(cid:15)

This derivation has the advantage that the bottom(cid:7)up activations (cid:4)t have an inter(cid:7)
pretation as Lagrange multipliers(cid:8)

Inverting the generative model by negative feedback can be interpreted as a process
of sequential hypothesis testing(cid:8) The top(cid:7)down connections generate a hypothesis
about the sensory data(cid:8) The bottom(cid:7)up connections propagate an error signal
that is the disagreement between the hypothesis and data(cid:8) When the error signal
reaches the top(cid:2) it is used to generate a revised hypothesis(cid:2) and the generate(cid:7)test(cid:7)
revise cycle starts all over again(cid:8) Perception is the convergence of this feedback loop
to the hypothesis that is most consistent with the data(cid:8)

 LEARNING THE GENERATIVE MODEL

The synaptic weights Wt determine the types of patterns that the network is able to
generate(cid:8) To learn from examples(cid:2) the weights are adjusted to improve the network(cid:25)s
generation ability(cid:8) A suitable cost function for learning is the reconstruction error
 jd (cid:2) xj averaged over an ensemble of examples(cid:8) Online gradient descent with
respect to the synaptic weights is performed by a learning rule of the form



(cid:22)Wt (cid:4) (cid:4)t(cid:0)xT
t

(cid:2)

(cid:14)(cid:15)

The same error signal (cid:4) that was used to invert the generative model is also used
to learn it(cid:8)

The batch form of the optimization is compactly written using matrix notation(cid:8)
To do this(cid:2) we de(cid:20)ne the matrices D(cid:3) X(cid:3) (cid:2) (cid:2) (cid:2) (cid:3) XL whose columns are the vectors d(cid:2)
x(cid:3) (cid:2) (cid:2) (cid:2) (cid:3) xL corresponding to examples in the training set(cid:8) Then computation and
learning are the minimization of

min
XL(cid:2)Wt




jD (cid:2) Xj (cid:3)

subject to the constraint that

Xt(cid:0) (cid:19) f (cid:14)WtXt(cid:15) (cid:3)

t (cid:19) (cid:3) (cid:2) (cid:2) (cid:2) (cid:3) L (cid:2)

(cid:14)(cid:15)

(cid:14)	(cid:15)

In other words(cid:2) up(cid:7)prop is a dual minimization with respect to hidden variables and
synaptic connections(cid:8) Computation minimizes with respect to the hidden variables
XL(cid:2) and learning minimizes with respect to the synaptic weight matrices Wt(cid:8)

From the geometric viewpoint(cid:2) up(cid:7)propagation is an algorithm for learning pattern
manifolds(cid:8) The top(cid:7)down pass (cid:14)(cid:15) maps an nL(cid:7)dimensional vector xL to an n(cid:7)
dimensional vector x(cid:8) Thus the generative model parametrizes a continuous nL(cid:7)
dimensional manifold embedded in n(cid:7)dimensional space(cid:8) Inverting the generative
model is equivalent to (cid:20)nding the point on the manifold that is closest to the sensory
data(cid:8) Learning the generative model is equivalent to deforming the manifold to (cid:20)t
a database of examples(cid:8)

W

principal components

Figure (cid:13) One(cid:7)step generation of handwritten digits(cid:8) Weights of the (cid:7)	 up(cid:7)prop
network (cid:14)left(cid:15) versus the top 	 principal components (cid:14)right(cid:15)

target image

x0 t=0

t=1

t=10

t=100

t=1000

x1

4

2

0

4

2

0

4

2

0

4

2

0

4

2

0

0

5

10

0

5

10

0

5

10

0

5

10

0

5

10

Figure (cid:13) Iterative inversion of a generative model as sequential hypothesis testing(cid:8)
A fully trained (cid:27)	 network is inverted to generate an approximation to a target
image that was not previously seen during training(cid:8) The stepsize of the dynamics
was (cid:20)xed to (cid:2) to show time evolution of the system(cid:8)

Pattern manifolds are relevant when patterns vary continuously(cid:8) For example(cid:2) the
variations in the image of a three(cid:7)dimensional object produced by changes of view(cid:7)
point are clearly continuous(cid:2) and can be described by the action of a transformation
group on a prototype pattern(cid:8) Other types of variation(cid:2) such as deformations in
the shape of the object(cid:2) are also continuous(cid:2) even though they may not be readily
describable in terms of transformation groups(cid:8) Continuous variability is clearly not
con(cid:20)ned to visual images(cid:2) but is present in many other domains(cid:8) Many existing
techniques for modeling pattern manifolds(cid:2) such as PCA or PCA mixtures(cid:10)(cid:12)(cid:2) de(cid:7)
pend on linear or locally linear approximations to the manifold(cid:8) Up(cid:7)prop constructs
a globally parametrized(cid:2) nonlinear manifold(cid:8)

 ONE(cid:5)STEP GENERATION

The simplest generative model of the form (cid:14)(cid:15) has just one step (cid:14)L (cid:19) (cid:15)(cid:8) Up(cid:7)
propagation minimizes the cost function

min
X(cid:2)W




jD (cid:2) f (cid:14)WX(cid:15)j (cid:2)

(cid:14)(cid:15)

For a linear f this reduces to PCA(cid:2) as the cost function is minimized when the vec(cid:7)
tors in the weight matrix W span the same space as the top principal components
of the data D(cid:8)

Up(cid:7)propagation with a one(cid:7)step generative model was applied to the USPS
database(cid:10)(cid:12)(cid:2) which consists of example images of handwritten digits(cid:8) Each of the
	 training and  testing images was normalized to a  (cid:5)  grid with pixel
intensities in the range (cid:10)(cid:3) (cid:12)(cid:8) A separate model was trained for each digit class(cid:8) The
nonlinearity f was the logistic function(cid:8) Batch optimization of (cid:14)(cid:15) was done by

Reconstruction Error

PCA, training 
Up−prop, training
PCA, test 
Up−prop, test 

0.025

0.02

r
o
r
r

E

0.015

0.01

0.005

0

5

10

15

20

25

number of vectors

30

35

40

Figure (cid:13) Reconstruction error for (cid:27)n networks as a function of n(cid:8) The error of
PCA with n principal components is shown for comparison(cid:8) The up(cid:7)prop network
performs better on both the training set and test set(cid:8)

gradient descent with adaptive stepsize control by the Armijo rule(cid:10)(cid:12)(cid:8) In most cases(cid:2)
the stepsize varied between (cid:0) and (cid:0)(cid:2) and the optimization usually converged
within  epochs(cid:8) Figure  shows the weights of a (cid:27)	 network that was trained
on  di(cid:9)erent images of the digit (cid:28)two(cid:8)(cid:29) Each of the 	 subimages is the weight
vector of a top(cid:7)level neuron(cid:8) The top 	 principal components are also shown for
comparison(cid:8)

Figure  shows the time evolution of a fully trained (cid:27)	 network during iterative
inversion(cid:8) The error signal from the bottom layer x quickly activates the top layer
x(cid:8) At early times(cid:2) all the top layer neurons have similar activation levels(cid:8) However(cid:2)
the neurons with weight vectors more relevant to the target image become dominant
soon(cid:2) and the other neurons are deactivated(cid:8)

The reconstruction error (cid:14)(cid:15) of the up(cid:7)prop network was much better than that of
PCA(cid:8) We trained  di(cid:9)erent up(cid:7)prop networks(cid:2) one for each digit(cid:2) and these were
compared with  corresponding PCA models(cid:8) Figure  shows the average squared
error per pixel that resulted(cid:8) A (cid:27) up(cid:7)prop network performed as well as PCA
with  principal components(cid:8)

 TWO(cid:5)STEP GENERATION

Two(cid:7)step generation is a richer model(cid:2) and is learned using the cost function

min

X(cid:2)W(cid:2)W




jD (cid:2) f (cid:14)Wf (cid:14)WX(cid:15)(cid:15)j (cid:2)

(cid:14)(cid:15)

Note that a nonlinear f is necessary for two(cid:7)step generation to have more represen(cid:7)
tational power than one(cid:7)step generation(cid:8) When this two(cid:7)step generative model was
trained on the USPS database(cid:2) the weight vectors in W learned features resembling
principal components(cid:8) The activities of the X neurons tended to be close to their
saturated values of one or zero(cid:8)

The reconstruction error of the two(cid:7)step generative network was compared to that of
the one(cid:7)step generative network with the same number of neurons in the top layer(cid:8)

Our (cid:27)(cid:27)	 network outperformed our (cid:27)	 network on the test set(cid:2) though
both networks used nine hidden variables to encode the sensory data(cid:8) However(cid:2)
the learning time was much longer(cid:2) and iterative inversion was also slow(cid:8) While
up(cid:7)prop for one(cid:7)step generation converged within several hundred epochs(cid:2) up(cid:7)prop
for two(cid:7)step generation often needed several thousand epochs or more to converge(cid:8)
We often found long plateaus in the learning curves(cid:2) which may be due to the
permutation symmetry of the network architecture(cid:10)(cid:12)(cid:8)

 DISCUSSION

To summarize the experiments discussed above(cid:2) we constructed separate generative
models(cid:2) one for each digit class(cid:8) Relative to PCA(cid:2) each generative model was
superior at encoding digits from its corresponding class(cid:8) This enhanced generative
ability was due to the use of nonlinearity(cid:8)

We also tried to use these generative models for recognition(cid:8) A test digit was
classi(cid:20)ed by inverting all the generative models(cid:2) and then choosing the one best able
to generate the digit(cid:8) Our tests of this recognition method were not encouraging(cid:8)
The nonlinearity of up(cid:7)propagation tended to improve the generation ability of
models corresponding to all classes(cid:2) not just the model corresponding to the correct
classi(cid:20)cation of the digit(cid:8) Therefore the improved encoding performance did not
immediately transfer to improved recognition(cid:8)

We have not tried the experiment of training one generative model on all the digits(cid:2)
with some of the hidden variables representing the digit class(cid:8) In this case(cid:2) pattern
recognition could be done by inverting a single generative model(cid:8) It remains to be
seen whether this method will work(cid:8)

Iterative inversion was surprisingly fast(cid:2) as shown in Figure (cid:2) and gave solutions
of surprisingly good quality in spite of potential problems with local minima(cid:2) as
shown in Figure (cid:8) In spite of these virtues(cid:2) iterative inversion is still a problematic
method(cid:8) We do not know whether it will perform well if a single generative model
is trained on multiple pattern classes(cid:8) Furthermore(cid:2) it seems a rather indirect way
of doing pattern recognition(cid:8)

The up(cid:7)prop generative model is deterministic(cid:2) which handicaps its modeling of
pattern variability(cid:8) The model can be dressed up in probabilistic language by de(cid:20)n(cid:7)
ing a prior distribution P (cid:14)xL(cid:15) for the hidden variables(cid:2) and adding Gaussian noise
to x to generate the sensory data(cid:8) However(cid:2) this probabilistic appearance is only
skin deep(cid:2) as the sequence of transformations from xL to x is still completely de(cid:7)
terministic(cid:8) In a truly probabilistic model(cid:2) like a belief network(cid:2) every layer of the
generation process adds variability(cid:8)

In conclusion(cid:2) we brie(cid:30)y compare up(cid:7)propagation to other algorithms and architec(cid:7)
tures(cid:8)

(cid:8) In backpropagation(cid:10)(cid:12)(cid:2) only the recognition model is explicit(cid:8) Iterative gra(cid:7)
dient descent methods can be used to invert the recognition model(cid:2) though
this implicit generative model generally appears to be inaccurate(cid:10)(cid:2) (cid:12)(cid:8)

(cid:8) Up(cid:7)propagation has an explicit generative model(cid:2) and recognition is done
by inverting the generative model(cid:8) The accuracy of this implicit recognition
model has not yet been tested empirically(cid:8) Iterative inversion of generative
models has also been proposed for linear networks(cid:10)(cid:2) 	(cid:12) and probabilistic
belief networks(cid:10)(cid:12)(cid:8)

(cid:8) In the autoencoder(cid:10)(cid:12) and the Helmholtz machine(cid:10)(cid:12)(cid:2) there are separate

models of recognition and generation(cid:2) both explicit(cid:8) Recognition uses only
bottom(cid:7)up connections(cid:2) and generation uses only top(cid:7)down connections(cid:8)
Neither process is iterative(cid:8) Both processes can operate completely inde(cid:7)
pendently(cid:31) they only interact during learning(cid:8)

(cid:8) In attractor neural networks(cid:10)(cid:2) (cid:12) and the Boltzmann machine(cid:10)(cid:12)(cid:2) recog(cid:7)
nition and generation are performed by the same recurrent network(cid:8) Each
process is iterative(cid:2) and each utilizes both bottom(cid:7)up and top(cid:7)down connec(cid:7)
tions(cid:8) Computation in these networks is chie(cid:30)y based on positive(cid:2) rather
than negative feedback(cid:8)

Backprop and up(cid:7)prop su(cid:9)er from a lack of balance in their treatment of bottom(cid:7)up
and top(cid:7)down processing(cid:8) The autoencoder and the Helmholtz machine su(cid:9)er from
inability to use iterative dynamics for computation(cid:8) Attractor neural networks lack
these de(cid:20)ciencies(cid:2) so there is incentive to solve the problem of learning attractors(cid:10)(cid:12)(cid:8)

This work was supported by Bell Laboratories(cid:8) JHO was partly supported by the
Research Professorship of the LG(cid:7)Yonam Foundation(cid:8) We are grateful to Dan Lee
for helpful discussions(cid:8)

References

(cid:5)(cid:7) D(cid:3) E(cid:3) Rumelhart(cid:4) G(cid:3) E(cid:3) Hinton(cid:4) and R(cid:3) J(cid:3) Williams(cid:3) Learning internal representations

by back(cid:2)propagating errors(cid:3) Nature(cid:4) (cid:10)(cid:12)(cid:4) 	(cid:3)

(cid:5)(cid:7) D(cid:3) D(cid:3) Lee and H(cid:3) S(cid:3) Seung(cid:3) Unsupervised learning by convex and conic coding(cid:3) Adv(cid:2)

Neural Info(cid:2) Proc(cid:2) Syst(cid:2)(cid:4) 	(cid:10)(cid:12)(cid:4) 		(cid:3)

(cid:5)(cid:7) G(cid:3) E(cid:3) Hinton(cid:4) P(cid:3) Dayan(cid:4) and M(cid:3) Revow(cid:3) Modeling the manifolds of images of hand(cid:2)

written digits(cid:3) IEEE Trans(cid:2) Neural Networks(cid:4) (cid:10)(cid:12)(cid:4) 		(cid:3)

(cid:5)(cid:7) Y(cid:3) LeCun et al(cid:3) Learning algorithms for classi(cid:18)cation(cid:10) a comparison on handwritten
digit recognition(cid:3) In J(cid:3)(cid:2)H(cid:3) Oh(cid:4) C(cid:3) Kwon(cid:4) and S(cid:3) Cho(cid:4) editors(cid:4) Neural networks(cid:3) the
statistical mechanics perspective(cid:4) pages (cid:12)(cid:4) Singapore(cid:4) 		(cid:3) World Scienti(cid:18)c(cid:3)

(cid:5)(cid:7) D(cid:3) P(cid:3) Bertsekas(cid:3) Nonlinear programming(cid:3) Athena Scienti(cid:18)c(cid:4) Belmont(cid:4) MA(cid:4) 		(cid:3)
(cid:5)(cid:7) K(cid:3) Kang(cid:4) J(cid:3)(cid:2)H(cid:3) Oh(cid:4) C(cid:3) Kwon(cid:4) and Y(cid:3) Park(cid:3) Generalization in a two(cid:2)layer neural

network(cid:3) Phys(cid:2) Rev(cid:2)(cid:4) E(cid:10)(cid:12)	(cid:4) 		(cid:3)

(cid:5)(cid:7) J(cid:3) Kindermann and A(cid:3) Linden(cid:3)

Inversion of neural networks by gradient descent(cid:3)

Parallel Computing(cid:4) (cid:10)(cid:12)(cid:4) 		(cid:3)

(cid:5)(cid:7) Y(cid:3) Lee(cid:3) Handwritten digit recognition using K nearest(cid:2)neighbor(cid:4) radial(cid:2)basis function(cid:4)

and backpropagation neural networks(cid:3) Neural Comput(cid:2)(cid:4) (cid:10)(cid:12)	(cid:4) 		(cid:3)

(cid:5)	(cid:7) R(cid:3) P(cid:3) N(cid:3) Rao and D(cid:3) H(cid:3) Ballard(cid:3) Dynamic model of visual recognition predicts neural

response properties in the visual cortex(cid:3) Neural Comput(cid:2)(cid:4) 	(cid:10)(cid:12)(cid:4) 		(cid:3)

(cid:5)(cid:7) L(cid:3) K(cid:3) Saul(cid:4) T(cid:3) Jaakkola(cid:4) and M(cid:3) I(cid:3) Jordan(cid:3) Mean (cid:18)eld theory for sigmoid belief

networks(cid:3) J(cid:2) Artif(cid:2) Intell(cid:2) Res(cid:2)(cid:4) (cid:10)(cid:12)(cid:4) 		(cid:3)

(cid:5)(cid:7) G(cid:3) W(cid:3) Cottrell(cid:4) P(cid:3) Munro(cid:4) and D(cid:3) Zipser(cid:3) Image compression by back propagation(cid:10) an
example of extensional programming(cid:3) In N(cid:3) E(cid:3) Sharkey(cid:4) editor(cid:4) Models of cognition(cid:3)
a review of cognitive science(cid:3) Ablex(cid:4) Norwood(cid:4) NJ(cid:4) 		(cid:3)

(cid:5)(cid:7) G(cid:3) E(cid:3) Hinton(cid:4) P(cid:3) Dayan(cid:4) B(cid:3) J(cid:3) Frey(cid:4) and R(cid:3) M(cid:3) Neal(cid:3) The (cid:20)wake(cid:2)sleep(cid:21) algorithm for

unsupervised neural networks(cid:3) Science(cid:4) (cid:10)(cid:12)(cid:4) 		(cid:3)

(cid:5)(cid:7) H(cid:3) S(cid:3) Seung(cid:3) Pattern analysis and synthesis in attractor neural networks(cid:3) In K(cid:3)(cid:2)Y(cid:3) M(cid:3)
Wong(cid:4) I(cid:3) King(cid:4) and D(cid:3)(cid:2)Y(cid:3) Yeung(cid:4) editors(cid:4) Theoretical Aspects of Neural Computation(cid:3)
A Multidisciplinary Perspective(cid:4) Singapore(cid:4) 		(cid:3) Springer(cid:2)Verlag(cid:3)

(cid:5)(cid:7) H(cid:3) S(cid:3) Seung(cid:3) Learning continuous attractors in recurrent networks(cid:3) Adv(cid:2) Neural Info(cid:2)

Proc(cid:2) Syst(cid:2)(cid:4) (cid:4) 		(cid:3)

(cid:5)(cid:7) D(cid:3) H(cid:3) Ackley(cid:4) G(cid:3) E(cid:3) Hinton(cid:4) and T(cid:3) J(cid:3) Sejnowski(cid:3) A learning algorithm for Boltzmann

machines(cid:3) Cognitive Science(cid:4) 	(cid:10)(cid:12)	(cid:4) 	(cid:3)

"
5221,1997,A Neural Network Based Head Tracking System,We have constructed an inexpensive video based motorized tracking system that learns to track a head. It uses real time graphical user inputs or an auxiliary infrared detector as supervisory signals to train a convolutional neural network. The inputs to the neural network consist of normalized luminance and chrominance images and motion information from frame differences. Subsampled images are also used to provide scale invariance. During the online training phases the neural network rapidly adjusts the input weights depending up on the reliability of the different channels in the surrounding environment. This quick adaptation allows the system to robustly track a head even when other objects are moving within a cluttered background.,"A Neural Network Based

Head Tracking System

D. D. Lee and H. S. Seung

Bell Laboratories, Lucent Technologies

 Mountain Ave.

Murray Hill, NJ 	

fddlee|seungg@bell-labs.com

Abstract

We have constructed an inexpensive, video-based, motorized track-
ing system that learns to track a head. It uses real time graphical
user inputs or an auxiliary infrared detector as supervisory signals
to train a convolutional neural network. The inputs to the neural
network consist of normalized luminance and chrominance images
and motion information from frame dierences. Subsampled im-
ages are also used to provide scale invariance. During the online
training phase, the neural network rapidly adjusts the input weights
depending upon the reliability of the dierent channels in the sur-
rounding environment. This quick adaptation allows the system to
robustly track a head even when other objects are moving within
a cluttered background.



Introduction

With the proliferation of inexpensive multimedia computers and peripheral equip-
ment, video conferencing nally appears ready to enter the mainstream. But per-
sonal video conferencing systems typically use a stationary camera, tying the user
to a xed location much as a corded telephone tethers one to the telephone jack. A
simple solution to this problem is to use a motorized video camera that can track
a specic person as he or she moves about. However, this presents the diculty of
having to continually control the movements of the camera while one is communi-
cating. In this paper, we present a prototype, neural network based system that
learns the characteristics of a person’s head in real time and automatically tracks
it around the room, thus alleviating the user of much of this burden.

The camera movements in this video conferencing system closely resemble the move-
ments of human eyes. The task of the biological oculomotor system is to direct

Color

CCD Camera

(Eye)

Directional

Microphones

(Ears)

PC

Serial
Port

Frame
Grabber

Sound
Card

Servo Motors
(Oculomotor

Muscles)

Reinforcement

Signals

IR Detector

GUI Mouse

Figure : Schematic hardware diagram of Marvin, our head tracking system.

interesting"" parts of the visual world onto the small, high resolution areas of the
retinas. For this task, complex neural circuits have evolved in order to control the
eye movements. Some examples include the saccadic and smooth pursuit systems
that allow the eyes to rapidly acquire and track moving objects , . Similarly,
an active video conferencing system also needs to determine the appropriate face
or feature to follow in the video stream. Then the camera must track that person’s
movements over time and transmit the image to the other party.

In the past few years, the problem of face detection in images and video has attracted
considerable attention , , . Rule-based methods have concentrated on looking
for generic characteristics of faces such as oval shapes or skin hue. Since these types
of algorithms are fairly simple to implement, they are commonly found in real-time
systems , . But because other objects have similar shapes and colors as faces,
these systems can also be easily fooled. A potentially more robust approach is to
use a convolutional neural network to learn the appropriate features of a face , 	.
Because most such implementations learn in batch mode, they are beset by the
diculty of constructing a large enough training set of labelled images with and
without faces.
In this paper, we present a video based system that uses online
supervisory signals to train a convolutional neural network. Fast online adaptation
of the network’s weights allows the neural network to learn how to discriminate an
individual head at the beginning of a session. This enables the system to robustly
track the head even in the presence of other moving objects.

 Hardware Implementation

Figure  shows a schematic of the tracking system we have constructed and have
named Marvin"" because of an early version’s similarity to a cartoon character.
Marvin’s eye consists of a small CCD camera with a  eld of view that is attached
to a motorized platform. Two RC servo motors give Marvin the ability to rapidly
pan and tilt over a wide range of viewing angles, with a typical maximum velocity of
 degsec. The system also includes two microphones or ears that give Marvin the
ability to locate auditory cues. Integrating auditory information with visual inputs
allows the system to nd salient objects better than with either sound or video
alone. But these proceedings will focus exclusively on how a visual representation
is learned.

RGB Images

Y

U

V

D

Figure : Preprocessing of the video stream. Luminance, chromatic and motion
information are separately represented in the Y, U, V, D channels at multiple res-
olutions.

Marvin is able to learn to track a visual target using two dierent sources of su-
pervisory signals. One method of training uses a small  KHz modulated infrared
light emitter   	 nm attached to the object that needs to be tracked. A
heat lter renders the infrared light invisible to Marvin’s video camera so that the
system does not merely learn to follow this signal. But mounted next to the CCD
camera and moving with it is a small infrared detector with a collimating lens that
signals when the object is located within a narrow angular cone in the direction
that the camera is pointing. This reinforcement signal can then be used to train
the weights of the neural network. Another more natural way for the system to
learn occurs in an actual video conferencing scenario. In this situation, a user who
is actively watching the video stream has manual override control of the camera
using graphical user interface inputs. Whenever the user repositions the camera to
a new location, the neural network would then adjust its weights to track whatever
is in the center portion of the image.

Since Marvin was built from readily available commercial components, the cost of
the system not including the PC was under $. The input devices and motors
are all controlled by the computer using custom-written Matlab drivers that are
available for both Microsoft Windows and the Linux operating system. The image
processing computations as well as the graphical user interface are then easily im-
plemented as simple Matlab operations and function calls. The following section
describes the head tracking neural network in more detail.

 Neural Network Architecture

Marvin uses a convolutional neural network architecture to detect a head within its
eld of view. The video stream from the CCD camera is rst digitized with a video
capture board into a series of raw  RGB images as shown in Figure . Each
RGB color image is then converted into its YUV representation, and a dierence D

Hidden
Units

Y

U

V

D

WY

WU

WV

WD

Saliency

Map

Winner Take All

Figure : Neural network uses a convolutional architecture to integrate the dierent
sources of information and determine the maximally salient object.

image is also computed as the absolute value of the dierence from the preceding
frame. Of the four resulting images, the Y component represents the luminance or
grayscale information while the U and V channels contain the chromatic or color
information. Motion information in the video stream is captured by the D image
where moving objects appear highlighted.

The four YUVD channels are then subsampled successively to yield representations
at lower and lower resolutions. The resulting image pyramids"" allow the network
to achieve recognition invariance across many dierent scales without having to
train separate neural networks for each resolution. Instead, a single neural network
with the same set weights is run with the dierent resolutions as inputs, and the
maximally active resolution and position is selected.

Marvin uses the convolutional neural network architecture shown in Figure  to
locate salient objects at the dierent resolutions. The YUVD input images are l-
tered with separate  kernels, denoted by WY , WU , WV , and WD respectively.
This results in the ltered images Y s, U s, V s, Ds:

Asi; j = WA  As = X

WAi; j  Asi + i; j + j 



i;j 

where s denotes the scale resolution of the inputs, and A is any of the Y , U , V ,
or D channels. These ltered images represent a single layer of hidden units in the
neural network. These hidden units are then combined to form the saliency map
X s in the following manner:

X si; j = cY g Y si; j + cU g U si; j + cV g V si; j + cD g Dsi; j + c:



Since gx = tanhx is sigmoidal, the saliency X s is computed as a nonlinear,
pixel-by-pixel combination of the hidden units. The scalar variables cY , cU , cV ,
and cD represent the relative importance of the dierent luminance, chromatic, and
motion channels in the overall saliency of an object.

With the bias term c, the function gX si; j may then be thought of as the
relative probability that a head exists at location i; j at input resolution s. The
nal output of the neural network is then determined in a competitive manner by
nding the location im; jm and scale sm of the best possible match:

gXm = gX smim; jm = max
i;j;s

gX si; j:



After processing the visual inputs in this manner, saccadic camera movements are
generated in order to keep the maximally salient object located near the center of
the eld of view.

 Training and Results

Either GUI user inputs or the infrared detector may be used as a supervisory signal
to train the kernels WA and scalar weights cA of the neural network. The neu-
ral network is updated when the maximally salient location of the neural network
im; jm does not correspond to the desired object’s true position in; jn as iden-
tied by the external supervisory signal. A cost function proportional to the sum
squared error terms at the maximal location and new desired location is used for
training:

m = jgm (cid:0) gX smim; jmj;
e
jgn (cid:0) gX sin; jnj:
e
n = min

s





In the following examples, the constants gm =  and gn =  are used. The gradients
to Eqs.  are then backpropagated through the convolutional network , ,
resulting in the following update rules:

cA =  emg Xmg Aim; jm +  engXng Ain; jn;
WA =  emg Xmg AmcAAm +  engXng AncAAn:




In typical batch learning applications of neural networks, the learning rate  is set
to be some small positive number. However in this case, it is desirable for Marvin
to learn to track a head in a new environment as quickly as possible. Thus, rapid
adaptation of the weights during even a single training example is needed. A natural
way of doing this is to use a fairly large learning rate  = :, and to repeatedly
apply the update rules in Eqs.  until the calculated maximally salient location
is very close to the actual desired position.

An example of how quickly Marvin is able to learn to track one of the authors
as he moved around his oce is given by the learning curve in Figure . The
weights were rst initialized to small random values, and Marvin was corrected in
an online fashion using mouse inputs to look at the author’s head. After only a few
seconds of training with a processing time loop of around  ms, the system was
able to locate the head to within four pixels of accuracy, as determined by hand
labelling the video data afterwards. As saccadic eye movements were initiated at

r
o
r
r

E

l

e
x
P

i

20

18

16

14

12

10

8

6

4

2

0

0

10

20

30

40

50

Frame Number

Figure : Fast online adaptation of the neural network. The head location error in
pixels in a    image is plotted as a function of frame number  framessec.

the times indicated by the arrows in Fig. , new environments of the oce were
sampled and an occasional large error is seen. However, over time as these errors
are corrected, the neural network learns to robustly discriminate the head from the
oce surroundings.

 Discussion

Figure  shows the inputs and weights of the network after a minute of training as
the author walked around his oce. The kernels necessarily appear a little smeared
because they are invariant to slight changes in head position, rotation, and scale.
But they clearly depict the dark hair, facial features, and skin color of the head. The
relative weighting cY ; cU ; cV  cD of the dierent input channels shows that the
luminance and color information are the most reliable for tracking the head. This
is probably because it is relatively dicult to distinguish in the frame dierence
images the head from other moving body parts.

We are currently considering more complicated neural network architectures for
combining the dierent input streams to give better tracking performance. How-
ever, this example shows how a simple convolutional architecture can be used to
automatically integrate dierent visual cues to robustly track a head. Moreover, by
using fast online adaptation of the neural network weights, the system is able to
learn without needing large hand-labelled training sets and is also able to rapidly
accomodate changing environments. Future improvements in hardware and neu-
ral network architectures and algorithms are still necessary, however, in order to
approach human speeds and performance in this type of sensory processing and
recognition task.

We acknowledge the support of Bell Laboratories, Lucent Technologies. We also
thank M. Fee, A. Jacquin, S. Levinson, E. Petajan, G. Pingali, and E. Rietman for
helpful discussions.

Y

U

V

D

c =0.15

Y

c =0.12

U

c =0.11

V

c =0.08

D

Figure : Example showing the inputs and weights used in tracking a head. The
head position as calculated by the neural network is marked with a box.

References

 Horiuchi, TK, Bishofberger, B & Koch, C 		. An analog VLSI saccadic
eye movement system. Advances in Neural Information Processing Systems ,
	.

 Rao, RPN, Zelinsky, GJ, Hayhoe, MM & Ballard, DH 		. Modeling sac-
cadic targeting in visual search. Advances in Neural Information Processing
Systems , .

 Sung, KK & Poggio, T 		. Example-based learning for view-based human

face detection. Proc. rd Image Understanding Workshop, .

 Eleftheriadis, A & Jacquin, A 		. Automatic face location detection and
tracking for model-assisted coding of video teleconferencing sequences at low
bit-rates. Signal Processing: Image Communication , .

 Petajan, E & Graf, HP 		. Robust face feature analysis for automatic
speechreading and character animation. Proc. nd Int. Conf. Automatic Face
and Gesture Recognition, -.

 Darrell, T, Maes, P, Blumberg, B, & Pentland, AP 		. A novel environment
for situated vision and behavior. Proc. IEEE Workshop for Visual Behaviors,
.

 Yang, J & Waibel, A 		. A real-time face tracker. Proc. rd IEEE Workshop

on Application of Computer Vision, .

 Nowlan, SJ & Platt, JC 		. A convolutional neural network hand tracker.

Advances in Neural Information Processing Systems , 		.

	 Rowley, HA, Baluja, S & Kanade, T 		. Human face detection in visual

scenes. Advances in Neural Information Processing Systems , .

 Le Cun, Y, et al. 		. Handwritten digit recognition with a back propagation

network. Advances in Neural Information Processing Systems , 	.

"
1861,2000,Algorithms for Non-negative Matrix Factorization,"Non-negative matrix factorization (NMF) has previously been shown to 
be a useful decomposition for multivariate data. Two different multi- 
plicative algorithms for NMF are analyzed. They differ only slightly in 
the multiplicative factor used in the update rules. One algorithm can be 
shown to minimize the conventional least squares error while the other 
minimizes the generalized Kullback-Leibler divergence. The monotonic 
convergence of both algorithms can be proven using an auxiliary func- 
tion analogous to that used for proving convergence of the Expectation- 
Maximization algorithm. The algorithms can also be interpreted as diag- 
onally rescaled gradient descent, where the rescaling factor is optimally 
chosen to ensure convergence. ","Algorithms for Non-negative Matrix 

Factorization 

Daniel D. Lee* 

*BelJ Laboratories 
Lucent Technologies 
Murray Hill, NJ 07974 

H. Sebastian Seung*t 

tDept.  of Brain and Cog.  Sci. 

Massachusetts Institute of Technology 

Cambridge, MA 02138 

Abstract 

Non-negative matrix factorization (NMF) has previously been shown to 
be  a useful  decomposition  for  multivariate  data.  Two  different multi(cid:173)
plicative algorithms for NMF are  analyzed.  They differ only slightly in 
the multiplicative factor used in  the  update rules.  One algorithm can  be 
shown to  minimize the conventional least squares  error while the  other 
minimizes the generalized Kullback-Leibler divergence.  The monotonic 
convergence of both  algorithms can be proven using an  auxiliary  func(cid:173)
tion analogous to that used for proving convergence of the Expectation(cid:173)
Maximization algorithm. The algorithms can also be interpreted as diag(cid:173)
onally rescaled gradient descent, where the rescaling factor is  optimally 
chosen to ensure convergence. 

1 

Introduction 

Unsupervised learning algorithms such as principal components analysis and vector quan(cid:173)
tization can be understood as factorizing a data matrix subject to different constraints.  De(cid:173)
pending upon the constraints utilized, the resulting factors  can be  shown to have very dif(cid:173)
ferent representational properties.  Principal components analysis enforces only a weak or(cid:173)
thogonality constraint, resulting in a very distributed representation that uses cancellations 
to  generate variability  [1,  2].  On the  other hand, vector quantization uses  a hard winner(cid:173)
take-all constraint that results in clustering the data into mutually exclusive prototypes [3]. 

We have previously shown that nonnegativity is a useful constraint for matrix factorization 
that can learn a parts representation of the data [4, 5].  The nonnegative basis vectors that are 
learned are used in distributed, yet still  sparse combinations to generate expressiveness in 
the reconstructions [6, 7].  In this submission, we analyze in detail two numerical algorithms 
for learning the optimal nonnegative factors from data. 

2  Non-negative matrix factorization 

We formally consider algorithms for solving the following problem: 

Non-negative matrix factorization (NMF) Given a non-negative matrix 
V, find non-negative matrix factors Wand H  such that: 

V~WH 

(1) 

NMF can be applied to the statistical analysis of multivariate data in the following manner. 
Given  a  set  of of multivariate  n-dimensional data  vectors,  the  vectors  are  placed  in  the 
columns of an n  x  m  matrix V  where m  is  the number of examples in the data set.  This 
matrix  is  then approximately factorized into an  n  x  r  matrix Wand an r  x  m  matrix  H. 
Usually r is chosen to be smaller than nor m, so that Wand H are smaller than the original 
matrix V. This results in a compressed version of the original data matrix. 

What is  the  significance of the  approximation in  Eq.  (1)?  It can  be  rewritten  column by 
column as v  ~ Wh, where v and h are the corresponding columns of V  and H.  In other 
words, each data vector v is approximated by  a linear combination of the columns of W, 
weighted  by  the  components  of h.  Therefore W  can  be  regarded  as  containing  a  basis 
that is  optimized for the linear approximation of the data in V.  Since relatively few  basis 
vectors are used to represent many data vectors, good approximation can only be achieved 
if the basis vectors discover structure that is latent in the data. 

The present submission is  not about applications of NMF, but focuses instead on the tech(cid:173)
nical  aspects of finding  non-negative matrix factorizations.  Of course, other types of ma(cid:173)
trix factorizations have  been extensively studied in numerical linear algebra, but the  non(cid:173)
negativity  constraint makes  much of this  previous  work inapplicable  to  the  present case 
[8]. 

Here we discuss two algorithms for NMF based on iterative updates of Wand H.  Because 
these  algorithms  are easy  to  implement and  their convergence properties are guaranteed, 
we have found  them very useful in practical applications.  Other algorithms may possibly 
be more efficient in overall computation time, but are more difficult to implement and may 
not generalize to different cost functions.  Algorithms similar to ours where only one of the 
factors is adapted have previously been used for the deconvolution of emission tomography 
and astronomical images [9,  10,  11,  12]. 

At each iteration of our algorithms, the new value of W  or H  is found by multiplying the 
current value by some factor that depends on the quality ofthe approximation in Eq. (1). We 
prove  that the quality  of the  approximation improves monotonically with  the  application 
of these  multiplicative update rules.  In  practice,  this  means that repeated iteration of the 
update rules is guaranteed to converge to a locally optimal matrix factorization. 

3  Cost functions 

To  find  an  approximate  factorization  V  ~ W H,  we  first  need  to  define  cost functions 
that quantify  the quality  of the  approximation.  Such a  cost function  can  be  constructed 
using some measure of distance between two non-negative matrices A  and B . One useful 
measure is  simply the square of the Euclidean distance between A and B  [13], 

IIA - BI12  = L(Aij - Bij)2 

ij 

This is lower bounded by zero, and clearly vanishes if and only if A  =  B . 

Another useful measure is 

D(AIIB) = 2:  Aij log B:~ - Aij + Bij 

k· 

( 

) 

(2) 

(3) 

""J 

Like the  Euclidean distance this  is  also  lower bounded by  zero,  and  vanishes if and only 
if A  =  B .  But it cannot be called a ""distance"", because it is  not symmetric in  A  and  B, 
so  we will refer to  it as  the ""divergence"" of A from B.  It reduces to the Kullback-Leibler 
divergence, or relative entropy, when 2:ij Aij  = 2:ij Bij  = 1,  so  that A  and  B  can be 
regarded as normalized probability distributions. 

We now consider two alternative formulations of NMF as  optimization problems: 
Problem 1  Minimize  IIV - W HI12  with  respect to  Wand H,  subject to  the  constraints 
W,H~O. 

Problem 2  Minimize  D(VIIW H)  with  re.lpect to  Wand H,  subject  to  the  constraints 
W,H~O. 
Although the functions IIV - W HI12 and D(VIIW H) are convex in W  only or H  only, they 
are not convex in both variables together. Therefore it is unrealistic to expect an algorithm 
to  solve Problems 1 and 2 in the sense of finding global minima.  However, there are many 
techniques from numerical optimization that can be applied to find  local minima. 

Gradient descent is  perhaps the  simplest technique to  implement, but convergence can  be 
slow.  Other methods  such  as  conjugate gradient have  faster  convergence,  at  least in  the 
vicinity  of local  minima,  but  are  more  complicated  to  implement  than  gradient descent 
[8] . The convergence of gradient based methods also have the disadvantage of being very 
sensitive to the choice of step size, which can be very inconvenient for large applications. 

4  Multiplicative update rules 

We  have  found  that  the  following  ""multiplicative update  rules""  are  a  good  compromise 
between speed and ease of implementation for solving Problems 1 and 2. 
Theorem 1  The Euclidean distance II V  - W H II  is non increasing under the update rules 

(WTV)att 
Hal'  +- Hal' (WTWH)att 

(V HT)ia 
Wia  +- Wia(WHHT)ia 

(4) 

The Euclidean distance  is  invariant under these updates if and only if Wand H  are  at a 
stationary point of the distance. 

Theorem 2  The divergence D(VIIW H) is nonincreasing under the update rules 

H 

att  +-

att 

H  2:i WiaVitt/(WH)itt 

""  W 
L..Jk 

ka 

Wia  +- Wia 

2:1' HattVitt/(WH)itt 

""  H 
L..Jv 

av 

(5) 

The divergence is invariant under these updates if and only ifW and H  are at a stationary 
point of the divergence. 

Proofs  of these theorems  are  given in  a later section.  For now,  we  note that each  update 
consists  of multiplication  by  a factor.  In  particular,  it  is  straightforward  to  see  that  this 
multiplicative factor is unity when V  = W H, so  that perfect reconstruction is  necessarily 
a fixed point of the update rules. 

5  Multiplicative versus additive update rules 

It is useful to contrast these multiplicative updates with those arising from gradient descent 
[14].  In particular, a simple additive update for H  that reduces the squared distance can be 
written as 

(6) 

If 'flatt  are all  set equal to  some  small positive number,  this  is  equivalent to  conventional 
gradient descent.  As  long as  this  number is  sufficiently  small,  the  update should reduce 
IIV - WHII· 

Now if we diagonally rescale the variables and set 

Halt 

""Ialt  = (WTW H)alt ' 

(7) 

then  we  obtain the update rule for  H  that is  given in Theorem 1.  Note that this rescaling 
results in a multiplicative factor with the positive component of the gradient in the denom(cid:173)
inator and the absolute value of the negative component in the numerator of the factor. 

For the divergence, diagonally rescaled gradient descent takes the form 

Halt  f- Halt + ""Ialt  [~Wia (:;;)ilt  - ~ Wia]. 

(8) 

Again, if the ""Ialt  are small and positive, this update should reduce D (V II W H). If we now 
set 

Halt 

""Ialt=  ~ W.  ' 
ui  za 

(9) 

then we  obtain the update rule for H  that is  given in  Theorem 2.  This rescaling can also 
be interpretated as  a multiplicative rule with the positive component of the gradient in  the 
denominator and negative component as the numerator of the multiplicative factor. 

Since our choices for ""Ialt  are not small, it may seem that there is  no guarantee that such a 
rescaled gradient descent should cause the cost function  to  decrease.  Surprisingly, this  is 
indeed the case as shown in  the next section. 

6  Proofs of convergence 

To prove Theorems 1 and 2, we will make use of an auxiliary function similar to that used 
in the Expectation-Maximization algorithm [15, 16]. 

Definition 1  G(h, h') is an auxiliary functionfor F(h) if the conditions 

G(h, h') ~ F(h), 

G(h, h) = F(h) 

(10) 

are satisfied. 

The auxiliary function is  a useful concept because of the following lemma, which is  also 
graphically illustrated in Fig. 1. 

Lemma 1  IfG is an auxiliary junction, then F  is nonincreasing under the update 

ht+1  =  argmlnG (h,ht ) 

(11) 

Proof:  F(ht+1)  ~ G(ht+1, ht) ~ G(ht, ht) = F(ht) • 
Note that F(ht+1)  = F(ht) only if ht is  a local  minimum of G(h, ht).  If the derivatives 
of F  exist and  are  continuous in  a  small  neighborhood  of ht ,  this  also  implies  that  the 
derivatives 'V F(ht)  =  O.  Thus,  by  iterating the update in Eq. (11) we obtain a  sequence 
of estimates  that converge  to  a  local  minimum hmin  = argminh F(h)  of the  objective 
function: 

We will show that by defining the appropriate auxiliary functions G(h, ht) for both IIV -
W HII and D(V, W H), the update rules in Theorems 1 and 2 easily follow from Eq.  (11). 

Figure 1:  Minimizing the auxiliary function G(h, ht)  2::  F(h) guarantees that F(ht+1)  :::; 
F(ht) for hn+1  = argminh G(h, ht). 

Lemma 2  If K(ht) is the diagonal matrix 

Kab(ht)  =  <5ab(WTwht)a/h~ 

then 

G(h, ht) = F(ht) + (h - ht)T\l F(ht) + ~(h - ht)T K(ht)(h - ht) 

is an auxiliary function for 

F(h) =  ~ ~)Vi - L  W ia ha)2 

i 

a 

(13) 

(14) 

(15) 

Proof:  Since G(h, h)  =  F(h)  is obvious, we need only show  that G(h, ht)  2::  F(h).  To 
do this, we compare 

F(h) =  F(ht) + (h - htf\l F(ht) + ~(h - ht)T(WTW)(h - ht) 

2 

with Eq.  (14) to  find that G(h, ht) 2::  F(h) is equivalent to 

0:::;  (h  - htf[K(ht) - WTW](h - ht) 

(16) 

(17) 

To prove positive semidefiniteness, consider the matrix 1: 

(18) 
which is just a rescaling of the components of K  - WTW.  Then K  - WTW is  positive 
semidefinite if and only if M  is, and 

VT M v  =  L  VaMabVb 

ab 
L  h~(WTW)abh~v~ - vah~(WTW)abh~Vb 
ab 

) 

t 

"" (   T 
L...J  W  W  abhahb 
ab 

t  [1  2  1 2 
2"" va + 2""Vb - VaVb 
=  ~ L(WTW)abh~h~(va - Vb)2 
>  0 

ab 

] 

(19) 

(20) 

(21) 

(22) 

(23) 

'One can also show that K  - WTW is positive semidefinite by considering the matrix K  (I-
K- 2 W  W K- 2  K 2. Then v M W  W ht  a  is a positive eigenvector of K- 2 W  W K- with 
1 
unity eigenvalue, and application of the Frobenius-Perron theorem shows that Eq.  17 holds. 

. /   (T 

T 

T 

1) 

) 

1 

1 

• 

We can now demonstrate the convergence of Theorem 1: 

Proof of Theorem 1 Replacing G(h, ht) in Eq.  (11) by Eq.  (14) results in the update rule: 
(24) 
Since Eq. (14) is an auxiliary function, F  is nonincreasing under this update rule, according 
to Lemma 1.  Writing the components of this equation explicitly, we obtain 

ht+1  =  ht - K(ht)-l\1F(ht) 

ht+1  =  ht 
a 

(WT V )a 

a (WTWht)a . 

(25) 

By reversing the roles  of Wand H  in  Lemma  1 and  2,  F  can  similarly  be  shown  to  be 
nonincreasing under the update rules for W .• 

We now consider the following auxiliary function for the divergence cost function: 

Lemma 3  Define 

G(h,ht) 

ia 

"" 
ia 
This is an auxiliary function for 

Wiah~  ( 
- ~ Vi,"""",  W - ht 
ub 
,b  b 

Wiah~  ) 
logWiaha -log,"""",  W - ht 
ub 
,b  b 

F(h) = L  Vi  log (~ ~_ h  )  - Vi  + LWiaha 

a 

'l,a  a 

a 

i 

(26) 

(27) 

(28) 

Proof: It is straightforward to verify that G(h, h) = F(h) . To show that G(h, ht) 2:  F(h), 
we use convexity of the log function to derive the inequality 

""W iaha  
-log ~ Wiaha  ::;  - ~ Q a  log - -

a 

Q a 

"" 
a 

which holds for all nonnegative Q a  that sum to  unity.  Setting 

Q a  = '""'"" 

Wiah~ 
t 
ub Wibhb 

we obtain 

Wiah~  ( 
-log ~ Wiaha ::;  - ~ '""'""  W- ht 
,b  b 

"" 
a  ub 

"" 
a 

log Wiaha -

Wiah~  ) 
log,"""",  W- ht 
,b  b 
ub 

(29) 

(30) 

(31) 

From this inequality it follows that F(h)  ::;  G(h, ht) .• 

Theorem 2 then follows from the application of Lemma 1: 

Proof of Theorem 2:  The minimum of G(h, ht) with respect to  h is determined by setting 
the gradient to zero: 

_dG---,(,---,h,_h--,-t)  __ "" 
~v, 
, 
_ 

dha 

-

_  Wiah~  1  ""W- - 0 
~b Wibhb  ha 

+ ~ za-

t 

, 
-

Thus, the update rule of Eq. (11) takes the form 

t+1  h~""  Vi 
ha  = '""'""  w  ~ '""'""  W- ht W ia · 

i  ub 

,b  b 

ub 

kb 

(32) 

(33) 

Since G is an auxiliary function, F  in  Eq.  (28) is  nonincreasing under this update.  Rewrit(cid:173)
ten in matrix form, this is equivalent to the update rule in Eq.  (5). By reversing the roles of 
Hand W, the update rule for W  can similarly be shown to be nonincreasing .• 

7  Discussion 

We  have  shown  that application of the update rules  in  Eqs.  (4)  and (5)  are guaranteed to 
find  at least locally optimal solutions of Problems  1 and 2,  respectively.  The convergence 
proofs rely upon defining an  appropriate auxiliary function.  We  are currently working to 
generalize these  theorems to  more complex constraints.  The update rules  themselves  are 
extremely easy to implement computationally, and will  hopefully be utilized by  others for 
a wide variety of applications. 

We  acknowledge the  support of Bell  Laboratories.  We  would  also  like  to  thank  Carlos 
Brody, Ken Clarkson, Corinna Cortes, Roland Freund, Linda Kaufman, Yann Le Cun, Sam 
Roweis, Larry Saul, and Margaret Wright for helpful discussions. 

References 

[1]  Jolliffe, IT (1986). Principal Component Analysis. New York:  Springer-Verlag. 
[2]  Turk, M &  Pentland, A (1991). Eigenfaces for recognition. J.  Cogn.  Neurosci. 3, 71- 86. 
[3]  Gersho, A  &  Gray, RM (1992).  Vector Quantization and Signal  Compression.  Kluwer Acad. 

Press. 

[4]  Lee, DD & Seung, HS . Unsupervised learning by convex and conic coding (1997). Proceedings 

of the Conference on Neural Information Processing Systems 9, 515- 521. 

[5]  Lee, DD &  Seung, HS  (1999). Learning the parts of objects by non-negative  matrix factoriza(cid:173)

tion. Nature 401, 788- 791. 

[6]  Field, DJ (1994). What is the goal of sensory coding? Neural Comput.  6, 559-601. 
[7]  Foldiak, P  &  Young, M  (1995).  Sparse coding in  the primate cortex.  The  Handbook of Brain 

Theory and Neural Networks, 895- 898.  (MIT Press, Cambridge, MA). 

[8]  Press, WH, Teukolsky, SA, Vetterling, WT &  Flannery, BP (1993). Numerical recipes:  the art 

of scientific computing.  (Cambridge University Press, Cambridge, England). 

[9]  Shepp, LA &  Vardi,  Y  (1982). Maximum likelihood reconstruction for emission tomography. 

IEEE Trans. MI-2, 113- 122. 

[10]  Richardson, WH  (1972). Bayesian-based  iterative  method  of image  restoration.  1.  Opt.  Soc. 

Am. 62, 55- 59. 

[11]  Lucy, LB  (1974). An iterative technique for the rectification of observed distributions. Astron. 

J.  74, 745- 754. 

[12]  Bouman, CA & Sauer, K (1996). A unified approach to statistical tomography using coordinate 

descent optimization. IEEE Trans.  Image  Proc.  5, 480--492. 

[13]  Paatero, P &  Tapper, U  (1997). Least squares formulation  of robust non-negative factor analy(cid:173)

sis. Chemometr. Intell. Lab.  37, 23- 35. 

[14]  Kivinen,  J  &  Warmuth,  M  (1997).  Additive  versus  exponentiated  gradient updates  for  linear 

prediction. Journal of Tnformation and Computation 132, 1-64. 

[15]  Dempster, AP, Laird, NM &  Rubin, DB (1977). Maximum likelihood from incomplete data via 

the EM algorithm. J.  Royal Stat. Soc.  39, 1-38. 

[16]  Saul, L &  Pereira, F (1997). Aggregate and mixed-order Markov models for statistical language 
processing. In  C.  Cardie and R.  Weischedel  (eds).  Proceedings  of the Second Conference  on 
Empirical Methods in Natural Language Processing, 81- 89. ACL Press. 

"
1975,2001,Characterizing Neural Gain Control using Spike-triggered Covariance,"Spike-triggered averaging techniques are effective for linear characterization of neural responses. But neurons exhibit important nonlinear behaviors, such as gain control, that are not captured by such analyses. We describe a spike-triggered covariance method for retrieving suppressive components of the gain control signal in a neuron. We demonstrate the method in simulation and on retinal ganglion cell data. Analysis of physiological data reveals significant suppressive axes and explains neural nonlinearities. This method should be applicable to other sensory areas and modalities.","Characterizing neural gain control using

spike-triggered covariance

Odelia Schwartz

E. J. Chichilnisky

Center for Neural Science

Systems Neurobiology

New York University
odelia@cns.nyu.edu

The Salk Institute

ej@salk.edu

Eero P. Simoncelli

Howard Hughes Medical Inst.

Center for Neural Science

New York University

eero.simoncelli@nyu.edu

Abstract

Spike-triggered averaging techniques are effective for linear characteri-
zation of neural responses. But neurons exhibit important nonlinear be-
haviors, such as gain control, that are not captured by such analyses.
We describe a spike-triggered covariance method for retrieving suppres-
sive components of the gain control signal in a neuron. We demonstrate
the method in simulation and on retinal ganglion cell data. Analysis
of physiological data reveals signiﬁcant suppressive axes and explains
neural nonlinearities. This method should be applicable to other sensory
areas and modalities.

White noise analysis has emerged as a powerful technique for characterizing response prop-
erties of spiking neurons. A sequence of stimuli are drawn randomly from an ensemble and
presented in rapid succession, and one examines the subset that elicit action potentials. This
“spike-triggered” stimulus ensemble can provide information about the neuron’s response
characteristics. In the most widely used form of this analysis, one estimates an excitatory
linear kernel by computing the spike-triggered average (STA); that is, the mean stimulus
that elicited a spike [e.g., 1, 2]. Under the assumption that spikes are generated by a
Poisson process with instantaneous rate determined by linear projection onto a kernel fol-
lowed by a static nonlinearity, the STA provides an unbiased estimate of this kernel [3].
Recently, a number of authors have developed interesting extensions of white noise anal-
ysis. Some have examined spike-triggered averages in a reduced linear subspace of input
stimuli [e.g., 4]. Others have recovered excitatory subspaces, by computing the spike-
triggered covariance (STC), followed by an eigenvector analysis to determine the subspace
axes [e.g., 5, 6].

Sensory neurons exhibit striking nonlinear behaviors that are not explained by fundamen-
tally linear mechanisms. For example, the response of a neuron typically saturates for large
amplitude stimuli; the response to the optimal stimulus is often suppressed by the presence
of a non-optimal mask [e.g., 7]; and the kernel recovered from STA analysis may change
shape as a function of stimulus amplitude [e.g., 8, 9]. A variety of these nonlinear behav-
iors can be attributed to gain control [e.g., 8, 10, 11, 12, 13, 14], in which neural responses
are suppressively modulated by a gain signal derived from the stimulus. Although the un-
derlying mechanisms and time scales associated with such gain control are current topics
of research, the basic functional properties appear to be ubiquitous, occurring throughout
the nervous system.

0

a

0

b

k

0

Figure 1: Geometric depiction of spike-triggered analyses. a, Spike-triggered averaging
with two-dimensional stimuli. Black points indicate raw stimuli. White points indicate stim-

sponds to their center of mass. b, Spike-triggered covariance analysis of suppressive axes.

uli eliciting a spike, and the STA (black vector), which provides an estimate of 
Shown are a set of stimuli lying on a plane perpendicular to the excitatory kernel, 

the plane, stimuli eliciting a spike are concentrated in an elliptical region. The minor axis of
the ellipse corresponds to a suppressive stimulus direction: stimuli with a signiﬁcant compo-
nent along this axis are less likely to elicit spikes. The stimulus component along the major
axis of the ellipse has no inﬂuence on spiking.

 , corre-
 . Within

Here we develop a white noise methodology for characterizing a neuron with gain control.
We show that a set of suppressive kernels may be recovered by ﬁnding the eigenvectors of
the spike-triggered covariance matrix associated with smallest variance. We apply the tech-
nique to electrophysiological data obtained from ganglion cells in salamander and macaque
retina, and recover a set of axes that are shown to reduce responses in the neuron. More-
over, when we ﬁt a gain control model to the data using a maximum likelihood procedure
within this subspace, the model accounts for changes in the STA as a function of contrast.

1 Characterizing suppressive axes

 .

	

As in all white noise approaches, we assume that stimuli correspond to vectors, 

ﬁnite-dimensional space (e.g., a neighborhood of pixels or an interval of time samples).
We assume a gain control model in which the probability of a stimulus eliciting a spike
grows monotonically with the halfwave-rectiﬁed projection onto an excitatory linear kernel,

 , in some

	

linear kernels, 
First, we recover the excitatory kernel, 

 , and is suppressively modulated by the fullwave-rectiﬁed projection onto a set of
 . This is achieved by presenting spherically sym-

metric input stimuli (e.g., Gaussian white noise) to the neuron and computing the STA
(Fig. 1a). STA correctly recovers the excitatory kernel, under the assumption that each
of the gain control kernels are orthogonal (or equal) to the excitatory kernel. The proof
is essentially the same as that given for recovering the kernel of a linear model followed
by a monotonic nonlinearity [3].
In particular, any stimulus can be decomposed into a
component in the direction of the excitatory kernel, and a component in a perpendicular
direction. This can be paired with another stimulus that is identical, except that its compo-
nent in the perpendicular direction is negated. The two stimuli are equally likely to occur
in a spherically Gaussian stimulus set (since they are equidistant from the origin), and they
are equally likely to elicit a spike (since their excitatory components are equal, and their
rectiﬁed perpendicular components are equal). Their vector average lies in the direction of
the excitatory kernel. Thus, the STA (which is an average over all such stimuli, or all such
stimulus pairs) must also lie in that direction. In a subsequent section we explain how to









Model:
Excitatory:

Retrieved: 
Excitatory:

Eigenvalues: 

Suppressive:

Suppressive:

Weights

1{
1.5{
2 {
2.5{
3 {

i

l

)
e
u
a
v
n
e
g
e
(
 
e
c
n
a
i
r
a
V

1

Arbitrary 

0

Axis number

350

Figure 2: Estimation of kernels from a simulated model (equation 2). Left: Model kernels.
Right: Sorted eigenvalues of covariance matrix of stimuli eliciting spikes (STC). Five eigen-
values fall signiﬁcantly below the others. Middle: STA (excitatory kernel) and eigenvectors
(suppressive kernels) associated with the lowest eigenvalues.

recover the excitatory kernel when it is not orthogonal to the suppressive kernels.

Next, we recover the suppressive subspace, assuming the excitatory kernel is known. Con-
sider the stimuli lying on a plane perpendicular to this kernel. These stimuli all elicit the
same response in the excitatory kernel, but they may produce different amounts of suppres-
sion. Figure 1b illustrates the behavior in a three-dimensional stimulus space, in which one
axis is assumed to be suppressive. The distribution of raw stimuli on the plane is spheri-
cally symmetric about the origin. But the distribution of stimuli eliciting a spike is narrower
along the suppressive direction: these stimuli have a component along the suppressive axis
and are therefore less likely to elicit a spike. This behavior is easily generalized from this
plane to the entire stimulus space. If we assume that the suppressive axes are ﬁxed, then
we expect to see reductions in variance in the same directions for any level of numerator
excitation.

Given this behavior of the spike-triggered stimulus ensemble, we can recover the suppres-
sive subspace using principal component analysis. We construct the sample covariance
matrix of the stimuli eliciting a spike:


(1)









where 
thogonal to the estimated 
subspace perpendicular to the estimated 

is the number of spikes. To ensure the estimated suppressive subspace is or-
 are ﬁrst projected onto the
that are
associated with small variance (eigenvalues) correspond to directions in which the response
of the neuron is modulated suppressively.

	 . The principal axes (eigenvectors) of 


(as in Figure 1b), the stimuli 

We illustrate the technique on simulated data for a neuron with a spatio-temporal receptive

input sequence are deﬁned over a 18-sample spatial region and a 18-sample time window

 of this
ﬁeld. The kernels are a set of orthogonal bandpass ﬁlters. The stimulus vectors 
(i.e., a ! #"" -dimensional space). Spikes are generated using a Poisson process with mean

rate determined by a speciﬁc form of gain control [14]:

$&%

+*

')(

-,



The goal of simulation is to recover excitatory kernel 
	
by 

, and constant 3

, weights 0

10

.

.
.1243
 , the suppressive subspace spanned

.65

(2)



	

















/









Retrieved kernels:

Eigenvalues:

Excitatory:

0

Suppressive:

actual
95 % confidence

1

Arbitrary 

l

)
e
u
a
v
n
e
g
e
(
 

i

e
c
n
a
i
r
a
V

0

Axis number

26

Figure 3: Left: Retrieved kernels from STA and STC analysis of ganglion cell data from a
salamander retina (cell 1999-11-12-B6A). Right: sorted eigenvalues of the spike-triggered
covariance matrix, with corresponding eigenvectors. Low eigenvalues correspond to suppres-
sive directions, while other eigenvalues correspond to arbitrary (ignored) directions. Raw
stimulus ensemble was sphered (whitened) prior to analysis and low-variance axes under-
represented in stimulus set were discarded.

Figure 2 shows the original and estimated kernels for a model simulation with 600K input
samples and 36.98K spikes. First, we note that STA recovers an accurate estimate of the
excitatory kernel. Next, consider the sorted eigenvalues of 
, as plotted in Figure 2. The
majority of the eigenvalues descend gradually (the covariance matrix of the white noise
source should have constant eigenvalues, but remember that those in Figure 2 are computed
from a ﬁnite set of samples). The last ﬁve eigenvalues are signiﬁcantly below the values
one would obtain with randomly selected stimulus subsets. The eigenvectors associated
with these lowest eigenvalues span approximately the same subspace as the suppressive
kernels. Note that some eigenvectors correspond to mixtures of the original suppressive
kernels, due to non-uniqueness of the eigenvector decomposition. In contrast, eigenvectors
corresponding to eigenvalues in the gradually-descending region appear arbitrary in their
structure.
Finally, we can recover the scalar parameters of this speciﬁc model (0
them to maximize the likelihood of the spike data according to equation (2). Note that a
direct maximum likelihood solution on the raw data would have been impractical due to
the high dimensionality of the stimulus space.

 and 3 ) by selecting

2 Suppressive Axes in Retinal Ganglion Cells

Retinal ganglion cells exhibit rapid [8, 15] as well as slow [9, 16, 17] gain control. We now
demonstrate that we can recover a rapid gain control signal by applying the method to data
from salamander retina [9]. The input sequence consists of 80K time samples of full-ﬁeld

33Hz ﬂickering binary white noise (contrast 8.5%). The stimulus vectors 

are deﬁned over a 60-segment time window. Since stimuli are ﬁnite in number and binary,
they are not spherically distributed. To correct for this, we discard low-variance axes and
whiten the stimuli within the remaining axes.

 of this sequence

Figure 3 depicts the kernels estimated from the 623 stimulus vectors eliciting spikes. Sim-
ilar to the model simulation, the eigenvalues gradually fall off, but four of the eigenvalues
appear to drop signiﬁcantly below the rest. To make this more concrete, we test the hy-
pothesis that the majority of the eigenvalues are consistent with those of randomly selected

ically, we perform a Monte Carlo simulation, drawing (with replacement) random subsets
(orthogonal)

stimulus vectors, but that the last "" eigenvalues fall signiﬁcantly below this range. Specif-
of 623 stimuli from the full set of raw stimuli. We also randomly select ""

b

0. 5

0

l

e
n
r
e
k
 
y
r
o
t
a
t
i
c
x
e
 
o
t
n
o
 
n
o
i
t
c
e
o
r
p

j

a

0. 5

0

-0. 5

-0. 5
0. 5
projection onto arbitrary kernel

0

l

i

e
n
r
e
k
 
e
v
s
s
e
r
p
p
u
s
 
o
t
n
o
 
n
o
i
t
c
e
o
r
p

j

-0. 5

-0. 5
0. 5
projection onto arbitrary kernel

0

Figure 4: Scatter plots from salamander ganglion cell data (cell 1999-11-12-B6A). Black
points indicate the raw stimulus set. White points indicate stimuli eliciting a spike. a, Pro-
jection of stimuli onto estimated excitatory kernel vs. arbitrary kernel. b, Projection of
stimuli onto an estimated suppressive kernel vs. arbitrary kernel.

axes, representing a suppressive subspace, and project this subspace out of the set of ran-
domly chosen stimuli. We then compute the eigenvalues of the sample covariance matrix
times, and estimate a 95 percent conﬁdence interval
for each of the eigenvalues. The ﬁgure shows that the ﬁrst eigenvalues lie within the conﬁ-
dence interval. In practice, we repeat this process in a nested fashion, assuming initially no
directions are signiﬁcantly suppressive, then one direction, and so on up to four directions.

of these stimuli. We repeat this 

These low eigenvalues correspond to eigenvectors that are concentrated in recent time (as is
the estimated excitatory kernel). The remaining eigenvectors appear to be arbitrary, span-
ning the full temporal window. We emphasize that these kernels should not be interpreted
to correspond to receptive ﬁelds of individual neurons underlying the suppressive signal,
but merely provide an orthogonal basis for a suppressive subspace.

We can now verify that the recovered STA axis is in fact excitatory, and the kernels corre-
sponding to the lowest eigenvalues are suppressive. Figure 4a shows a scatter plot of the
stimuli projected onto the excitatory axis vs. an arbitrary axis. Spikes are seen to occur
only when the component along the excitatory axis is high, as expected. Figure 4b is a
scatter plot of the stimuli projected onto one of the suppressive axes vs. an arbitrary (ig-
nored) axis. The spiking stimuli lie within an ellipse, with the minor axis corresponding to
the suppressive kernel. This is exactly what we would expect in a suppressive gain control
system (see Figure 1b).

Figure 5 illustrates recovery of a two-dimensional suppressive subspace for a macaque reti-
nal ganglion cell. The subspace was computed from the 36.43K stimulus vectors eliciting
spikes out of a total of 284.74K vectors. The data are qualitatively similar to those of the
salamander cell, although both the strength of suppression and speciﬁc shapes of the scatter
plots differs. In addition to suppression, the method recovers facilitation (i.e., high-variance
axes) in some cells (not shown here).

3 Correcting for Bias in Kernel Estimates

The kernels in the previous section were all recovered from stimuli of a single contrast.
However, when the STA is computed in a ganglion cell for low and high contrast stimuli,
the low-contrast kernel shows a slower time course [9] (ﬁgure 7,a). This would appear
inconsistent with the method we describe, in which the STA is meant to provide an estimate
of a single excitatory kernel. This behavior can be explained by assuming a model of the
form given in equation 2, and in addition dropping the constraint that the gain control
kernels are orthogonal (or identical) to the excitatory kernel.






 
!""
#$

%&	'	'

()*)

+($

a

actual

 95% confidence



	
	

60

0.5

0

-0. 5

l

e
n
r
e
k
 
y
r
o
t
a
t
i
c
x
e
 
o
t
n
o
 
n
o
i
t
c
e
o
r
p

j

1

0

l

i

)
e
u
a
v
n
e
g
e
(
 
e
c
n
a
i
r
a
V

b

l

c

0.5

0

-0. 5

i

e
n
r
e
k
 
e
v
s
s
e
r
p
p
u
s
 
o
t
n
o
 
n
o
i
t
c
e
o
r
p

j

0. 5
0.5
projection onto arbitrary kernel

0

0. 5
0.5
projection onto arbitrary kernel

0

Figure 5: a, Sorted eigenvalues of stimuli eliciting spikes from a macaque retina (cell 2001-
09-29-E6A). b-c, Scatter plots of stimuli projected onto recovered axes.

k0kk

Gain kernel

STA estimate

Figure 6: Demonstration of estimator bias. When a gain control kernel is not orthogonal to
the excitatory kernel, the responses to one side of the excitatory kernel are suppressed more
than those on the other side. The resulting STA estimate is thus biased away from the true

excitatory kernel, 

 .

First we show that when the orthogonality constraint is dropped, the STA estimate of the
excitatory kernel is biased by the gain control signal. Consider a situation in which a
suppressive kernel contains a component in the direction of the excitatory kernel,

 . We
-,
write 
 equal to
, produces a suppressive component along 
a stimulus 
2.
250
 produces
a suppressive component of ,
. . Thus, the two stimuli are equally likely
to occur but not equally likely to elicit a spike. As a result, the STA will be biased in the
direction 
. Figure 6 illustrates an example in which a non-orthogonal suppressive axis
biases the estimate of the STA.

/.
/.
, where 
2.
210
. , but the corresponding paired stimulus vector 

is perpendicular to the excitatory kernel. Then, for example,

, with 043
	

/.

2.

2.

Now consider the model in equation 2 in the presence of a non-orthogonal suppressive
subspace. Note that the bias is stronger for larger amplitude stimuli because the constant
term 3
. dominates the gain control signal for weak stimuli. Indeed, we have previously
hypothesized that changes in receptive ﬁeld tuning can arise from divisive gain control
models that include an additive constant [14].

Even when the STA estimate is biased by the gain control signal, we can still obtain an
(asymptotically) unbiased estimate of the excitatory kernel. Speciﬁcally, the true exci-
tatory kernel lies within the subspace spanned by the estimated (biased) excitatory and
suppressive kernels. So, assuming a particular gain control model, we can again maximize
the likelihood of the data, but now allowing both the excitatory and suppressive kernels to
move within the subspace spanned by the initial estimated kernels. The resulting suppres-


""








2












,







.












0






.

0








a

b

0.1

0

0.1

0

Low contrast STA
High contrast STA

Time preceding spike (sec)

-0.5

-1.8

0

-0.5

-1.8

Low contrast STA
High contrast STA

Time preceding spike (sec)

0

c
(cid:0)(cid:1)
Excitatory:

Suppressive:

{
{
{

0.99

0.97

0.87

Weights

{ 0.52
{

0.46

Figure 7: STA kernels estimated from low (8.5%) and high (34%) contrast salamander reti-
nal ganglion cell data (cell 1999-11-12-B6A). Kernels are normalized to unit energy. a, STA
kernels derived from ganglion cell spikes. b, STA kernels derived from simulated spikes
using ML-estimated model. c, Kernels and corresponding weights of ML-estimated model.

sive kernels need not be orthogonal to the excitatory kernel.

We maximize the likelihood of the full two-contrast data set using a model that is a gener-
alization of that given by equation (2):

+*

')(

,

10



243

(3)



The exponent '
is incorporated to allow for more realistic contrast-response functions.
The excitatory axis is initially set to the STA and the suppressive axes are set to the
low-eigenvalue eigenvectors of the STC, along with the STA (e.g., to allow for self-
suppression). The recovered axes and weights are shown in Figure 7b, and remaining model
parameters are: '
. Whereas the axes recovered from the STA/STC
analysis are orthogonal, the axes determined during the maximum likelihood stage need not
be (and in the data example are not) orthogonal. Figure 7b also demonstrates that the ﬁtted
model accounts for the change in STA observed at different contrast levels. Speciﬁcally,
we simulate responses of the model (equation (3) with Poisson spike generation) on each
of the two contrast stimulus sets, and then compute the STA based on these simulated spike
trains. Although it is based on a single ﬁxed excitatory kernel, the model exhibits a change
in STA shape as a function of contrast very much like the salamander neuron.

, 3

 



4 Discussion

We have described a spike-triggered covariance method for characterizing a neuron with
gain control, and demonstrated the plausibility of the technique through simulation and
analysis of neural data. The suppressive axes recovered from retinal ganglion cell data
appear to be signiﬁcant because: (1) As in the model simulation, a small number of eigen-
values are signiﬁcantly below the rest; (2) The eigenvectors associated with these axes are
concentrated in a temporal region immediately preceding the spike, unlike the remaining
axes; (3) Projection of the multi-dimensional stimulus vectors onto these axes reveal reduc-
tions of spike probability; (4) The full model, with parameters recovered through maximum
likelihood, explains changes in STA as a function of contrast.

Models of retinal processing often incorporate gain control [e.g., 8, 10, 15, 17, 18]. We
have shown for the ﬁrst time how one can use white noise analysis to recover a gain con-
trol subspace. The kernels deﬁning this subspace correspond to relatively short timescales.
Thus, it is interesting to compare the recovered subspace to models of rapid gain control.
In particular, Victor [15] proposed a retinal gain model in which the gain signal consists

$
%










%
/









.
,


.

5


5
""

of time-delayed copies of the excitatory kernel. In fact, for the cell shown in Figure 3,
the recovered suppressive subspace lies within the space spanned by shifted copies of the
excitatory kernel. The fact that we do not see evidence for slow gain control in the analysis
might indicate that these signals do not lie within a low-dimensional stimulus subspace. In
addition, the analysis is not capable of distinguishing between physiological mechanisms
that could underlie gain control behaviors. Potential candidates may include internal bio-
chemical adjustments, non-Poisson spike generation mechanisms, synaptic depression, and
shunting inhibition due to other neurons.

This technique should be applicable to a far wider range of neural data than has been
shown here. Future work will incorporate analysis of data gathered using stimuli that vary
in both time and space (as in the simulated example of Figure 2). We are also exploring
applicability of the technique to other visual areas.

Acknowledgments We thank Liam Paninski and Jonathan Pillow for helpful discussions
and comments, and Divya Chander for data collection.

References
[1] E deBoer and P Kuyper. Triggered correlation. In IEEE Transact. Biomed. Eng., volume 15,

pages 169–179, 1968.

[2] J P Jones and L A Palmer. The two-dimensional spatial structure of simple receptive ﬁelds in

the cat striate cortex. J Neurophysiology, 58:1187–11211, 1987.

[3] E J Chichilnisky. A simple white noise analysis of neuronal light responses. Network: Compu-

tation in Neural Systems, 12(2):199–213, 2001.

[4] D L Ringach, G Sapiro, and R Shapley. A subspace reverse-correlation technique for the study

of visual neurons. Vision Research, 37:2455–2464, 1997.

[5] R de Ruyter van Steveninck and W Bialek. Coding and information transfer in short spike

sequences. In Proc.Soc. Lond. B. Biol. Sci., volume 234, pages 379–414, 1988.

[6] B A Y Arcas, A L Fairhall, and W Bialek. What can a single neuron compute? In Advances in

Neural Information Processing Systems, volume 13, pages 75–81, 2000.

[7] M Carandini, D J Heeger, and J A Movshon. Linearity and normalization in simple cells of the

macaque primary visual cortex. Journal of Neuroscience, 17:8621–8644, 1997.

[8] R M Shapley and J D Victor. The effect of contrast on the transfer properties of cat retinal

ganglion cells. J. Physiol. (Lond), 285:275–298, 1978.

[9] D Chander and E J Chichilnisky. Adaptation to temporal contrast in primate and salamander

retina. J Neurosci, 21(24):9904–9916, 2001.

[10] R Shapley and C Enroth-Cugell. Visual adaptation and retinal gain control. Progress in Retinal

Research, 3:263–346, 1984.

[11] R F Lyon. Automatic gain control in cochlear mechanics. In P Dallos et al., editor, The Me-

chanics and Biophysics of Hearing, pages 395–420. Springer-Verlag, 1990.

[12] W S Geisler and D G Albrecht. Cortical neurons: Isolation of contrast gain control. Vision

Research, 8:1409–1410, 1992.

[13] D J Heeger. Normalization of cell responses in cat striate cortex. Vis. Neuro., 9:181–198, 1992.
[14] O Schwartz and E P Simoncelli. Natural signal statistics and sensory gain control. Nature

Neuroscience, 4(8):819–825, August 2001.

[15] J D Victor. The dynamics of the cat retinal X cell centre. J. Physiol., 386:219–246, 1987.
[16] S M Smirnakis, M J Berry, David K Warland, W Bialek, and M Meister. Adaptation of retinal

processing to image contrast and spatial scale. Nature, 386:69–73, March 1997.

[17] K J Kim and F Rieke. Temporal contrast adaptation in the input and output signals of salamander

retinal ganglion cells. J. Neurosci., 21(1):287–299, 2001.

[18] M Meister and M J Berry. The neural code of the retina. Neuron, 22:435–450, 1999.

"
195,2007,Compressed Regression,"Recent research has studied the role of sparsity in high dimensional regression and signal reconstruction, establishing theoretical limits for recovering sparse models from sparse data. In this paper we study a variant of this problem where the original $n$ input variables are compressed by a random linear transformation to $m \ll n$ examples in $p$ dimensions, and establish conditions under which a sparse linear model can be successfully recovered from the compressed data. A primary motivation for this compression procedure is to anonymize the data and preserve privacy by revealing little information about the original data. We characterize the number of random projections that are required for $\ell_1$-regularized compressed regression to identify the nonzero coefficients in the true model with probability approaching one, a property called ``sparsistence.'' In addition, we show that $\ell_1$-regularized compressed regression asymptotically predicts as well as an oracle linear model, a property called ``persistence.'' Finally, we characterize the privacy properties of the compression procedure in information-theoretic terms, establishing upper bounds on the rate of information communicated between the compressed and uncompressed data that decay to zero.","Compressed Regression

Shuheng Zhou∗ John Lafferty∗† Larry Wasserman‡†

∗Computer Science Department

‡Department of Statistics

†Machine Learning Department

Carnegie Mellon University

Pittsburgh, PA 15213

Abstract

Recent research has studied the role of sparsity in high dimensional regression and
signal reconstruction, establishing theoretical limits for recovering sparse models
from sparse data.
In this paper we study a variant of this problem where the
original n input variables are compressed by a random linear transformation to
m (cid:28) n examples in p dimensions, and establish conditions under which a sparse
linear model can be successfully recovered from the compressed data. A primary
motivation for this compression procedure is to anonymize the data and preserve
privacy by revealing little information about the original data. We characterize
the number of random projections that are required for `1-regularized compressed
regression to identify the nonzero coefﬁcients in the true model with probabil-
ity approaching one, a property called “sparsistence.” In addition, we show that
`1-regularized compressed regression asymptotically predicts as well as an or-
acle linear model, a property called “persistence.” Finally, we characterize the
privacy properties of the compression procedure in information-theoretic terms,
establishing upper bounds on the rate of information communicated between the
compressed and uncompressed data that decay to zero.

1 Introduction

Two issues facing the use of statistical learning methods in applications are scale and privacy. Scale
is an issue in storing, manipulating and analyzing extremely large, high dimensional data. Privacy
is, increasingly, a concern whenever large amounts of conﬁdential data are manipulated within an
organization. It is often important to allow researchers to analyze data without compromising the
privacy of customers or leaking conﬁdential information outside the organization. In this paper we
show that sparse regression for high dimensional data can be carried out directly on a compressed
form of the data, in a manner that can be shown to guard privacy in an information theoretic sense.

The approach we develop here compresses the data by a random linear or afﬁne transformation,
reducing the number of data records exponentially, while preserving the number of original input
variables. These compressed data can then be made available for statistical analyses; we focus on
the problem of sparse linear regression for high dimensional data. Informally, our theory ensures
that the relevant predictors can be learned from the compressed data as well as they could be from
the original uncompressed data. Moreover, the actual predictions based on new examples are as
accurate as they would be had the original data been made available. However, the original data
are not recoverable from the compressed data, and the compressed data effectively reveal no more
information than would be revealed by a completely new sample. At the same time, the inference
algorithms run faster and require fewer resources than the much larger uncompressed data would
require. The original data need not be stored; they can be transformed “on the ﬂy” as they come in.

1

In more detail, the data are represented as a n × p matrix X. Each of the p columns is an attribute,
and each of the n rows is the vector of attributes for an individual record. The data are compressed

is a random m × p matrix. Such transformations have been called “matrix masking” in the privacy
literature [6]. The entries of  and  are taken to be independent Gaussian random variables, but

by a random linear transformation X 7→ eX ≡ X, where  is a random m × n matrix with
m (cid:28) n. It is also natural to consider a random afﬁne transformation X 7→eX ≡ X + , where 
other distributions are possible. We think of eX as “public,” while  and  are private and only
needed at the time of compression. However, even with  = 0 and  known, recovering X from
eX requires solving a highly under-determined linear system and comes with information theoretic
privacy guarantees, as we demonstrate.
In standard regression, a response variable Y = Xβ +  ∈ Rn is associated with the input variables,
where i are independent, mean zero additive noise variables. In compressed regression, we assume
that the response is also compressed, resulting in the transformed responseeY ∈ Rm given by Y 7→
eY ≡ Y = Xβ +  = eX β +e. Note that under compression,ei , i ∈ {1, . . . , m}, in the
transformed noisee =  are no longer independent. In the sparse setting, the parameter β ∈ R p

is sparse, with a relatively small number s = kβk0 of nonzero coefﬁcients in β. The method we
focus on is `1-regularized least squares, also known as the lasso [17]. We study the ability of the
compressed lasso estimator to identify the correct sparse set of relevant variables and to predict well.

1

j 6= 0}.

2mkeY −eX βk2

We omit details and technical assumptions in the following theorems for clarity. Our ﬁrst result
shows that the lasso is sparsistent under compression, meaning that the correct sparse set of relevant
variables is identiﬁed asymptotically.
Sparsistence (Theorem 3.3): Ifthenumberofcompressedexamples m satisﬁes C1s2 log nps ≤
m ≤ √C2n/ log n,andtheregularizationparameter λm satisﬁes λm → 0 and mλ2
m / log p →
∞, then the compressed lasso estimatoreβm = arg minβ
2 + λmkβk1 is sparsistent:
P(cid:0)supp(eβm ) = supp(β)(cid:1) → 1 asm → ∞,wheresupp(β) = {j :
Our second result shows that the lasso is persistent under compression. Roughly speaking, per-
sistence [10] means that the procedure predicts well, as measured by the predictive risk R(β) =
E(cid:0)Y − β T X(cid:1)2, where X ∈ R p is a new input vector and Y is the associated response. Persistence is
a weaker condition than sparsistency, and in particular does not assume that the true model is linear.
Persistence (Theorem 4.1): Givenasequenceofsetsofestimators Bn,m ⊂ R p suchthat Bn,m =
{β : kβk1 ≤ Ln,m} with log2(np) ≤ m ≤ n,thesequenceofcompressedlassoestimatorseβn,m =
2 is persistent with the predictive risk R(β) = E(cid:0)Y − β T X(cid:1)2 over
argmin
uncompressed data with respect to Bn,m, meaning that R(eβn,m ) − infkβk1≤Ln,m R(β)
n → ∞,incase Ln,m = o (m/ log(np))1/4.
Our third result analyzes the privacy properties of compressed regression. We evaluate privacy in
information theoretic terms by bounding the average mutual information I (eX; X )/np per matrix

entry in the original data matrix X, which can be viewed as a communication rate. Bounding this
mutual information is intimately connected with the problem of computing the channel capacity of
certain multiple-antenna wireless communication systems [13].
Information Resistence (Propositions 5.1 and 5.2): The rate at which information about X is

kβk1≤Ln,m keY − eX βk2

−→ 0, as

P

revealed by the compressed dataeX satisﬁes rn,m = sup I (X;eX )

supremumisoverdistributionsontheoriginaldata X.

np

= O(cid:0) m

n(cid:1) → 0, where the

As summarized by these results, compressed regression is a practical procedure for sparse learning
in high dimensional data that has provably good properties. Connections with related literature are
brieﬂy reviewed in Section 2. Analyses of sparsistence, persistence and privacy properties appear in
Section 3–5. Simulations for sparsistence and persistence of the compressed lasso are presented in
Section 6. The proofs are included in the full version of the paper, available at http://arxiv.
org/abs/0706.0534.

2

2 Background and Related Work

In this section we brieﬂy review related work in high dimensional statistical inference, compressed
sensing, and privacy, to place our work in context.
Sparse Regression. An estimator that has received much attention in the recent literature is the
2 + λnkβk1, where λn is a regularization param-
eter. In [14] it was shown that the lasso is consistent in the high dimensional setting under certain
assumptions. Sparsistency proofs for high dimensional problems have appeared recently in [20]
and [19]. The results and method of analysis of Wainwright [19], where X comes from a Gaussian
ensemble and i is i.i.d. Gaussian, are particularly relevant to the current paper. We describe this
Gaussian Ensemble result, and compare our results to it in Sections 3, 6.Given that under com-

lassobβn [17], deﬁned asbβn = arg min 1

2nkY − Xβk2

pression, the noisee =  is not i.i.d, one cannot simply apply this result to the compressed case.

Persistence for the lasso was ﬁrst deﬁned and studied by Greenshtein and Ritov in [10]; we review
their result in Section 4.
Compressed Sensing. Compressed regression has close connections to, and draws motivation from
compressed sensing [4, 2]. However, in a sense, our motivation is the opposite of compressed
sensing. While compressed sensing of X allows a sparse X to be reconstructed from a small number
of random measurements, our goal is to reconstruct a sparse function of X. Indeed, from the point
of view of privacy, approximately reconstructing X, which compressed sensing shows is possible
if X is sparse, should be viewed as undesirable; we return to this point in Section ??. Several
authors have considered variations on compressed sensing for statistical signal processing tasks
[5, 11]. They focus on certain hypothesis testing problems under sparse random measurements, and
a generalization to classiﬁcation of a signal into two or more classes. Here one observes y = x,
where y ∈ Rm, x ∈ Rn and  is a known random measurement matrix. The problem is to select
between the hypotheses eHi : y = (si + ). The proofs use concentration properties of random
projection, which underlie the celebrated Johnson-Lindenstrauss lemma. The compressed regression
problem we introduce can be considered as a more challenging statistical inference task, where the
problem is to select from an exponentially large set of linear models, each with a certain set of
relevant variables with unknown parameters, or to predict as well as the best linear model in some
class.
Privacy. Research on privacy in statistical data analysis has a long history, going back at least to [3].
We refer to [6] for discussion and further pointers into this literature; recent work includes [16]. The
work of [12] is closely related to our work at a high level, in that it considers low rank random linear
transformations of either the row space or column space of the data X. The authors note the Johnson-
Lindenstrauss lemma, and argue heuristically that data mining procedures that exploit correlations
or pairwise distances in the data are just as effective under random projection. The privacy analysis

system. We are not aware of previous work that analyzes the asymptotic properties of a statistical
estimator under random projection in the high dimensional setting, giving information-theoretic
guarantees, although an information-theoretic quantiﬁcation of privacy was proposed in [1]. We

is restricted to observing that recovering X from eX requires solving an under-determined linear
cast privacy in terms of the rate of information communicated about X througheX, maximizing over

all distributions on X, and identify this with the problem of bounding the Shannon capacity of a
multi-antenna wireless channel, as modeled in [13]. Finally, it is important to mention the active
area of cryptographic approaches to privacy from the theoretical computer science community, for
instance [9, 7]; however, this line of work is quite different from our approach.

3 Compressed Regression is Sparsistent

In the standard setting, X is a n × p matrix, Y = Xβ +  is a vector of noisy observations under
a linear model, and p is considered to be a constant. In the high-dimensional setting we allow p to
grow with n. The lasso refers to the following: (P1) min kY − Xβk2
2 such that kβk1 ≤ L. In
Lagrangian form, this becomes: (P2) min 1
2 + λnkβk1. For an appropriate choice of
the regularization parameter λ = λ(Y, L), the solutions of these two problems coincide.
In compressed regression we project each column X j ∈ Rn of X to a subspace of m dimensions,
using an m × n random projection matrix . LeteX = X be the compressed design matrix, and

2nkY − Xβk2

3

being the set of optimal solutions:

leteY = Y be the compressed response. Thus, the transformed noisee is no longer i.i.d.. The
compressed lasso is the following optimization problem, foreY = Xβ +  = eX +e, withem
(a) (eP2) min

2 + λmkβk1, (b) em = arg min

Although sparsistency is the primary goal in selecting the correct variables, our analysis establishes
conditions for the stronger property of sign consistency:

2m keY −eX βk2

2m keY −eX βk2

2 + λmkβk1.

β∈R p

(1)

1

1

Deﬁnition 3.1. (Sign Consistency) A set of estimators n is sign consistent with the true β if

−1 for x >,=, or < 0 respectively. Asashorthand,denotetheeventthatasignconsistentsolution

P(cid:0)∃bβn ∈ n s.t.sgn(bβn) = sgn(β)(cid:1) → 1 as n → ∞,wheresgn(·) isgivenbysgn(x) = 1, 0, and
existswith E(cid:0)sgn(bβn) = sgn(β∗)(cid:1) :=(cid:8)∃bβ ∈ n suchthatsgn(bβ) = sgn(β∗)(cid:9).

Clearly, if a set of estimators is sign consistent then it is sparsistent.

All recent work establishing results on sparsity recovery assumes some form of incoherence condi-
tion on the data matrix X. To formulate such a condition, it is convenient to introduce an additional
6= 0} be the set of relevant variables and let Sc = {1, . . . , p} \ S
piece of notation. Let S = {j : β j
be the set of irrelevant variables. Then X S and X Sc denote the corresponding sets of columns of the
matrix X. We will impose the following incoherence condition; related conditions are used by [18]

in a deterministic setting. Let kAk∞ = maxiP p
Deﬁnition 3.2. (S-Incoherence) Let X be an n × p matrix and let S ⊂ {1, . . . , p} be nonempty.
Wesaythat X is S-incoherent incase
n X T

j=1 |Ai j| denote the matrix ∞-norm.

n X T

(2)

Sc X S(cid:13)(cid:13)(cid:13)∞ +(cid:13)(cid:13)(cid:13) 1

S X S − I|S|(cid:13)(cid:13)(cid:13)∞ ≤ 1 − η, forsome η ∈ (0, 1].

(cid:13)(cid:13)(cid:13) 1

Although not explicitly required, we only apply this deﬁnition to X such that columns of X satisfy

2 = (n),∀ j ∈ {1, . . . , p}. We can now state our main result on sparsistency.

(cid:13)(cid:13)X j(cid:13)(cid:13)2
Theorem 3.3. Suppose that, before compression, Y = Xβ∗ + , where each column of X is
normalized to have `2-norm n, and ε ∼ N (0, σ 2 In). Assume that X is S-incoherent, where S =
supp(β∗),anddeﬁnes = |S| andρm = mini∈S |β∗i |. Weobserve,aftercompression,eY =eX β∗ +e,
whereeY = Y,eX = X,ande = ,where i j ∼ N (0, 1
withC1 = 4e

η (cid:19) (ln p + 2 log n + log 2(s + 1)) ≤ m ≤r n
S X S)−1(cid:13)(cid:13)(cid:13)∞) → 0.
Thenthecompressedlassoissparsistent: P(cid:0)supp(eβm ) = supp(β)(cid:1) → 1 asm → ∞.

√6π ≈ 2.5044 andC2 =
log( p − s) → ∞, and (b)

n ). Leteβm ∈em asin(1b). If

√8e ≈ 7.6885,and λm → 0 satisﬁes

m + λm(cid:13)(cid:13)(cid:13)( 1

ρm (r log s

(cid:18) 16C1s2

4 Compressed Regression is Persistent

η2 +

mη2λ2
m

16 log n

n X T

4C2s

(a)

(4)

(3)

1

Persistence (Greenshtein and Ritov [10]) is a weaker condition than sparsistency. In particular, the
assumption that E(Y|X ) = β T X is dropped. Roughly speaking, persistence implies that a procedure
predicts well. We review the arguments in [10] ﬁrst; we then adapt it to the compressed case.
Uncompressed Persistence. Consider a new pair (X, Y ) and suppose we want to predict Y from X.
The predictive risk using predictor β T X is R(β) = E(Y − β T X )2. Note that this is a well-deﬁned
quantity even though we do not assume that E(Y|X ) = β T X. It is convenient to rewrite the risk in
the following way: deﬁne Q = (Y, X1, . . . , X p) and γ = (−1, β1, . . . , β p)T , then

R(β) = γ T γ , where  = E(Q QT ).

(5)

4

1
n

arg min

(6)

(7)

.

1
n

QT Q.

Compressed Persistence.

bRn(β) =

Let Q = (Q†
vectors and the training error is

2 · ·· Q†

1 Q†

n)T , where Q†

nXi=1
(Yi − X T

kβk1≤Ln bRn(β).

For the compressed case, again we want to predict (X, Y ), but

constants M and s, where Z = Q j Qk − E(Q j Qk ), where Q j , Qk denote elements of Q.
Following arguments in [10], it can be shown that under Assumption 1 and given a sequence of sets

i = (Yi , X1i , . . . , X pi )T ∼ Q,∀i = 1, . . . , n are i.i.d. random
i β)2 = γ Tbnγ , where bn =
kβk1≤Ln R(β), and the uncompressed lasso estimatorbβn = arg min

Given Bn = {β : kβk1 ≤ Ln} for Ln = o(cid:0)(n/ log n)1/4(cid:1), we deﬁne the oracle predictor β∗,n =
Assumption 1. Suppose that, for each j and k, E(cid:0)|Z|q(cid:1) ≤ q!Mq−2s/2, for every q ≥ 2 and some
of estimators Bn = {β : kβk1 ≤ Ln} for Ln = o(cid:0)(n/ log n)1/4(cid:1), the sequence of uncompressed
lasso estimatorsbβn = arg min β∈Bn bRn(β) is persistent, i.e., R(bβn) − R(β∗,n)
now the estimator bβn,m is based on the lasso from the compressed data of size mn. Let γ =
(−1, β1, . . . , β p)T as before and we replacebRn with
log(npn )(cid:17)1/4
Given compressed sample size mn, let Bn,m = {β : kβk1 ≤ Ln,m}, where Ln,m = o(cid:16) mn
We deﬁne the compressed oracle predictor β∗,n,m = arg min β : kβk1≤Ln,m R(β) and the compressed
lasso estimatorbβn,m = arg min β : kβk1≤Ln,m bRn,m (β).
Theorem 4.1. Under Assumption 1, we further assume that there exists a constant M1 > 0 such
that E(Q2
j ) < M1,∀ j,where Q j denotesthe j th elementof Q. Foranysequence Bn,m ⊂ R p with
log2(npn) ≤ mn ≤ n, where Bn,m consists of all coefﬁcient vectors β such thatkβk1 ≤ Ln,m =
o(cid:0)(mn/ log(npn))1/4(cid:1),thesequenceofcompressedlassoproceduresbβn,m = argminβ∈Bn,m bRn,m (β)
ispersistent: R(bβn,m ) − R(β∗,n,m )
(cid:8)bβn,m , ∀mn such that log2(np) < mn ≤ n(cid:9) deﬁnes a subsequence of estimators. In Section 6 we

The main difference between the sequence of compressed lasso estimators and the original un-
compressed sequence is that n and mn together deﬁne the sequence of estimators for the com-
pressed data. Here mn is allowed to grow from (log2(np)) to n; hence for each ﬁxed n,

illustrate the compressed lasso persistency via simulations to compare the empirical risks with the
oracle risks on such a subsequence for a ﬁxed n.

bRn,m (β) = γ Tbn,m γ , wherebn,m =

→ 0,when pn = O(cid:0)enc(cid:1) forc < 1/2.

1
mn

QT T Q.

P

→ 0.

P

5 Information Theoretic Analysis of Privacy

Next we derive bounds on the rate at which the compressed data eX reveal information about the

uncompressed data X. Our general approach is to consider the mapping X 7→ X +  as a noisy
communication channel, where the channel is characterized by multiplicative noise  and additive
noise . Since the number of symbols in X is np we normalize by this effective block length to
deﬁne the information rate rn,m per symbol as rn,m = sup p(X )
. Thus, we seek bounds on
the capacity of this channel. A privacy guarantee is given in terms of bounds on the rate rn,m → 0

decaying to zero. Intuitively, if the mutual information satisﬁes I (X;eX ) = H (X ) − H (X |eX ) ≈ 0,
then the compressed dataeX reveal, on average, no more information about the original data X than

The underlying channel is equivalent to the multiple antenna model for wireless communication
[13], where there are n transmitter and m receiver antennas in a Raleigh ﬂat-fading environment.
The propagation coefﬁcients between pairs of transmitter and receiver antennas are modeled by the
matrix entries i j ; they remain constant for a coherence interval of p time periods. Computing the

could be obtained from an independent sample.

I (X;eX )

np

5

channel capacity over multiple intervals requires optimization of the joint density of pn transmitted
signals, the problem studied in [13]. Formally, the channel is modeled as Z = X + γ , where
nPn
γ > 0, i j ∼ N (0, 1), i j ∼ N (0, 1/n) and 1
i=1 E[X 2
i j ] ≤ P, where the latter is a power
constraint.

Theorem 5.1. Supposethat E[X 2
j ] ≤ P andthecompresseddataareformedby Z = X + γ ,
where ism×n withindependententriesi j ∼ N (0, 1/n) and ism× p withindependententries
i j ∼ N (0, 1). Thentheinformationratern,m satisﬁesrn,m = sup p(X )
This result is implicitly contained in [13]. When  = 0, or equivalently γ = 0, which is the
case assumed in our sparsistence and persistence results, the above analysis yields the trivial bound
rn,m ≤ ∞. We thus derive a separate bound for this case; however, the resulting asymptotic order
of the information rate is the same.

n log(cid:16)1 + P
γ 2(cid:17) .

I (X; Z )
np ≤ m

Theorem 5.2. Supposethat E[X 2
j ] ≤ P andthecompresseddataareformedby Z = X,where
 is m × n with independent entries i j ∼ N (0, 1/n). Then the information rate rn,m satisﬁes
rn,m = sup p(X )
Under our sparsistency lower bound on m, the above upper bounds are rn,m = O(log(np)/n). We
note that these bounds may not be the best possible since they are obtained assuming knowledge of
the compression matrix , when in fact the privacy protocol requires that  and  are not public.

I (X; Z )
np ≤ m

2n log (2πeP) .

6 Experiments

In this section, we report results of simulations designed to validate the theoretical analysis presented
in previous sections. We ﬁrst present results that show the compressed lasso is comparable to the
uncompressed lasso in recovering the sparsity pattern of the true linear model. We then show results
on persistence that are in close agreement with the theoretical results of Section 4. We only include
Figures 1–2 here; additional plots are included in the full version.
Sparsistency. Here we run simulations to compare the compressed lasso with the uncompressed
lasso in terms of the probability of success in recovering the sparsity pattern of β∗. We use random
matrices for both X and , and reproduce the experimental conditions of [19]. A design parameter
is the compression factor f = n
m , which indicates how much the original data are compressed.
The results show that when the compression factor f is large enough, the thresholding behaviors
as speciﬁed in (8) and (9) for the uncompressed lasso carry over to the compressed lasso, when
X is drawn from a Gaussian ensemble.
is well below the
requirement that we have in Theorem 3.3 in case X is deterministic. In more detail, we consider the
Gaussian ensemble for the projection matrix , where i, j ∼ N (0, 1/n) are independent. The noise
is  ∼ N (0, σ 2), where σ 2 = 1. We consider Gaussian ensembles for the design matrix X with both
diagonal and Toeplitz covariance. In the Toeplitz case, the covariance is given by T (ρ)i, j = ρ|i− j|;
we use ρ = 0.1. [19] shows that when X comes from a Gaussian ensemble under these conditions,
there exist ﬁxed constants θ` and θu such that for any ν > 0 and s = supp(β), if
(8)

In general, the compression factor f

n > 2(θu + ν)s log( p − s) + s + 1,

then the lasso identiﬁes true variables with probability approaching one. Conversely, if

n < 2(θ` − ν)s log( p − s) + s + 1,

(9)
then the probability of recovering the true variables using the lasso approaches zero. In the follow-
ing simulations, we carry out the lasso using procedure lars(Y, X ) that implements the LARS
algorithm of [8] to calculate the full regularization path. For the uncompressed case, we run
lars(Y, X ) such that Y = Xβ∗ + , and for the compressed case we run lars(Y, X ) such
show that the behavior under compression is close to the uncompressed case.

that Y = Xβ∗+ . The regularization parameter is λm = cp(log( p − s) log s)/m. The results
Persistence. Here we solve the following `1-constrained optimization problem eβ =
kβk1≤L kY − Xβk2 directly, based on algorithms described by [15]. We constrain the solu-
tion to lie in the ball Bn = {kβk1 ≤ Ln}, where Ln = n1/4/√log n. By [10], the uncompressed lasso

arg min

6

Toeplitz r= 0.1; Fractional Power g=0.5, a=0.2

p=128

256

512

1024

Uncompressed
f = 120

s
s
e
c
c
u
s
 
f
o
 
b
o
r
P

0
.
1

8
.
0

6
.
0

4
.
0

2
.
0

0
.
0

0

50

150

100
Compressed dimension m

200

250

300

s
s
e
c
c
u
s
 
f

o

 

b
o
r
P

s
s
e
c
c
u
s
 
f

o

 

b
o
r
P

0

.

1

8
0

.

6
0

.

4
0

.

2
0

.

0
0

.

0

.

1

8
0

.

6
0

.

4
0

.

2
0

.

0

.

0

Identity; FP g= 0.5, a=0.2; p=1024

Uncomp.
f = 5
f = 10
f = 20
f = 40
f = 80
f = 120

0.0

0.5

1.0
2.0
Control parameter q

1.5

2.5

3.0

Toeplitz r= 0.1; FP g=0.5, a=0.2; p=1024

Uncomp.
f = 5
f = 10
f = 20
f = 40
f = 80
f = 120

0.0

0.5

1.0
2.0
Control parameter q

1.5

2.5

3.0

1

Figure 1: Plots of the number of samples versus the probability of success for recovering sgn(β∗).
Each point on a curve for a particular θ or m, where m = 2θ σ 2s log( p − s) + s + 1, is an average
over 200 trials; for each trial, we randomly draw Xn× p, m×n, and  ∈ Rn. The covariance  =
n E(cid:0)X T X(cid:1) and model β∗ are ﬁxed across all curves in the plot. The sparsity level is s( p) = 0.2 p1/2.
The four sets of curves in the left plot are for p = 128, 256, 512 and 1024, with dashed lines
marking m for θ = 1 and s = 2, 3, 5 and 6 respectively. In the plots on the right, each curve has
a compression factor f ∈ {5, 10, 20, 40, 80, 120} for the compressed lasso, thus n = f m; dashed
lines mark θ = 1. For  = I , θu = θ` = 1, while for  = T (0.1), θu ≈ 1.84 and θ` ≈ 0.46 [19],
for the uncompressed lasso in (8) and in (9).

n=9000, p=128, s=9

Uncompressed predictive
Compressed predictive
Compressed empirical

k
s
R

i

8
1

6
1

4
1

2
1

0
1

8

0

2000

4000

6000

8000

Compressed dimension m

that(cid:13)(cid:13)β∗b(cid:13)(cid:13)1 > Ln and β∗b 6∈ Bn, and the uncompressed oracle predictive risk is R = 9.81. For each

Figure 2: Risk versus compressed dimension. We ﬁx n = 9000 and p = 128, and set s( p) = 3 and
Ln = 2.6874. The model is β∗ = (−0.9,−1.7, 1.1, 1.3,−0.5, 2,−1.7,−1.3,−0.9, 0, . . . , 0)T so
value of m, a data point corresponds to the mean empirical risk, which is deﬁned in (7), over 100
trials, and each vertical bar shows one standard deviation. For each trial, we randomly draw Xn× p
with i.i.d. row vectors xi ∼ N (0, T (0.1)), and Y = Xβ∗ + .

7

estimatorbβn is persistent over Bn. For the compressed lasso, given n and pn, and a varying com-
pressed sample size m, we take the ball Bn,m = {β : kβk1 ≤ Ln,m} where Ln,m = m1/4/plog(npn).
The compressed lasso estimator bβn,m for log2(npn) ≤ m ≤ n, is persistent over Bn,m by Theo-

rem 4.1. The simulations conﬁrm this behavior.

7 Acknowlegments

This work was supported in part by NSF grant CCF-0625879.

References

[1] D. Agrawal and C. C. Aggarwal. On the design and quantiﬁcation of privacy preserving data mining

algorithms. In In Proceedings of the 20th Symposium on Principles of Database Systems, May 2001.

[2] E. Candès, J. Romberg, and T. Tao. Stable signal recovery from incomplete and inaccurate measurements.

Communications in Pure and Applied Mathematics, 59(8):1207–1223, August 2006.

[3] T. Dalenius. Towards a methodology for statistical disclosure control. Statistik Tidskrift, 15:429–444,

1977.

[4] D. Donoho. Compressed sensing. IEEE Trans. Info. Theory, 52(4):1289–1306, April 2006.
[5] M. Duarte, M. Davenport, M. Wakin, and R. Baraniuk. Sparse signal detection from incoherent projections.

In Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing, 2006.

[6] G. Duncan and R. Pearson. Enhancing access to microdata while protecting conﬁdentiality: Prospects for

the future. Statistical Science, 6(3):219–232, August 1991.

[7] C. Dwork. Differential privacy.

In 33rd International Colloquium on Automata, Languages and

Programming–ICALP 2006, pages 1–12, 2006.

[8] B. Efron, T. Hastie, I. Johnstone, and R. Tibshirani. Least angle regression. Annals of Statistics, 32(2):407–

499, 2004.

[9] J. Feigenbaum, Y. Ishai, T. Malkin, K. Nissim, M. J. Strauss, and R. N. Wright. Secure multiparty compu-

tation of approximations. ACM Trans. Algorithms, 2(3):435–472, 2006.

[10] E. Greenshtein and Y. Ritov. Persistency in high dimensional linear predictor-selection and the virtue of

over-parametrization. Journal of Bernoulli, 10:971–988, 2004.

[11] J. Haupt, R. Castro, R. Nowak, G. Fudge, and A. Yeh. Compressive sampling for signal classiﬁcation. In

Proc. Asilomar Conference on Signals, Systems, and Computers, October 2006.

[12] K. Liu, H. Kargupta, and J. Ryan. Random projection-based multiplicative data perturbation for privacy

preserving distributed data mining. IEEE Trans. on Knowl. and Data Engin., 18(1), Jan. 2006.

[13] T. L. Marzetta and B. M. Hochwald. Capacity of a mobile multiple-antenna communication link in

Rayleigh ﬂat fading. IEEE Trans. Info. Theory, 45(1):139–157, January 1999.

[14] N. Meinshausen and B. Yu. Lasso-type recovery of sparse representations for high-dimensional data.

Technical Report 720, Department of Statistics, UC Berkeley, 2006.

[15] M. Osborne, B. Presnell, and B. Turlach. On the lasso and its dual. J. Comp. and Graph. Stat., 9(2):319–

337, 2000.

[16] A. P. Sanil, A. Karr, X. Lin, and J. P. Reiter. Privacy preserving regression modelling via distributed

computation. In Proceedings of Tenth ACM SIGKDD, 2004.

[17] R. Tibshirani. Regression shrinkage and selection via the lasso. J. Roy. Statist. Soc. Ser. B, 58(1):267–288,

1996.

[18] J. Tropp. Greed is good: Algorithmic results for sparse approximation. IEEE Transactions on Information

Theory, 50(10):2231–2242, 2004.

[19] M. Wainwright. Sharp thresholds for high-dimensional and noisy recovery of sparsity. Technical Report

709, Department of Statistics, UC Berkeley, May 2006.

[20] P. Zhao and B. Yu. On model selection consistency of lasso. J. Mach. Learn. Research, 7:2541–2567,

2007.

8

"
896,2007,Predictive Matrix-Variate t Models,"It is becoming increasingly important to learn from a partially-observed random matrix and predict its missing elements. We assume that the entire matrix is a single sample drawn from a matrix-variate t distribution and suggest a matrix-variate t model (MVTM) to predict those missing elements. We show that MVTM generalizes a range of known probabilistic models, and automatically performs model selection to encourage sparse predictive models. Due to the non-conjugacy of its prior, it is difficult to make predictions by computing the mode or mean of the posterior distribution. We suggest an optimization method that sequentially minimizes a convex upper-bound of the log-likelihood, which is very efficient and scalable. The experiments on a toy data and EachMovie dataset show a good predictive accuracy of the model.","Predictive Matrix-Variate t Models

Shenghuo Zhu

Kai Yu

Yihong Gong

NEC Labs America, Inc.

10080 N. Wolfe Rd. SW3-350

Cupertino, CA 95014

{zsh,kyu,ygong}@sv.nec-labs.com

Abstract

It is becoming increasingly important to learn from a partially-observed random
matrix and predict its missing elements. We assume that the entire matrix is a
single sample drawn from a matrix-variate t distribution and suggest a matrix-
variate t model (MVTM) to predict those missing elements. We show that MVTM
generalizes a range of known probabilistic models, and automatically performs
model selection to encourage sparse predictive models. Due to the non-conjugacy
of its prior, it is difﬁcult to make predictions by computing the mode or mean of
the posterior distribution. We suggest an optimization method that sequentially
minimizes a convex upper-bound of the log-likelihood, which is very efﬁcient and
scalable. The experiments on a toy data and EachMovie dataset show a good
predictive accuracy of the model.

1

Introduction

Matrix analysis techniques, e.g., singular value decomposition (SVD), have been widely used in
various data analysis applications. An important class of applications is to predict missing elements
given a partially observed random matrix. For example, putting ratings of users into a matrix form,
the goal of collaborative ﬁltering is to predict those unseen ratings in the matrix.

To predict unobserved elements in matrices, the structures of the matrices play an importance role,
for example, the similarity between columns and between rows. Such structures imply that elements
in a random matrix are no longer independent and identically-distributed (i.i.d.). Without the i.i.d.
assumption, many machine learning models are not applicable.

In this paper, we model the random matrix of interest as a single sample drawn from a matrix-
variate t distribution, which is a generalization of Student-t distribution. We call the predictive
model under such a prior by matrix-variate t model (MVTM). Our study shows several interesting
properties of the model. First, it continues the line of gradual generalizations across several known
probabilistic models on random matrices, namely, from probabilistic principle component analysis
(PPCA) [11], to Gaussian process latent-variable models (GPLVMs)[7], and to multi-task Gaussian
processes (MTGPs) [13]. MVTMs can be further derived by analytically marginalizing out the
hyper-parameters of these models. From a Bayesian modeling point of view, the marginalization of
hyper-parameters means an automatic model selection and usually leads to a better generalization
performance [8]; Second, the model selection by MVTMs explicitly encourages simpler predictive
models that have lower ranks. Unlike the direct rank minimization, the log-determinant terms in the
form of matrix-variate t prior offers a continuous optimization surface (though non-convex) for rank
constraint; Third, like multivariate Gaussian distributions, a matrix-variate t prior is consistent under
marginalization, that means, if a matrix follows a matrix-variate t distribution, its any sub-matrix
follows a matrix-variate t distribution as well. This property allows to generalize distributions for
ﬁnite matrices to inﬁnite stochastic processes.

S

R

S

T

Y

(a)

T

Y

(b)

T

Y

(c)

I

T

Y

(d)

Figure 1: Models for matrix prediction. (a) MVTM. (b) and (c) are two normal-inverse-Wishart
models, equivalent to MVTM when the covariance variable S (or R) is marginalized. (d) MTGP,
which requires to optimize the covariance variable S. Circle nodes represent for random variables,
shaded nodes for (partially) observable variables, text nodes for given parameters.

Under a Gaussian noise model, the matrix-variate t distribution is not a conjugate prior. It is thus dif-
ﬁcult to make predictions by computing the mode or mean of the posterior distribution. We suggest
an optimization method that sequentially minimizes a convex upper-bound of the log-likelihood,
which is highly efﬁcient and scalable. In the experiments, the algorithm shows very good efﬁciency
and excellent prediction accuracy.

This paper is organized as follows. We review three existing models and introduce the matrix-variate
t models in Section 2. The prediction methods are proposed in Section 3. In Section 4, the MVTM is
compared with some other models. We illustrate the MVTM with the experiments on a toy example
and on the movie-rating data in Section 5. We conclude in Section 6.

2 Predictive Matrix-Variate t Models

2.1 A Family of Probabilistic Models for Matrix Data

In this section we introduce three probabilistic models in the literature. Let Y be a p × m
observational matrix and T be the underlying p × m noise-free random matrix. We assume
Yi,j = Ti,j + i,j , i,j ∼ N (0, σ2), where Yi,j denotes the (i, j)-th element of Y.
If Y is
partially observed, then YI denotes the set of observed elements and I is the corresponding index
set.

Probabilistic Principal Component Analysis (PPCA) [11] assumes that yj, the j-th column vector
of Y, can be generated from a latent vector vj in a k-dimensional linear space (k < p). The model
is deﬁned as yj = Wvj + µ + j and vj ∼ Nk(vj; 0, Ik), where j ∼ Np(j; 0, σ2Ip), and
W is a p × k loading matrix. By integrating out vj, we obtain the marginal distribution yj ∼
Np(yj; µ, WW> + σ2Ip). Since the columns of Y are conditionally independent, letting S take
the place of WW>, PPCA is similar1 to

Yi,j = Ti,j + i,j,

T ∼ Np,m(T; 0, S, Im),

where Np,m(·; 0, S, Im) is a matrix-variate normal prior with zero mean, covariance S between
rows, and identity covariance Im between columns. PPCA aims to estimate the parameter W by
maximum likelihood.

Gaussian Process Latent-Variable Model (GPLVM) [7] formulates a latent-variable model in a
slightly unconventional way. It considers the same linear relationship from latent representation vj
to observations yj. Instead of treating vj as random variables, GPLVM assigns a prior on W and
see {vj} as parameters yj = Wvj + j, and W ∼ Np,k(W; 0, Ip, Ik), where the elements of W
are independent Gaussian random variables. By marginalizing out W, we obtain a distribution that
each row of Y is an i.i.d. sample from a Gaussian process prior with the covariance VV> + σ2Im
and V = [v1, . . . , vm]>. Letting R take the place of VV>, we rewrite a similar model as

Yi,j = Ti,j + i,j,

T ∼ Np,m(T; 0, Ip, R).

1Because it requires S to be positive deﬁnite and W is usually low rank, they are not equivalent.

S
W
S
W
S
W
S
From a matrix modeling point of view, GPLVM estimates the covariance between the rows and
assume the columns to be conditionally independent.

Multi-task Gaussian Process (MTGP) [13] is a multi-task learning model where each column of
Y is a predictive function of one task, sampled from a Gaussian process prior, yj = tj + j, and
tj ∼ Np(0, S), where j ∼ Np(0, σ2Ip). It introduces a hierarchical model where an inverse-
Wishart prior is added for the covariance,

Yi,j = Ti,j + i,j,

T ∼ Np,m(T; 0, S, Im),

S ∼ IW p(S; ν, Ip)

MTGP utilizes the inverse-Wishart prior as the regularization and obtains a maximum a posteriori
(MAP) estimate of S.

2.2 Matrix-Variate t Models

The models introduced in the previous section are closely related to each other. PPCA models the
row covariance of Y, GPLVM models the column covariance, and MTGP assigns a hyper prior to
prevent over-ﬁtting when estimating the (row) covariance. From a matrix modeling point of view,
capturing the dependence structure of Y by its row or column covariance is a matter of choices,
which are not fundamentally different.2 There is no reason to favor one choice over the other. By
introducing the matrix-variate t models (MVTMs), they can be uniﬁed to be the same model.

From a Bayesian modeling viewpoint, one should marginalize out as many variables as possible
[8]. We thus extend the MTGP model in two directions: (1) assume T ∼ Np,m(T; 0, S, Im) that
have covariances on both sides of the matrix; (2) marginalize the covariance S on one side (see
Figure 1(b)). Then we have a marginal distribution of T

Pr(T) =Z Np,m(T; 0, S, Im)IW p(S; ν, Ip)dS = tp,m(T; ν, 0, Ip, Im),

(1)

which is a matrix-variate t distribution. Because the inverse-Wishart distribution may have different
degree-of-freedom deﬁnition in literature, we use the deﬁnition in [5].

Following the deﬁnition in [6], the matrix-variate t distribution of p × m matrix T is given by

tp,m(T; ν, M, Σ, Ω) def=

1
Z

|Σ|− m

2 |Ω|− p

2 (cid:12)(cid:12)Ip + Σ−1(T − M)Ω−1(T − M)>(cid:12)(cid:12)− ν+m+p−1

2

,

where ν is the degree of freedom; M is a p × m matrix; Σ and Ω are positive deﬁnite matrices of
size p × p and m × m, respectively; Z = (νπ)
); Γp(·) is a multivariate
gamma function, and | · | stands for determinant.

)/Γp( ν+m+p−1

2 Γp( ν+p−1

mp

2

2

The model can be depicted as Figure 1(a). One important property of matrix-variate t distribution
is that the marginal distribution of its sub-matrix still follows a matrix-variate t distribution with the
same degree of freedom (see Section 3.1). Therefore, we can expand it to the inﬁnite dimensional
stochastic process. By Eq. (1), we can see that Figure 1(a) and Figure 1(b) describe two equivalent
models. Comparing them with the MTGP model represented in Figure 1(d), we can see that the
difference lies in whether S is point estimated or integrated out.

Interestingly, the same matrix-variate t distribution can be equivalently derived by putting another
hierarchical generative process on the covariance R, as described in Figure 1(c), where R follows
an inverse-Wishart distribution. In other words, integrating the covariance on either side, we obtain
the same model. This implies that the model controls the complexity of the covariances on both
sides of the matrix. Neither PPCA nor GPLVM has such a property.

The matrix-variate t distribution involves a determinant term of T, which becomes a log-determinant
term in log-likelihood or KL-divergence. The log-determinant term encourages the sparsity of ma-
trix T with lower rank. This property has been used as the heuristic for minimizing the rank of the
matrix in [3]. Student’s t priors were applied to enforce sparse kernel machine [10].

Here we say a few words about the given parameters. Though we can use evidence framework[8]
or other methods to estimate ν, the results are not good in many cases(see [4]). Usually we just set

2GPLVM offers an advantage of using nonlinear covariance function based on attributes.

ν to a small number. Similar to ν, the estimated σ2 does not give us a good result either, but cross-
validation is a good choice. For the mean matrix M, in our experiments, we just use sample average
for all observed elements. For some tasks, when we have prior knowledge about the covariance
between columns or between rows, we can use the covariance matrices in the places of Im or Ip.

3 Prediction Methods

When the evaluation of the prediction is the sum of individual losses, the optimal prediction is to ﬁnd
the individual mode of the marginal posterior distribution, i.e., arg maxTij Pr(Tij|YI). However,
there is no exact solution for the marginal posterior. We have two ways to approximate the optimal
prediction.

One way to make prediction is to compute the mode of the joint posterior distribution of T, i.e. the
prediction problem is

{ln Pr(YI|T) + ln Pr(T)} .

(2)

bT = arg max

T

The computation of this estimation is usually easy. We discuss it in Section 3.3.

An alternative way is to use the individual mean of the posterior distribution to approximate the
individual mode. Since the joint of individual mean happens to be the mean of the joint distribution,
we only need to compute the joint posterior distribution. The problem of prediction by means is
written as

T = E(T|YI).

(3)

However, it is usually difﬁcult to compute the exact mean. One estimation method is the Monte
Carlo method, which is computationally intensive. In Section 3.4, we discuss an approximation
to compute the mean. From our experiments, the prediction by means usually outperforms the
prediction by modes.

Before discussing the prediction methods, we introduce a few useful properties in Section 3.1 and
suggest an optimization method as the efﬁcient tool for prediction in Section 3.2.

3.1 Properties

The MVTM has a rich set of properties. We list a few in the following Theorem.

Theorem 1. If

then

n m
q Θ Φ

(cid:18)
p Ψ T(cid:19) ∼ tp+q,m+n(·; ν, 0, (cid:18)

q
Iq
0

p
0

Ip(cid:19), (cid:18)

n m
In
0
0

Im(cid:19)),

Pr(T) =tp,m(T; ν, 0, Ip, Im),

Pr(T|Θ, Φ, Ψ) =tp,m(T; ν + q + n, M, (Ip + ΨBΨ>), (Im + Φ>AΦ)),

Pr(Θ) =tq,n(Θ; ν, 0, Iq, In),

Pr(Φ|Θ) =tq,m(Φ; ν + n, 0, A−1, Im),

Pr(Ψ|Θ, Φ) =tp,n(Ψ; ν + q, 0, Ip, B−1) = Pr(Ψ|Θ),

E(T|Θ, Φ, Ψ) =M,

Cov(cid:16)vec(cid:16)T>(cid:17) |Θ, Φ, Ψ(cid:17) =(ν + q + n − 2)−1(Ip + ΨBΨ>) ⊗ (Im + Φ>AΦ),

where A

def

= (ΘΘ> + Iq)−1, B

def

= (Θ>Θ + In)−1, and M

def
= ΨΘ>AΦ = ΨBΘ>Φ.

(4)

(5)

(6)

(7)

(8)

(9)

(10)

(11)

This theorem can be directly derived from Theorem 4.3.1 and 4.3.9 in [6] with a little calculus. It
provides some insights about MVTMs. The marginal distribution in Eq. (5) has the same form as the
joint distribution, therefore the matrix-variate t distribution is extensible to an inﬁnite dimensional
stochastic process. As conditional distribution in Eq. (6) is still a matrix-variate t distribution, we
can use it to approximate the posterior distribution, which we use in Section 3.4.

We encounter log-determinant terms in computation of the mode or mean estimation. The following
theorem provides a quadratic upper bounds for the log-determinant terms, which makes it possible
to apply the optimization method in Section 3.2.
Lemma 1. If X is a p × p positive deﬁnite matrices, it holds that ln |X| ≤ tr (X) − p. The equality
holds when X is an orthonormal matrix.

Proof. Let {λ1, · · · , λp} be the eigenvalues of X. We have ln |X| =Pi ln λi and tr (X) =Pi λi.

Since ln λi ≤ λi − 1, we have the inequality. The equality holds when λi = 1. Therefore, when X
is an orthonormal matrix (especially X = Ip), the equality holds.

Theorem 2. If Σ is a p × p positive deﬁnite matrix, Ω is an m × m positive deﬁnite matrix, and T
and T0 are p × m matrices, it holds that

ln |Σ + TΩ−1T>| ≤ h(T; T0, Σ, Ω) + h0(T0, Σ, Ω),

where

h(T; T0, Σ, Ω)

h0(T0, Σ, Ω)

∂
∂T

h(T; T0, Σ, Ω)(cid:12)(cid:12)(cid:12)(cid:12)T=T0

The equality holds when T = T0. Also it holds that

def

=tr(cid:16)(Σ + T0Ω−1T>

def

= ln |Σ + T0Ω−1T>

0 )−1TΩ−1T>(cid:17) ,
0 | + tr(cid:16)(Σ + T0Ω−1T>

= 2(Σ + T0Ω−1T>

0 )−1T0Ω−1 =

∂
∂T

0 )−1Σ(cid:17) − p
ln |Σ + TΩ−1T>|(cid:12)(cid:12)(cid:12)(cid:12)T=T0

.

Applying Lemma 1 with X = (Σ + T0Ω−1T>
0 )−1(Σ + TΩ−1T>), we obtain the inequality. By
some calculus we have the equality of the ﬁrst-order derivative. Actually h(·) is a quadratic convex
function with respect to T, as (Σ + T0Ω−1T>

0 )−1 and Ω−1 are positive deﬁnite matrices.

3.2 Optimization Method

Once the objective is given, the prediction becomes an optimization problem. We use an EM-
style optimization method to make the prediction. Suppose J (T) be the objective function to be
minimized. If we can ﬁnd an auxiliary function, Q(T; T0), having the following properties, we can
apply this method.

1. J (T) ≤ Q(T; T0) and J (T0) = Q(T0; T0),

2. ∂J (T)/∂T|T=T0 = ∂Q(T; T0)/∂T(cid:12)(cid:12)T=T0 ,

3. For a ﬁxed T0, Q(T; T0) is quadratic and convex with respect to T.

Starting from any T0, as long as we can ﬁnd a T1 such that Q(T1, T0) < Q(T0, T0), we have
J (T0) = Q(T0, T0) > Q(T1, T0) ≥ J (T1). If there exists a global minimum point of J (T),
there exists a global minimum point of Q(T; T0) as well, because Q(T; T0) is upper bound of
J (T). Since Q(T; T0) is quadratic with the respect to T, we can apply the Newton-Raphson
method to minimize Q(T; T0). As long as T0 is not a local minimum, maximum or saddle point of
J , we can ﬁnd a T to reduce Q(T; T0), because Q(T; T0) has the same derivative as J (T) at T0.
Usually, a random starting point, T0 is unlikely to be a local maximum, then T1 can not be a local
maximum. If T0 is a local maximum, we can reselect a point, which is not. After we ﬁnd a Ti, we
repeat the procedure to ﬁnd a Ti+1 so that J (Ti+1) < J (Ti), unless Ti is a local minimum or
saddle point of J . Repeating this procedure, Ti converges a local minimum or saddle point of J ,
as long as T0 is not a local maximum.

3.3 Mode Prediction

Following Eq. (2), the goal is to minimize the objective function

bJ (T) def= `(T) +

ν+m+p−1

2

ln(cid:12)(cid:12)(cid:12)Ip + TT>(cid:12)(cid:12)(cid:12) ,

(12)

where `(T) def= − ln Pr(YI) = 1

2σ2 P(i,j)∈I(Tij − Yij)2 + const.

introduce an auxiliary function,

Q(T; T0) def= `(T) + h(T; T0, Ip, Im) + h0(T0, Ip, Im).

As bJ contains a log-determinant term, minimizing bJ by nonlinear optimization is slow. Here, we
By Corollary 2, we have that bJ (T) ≤ Q(T; T0), bJ (T0) = Q(T0, T0), and Q(T, T0) has the same
ﬁrst-order derivative as bJ (T) at T0. Because l and h are quadratic and convex, Q is quadratic and
convex as well. Therefore, we can apply the optimization method in Section 3.2 to minimize bJ .
However, when the size of T is large, to ﬁnd bT is still time consuming and requires a very large
space. In many tasks, we only need to infer a small portion of bT. Therefore, we consider a low
matrix. The problem of Eq. (2) is approximated by arg minU,V bJ (UV>). We can minimize J1 by
alternatively optimizing U and V. We can put the ﬁnal result in a canonical format as bT ≈ USV>,

where U and V are semi-orthonormal and S is a k × k diagonal matrix. This result can be consider
as the SVD of an incomplete matrix using matrix-variate t regularization. The details are skipped
because of the limit space.

rank approximation, using UV> to approximate T, where U is a p × k matrix and V is an m × k

(13)

3.4 Variational Mean Prediction

As the difﬁculty in explicitly computing the posterior distribution of T, we take a variational ap-
proach to approximate its posterior distribution by a matrix-variate t distribution via an expanded
model. We expand the model by adding matrix variate Θ, Φ and Ψ with distribution as Eq. (4).
Since the marginal distribution, Eq. (5), is the same as the prior of T, we can derive the original
model by marginalizing out Θ, Φ and Ψ. However, instead of integrating out Θ, Φ and Ψ, we use
them as the parameters to approximate T’s posterior distribution. Therefore, the estimation of the
parameters is to minimize

− ln Pr(YI, Θ, Φ, Ψ) = − ln Pr(Θ, Φ, Ψ) − lnZ Pr(T|Θ, Φ, Ψ) Pr(YI|T)dT

(14)

over Θ, Φ and Ψ. The ﬁrst term in the RHS of Eq. (14) can be written as

− ln Pr(Θ, Φ, Ψ) = − ln Pr(Θ) − ln Pr(Φ|Θ) − ln Pr(Ψ|Θ, Φ)
ν+q+n+m−1

ν+q+n+p+m−1

=

2

ν+q+n+p−1

2

+

ln |Iq + ΘΘ>| +

2
ln |Ip + ΨBΨ>| + const.

ln |Im + Φ>AΦ|

(15)

Due to the convexity of negative logarithm, the second term in the RHS of Eq. (14) is bounded by

`(ΨB

1
2 Θ>A

1
2 Φ) +

1

2σ2(ν+q+n−2) X(i,j)∈I

(1 + [ΨBΨ>]ii)(1 + [Φ>AΦ]jj) + const.

(16)

because − ln Pr(YI|T) is quadratic respective to T, thus we only need integration using the mean
and variance of Tij of Pr(T|Θ, Φ, Ψ), which is given by Eq. (10) and (11). The parameter estima-
tion not only reduce the loss (the term of `(·)), but also reduce the variance. Because of this, the
prediction by means usually outperforms the prediction by modes.

Let J be the sum of the right-hand-side of Eq. (15) and (16), which can be considered as the upper
bound of Eq. (14) (ignoring constants). Here, we estimate the parameters by minimizing J . Because
A and B involve the inverse of quadratic term of Θ, it is awkward to directly optimize Θ, Φ, Ψ.
def= Θ. We can easily apply the
We reparameterize J by U
optimization method in Section 3.2 to ﬁnd optimal U, V and S. After estimation U, V and S, by
Theorem 1, we can compute T = M = USV>. The details are skipped because of the limit space.

def= Φ>A1/2, and S

def= ΨB1/2, V

4 Related work

Maximum Margin Matrix Factorization (MMMF) [9] is not in the framework of stochastic matrix
analysis, but there are some similarities between MMMF and our mode estimation in Section 3.3.

Using trace norm on the matrix as regularization, MMMF overcomes the over-ﬁtting problem in
factorizing matrix with missing values. From the regularization viewpoint, the prediction by mode
of MVTM uses log-determinants as the regularization term in Eq. (12). The log-determinants en-
courage sparsity predictive models.

Stochastic Relational Models (SRMs) [12] extend MTGPs by estimating the covariance matrices
for each side. The covariance functions are required to be estimated from observation. By maxi-
mizing marginalized likelihood, the estimated S and R reﬂect the information of the dependency
structure. Then the relationship can be predicted with S and R. During estimating S and R, inverse-
Wishart priors with parameter Σ and Ω are imposed to S and R respectively. MVTM differs from
SRM in integrating out the hyper-parameters or maximizing out. As MacKay suggests [8], “one
should integrate over as many variables as possible”.

Robust Probabilistic Projections (RPP)[1] uses Student-t distribution to extends PPCA by scaling
each feature vector by an independent random variable. Written in a matrix format, RPP is

T ∼ Np,m(T; µ1>, WW>, U),

U = diag {ui} ,

ui ∼ IG(ui|

ν
2

,

ν
2

),

where IG is inverse Gamma distribution. Though RPP unties the scale factors between feature vec-
tors, which could make the estimation more robust, it does not integrate out the covariance matrix,
which we did in MVTM. Moreover inherited from PPCA, RPP implicitly uses independence as-
sumption of feature vectors. Also RPP results different models depending on which side we assume
to be independent, therefore it is not suitable for matrix prediction.

5 Experiments

5

10

15

20

25

30

5

10

15

20

25

30

5

10

15

20

25

30

5

10

15

20

25

30

5

10

15

20

25

30

2

4

6

8

10

12

14

16

18

20

2

4

6

8

10

12

14

16

18

20

2

4

6

8

10

12

14

16

18

20

2

4

6

8

10

12

14

16

18

20

(a) Original Matrix

(b) With Noise (0.32)

(c) MMMF (0.27)

(d) PPCA (0.26)

5

10

15

20

25

30

5

10

15

20

25

30

5

10

15

20

25

30

2

4

6

8

10

12

14

16

18

20

2

4

6

8

10

12

14

16

18

20

2

4

6

8

10

12

14

16

18

20

2

4

6

8

10

12

14

16

18

20

(e) SRM (0.22)

(f) MVTM mode (0.20)

(g) MVTM mean (0.192)

(h) MCMC (0.185)

Figure 2: Experiments on synthetic data. RMSEs are shown in parentheses.

Synthetic data: We generate a 30 × 20 matrix (Fig-
ure 2(a)), then add noise with σ2 = 0.1 (Figure 2(b)). The
root mean squared noise is 0.32. We select 70% elements
as the observed data and the rest elements are for predic-
tion. We apply MMMF [9], PPCA[11], MTGP[13], SRM
[12], our MVTM prediction-by-means and prediction-
by-modes methods. The number of dimensions for low
rank approximation is 10. We also apply MCMC method
to infer the matrix. The reconstruction matrix and root
mean squared errors of prediction on the unobserved el-
ements (comparing to the original matrix) are shown in
Figure 2(c)-2(g), respectively. MTGP has the similar re-
sult as PPCA, we do not show the result.

l

s
e
u
a
v
 
r
a
u
g
n
s

l

i

MMMF
MVTM-mode
MVTM-mean

 4
 3.5
 3
 2.5
 2
 1.5
 1
 0.5
 0

 1 2 3 4 5 6 7 8 9 10

index

Figure 3: Singular values of recovered
matrices in descent order.

MVTM is in favor of sparse predictive models. To verify this, we depict the singular values of
the MMMF method and two MVTM prediction methods in Figure 3. There are only two singular

RMSE
MAE

user mean movie mean MMMF
1.186
0.943

1.387
1.103

1.425
1.141

PPCA MVTM (mode) MVTM (mean)
1.165
0.915

1.162
0.898

1.151
0.887

Table 1: RMSE (root mean squred error) and MAE (mean absolute error) of experiments on Each-
movie data. All standard errors are 0.001 or less.

values of the MVTM prediction-by-means method are non-zeros. The singular values of the mode
estimation decrease faster than the MMMF ones at beginning, but decrease slower after a threshold.
This conﬁrms that the log-determinants automatically determine the intrinsic rank of the matrices.

Eachmovie data: We test our algorithms on Eachmovie from [2]. The dataset contains 74, 424
users’ 2, 811, 718 ratings on 1, 648 movies, i.e. about 2.29% are rated by zero-to-ﬁve stars. We put
all ratings into a matrix, and randomly select 80% as observed data to predict the remaining ratings.
The random selection was carried out 10 times independently. We compare our approach with other
three approaches: 1) USER MEAN predicting rating by the sample mean of the same user’ ratings;
2) MOVIE MEAN, predicting rating by the sample mean of users’ ratings of the same movie; 3)
MMMF[9]; 4) PPCA[11]. We do not have a scalable implementation for other approaches compared
in the previous experiment. The number of dimensions is 10. The results are shown in Table 1. Two
MVTM prediction methods outperform the other methods.

6 Conclusions

In this paper we introduce matrix-variate t models for matrix prediction. The entire matrix is mod-
eled as a sample drawn from a matrix-variate t distribution. An MVTM does not require the inde-
pendence assumption over elements. The implicit model selection of the MVTM encourages sparse
models with lower ranks. To minimize the log-likelihood with log-determinant terms, we propose an
optimization method by sequentially minimizing its convex quadratic upper bound. The experiments
show that the approach is accurate, efﬁcient and scalable.

References

[1] C. Archambeau, N. Delannay, and M. Verleysen. Robust probabilistic projections. In ICML, 2006.

[2] J. Breese, D. Heckerman, and C. Kadie. Empirical analysis of predictive algorithms for collaborative

ﬁltering. In UAI-98, pages 43–52, 1998.

[3] M. Fazel, H. Haitham, and S. P. Boyd. Log-det heuristic for matrix rank minimization with applications

to hankel and euclidean distance matrices. In Proceedings of the American Control Conference, 2003.

[4] C. Fernandez and M. F. J. Steel. Multivariate Student-t regression models: Pitfalls and inference.

Biometrika, 86(1):153–167, 1999.

[5] A. Gelman, J. B. Carlin, H. S. Stern, and D. B. Rubin. Bayesian Data Analysis. Chapman & Hall/CRC,

New York, 2nd edition, 2004.

[6] A. K. Gupta and D. K. Nagar. Matrix Variate Distributions. Chapman & Hall/CRC, 2000.

[7] N. Lawrence. Probabilistic non-linear principal component analysis with gaussian process latent variable

models. J. Mach. Learn. Res., 6:1783–1816, 2005.

[8] D. J. C. MacKay. Comparison of approximate methods for handling hyperparameters. Neural Comput.,

11(5):1035–1068, 1999.

[9] J. D. M. Rennie and N. Srebro. Fast maximum margin matrix factorization for collaborative prediction.

In ICML, 2005.

[10] M. E. Tipping. Sparse bayesian learning and the relevance vector machine. Journal of Machine Learning

Research, 1:211–244, 2001.

[11] M. E. Tipping and C. M. Bishop. Probabilistic principal component analysis. Journal of the Royal

Statisitical Scoiety, B(61):611–622, 1999.

[12] K. Yu, W. Chu, S. Yu, V. Tresp, and Z. Xu. Stochastic relational models for discriminative link prediction.

In Advances in Neural Information Processing Systems 19 (NIPS), 2006.

[13] K. Yu, V. Tresp, and A. Schwaighofer. Learning Gaussian processes from multiple tasks. In ICML, 2005.

"
1077,2007,Loop Series and Bethe Variational Bounds in Attractive Graphical Models,"Variational methods are frequently used to approximate or bound the partition or likelihood function of a Markov random field. Methods based on mean field theory are guaranteed to provide lower bounds, whereas certain types of convex relaxations provide upper bounds. In general, loopy belief propagation (BP) provides (often accurate) approximations, but not bounds. We prove that for a class of attractive binary models, the value specified by any fixed point of loopy BP always provides a lower bound on the true likelihood. Empirically, this bound is much better than the naive mean field bound, and requires no further work than running BP. We establish these lower bounds using a loop series expansion due to Chertkov and Chernyak, which we show can be derived as a consequence of the tree reparameterization characterization of BP fixed points.","Loop Series and Bethe Variational Bounds

in Attractive Graphical Models

Erik B. Sudderth and Martin J. Wainwright

Electrical Engineering & Computer Science, University of California, Berkeley

sudderth@eecs.berkeley.edu, wainwrig@eecs.berkeley.edu

Electrical Engineering & Computer Science, Massachusetts Institute of Technology

Alan S. Willsky

willsky@mit.edu

Abstract

Variational methods are frequently used to approximate or bound the partition
or likelihood function of a Markov random ﬁeld. Methods based on mean ﬁeld
theory are guaranteed to provide lower bounds, whereas certain types of convex
relaxations provide upper bounds. In general, loopy belief propagation (BP) pro-
vides often accurate approximations, but not bounds. We prove that for a class of
attractive binary models, the so–called Bethe approximation associated with any
ﬁxed point of loopy BP always lower bounds the true likelihood. Empirically,
this bound is much tighter than the naive mean ﬁeld bound, and requires no fur-
ther work than running BP. We establish these lower bounds using a loop series
expansion due to Chertkov and Chernyak, which we show can be derived as a
consequence of the tree reparameterization characterization of BP ﬁxed points.

1 Introduction
Graphical models are widely used in many areas, including statistical machine learning, computer
vision, bioinformatics, and communications. Such applications typically require computationally
efﬁcient methods for (approximately) solving various problems, including computing marginal dis-
tributions and likelihood functions. The variational framework provides a suite of candidate meth-
ods, including mean ﬁeld approximations [3, 9], the sum–product or belief propagation (BP) algo-
rithm [11, 14], Kikuchi and cluster variational methods [23], and related convex relaxations [21].

The likelihood or partition function of an undirected graphical model is of fundamental interest in
many contexts, including parameter estimation, error bounds in hypothesis testing, and combina-
torial enumeration. In rough terms, particular variational methods can be understood as solving
optimization problems whose optima approximate the log partition function. For mean ﬁeld meth-
ods, this optimal value is desirably guaranteed to lower bound the true likelihood [9]. For other
methods, including the Bethe variational problem underlying loopy BP [23], optima may either
over–estimate or under–estimate the truth. Although “convexiﬁed” relaxations of the Bethe problem
yield upper bounds [21], to date the best known lower bounds on the partition function are based on
mean ﬁeld theory. Recent work has studied loop series expansions [2, 4] of the partition function,
which generate better approximations but not, in general, bounds.

Several existing theoretical results show that loopy BP, and the corresponding Bethe approximation,
have desirable properties for graphical models with long cycles [15] or sufﬁciently weak depen-
dencies [6, 7, 12, 19]. However, these results do not explain the excellent empirical performance
of BP in many graphs with short cycles, like the nearest–neighbor grids arising in spatial statistics
and low–level vision [3, 18, 22]. Such models often encode “smoothness” priors, and thus have
attractive interactions which encourage connected variables to share common values. The ﬁrst main
contribution of this paper is to demonstrate a family of attractive models for which the Bethe varia-
tional method always yields lower bounds on the true likelihood. Although we focus on models with
binary variables (but arbitrary order of interactions), we suspect that some ideas are more generally
applicable. For such models, these lower bounds are easily computed from any ﬁxed point of loopy
BP, and empirically improve substantially on naive mean ﬁeld bounds.

1

Our second main contribution lies in the route used to establish the Bethe lower bounds. In partic-
ular, Sec. 3 uses the reparameterization characterization of BP ﬁxed points [20] to provide a simple
derivation for the loop series expansion of Chertkov and Chernyak [2]. The Bethe approximation
is the ﬁrst term in this representation of the true partition function. Sec. 4 then identiﬁes attrac-
tive models for which all terms in this expansion are positive, thus establishing the Bethe lower
bound. We conclude with empirical results demonstrating the accuracy of this bound, and discuss
implications for future analysis and applications of loopy BP.

2 Undirected Graphical Models
Given an undirected graph G = (V, E), with edges (s, t) ∈ E connecting n vertices s ∈ V , a graph-
ical model associates each node with a random variable Xs taking values xs ∈ X . For pairwise
Markov random ﬁelds (MRFs) as in Fig. 1, the joint distribution of x := {xs | s ∈ V } is speciﬁed
via a normalized product of local compatibility functions:

p(x) =

1

Z(ψ) Ys∈V

ψs(xs) Y(s,t)∈E

ψst(xs, xt)

(1)

The partition function Z(ψ) := Px∈X n Qs ψs(xs) Q(s,t) ψst(xs, xt), whose value depends on

the compatibilities ψ, is deﬁned so that p(x) is properly normalized. We also consider distributions
deﬁned by hypergraphs G = (V, C), where each hyperedge c ∈ C connects some subset of the
vertices (c ⊂ V ). Letting xc := {xs | s ∈ c}, the corresponding joint distribution equals

p(x) =

1

Z(ψ) Ys∈V

ψs(xs) Yc∈C

ψc(xc)

(2)

where as before Z(ψ) = Px∈X n Qs ψs(xs) Qc ψc(xc). Such higher–order random ﬁelds are

conveniently described by the bipartite factor graphs [11] of Fig. 2.

In statistical physics, the partition function arises in the study of how physical systems respond to
changes in external stimuli or temperature [23]. Alternatively, when compatibility functions are
parameterized by exponential families [20], log Z(ψ) is the family’s cumulant generating function,
and thus intrinsically related to the model’s marginal statistics. For directed Bayesian networks
(which can be factored as in eq. (2)), Z(ψ) is the marginal likelihood of observed data, and plays a
central role in learning and model selection [9]. However, for general graphs coupling discrete ran-
dom variables, the cost of exactly evaluating Z(ψ) grows exponentially with n [8]. Computationally
tractable families of bounds on the true partition function are thus of great practical interest.

2.1 Attractive Discrete Random Fields
In this paper, we focus on binary random vectors x ∈ {0, 1}n. We say that a pairwise MRF, with
compatibility functions ψst : {0, 1}2 → R+, has attractive interactions if

(3)
for each edge (s, t) ∈ E. Intuitively, this condition requires all potentials to place greater weight
on conﬁgurations where neighboring variables take the same value. Our later analysis is based on
pairwise marginal distributions τst(xs, xt), which we parameterize as follows:

ψst(0, 0) ψst(1, 1) ≥ ψst(0, 1) ψst(1, 0)

τst(xs, xt) = (cid:20)1 − τs − τt + τst

τs − τst

τt − τst

τst (cid:21)

τs := Eτst [Xs]
τst := Eτst [XsXt]

(4)

We let Eτst [·] denote expectation with respect to τst(xs, xt), so that τst is the probability that
Xs = Xt = 1. This normalized matrix is attractive, satisfying eq. (3), if and only if τst ≥ τsτt.
For binary variables, the pairwise MRF of eq. (1) provides one representation of a general, inho-
mogeneous Ising model. In the statistical physics literature, Ising models are typically expressed
by coupling random spins zs ∈ {−1, +1} with symmetric potentials log ψst(zs, zt) = θstzszt. The
attractiveness condition of eq. (3) then becomes θst ≥ 0, and the resulting model has ferromagnetic
interactions. Furthermore, pairwise MRFs satisfy the regularity condition of [10], and thus allow
tractable MAP estimation via graph cuts [5], if and only if they are attractive. Even for attractive
models, however, calculation of the partition function in non–planar graphs is #P–complete [8].

To deﬁne families of higher–order attractive potentials, we ﬁrst consider a probability distribution
τc(xc) on k = |c| binary variables. Generalizing eq. (4), we parameterize such distributions by the

2

following collection of 2k − 1 mean parameters:

τa := Eτc(cid:20)Ys∈a

Xs(cid:21)

∅ 6= a ⊆ c

(5)

For example, τstu(xs, xt, xu) would be parameterized by {τs, τt, τu, τst, τsu, τtu, τstu}. For any
subset a ⊆ c, we then deﬁne the following central moment statistic:

κa := Eτc(cid:20)Ys∈a

(Xs − τs)(cid:21)

∅ 6= a ⊆ c

(6)

Note that κs = 0, while κst = Covτ (Xs, Xt) = τst − τsτt. The third–order central moment then
equals the cumulant κstu = τstu − τstτu − τsuτt − τtuτs + 2τsτtτu.
Given these deﬁnitions, we say that a probability distribution τc(xc) is attractive if the central mo-
ments associated with all subsets a ⊆ c of binary variables are non–negative (κa ≥ 0). Similarly, a
compatibility function ψc(xc) is attractive if the probability distribution attained by normalizing its
values has non–negative central moments. For example, the following potential is easily shown to
satisfy this condition for all degrees k = |c|, and any scalar θc > 0:

log ψc(x1, . . . , xk) = (cid:26) θc

−θc

x1 = x2 = · · · = xk
otherwise

(7)

2.2 Belief Propagation and the Bethe Variational Principle
Many applications of graphical models require estimates of the posterior marginal distributions of
individual variables τs(xs) or factors τc(xc). Loopy belief propagation (BP) approximates these
marginals via a series of messages passed among nodes of the graphical model [14, 23]. Let Γ(s)
denote the set of factors which depend on Xs, or equivalently the neighbors of node s in the corre-
sponding factor graph. The BP algorithm then iterates the following message updates:

¯msc(xs) ← ψs(xs) Yd∈Γ(s)\c

mds(xs)

mcs(xs) ← Xxc\s

ψc(xc) Yt∈c\s

¯mtc(xt)

(8)

The left–hand expression updates the message ¯msc(xs) passed from variable node s to factor c. New
outgoing messages mcs(xs) from factor c to each s ∈ c are then determined by marginalizing the
incoming messages from other nodes. At any iteration, appropriately normalized products of these
messages deﬁne estimates of the desired marginals:

τs(xs) ∝ ψs(xs) Yc∈Γ(s)

mcs(xs)

τc(xc) ∝ ψc(xc) Yt∈c

¯mtc(xt)

(9)

In tree–structured graphs, BP deﬁnes a dynamic programming recursion which converges to the
exact marginals after ﬁnitely many iterations [11, 14]. In graphs with cycles, however, convergence
is not guaranteed, and pseudo–marginals computed via eq. (9) are (often good) approximations.

A wide range of inference algorithms can be derived via variational approximations [9] to the true
partition function. Loopy BP is implicitly associated with the following Bethe approximation:

log Zβ(ψ; τ ) = Xs∈V Xxs
− Xs∈V Xxs

τs(xs) log ψs(xs) + Xc∈CXxc
τs(xs) log τs(xs) − Xc∈CXxc

τc(xc) log ψc(xc)

τc(xc) log

(10)

τc(xc)

Qt∈c τt(xt)

Fixed points of loopy BP correspond to stationary points of this Bethe approximation [23], subject

τc(xc) = τs(xs).

to the local marginalization constraints Pxc\s
3 Reparameterization and Loop Series Expansions
As discussed in Sec. 2.2, any BP ﬁxed point is in one–to–one correspondence with a set {τs, τc}
of pseudo–marginals associated with each of the graph’s nodes s ∈ V and factors c ∈ C. These
pseudo–marginals then lead to an alternative parameterization [20] of the factor graph of eq. (2):

p(x) =

1

Z(τ ) Ys∈V

τs(xs) Yc∈C

τc(xc)

Qt∈c τt(xt)

(11)

For pairwise MRFs, the reparameterized compatibility functions equal τst(xs, xt)/τs(xs)τt(xt).
The BP algorithm effectively searches for reparameterizations which are tree–consistent, so that

3

τc(xc) is the exact marginal distribution of Xc for any tree (or forest) embedded in the original
graph [20]. In later sections, we take expectations with respect to τc(xc) of functions f (xc) de-
ﬁned over individual factors. Although these pseudo–marginals will in general not equal the true
marginals pc(xc), BP ﬁxed points ensure local consistency so that Eτc [f (Xc)] is well–deﬁned.
Using eq. (10), it is easily shown that the Bethe approximation Zβ(τ ; τ ) = 1 for any joint distribu-
tion deﬁned by reparameterized potentials as in eq. (11). For simplicity, the remainder of this paper
focuses on reparameterized models of this form, and analyzes properties of the corresponding exact
partition function Z(τ ). The resulting expansions and bounds are then related to the original MRF’s
partition function via the positive constant Z(ψ)/Z(τ ) = Zβ(ψ; τ ) of eq. (10).
Recently, Chertkov and Chernyak proposed a ﬁnite loop series expansion [2] of the partition func-
tion, whose ﬁrst term coincides with the Bethe approximation. They provide two derivations: one
applies a trigonometric identity to Fourier representations of binary variables, while the second em-
ploys a saddle point approximation obtained via an auxiliary ﬁeld of complex variables. The gauge
transformations underlying these derivations are a type of reparameterization, but their form is com-
plicated by auxiliary variables and extraneous degrees of freedom. In this section, we show that the
ﬁxed point characterization of eq. (11) leads to a more direct, and arguably simpler, derivation.

3.1 Pairwise Loop Series Expansions
We begin by developing a loop series expansion for pairwise MRFs. Given an undirected graph
G = (V, E), and some subset F ⊆ E of the graph’s edges, let ds(F ) denote the degree (number of
neighbors) of node s in the subgraph induced by F . As illustrated in Fig. 1, any subset F for which
all nodes s ∈ V have degree ds(F ) 6= 1 deﬁnes a generalized loop [2]. The partition function for
any binary, pairwise MRF can then be expanded via an associated set of loop corrections.
Proposition 1. Consider a pairwise MRF deﬁned on an undirected G = (V, E), with reparameter-
ized potentials as in eq. (11). The associated partition function then equals

Z(τ ) = 1 + X∅6=F ⊆E

βF Ys∈V

Eτsh(Xs − τs)ds(F )i

βF := Y(s,t)∈F

βst

βst :=

τst − τsτt

τs(1 − τs)τt(1 − τt)

=

Covτst (Xs, Xt)

Varτs (Xs) Varτt (Xt)

where only generalized loops F lead to non–zero terms in the sum of eq. (12), and

are central moments of the binary variables at individual nodes.

Eτs(cid:2)(Xs − τs)d(cid:3) = τs(1 − τs)(cid:2)(1 − τs)d−1 + (−1)d (τs)d−1(cid:3)

(12)

(13)

(14)

Proof. To establish the expansion of eq. (12), we exploit the following polynomial representation of
reparameterized pairwise compatibility functions:

τst(xs, xt)
τs(xs)τt(xt)

= 1 + βst(xs − τs)(xt − τt)

(15)

As veriﬁed in [17], this expression is satisﬁed for any (xs, xt) ∈ {0, 1}2 if βst is deﬁned as in
eq. (13). For attractive models satisfying eq. (3), βst ≥ 0 for all edges. Using E˜τ [·] to denote

expectation with respect to the fully factorized distribution ˜τ (x) = Qs τs(xs), we then have

Z(τ ) = Xx∈{0,1}n Ys∈V
= E˜τ(cid:20) Y(s,t)∈E

τst(xs, xt)
τs(xs)τt(xt)

τs(xs) Y(s,t)∈E
τs(Xs)τt(Xt)(cid:21) = E˜τ(cid:20) Y(s,t)∈E

τst(Xs, Xt)

1 + βst(Xs − τs)(Xt − τt)(cid:21)

(16)

Expanding this polynomial via the expectation operator’s linearity, we recover one term for each
non–empty subset F ⊆ E of the graph’s edges:

Z(τ ) = 1 + X∅6=F ⊆E

E˜τ(cid:20) Y(s,t)∈F

βst(Xs − τs)(Xt − τt)(cid:21)

(17)

The expression in eq. (12) then follows from the independence structure of ˜τ (x), and standard
formulas for the moments of Bernoulli random variables. To evaluate these terms, note that if
ds(F ) = 1, it follows that Eτs [Xs − τs] = 0. There is thus one loop correction for each generalized
loop F , in which all connected nodes have degree at least two.

4

Figure 1: A pairwise MRF coupling ten binary variables (left), and the nine generalized loops in its loop series
expansion (right). For attractive potentials, two of the generalized loops may have negative signs (second &
third from right), while the core graph of Thm. 1 contains eight variables (far right).

Figure 1 illustrates the set of generalized loops associated with a particular pairwise MRF. These
loops effectively deﬁne corrections to the Bethe estimate Z(τ ) ≈ 1 of the partition function for
reparameterized models. Tree–structured graphs do not contain any non–trivial generalized loops,
and the Bethe variational approximation is thus exact.

The loop expansion formulas of [2] can be precisely recovered by transforming binary variables to
a spin representation, and refactoring terms from the denominator of edge weights βst to adjacent
vertices. Explicit computation of these loop corrections is in general intractable; for example, fully
connected graphs with n ≥ 5 nodes have more than 2n generalized loops. In some cases, accounting
for a small set of signiﬁcant loop corrections may lead to improved approximations to Z(ψ) [4], or
more accurate belief estimates for LDPC codes [1]. We instead use the series expansion of Prop. 1
to establish analytic properties of BP ﬁxed points.

3.2 Factor Graph Loop Series Expansions
We now extend the loop series expansion to higher–order MRFs deﬁned on hypergraphs G = (V, C).
Let E = {(s, c) | c ∈ C, s ∈ c} denote the set of edges in the factor graph representation of this
MRF. As illustrated in Fig. 2, we deﬁne a generalized loop to be a subset F ⊆ E of edges such that
all connected factor and variable nodes have degree at least two.
Proposition 2. Consider any factor graph G = (V, C) with reparameterized potentials as in
eq. (11), and associated edges E. The partition function then equals

Z(τ ) = 1 + X∅6=F ⊆E

βF Ys∈V

βa :=

κa

Qt∈a τt(1 − τt)

Eτsh(Xs − τs)ds(F )i
Eτc(cid:2)Qs∈a(Xs − τs)(cid:3)
Qt∈a Varτt (Xt)

=

βF :=Yc∈C

βac(F )

(18)

(19)

where ac(F ) := {s ∈ c | (s, c) ∈ F } denotes the subset of variables linked to factor node c by the
edges in F . Only generalized loops F lead to non–zero terms in the sum of eq. (18).
Proof. As before, we employ a polynomial representation of the reparameterized factors in eq. (11):

τc(xc)

Qt∈c τt(xt)

= 1 + Xa⊆c,|a|≥2

βa Ys∈a

(xs − τs)

(20)

For factor graphs with attractive reparameterized potentials, the constant βa ≥ 0 for all a ⊆ c.
Note that this representation, which is derived in [17], reduces to that of eq. (15) when c = {s, t}.
Single–variable subsets are excluded in eq. (20) because κs = Eτs [Xs − τs] = 0.
Applying eq. (20) as in our earlier derivation for pairwise MRFs (see eq. (16)), we may express the
partition function of the reparameterized factor graph as follows:

Z(τ ) = E˜τ(cid:20) Yc∈C

τc(Xc)

Qt∈c τt(Xt)(cid:21) = E˜τ(cid:20) Yc∈C

1 + X∅6=a⊆c

βa Ys∈a

(Xs − τs)(cid:21)

(21)

Note that βa = 0 for any subset where |a| = 1. There is then a one–to–one correspondence between
variable node subsets a ⊆ c, and subsets {(s, c) | s ∈ a} of the factor graph’s edges E. Expanding
this expression by F ⊆ E, it follows that each factor c ∈ C contributes a term corresponding to the
chosen subset ac(F ) of its edges:

Z(τ ) = 1 + X∅6=F ⊆E

E˜τ(cid:20) Yc∈C

βac(F ) Ys∈ac(F )

(Xs − τs)(cid:21)

(22)

Note that β∅ = 1. Equation (18) then follows from the independence properties of ˜τ (x). For a term
in this loop series to be non–zero, there must be no degree one variables, since Eτs[Xs − τs] = 0.
In addition, the deﬁnition of βa implies that there can be no degree one factor nodes.

5

Figure 2: A factor graph (left) with three binary variables (circles) and four factor nodes (squares), and the
thirteen generalized loops in its loop series expansion (right, along with the full graph).

4 Lower Bounds in Attractive Binary Models
The Bethe approximation underlying loopy BP differs from mean ﬁeld methods [9], which lower
bound the true log partition function Z(ψ), in two key ways. First, while the Bethe entropy (second
line of eq. (10)) is exact for tree–structured graphs, it approximates (rather than bounds) the true
entropy in graphs with cycles. Second, the marginalization condition imposed by loopy BP relaxes
(rather than strengthens) the global constraints characterizing valid distributions [21]. Neverthe-
less, we now show that for a large family of attractive graphical models, the Bethe approximation
Zβ(ψ; τ ) of eq. (10) lower bounds Z(ψ). In contrast with mean ﬁeld methods, these bounds hold
only at appropriate BP ﬁxed points, not for arbitrarily chosen pseudo–marginals τc(xc).

4.1 Partition Function Bounds for Pairwise Graphical Models
Consider a pairwise MRF deﬁned on G = (V, E), as in eq. (1). Let VH ⊆ V denote the set of
nodes which either belong to some cycle in G, or lie on a path (sequence of edges) connecting two
cycles. We then deﬁne the core graph H = (VH , EH ) as the node–induced subgraph obtained by
discarding edges from nodes outside VH, so that EH = {(s, t) ∈ E | s, t ∈ VH }. The unique core
graph H underlying any graph G can be efﬁciently constructed by iteratively pruning degree one
nodes, or leaves, until all remaining nodes have two or more neighbors. The following theorem
identiﬁes conditions under which all terms in the loop series expansion must be non–negative.
Theorem 1. Let H = (VH , EH ) be the core graph for a pairwise binary MRF, with attractive
potentials satisfying eq. (3). Consider any BP ﬁxed point for which all nodes s ∈ VH with three or
more neighbors in H have marginals τs ≤ 1
2 ). The corresponding Bethe
variational approximation Zβ(ψ; τ ) then lower bounds the true partition function Z(ψ).

2 (or equivalently, τs ≥ 1

Proof. It is sufﬁcient to show that Z(τ ) ≥ 1 for any reparameterized pairwise MRF, as in eq. (11).
From eq. (9), note that loopy BP estimates the pseudo–marginal τst(xs, xt) via the product of
ψst(xs, xt) with message functions of single variables. For this reason, attractive pairwise com-
patibilities always lead to BP ﬁxed points with attractive pseudo–marginals satisfying τst ≥ τsτt.
Consider the pairwise loop series expansion of eq. (12). As shown by eq. (13), attractive models

each generalized loop F ⊆ E. Suppose ﬁrst that the graph has a single cycle, and thus exactly one
non–zero generalized loop F . Because all connected nodes in this cycle have degree two, the bound

lead to edge weights βst ≥ 0. It is thus sufﬁcient to show that Qs
Eτs(cid:2)(Xs − τs)ds(F )(cid:3) ≥ 0 for
follows because Eτs(cid:2)(Xs − τs)2(cid:3) ≥ 0. More generally, we clearly have Z(τ ) ≥ 1 in graphs where

every generalized loop F associates an even number of neighbors ds(F ) with each node.
Focusing on generalized loops containing nodes with odd degree d ≥ 3, eq. (14) implies that

Eτs(cid:2)(Xs − τs)d(cid:3) ≥ 0 for marginals satisfying 1 − τs ≥ τs. For BP ﬁxed points in which τs ≤ 1
for all nodes, we thus have Z(τ ) ≥ 1. In particular, the symmetric ﬁxed point τs = 1
2 leads to uni-
formly positive generalized loop corrections. More generally, the marginals of nodes s for which
ds(F ) ≤ 2 for every generalized loop F do not inﬂuence the expansion’s positivity. Theorem 1
discards these nodes by examining the topology of the core graph H (see Fig. 1 for an example).
For ﬁxed points where τs ≥ 1
2 for all nodes, we rewrite the polynomial in the loop expansion of
eq. (15) as (1 + βst(τs − xs)(τt − xt)), and employ an analogous line of reasoning.
In addition to establishing Thm. 1, our arguments show that the true partition function monotonically
increases as additional edges, with attractive reparameterized potentials as in eq. (11), are added to
a graph with ﬁxed pseudo–marginals τs ≤ 1
2 . For such models, the accumulation of particular
loop corrections, as explored by [4], produces a sequence of increasingly tight bounds on Z(ψ). In
addition, we note that the conditions required by Thm. 1 are similar to those underlying classical

2

6

correlation inequalities [16] from the statistical physics literature.
Sherman (GKS) inequality leads to an alternative proof in cases where τs = 1

Indeed, the Grifﬁths–Kelly–

2 for all nodes.

For attractive Ising models in which some nodes have marginals τs > 1
2 , the loop
series expansion may contain negative terms. For small graphs like that in Fig. 1, it is possible to
use upper bounds on the edge weights βst, which follow from τst ≤ min(τs, τt), to cancel negative
loop corrections with larger positive terms. As conﬁrmed by the empirical results in Sec. 4.3, the
lower bound Z(ψ) ≥ Zβ(ψ; τ ) thus continues to hold for many (perhaps all) attractive Ising models
with less homogeneous marginal biases.

2 and others τt < 1

4.2 Partition Function Bounds for Factor Graphs
Given a factor graph G = (V, C) relating binary variables, deﬁne a core graph H = (VH , CH ) by
excluding variable and factor nodes which are not members of any generalized loops. As in Sec. 2.2,
let Γ(s) denote the set of factor nodes neighboring variable node s in the core graph H.
Theorem 2. Let H = (VH , CH ) be the core graph for a binary factor graph, and consider an
attractive BP ﬁxed point for which one of the following conditions holds:

(i) τs ≤ 1
(ii) τs ≥ 1

2 for all nodes s ∈ VH with |Γ(s)| ≥ 3, and κa ≥ 0 for all a ⊆ c, c ∈ CH.
2 for all nodes s ∈ VH with |Γ(s)| ≥ 3, and (−1)|a|κa ≥ 0 for all a ⊆ c, c ∈ CH.

The Bethe approximation Zβ(ψ; τ ) then lower bounds the true partition function Z(ψ).

For the case where τs ≤ 1
arguments in Sec. 4.1. When τs ≥ 1
of eq. (20), and again recover uniformly positive loop corrections.

2 , the proof of this theorem is a straightforward generalization of the
2 , we replace all (xs − τs) terms by (τs − xs) in the expansion

For any given BP ﬁxed point, the conditions of Thm. 2 are easy to verify. For factor graphs, it is
more challenging to determine which compatibility functions ψc(xc) necessarily lead to attractive
ﬁxed points. For symmetric potentials as in eq. (7), however, one can show that the conditions on
κa, a ⊆ c are necessarily satisﬁed whenever all variable nodes s ∈ VH have the same bias.

4.3 Empirical Comparison of Mean Field and Bethe Lower Bounds
In this section, we compare the accuracy of the Bethe variational bounds established by Thm. 1
to those produced by a naive, fully factored mean ﬁeld approximation [3, 9]. Using the
spin representation zs ∈ {−1, +1}, we examine Ising models with attractive pairwise potentials
log ψst(zs, zt) = θstzszt of varying strengths θst ≥ 0. We ﬁrst examine a 2D torus, with potentials
of uniform strength θst = ¯θ and no local observations. For such MRFs, the exact partition func-
tion may be computed via Onsager’s classical eigenvector method [13]. As shown in Fig. 3(a), for
moderate ¯θ the Bethe bound Zβ(ψ; τ ) is substantially tighter than mean ﬁeld. For large ¯θ, only two
states (all spins “up” or “down”) have signiﬁcant probability, so that Z(ψ) ≈ 2 exp(¯θ|E|). In this
regime, loopy BP exhibits “symmetry breaking” [6], and converges to one of these states at random
with corresponding bound Zβ(ψ; τ ) ≈ exp(¯θ|E|). As veriﬁed in Fig. 3(a), as ¯θ → ∞ the difference
log Z(ψ) − log Zβ(ψ; τ ) ≈ log 2 ≈ 0.69 thus remains bounded.
We also consider a set of random 10 × 10 nearest–neighbor grids, with inhomogeneous pairwise

potentials sampled according to |θst| ∼ N(cid:0)0, ¯θ 2(cid:1), and observation potentials log ψs(zs) = θszs,
|θs| ∼ N(cid:0)0, 0.12(cid:1). For each candidate ¯θ, we sample 100 random MRFs, and plot the average differ-

ence log Zβ(ψ; τ ) − log Z(ψ) between the true partition function and the BP (or mean ﬁeld) ﬁxed
point reached from a random initialization. Fig. 3(b) ﬁrst considers MRFs where θs > 0 for all
nodes, so that the conditions of Thm. 1 are satisﬁed for all BP ﬁxed points. For these models, the
Bethe bound is extremely accurate.
In Fig. 3(c), we also consider MRFs where the observation
potentials θs are of mixed signs. Although this sometimes leads to BP ﬁxed points with negative
associated loop corrections, the Bethe variational approximation nevertheless always lower bounds
the true partition function in these examples. We hypothesize that this bound in fact holds for all
attractive, binary pairwise MRFs, regardless of the observation potentials.
5 Discussion
We have provided an alternative, direct derivation of the partition function’s loop series expansion,
based on the reparameterization characterization of BP ﬁxed points. We use this expansion to prove
that the Bethe approximation lower bounds the true partition function in a family of binary attractive

7

10

0

−10

−20

−30

−40

−50

n
o

i
t
i
t
r
a
P
g
o
L

 

 

 

e
u
r
T
m
o
r
f
 

e
c
n
e
r
e

f
f
i

D

−60

 

−70
0

0.2

Belief Propagation
Mean Field
0.8

0.4
0.6
Edge Strength
(a)

 

0.5

0

−0.5

−1

−1.5

−2

−2.5

n
o

i
t
i
t
r
a
P
g
o
L

 

 

 

e
u
r
T
m
o
r
f
 

e
c
n
e
r
e

f
f
i

D

1

 

−3
0

0.2

 

n
o

 

 

 

i
t
i
t
r
a
P
g
o
L
e
u
r
T
m
o
r
f
 

e
c
n
e
r
e

2

0

−2

−4

−6

Belief Propagation
Mean Field
0.8

0.4
0.6
Edge Strength
(b)

f
f
i

D

1

 

−8
0

0.2

Belief Propagation
Mean Field
0.8

0.4
0.6
Edge Strength
(c)

 

1

Figure 3: Bethe (dark blue, top) and naive mean ﬁeld (light green, bottom) lower bounds on log Z(ψ) for three
families of attractive, pairwise Ising models. (a) 30 × 30 torus with no local observations and homogeneous
potentials. (b) 10 × 10 grid with random, inhomogeneous potentials and all pseudo–marginals τs > 1
2 , satisfy-
ing the conditions of Thm. 1. (c) 10 × 10 grid with random, inhomogeneous potentials and pseudo–marginals
of mixed biases. Empirically, the Bethe lower bound also holds for these models.

models. These results have potential implications for the suitability of loopy BP in approximate
parameter estimation [3], as well as its convergence dynamics. We are currently exploring general-
izations of our results to other families of attractive, or “nearly” attractive, graphical models.
Acknowledgments The authors thank Yair Weiss for suggesting connections to loop series expansions,
and helpful conversations. Funding provided by Army Research Ofﬁce Grant W911NF-05-1-0207, National
Science Foundation Grant DMS-0528488, and NSF Career Grant CCF-0545862.
References
[1] M. Chertkov and V. Y. Chernyak. Loop calculus helps to improve belief propagation and linear program-

ming decodings of low density parity check codes. In Allerton Conf., 2006.

[2] M. Chertkov and V. Y. Chernyak. Loop series for discrete statistical models on graphs. J. Stat. Mech.,

2006:P06009, June 2006.

[3] B. J. Frey and N. Jojic. A comparison of algorithms for inference and learning in probabilistic graphical

models. IEEE Trans. PAMI, 27(9):1392–1416, Sept. 2005.

[4] V. G´omez, J. M. Mooij, and H. J. Kappen. Truncating the loop series expansion for BP. JMLR, 8:1987–

2016, 2007.

2004.

[5] D. M. Greig, B. T. Porteous, and A. H. Seheult. Exact maximum a posteriori estimation for binary images.

J. R. Stat. Soc. B, 51(2):271–279, 1989.

[6] T. Heskes. On the uniqueness of loopy belief propagation ﬁxed points. Neural Comp., 16:2379–2413,

[7] A. T. Ihler, J. W. Fisher, and A. S. Willsky. Loopy belief propagation: Convergence and effects of message

[8] M. Jerrum and A. Sinclair. Polynomial-time approximation algorithms for the Ising model. SIAM J.

errors. JMLR, 6:905–936, 2005.

Comput., 22(5):1087–1116, Oct. 1993.

[9] M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K. Saul. An introduction to variational methods for

graphical models. Machine Learning, 37:183–233, 1999.

[10] V. Kolmogorov and R. Zabih. What energy functions can be minimized via graph cuts? IEEE Trans.

PAMI, 26(2):147–159, Feb. 2004.

Trans. IT, 47(2):498–519, Feb. 2001.

21, pages 396–403. AUAI Press, 2005.

Review, 65:117–149, 1944.

[11] F. R. Kschischang, B. J. Frey, and H.-A. Loeliger. Factor graphs and the sum–product algorithm. IEEE

[12] J. M. Mooij and H. J. Kappen. Sufﬁcient conditions for convergence of loopy belief propagation. In UAI

[13] L. Onsager. Crystal statistics I: A two–dimensional model with an order–disorder transition. Physical

[14] J. Pearl. Probabilistic Reasoning in Intelligent Systems. Morgan Kaufman, San Mateo, 1988.
[15] T. J. Richardson and R. L. Urbanke. The capacity of low-density parity-check codes under message-

passing decoding. IEEE Trans. IT, 47(2):599–618, Feb. 2001.

[16] S. B. Shlosman. Correlation inequalities and their applications. J. Math. Sci., 15(2):79–101, Jan. 1981.
[17] E. B. Sudderth, M. J. Wainwright, and A. S. Willsky. Loop series and Bethe variational bounds in attractive

graphical models. UC Berkeley, EECS department technical report, in preparation, 2008.

[18] M. F. Tappen and W. T. Freeman. Comparison of graph cuts with belief propagation for stereo, using

identical MRF parameters. In ICCV, volume 2, pages 900–907, 2003.

[19] S. C. Tatikonda and M. I. Jordan. Loopy belief propagation and Gibbs measures.

In UAI 18, pages

493–500. Morgan Kaufmann, 2002.

[20] M. J. Wainwright, T. S. Jaakkola, and A. S. Willsky. Tree–based reparameterization framework for anal-

ysis of sum–product and related algorithms. IEEE Trans. IT, 49(5):1120–1146, May 2003.

[21] M. J. Wainwright, T. S. Jaakkola, and A. S. Willsky. A new class of upper bounds on the log partition

function. IEEE Trans. IT, 51(7):2313–2335, July 2005.

[22] Y. Weiss. Comparing the mean ﬁeld method and belief propagation for approximate inference in MRFs.

In D. Saad and M. Opper, editors, Advanced Mean Field Methods. MIT Press, 2001.

[23] J. S. Yedidia, W. T. Freeman, and Y. Weiss. Constructing free energy approximations and generalized

belief propagation algorithms. IEEE Trans. IT, 51(7):2282–2312, July 2005.

8

"
1021,2007,Efficient Principled Learning of Thin Junction Trees,"We present the first truly polynomial algorithm for learning the structure of bounded-treewidth junction trees -- an attractive subclass of probabilistic graphical models that permits both the compact representation of probability distributions and efficient exact inference. For a constant treewidth, our algorithm has polynomial time and sample complexity, and provides strong theoretical guarantees in terms of $KL$ divergence from the true distribution. We also present a lazy extension of our approach that leads to very significant speed ups in practice, and demonstrate the viability of our method empirically, on several real world datasets. One of our key new theoretical insights is a method for bounding the conditional mutual information of arbitrarily large sets of random variables with only a polynomial number of mutual information computations on fixed-size subsets of variables, when the underlying distribution can be approximated by a bounded treewidth junction tree.","Efﬁcient Principled Learning of Thin Junction Trees

Anton Chechetka Carlos Guestrin

Carnegie Mellon University

Abstract

We present the ﬁrst truly polynomial algorithm for PAC-learning the structure of
bounded-treewidth junction trees – an attractive subclass of probabilistic graphical
models that permits both the compact representation of probability distributions
and efﬁcient exact inference. For a constant treewidth, our algorithm has polyno-
mial time and sample complexity. If a junction tree with sufﬁciently strong intra-
clique dependencies exists, we provide strong theoretical guarantees in terms of
KL divergence of the result from the true distribution. We also present a lazy
extension of our approach that leads to very signiﬁcant speed ups in practice, and
demonstrate the viability of our method empirically, on several real world datasets.
One of our key new theoretical insights is a method for bounding the conditional
mutual information of arbitrarily large sets of variables with only polynomially
many mutual information computations on ﬁxed-size subsets of variables, if the
underlying distribution can be approximated by a bounded-treewidth junction tree.

1 Introduction
In many applications, e.g., medical diagnosis or datacenter performance monitoring, probabilistic
inference plays an important role: to decide on a patient’s treatment, it is useful to know the prob-
ability of various illnesses given the known symptoms. Thus, it is important to be able to represent
probability distributions compactly and perform inference efﬁciently. Here, probabilistic graphical
models (PGMs) have been successful as compact representations for probability distributions.

In order to use a PGM, one needs to deﬁne its structure and parameter values. Usually, we only
have data (i.e., samples from a probability distribution), and learning the structure from data is thus
a crucial task. For most formulations, the structure learning problem is NP-complete, c.f., [10].
Most structure learning algorithms only guarantee that their output is a local optimum. One of the
few notable exceptions is the work of Abbeel et al. [1], for learning structure of factor graphs, that
provides probably approximately correct (PAC) learnability guarantees.

While PGMs can represent probability distributions compactly, exact inference in compact models,
such as those of Abbeel et al., remains intractable [7]. An attractive solution is to use junction
trees (JTs) of limited treewidth – a subclass of PGMs that permits efﬁcient exact inference. For
treewidth k = 1 (trees), the most likely (MLE) structure of a junction tree can be learned efﬁciently
using the Chow-Liu algorithm [6], but the representational power of trees is often insufﬁcient. We
address the problem of learning JTs for ﬁxed treewidth k > 1. Learning the most likely such JT is
NP-complete [10]. While there are algorithms with global guarantees for learning ﬁxed-treewidth
JTs [10, 13], there has been no polynomial algorithm with PAC guarantees. The guarantee of [10]
is in terms of the difference in log-likelihood of the MLE JT and the model where all variables are
independent: the result is guaranteed to achieve at least a constant fraction of that difference. The
constant does not improve as the amount of data increases, so it does not imply PAC learnability.
The algorithm of [13] has PAC guarantees, but its complexity is exponential. In contrast, we provide
a truly polynomial algorithm with PAC guarantees. The contributions of this paper are as follows:

• A theoretical result (Lemma 4) that upper bounds the conditional mutual information of
In particular, we do not

arbitrarily large sets of random variables in polynomial time.
assume that an efﬁciently computable mutual information oracle exists.

• The ﬁrst polynomial algorithm for PAC-learning the structure of limited-treewidth junction
trees with strong intra-clique dependencies. We provide graceful degradation guarantees
for distributions that are only approximately representable by JTs with ﬁxed treewidth.

1

x4,x5
1

x2,x5
2

x4,x5,x6

x2,x3,x5

4

5

x1,x4,x5

x1,x5

x1,x2,x5

x1,x2

x1,x2,x7

3

Figure 1: A junction tree. Rectangles denote
cliques, separators are marked on the edges.

Algorithm 1: Na¨ıve approach to structure learning

Input: V , oracle I (·, · | ·), treewidth k, threshold δ
L ← ∅ ; // L is a set of “useful components”
for S ⊂ V s.t. |S| = k do

for Q ⊂ V-S do

if I (Q, V-SQ | S) ≤ δ then

L ← L ∪ (S, Q)

return FindConsistentTree(L)

1
2
3
4
5

6

• A lazy heuristics that allows to make the algorithm practical.
• Empirical evidence of the viability of our approach on real-world datasets.

2 Bounded treewidth graphical models
In general, even to represent a probability distribution P (V ) over discrete variables1 V we need
space exponential in the size n of V . However, junction trees of limited treewidth allow compact
representation and tractable exact inference. We brieﬂy review junction trees (for details see [7]).
Let C = {C1, . . . , Cm} be a collection of subsets of V . Elements of C are called cliques. Let T be
a set of edges connecting pairs of cliques such that (T, C) is a tree.
Deﬁnition 1. Tree (T, C) is a junction tree iff it satisﬁes the running intersection property (RIP):
∀Ci, Cj ∈ C and ∀Ck on the (unique) simple path between Ci and Cj, x ∈ Ci ∩ Cj ⇒ x ∈ Ck.

A set Sij ≡ Ci ∩ Cj is called the separator corresponding to an edge (i−j) from T . The size of
a largest clique in a junction tree minus one is called the treewidth of that tree. For example, in a
junction tree in Fig. 1, variable x2 is contained in both clique 3 and 5, so it has to be contained in
clique 2, because 2 is on the simple path between 3 and 5. The largest clique in Fig. 1 has size 3, so
the treewidth of that junction tree is 2.

A distribution P (V ) is representable using junction tree (T, C) if instantiating all variables in a sep-
arator Sij renders the variables on different sides of Sij independent. Denote the fact that A is inde-
ij be cliques that can be reached from Ci in the (T, C)
pendent of B given C by (A ⊥ B | C). Let Ci
without using edge (i−j), and denote these reachable variables by V i
Ck \ Sij.
For example, in Fig. 1, S12 = {x1, x5}, V 1
Deﬁnition 2. P (V ) factors according to junction tree (T, C) iff ∀(i − j) ∈ T ,(cid:16)V i

ij ≡ V i
12 = {x2, x3, x7}.

12 = {x4, x6} , V 2

ij | Sij(cid:17).

ji ≡ SCk∈Ci

ij

ij ⊥ V j

If a distribution P (V ) factors according to some junction tree of treewidth k, we will say that P (V )
is k-JT representable. In this case, a projection P(T,C) of P on (T, C), deﬁned as

P(T,C) = QCi∈C P (Ci)
Q(i−j)∈T P (Sij)

,

(1)

is equal to P itself. For clarity, we will only consider maximal junction trees, where all separators
have size k. If P is k-JT representable, it also factors according to some maximal JT of treewidth k.

In practice the notion of conditional independence is too strong. Instead, a natural relaxation is to
require sets of variables to have low conditional mutual information I. Denote H(A) the entropy
of A, then I(A,B | S) ≡ H(A | S)−H(A | BS) is nonnegative, and zero iff (A ⊥ B | S). Intuitively,
I (A, B | S) shows how much new information about A can we extract from B if we already know S.
Deﬁnition 3. (T, C) is an ε-junction tree for P (V ) iff ∀(i − j) ∈ T : I (cid:16)V i

ij | Sij(cid:17) ≤ ε.

ij, V j

1Notation note: throughout the paper, we use small letters (x, y) to denote variables, capital letters (V, C)

to denote sets of variables, and double-barred font (C, D) to denote sets of sets.

2

If there exists an ε-junction tree (T, C) for P (V ), we will say that P is k-JT ε-representable. In
this case, the Kullback-Leibler divergence of projection (1) of P on (T, C) from P is bounded [13]:

KL(cid:0)P, P(T,C)(cid:1) ≤ nε.

(2)

This bound means that if we have an ε-junction tree for P (V ), then instead of P we can use its
tractable principled approximation P(T,C) for inference. In this paper, we address the problem of
learning structure of such junction tree from data (samples from P ).
3 Structure learning
In this paper, we address the following problem: given data, such as multiple temperature read-
ings from sensors in a sensor network, we treat each datapoint as an instantiation of the random
variables V and seek to ﬁnd a good approximation of P (V ). We will assume that P (V ) is k-JT
ε-representable for some ε and aim to ﬁnd a ˆε-junction tree for P with the same treewidth k and
with ˆε as small as possible. Note that the maximal treewidth k is considered to be a constant and not
a part of problem input. The complexity of our approach is exponential in k.

1
2
3

4

5

return QS

Input: V , separator S, oracle I (·, · | ·),

threshold δ, max set size q

// QS is a set of singletons

QS ← ∪x∈V {x} ;
for A ⊂ V-S s.t. |A| ≤ q do

Algorithm 2: LTCI: ﬁnd Conditional Indepen-
dencies in Low-Treewidth distributions

if minX⊂A I (X, A-X | S) > δ then
// ﬁnd min with Queyranne’s alg.
merge all Qi ∈ QS, s.t. Qi ∩ A 6= ∅

Let us initially assume that we have an ora-
cle I (·, · | ·) that can compute the mutual in-
formation I (A, B | C) exactly for any disjoint
subsets A, B, C ⊂ V . This is a very strict re-
quirement, which we address in the next sec-
tion. Using the oracle I, a na¨ıve approach
would be to evaluate2 I(Q, V-QS | S) for all
possible Q, S ⊂ V s.t. |S| = k and record all
pairs (S, Q) with I(Q, V-QS | S) ≤ ε into a
list L. We will say that a junction tree (T, C)
is consistent with a list L iff for every separa-
ij) ∈ L.
tor Sij of (T, C) it holds that (Sij, V i
After L is formed, any junction tree consistent with L would be an ε-junction tree for P (V ). Such
tree would be found by some FindConsistentTree procedure, implemented, e.g., using constraint
satisfaction. Alg. 1 summarizes this idea. Algorithms that follow this outline, including ours, form a
class of constraint-based approaches. These algorithms use mutual information tests to constrain the
set of possible structures and return one that is consistent with the constraints. Unfortunately, using
Alg. 1 directly is impractical because its complexity is exponential in the total number of variables
n. In the following sections we discuss inefﬁciencies of Alg. 1 and present efﬁcient solutions.
3.1 Global independence assertions from local tests
One can see two problems with the inner loop of Alg. 1 (lines 3-5). First, for each separator we
need to call the oracle exponentially many times (2n−k−1, once for every Q ⊂ V-S). This drawback
is addressed in the next section. Second, the mutual information oracle, I (A, B | S), is called on
subsets A and B of size O(n). Unfortunately, the best known way of computing mutual information
(and estimating I from data) has time and sample complexity exponential in |A|+|B|+|S|. Previous
work has not addressed this problem. In particular, the approach of [13] has exponential complexity,
in general, because it needs to estimate I for subsets of size O(n). Our ﬁrst new result states that we
can limit ourselves to computing mutual information over small subsets of variables:
Lemma 4. Let P (V ) be a k-JT ε-representable distribution. Let S ⊂ V , A ⊂ V-S. If ∀X ⊆ V-S
s.t. |X| ≤ k + 1, it holds that I(A ∩ X, V-SA ∩ X | S) ≤ δ, then I(A, V-SA | S) ≤ n(ε + δ).
We can thus compute an upper bound on I(A, V-SA | S) using O (cid:0)(cid:0)n
k(cid:1)(cid:1) ≡ O(nk) (i.e., polynomially
many) calls to the oracle I (·, · | ·), and each call will involve at most |S| + k + 1 variables. Lemma 4
also bounds the quality of approximation of P by a projection on any junction tree (T, C):
Corollary 5. If conditions of Lemma 4 hold for P (V ) with S = Sij and A = V i
Sij of a junction tree (T, C), then (T, C) is a n(ε + δ)-junction tree for P (V ).

ij for every separator

3.2 Partitioning algorithm for weak conditional independencies
Now that we have an efﬁcient upper bound for I (·, · | ·) oracle, let us turn to reducing the number of
oracle calls by Alg. 1 from exponential (2n−k−1) to polynomial.
In [13], Narasimhan and Bilmes

2Notation note: for any sets A, B, C we will denote A \ (B ∪ C) as A-BC to lighten the notation.

3

Algorithm 3: Efﬁcient approach to struc-
ture learning

Input: V , oracle I (·, · | ·), treewidth k,

1
2
3

4

threshold ε, L = ∅
for S ⊂ V s.t. |S| = k do

for Q ∈ LTCI(V ,S,I,ε,k + 2) do

L ← L ∪ (S, Q)

return FindConsistentTreeDPGreedy(L)

Algorithm 4: FindConsistentTreeDPGreedy

Input: List L of components (S, Q)
for (S, Q) ∈ L in the order of increasing |Q| do
greedily check if (S, Q) is L-decomposable
record the decomposition if it exists
if ∃S : (S, V-S) is L-decomposable then

return corresponding junction tree

else return no tree found

1
2
3

4
5

6

present an approximate solution to this problem, assuming that an efﬁcient approximation of oracle
I (·, · | ·) exists. A key observation that they relied on is that the function FS(A) ≡ I (A, V-SA | S)
is submodular: FS(A)+FS(B) ≥ FS(A∪B)+FS(A∩B). Queyranne’s algorithm [14] allows the
minimization of a submodular function F using O(n3) evaluations of F . [13] combines Queyranne’s
algorithm with divide-and-conquer approach to partition V-S into conditionally independent subsets
using O(n3) evaluations of I (·, · | ·). However, since I (·, · | ·) is computed for sets of size O(n),
complexity of their approach is still exponential in n, in general.

Our approach, called LTCI (Alg. 2), in contrast, has polynomial complexity for q = O(1). We
will show that q = O(1) in our approach that uses LTCI as a subroutine. To gain intuition for LTCI,
suppose there exists a ε-junction tree for P (V ), such that S is a separator and subsets B and C are on
different sides of S in the junction tree. By deﬁnition, this means I (B, C | S) ≤ ε. When we look
at subset A ≡ B ∪ C, the true partitioning is not known, but setting δ = ε, we can test all possible
2|A|−1 ways to partition A into two subsets (X and A-X). If none of the possible partitionings have
I (X, A-X | S) ≤ ε, we can conclude that all variables in A are on the same side of separator S in
any ε-junction tree that includes S as a separator. Notice also that

∀X ⊂ A I (X, A-X | S) > δ ⇔ min
X⊂A

I (X, A-X | S) > δ,

so we can use Queyranne’s algorithm to evaluate I (·, · | ·) only O(|A|3) times instead of 2|A|−1
times for minimization by exhaustive search. LTCI initially assumes that every variable x forms
its own partition Q = {x}. If a test shows that two variables x and y are on the same side of the
separator, it follows that their container partitions Q1 ∋ x, Q2 ∋ y cannot be separated by S, so
LTCI merges Q1 and Q2 (line 3 of Alg. 2). This process is then repeated for larger sets of variables,
of size up to q, until we converge to a set of partitions that are “almost independent” given S.
Proposition 6. The time complexity of LTCI with |S| = k is O (cid:16)(cid:0)n
where J M I

k+q is the time complexity of computing I (A, B | C) for |A| + |B| + |C| = k + q.

q(cid:1)nJ M I

k+q(cid:17) ≡ O (cid:16)nq+1J M I

k+q(cid:17) ,

ij or Q ⊆ V j

It is important that the partitioning algorithm returns partitions that are similar to connected com-
ponents of V i
ij of the true junction tree for P (V ). Formally, let us deﬁne two desirable properties.
Suppose (T, C) is an ε-junction tree for P (V ), and QSij is an output of the algorithm for separator
Sij and threshold δ. We will say that partitioning algorithm is correct iff for δ = ε, ∀Q ∈ QSij
either Q ⊆ V i
ij. A correct algorithm will never mistakenly put two variables on the same
side of a separator. We will say that an algorithm is α-weak iff ∀Q ∈ QSij I (cid:0)Q, V-QSij | Sij(cid:1) ≤ α.
For small α, an α-weak algorithm puts variables on different sides of a separator only if correspond-
ing mutual information between those variables is not too large. Ideally, we want a correct and
δ-weak algorithm; for δ = ε it would separate variables that are on different sides of S in a true
junction tree, but not introduce any spurious independencies. LTCI, which we use instead of lines
3-5 in Alg. 1, satisﬁes the ﬁrst requirement and a relaxed version of the second:
Lemma 7. LTCI, for q ≥ k + 1, is correct and n(ε + (k − 1)δ)-weak.

Implementing FindConsistentTree using dynamic programming

3.3
A concrete form of FindConsistentTree procedure is the last step needed to make Alg. 1 practical.
For FindConsistentTree, we adopt a dynamic programming approach from [2] that was also used in
[13] for the same purpose. We brieﬂy review the intuition; see [2] for details.
Consider a junction tree (T, C). Let Sij be a separator in (T, C) and Ci
reachable from Ci without using edge (i − j). Denote T i

ij be the set of cliques
ij the set of edges from T that connect

4

ij, T i

ij ∪ Sij). Moreover, the subtree (Ci

ij. If (T, C) is an ε-junction tree for P (V ), then (Ci

ij) is an ε-junction tree for
cliques from Ci
ij) consists of a clique Ci and several sub-subtrees that
P (V i
are each connected to Ci. For example, in Fig. 1 the subtree over cliques 1,2,4,5 can be decom-
posed into clique 2 and two sub-subtrees: one including cliques {1,4} and one with clique 5. The
recursive structure suggests dynamic programming approach: given a component (S, Q) such that
I (Q, V-QS | S) < δ, check if smaller subtrees can be put together to cover the variables of (S, Q).
Formally, we require the following property:
Deﬁnition 8. (S, Q) ∈ L is L-decomposable iff ∃D = ∪i{(Si, Qi)}, x ∈ Q s.t.

ij, T i

1. ∀i(Si, Qi) is L-decomposable and ∪m
2. Si ⊂ S ∪ {x}, i.e., each subcomponent can be connected directly to the clique (S, x);
3. Qi ∩ Qj = ∅, ensuring the running intersection property within the subtree over S ∪ Q.

i=1Qi = Q \ {x};

The set {(S1, Q1), . . . , (Sm, Qm)} is called a decomposition of (S, Q).

Unfortunately, checking whether a decomposition exists is equivalent to an NP-complete exact set
cover problem because of the requirement Qi ∩ Qj = ∅ in part 3 of Def. 8. Unfortunately, this chal-
lenging issue was not addressed by [13], where the same algorithm was used. To keep complexity
polynomial, we use a simple greedy approach: for every x ∈ Qi, starting with an empty candidate
decomposition D, add (Si, Qi) ∈ L to D if the last two properties of Def. 8 hold for (Si, Qi). If
eventually Def. 8 holds, return the decomposition D, otherwise return that no decomposition exists.
We call the resulting procedure FindConsistentTreeDPGreedy.
Proposition 9. For separator size k, time complexity of FindConsistentTreeDPGreedy is O(nk+2)

2k+2).

Combining Alg. 2 and FindConsistentTreeDPGreedy, we arrive at Alg. 3. Overall complexity of
Alg. 3 is dominated by Alg. 2 and is equal to O(n2k+3J M I
In general, FindConsistentTreeDP with greedy decomposition checks may miss a junction tree that
is consistent with the list of components L, but there is a class of distributions for which Alg. 3 is
guaranteed to ﬁnd a junction tree. Intuitively, we require that for every (Sij, V i
ij) from a ε-junction
tree (T, C), Alg. 2 adds all the components from decomposition of (Sij, V i
ij) to L and nothing else.
This requirement is guaranteed for distributions where variables inside every clique of the junction
tree are sufﬁciently strongly interdependent (have a certain level of mutual information):
Lemma 10. If ∃ an ε-JT (T, C) for P (V ) s.t. no two edges of T have the same separator, and
for every separator S, clique C ∈ C, minX⊂C-S I (X, C-XS | S) > (k + 3)ε (we will call (T, C)
(k + 3)ε-strongly connected), then Alg. 3, called with δ = ε, will output a nkε-JT for P (V ).

4 Sample complexity
So far we have assumed that a mutual information oracle I (·, · | ·) exists for the distribution P (V )
and can be efﬁciently queried. In real life, however, one only has data (i.e., samples from P (V ))
to work with. However, we can get a probabilistic estimate of I (A, B | C), that has accuracy ±∆
with probability 1 − γ, using number of samples and computation time polynomial in 1
∆ and log 1
γ :
Theorem 11. (H¨offgen, [9]). The entropy of a probability distribution over 2k + 2 discrete vari-
ables with domain size R can be estimated with accuracy ∆ with probability at least (1 − γ) using
F (k, R, ∆, γ) ≡ O(cid:16) R4k+4
γ (cid:17)(cid:17) samples from P and the same amount of time.

∆2 (cid:17) log (cid:16) R2k+2

log2 (cid:16) R2k+2

∆2

If we employ this oracle in our algorithms, the performance guarantee becomes probabilistic:
Theorem 12. If there exists a (k + 3)(ε + 2∆)-strongly connected ε-junction tree for P (V ), then
Alg. 3, called with δ = ε + ∆ and ˆI (·, ·, ·) based on Thm. 11, using U ≡ F (k, R, ∆,
n2k+2 ) samples
and O(n2k+3U ) time, will ﬁnd a kn(ε+2∆)-junction tree for P (V ) with probability at least (1−γ).

γ

Finally, if P (V ) is k-JT representable (i.e., ε = 0), and the corresponding junction tree is strongly
connected, then we can let both ∆ and γ go to zero and use Alg. 3 to ﬁnd, with probability arbitrarily
∆ and log 1
close to one, a junction tree that approximates P arbitrarily well in time polynomial in 1
γ ,
i.e., the class of strongly connected k-junction trees is probably approximately correctly learnable3.

3A class P of distributions is PAC learnable if for any P ∈ P, δ > 0, γ > 0 a learning algorithm will output

P ′ : KL(P, P ′) < δ with probability 1 − γ in time polynomial in 1

δ and log 1
γ .

5

Corollary 13. If there exists an α-strongly connected junction tree for P (V ) with α > 0, then
for β < αn, Alg. 3 will learn a β-junction tree for P with probability at least 1 − γ using
O (cid:16) n4

γ(cid:17) samples from P (V ) and O (cid:16) n2k+7

γ(cid:17) computation time.

β2 log2 n

β log n

log2 n

β log n

β2

5 Lazy evaluation of mutual information
Alg. 3 requires the value of threshold δ as an input. To get tighter quality guarantees, we need to
choose the smallest δ for which Alg. 3 ﬁnds a junction tree. A priori, this value is not known, so we
need a procedure to choose the optimal δ. A natural way to select δ is binary search. For discrete
random variables with domain size R, for any P (V ), S, x it holds that I (x, V-Sx | S) ≤ logR, so for
any δ > logR Alg. 3 is guaranteed to ﬁnd a junction tree (with all cliques connected to the same
separator). Thus, we can restrict binary search to range δ ∈ [0, log R].
In binary search, for every value of δ, Alg. 2 checks the result of Queyranne’s algorithm minimizing
minX⊂A I (X, A-X | S) for every |S| = k, |A| ≤ k+2, which amounts to O(n2k+2) complexity per
value of δ. It is possible, however, to ﬁnd the optimal δ while only checking minX⊂A I (X, A-X | S)
for every S and A once over the course of the search process.
Intuitively, think of the set of
partitions QS in Alg. 2 as a set of connected components of a graph with variables as vertices,
and a hyper-edge connecting all variables from A whenever minX⊂A I (X, A-X | S) > δ. As δ
increases, some of the hyper-edges disappear, and the number of connected components (or in-
dependent sets) may increase. More speciﬁcally, a graph QS is maintained for each separator
S. For all S, A add a hyper-edge connecting all variables in A annotated with strengthS(A) ≡
minX⊂A I (X, A-X | S) to QS. Until F indConsistentT ree(∪S QS) returns a tree, increase δ to
be minS,A:hyperedgeS (A)∈QS strengthS(A) (i.e., strength of the weakest remaining hyper-edge), and
remove hyperedgeS(A) from QS. Fig. 2(a) shows an example evolution of Qx4 for k = 1.
To further save computation time, we exploit two observations: First, if A is a subset of a connected
component Q ∈ QS, adding hyperedgeS(A) to QS will not change QS. Thus, we do not test any
hyper-edge A which is contained in a connected component. However, as δ increases, a component
may become disconnected, because such an edge was not added. Therefore, we may have more
components than we should (inducing incorrect independencies). This issue is addressed by our
second insight: If we ﬁnd a junction tree for a particular value of δ, we only need to recheck the
components used in this tree. These insights lead to a simple, lazy procedure: If FindConsistentTree
returns a tree (T, C), we check the hyper-edges that intersect the components used to form (T, C).
If none of these edges are added, then we can return (T, C) for this value of δ. Otherwise, some of
QS have changed; we can iterate this procedure until we ﬁnd a solution.

6 Evaluation
To evaluate our approach, we have applied it to two real-world (sensor network temperature [8] and
San Francisco Bay area trafﬁc [11]) and one artiﬁcial (samples from ALARM Bayesian network [4])
datasets. Our implementation, called LPACJT, uses lazy evaluations of I (·, · | ·) from section 5.
As baselines for comparison, we used a simple hill-climbing heuristic4, a combination of LPACJT
with hill-climbing, where intermediate results returned by FindConsistentTree were used as starting
points for hill-climbing, Chow-Liu algorithm, and algorithms of [10] (denoted Karger-Srebro) and
[17] (denoted OBS). All experiments were run on a Pentium D 3.4 GHz, with runtimes capped to
10 hours. The necessary entropies were cached in advance.
ALARM. This discrete-valued data was sampled from a known Bayesian network with treewidth 4.
We learned models with treewidth 3 because of computational concerns. Fig. 2(b) shows the per-
point log-likelihood of learned models on test data depending on the amount of training data. We see
that on small training datasets both LPACJT ﬁnds better models than a basic hill-climbing approach,
but worse than the OBS of [17] and Chow-Liu. The implementation of OBS was the only one to
use regularization, so this outcome can be expected. We can also conclude that on this dataset our
approach overﬁts than hill-climbing. For large enough training sets, LPACJT results achieve the
likelihood of the true model, despite being limited to models with smaller treewidth. Chow-Liu
performs much worse, since it is limited to models with treewidth 1. Fig. 2(c) shows an example of
a structure found by LPACJT for ALARM data. LPACJT only missed 3 edges of the true model.

4Hill-climbing had 2 kinds of moves available: replace variable x with variable y in a connected sub-
junction tree, or relpace a leaf clique Ci with another clique (Ci \ Sij) ∪ Smr connected to a separator Smr.

6

x3

0.1
x1 0.2
0=d

0
.
4

x2

x3

0
.
4

x1
x2
2.0=d

x3

0
.
4

x2

x1 0.2
1.0=d

x3

x2

x1
4.0=d

d
o
o
h

i
l

e
k

i
l

−
g
o
L

−15
OBS
−20

−25

−30

102

ALARM

True model

Chow−Liu

Karger−Srebro

LPACJT

LPACJT+Local
Local

103

Training set size

104

(a) Example QS evolution

(b) ALARM - loglikelihood

(c) ALARM - structure

Temperature

OBS

−40
Chow−Liu
−50

d
o
o
h

i
l
e
k
i
l

−
g
o
L

−60

−70

Local
LPACJT

Karger−Srebro

LPACJT+Local

−80

102

103

Training set size

104

d
o
o
h

i
l

i

e
k
L
−
g
o
L

TEMPERATURE sample run,

 2K training points

 

LPACJT

1
Time, seconds

2

3
x 104

−46

−47

 

−48
0

TRAFFIC

Chow−Liu

OBS

Local
LPACJT+Local
Karger−Srebro

LPACJT

d
o
o
h

i
l
e
k
i
l

−
g
o
L

−30

−40

−50

−60

102

Training set size

103

(d) TEMPERATURE loglikelihood

(e) TEMPERATURE sample run

(f) TRAFFIC loglikelihood

Figure 2: An example of evolution of QS for section 5 (2(a)), one structure learned by LPACJT(2(c)), experi-
mental results (2(b),2(d),2(f)), and an example evolution of the test set likelihood of the best found model (2(e)).
In 2(c), nodes denote variables, edges connect variables that belong to the same clique, green edges belong to
both true and learned models, blue edges belong only to the learned model, red - only to the true one.

TEMPERATURE. This data is from a 2-month deployment of 54 sensor nodes (15K data-
points) [8]. Each variable was discretized into 4 bins and we learned models of treewidth 2. Since
the locations of the sensor have an ∞-like shape with two loops, the problem of learning a thin
junction tree for this data is hard. In Fig. 2(d) one can see that LPACJT performs almost as good
as hill-climbing-based approaches, and, on large training sets, much better than Karger-Srebro al-
gorithm. Again, as expected, LPACJT outperforms Chow-Liu algorithm by a signiﬁcant margin if
there is enough data available, but overﬁts on the smallest training sets. Fig 2(e) shows the evolution
of the test set likelihood of the best (highest training set likelihood) structure identiﬁed by LPACJT
over time. The ﬁrst structure was identiﬁed within 5 minutes, and the ﬁnal result within 1 hour.
TRAFFIC. This dataset contains trafﬁc ﬂow information measured every 5 minutes in 8K loca-
tions in California for 1 month [11]. We selected 32 locations in San Francisco Bay area for the
experiments, discretized trafﬁc ﬂow values into 4 bins and learned models of treewidth 3. All non-
regularized algorithms, including LPACJT, give results of essentially the same quality.

7 Relation to prior work and conclusions
For a brief overview of the prior work, we refer the reader to Fig. 3. Most closely related to LPACJT
are learning factor graphs of [1] and learning limited-treewidth Markov nets of [13, 10]. Unlike our
approach, [1] does not guarantee low treewidth of the result, instead settling for compactness. [13,
10] guarantee low treewidth. However, [10] only guarantees that the difference of the log-likelihood
of the result from the fully independent model is within a constant factor from the difference of the
most likely JT: LLH(optimal) − LLH(indep.) ≤ 8kk!2(LLH(learned) − LLH(indep.)). [13] has
exponential complexity. Our approach has polynomial complexity and quality guarantees that hold
for strongly connected k-JT ε-representable distributions, while those of [13] only hold for ε = 0.
We have presented the ﬁrst truly polynomial algorithm for learning junction trees with limited
treewidth. Based on a new upper bound for conditional mutual information that can be computed us-
ing polynomial time and number of samples, our algorithm is guaranteed to ﬁnd a junction tree that
is close in KL divergence to the true distribution, for strongly connected k-JT ε-representable distri-
butions. As a special case of these guarantees, we show PAC-learnability of strongly connected k-JT
representable distributions. We believe that the new theoretical insights herein provide signiﬁcant
step in the understanding of structure learning in graphical models, and are useful for the analysis of
other approaches to the problem. In addition to the theory, we have also demonstrated experimen-
tally that these theoretical ideas are viable, and can, in the future, be used in the development of fast
and effective structure learning heuristics.

7

approach

model class

guarantees

true distribution

samples

score
score
score
score
score
score

constraint
constraint
constraint
constraint

tractable

tree

tree mixture

compact

tractable
compact

all

all

tractable
tractable

local
global
local
local
global

PAC◦
global
PAC
PAC§

const-factor

any
any
any
any
any
any

any

positive

strong k-JT
strong k-JT

any
any
any
any
any
any
poly
∞
exp‡
poly

time
poly†
O(n2)
O(n2)†
poly†
exp
poly
poly

poly(tests)

exp‡
poly

reference

[3, 5]
[6]
[12]
[17]
[15]
[10]
[1]
[16]
[13]

this paper

Figure 3: Prior work. The majority of the literature can be subdivided into score-based [3, 5, 6, 12, 15, 10] and
constraint-based [13, 16, 1] approaches. The former try to maximize some target function, usually regularized
likelihood, while the latter perform conditional independence tests and restrict the set of candidate structures
to those consistent with the results of the tests. Tractable means that the result is guaranteed to be of limited
treewidth, compact - with limited connectivity of the graph. Guarantees column shows whether the result is a
local or global optimum, whether there are PAC guarantees, or whether the difference of the log-likelihood of
the result from the fully independent model is within a const-factor from the difference of the most likely JT.
True distribution shows for what class of distributions the guarantees hold. † superscript means per-iteration
complexity, poly - O(nO(k)), exp‡ - exponential in general, but poly for special cases. PAC◦ and PAC§ mean
PAC with (different) graceful degradation guarantees.

8 Acknowledgments
This work is supported in part by NSF grant IIS-0644225 and by the ONR under MURI
N000140710747. C. Guestrin was also supported in part by an Alfred P. Sloan Fellowship. We
thank Nathan Srebro for helpful discussions, and Josep Roure, Ajit Singh, CMU AUTON lab, Mark
Teyssier, Daphne Koller, Percy Liang and Nathan Srebro for sharing their source code.
References

[1] P. Abbeel, D. Koller, and A. Y. Ng. Learning factor graphs in polynomial time and sample complexity.

JMLR, 7, 2006.

[2] S. Arnborg, D. G. Corneil, and A. Proskurowski. Complexity of ﬁnding embeddings in a k-tree. SIAM

Journal on Algebraic and Discrete Methods, 8(2):277–284, 1987.
[3] F. R. Bach and M. I. Jordan. Thin junction trees. In NIPS, 2002.
[4] I. Beinlich, J. Suermondt, M. Chavez, and G. Cooper. The ALARM monitoring system: A case study

with two probablistic inference techniques for belief networks. In Euro. Conf. on AI in Medicine, 1988.

[5] A. Choi, H. Chan, and A. Darwiche. On Bayesian network approximation by edge deletion. In UAI, 2005.
[6] C. Chow and C. Liu. Approximating discrete probability distributions with dependence trees.
IEEE

Transactions on Information Theory, 14(3):462–467, 1968.

[7] R. G. Cowell, P. A. Dawid, S. L. Lauritzen, and D. J. Spiegelhalter. Probabilistic Networks and Expert

Systems (Information Science and Statistics). Springer, May 2003.

[8] A. Deshpande, C. Guestrin, S. Madden, J. Hellerstein, and W. Hong. Model-driven data acquisition in

sensor networks. In VLDB, 2004.

[9] K. U. H¨offgen. Learning and robust learning of product distributions. In COLT, 1993.
[10] D. Karger and N. Srebro. Learning Markov networks: Maximum bounded tree-width graphs. SODA-01.
[11] A. Krause and C. Guestrin. Near-optimal nonmyopic value of information in graphical models. UAI-05.
[12] M. Meil˘a and M. I. Jordan. Learning with mixtures of trees. JMLR, 1:1–48, 2001.
[13] M. Narasimhan and J. Bilmes. PAC-learning bounded tree-width graphical models. In UAI, 2004.
[14] M. Queyranne. Minimizing symmetric submodular functions. Math. Programming, 82(1):3–12, 1998.
[15] A. Singh and A. Moore. Finding optimal Bayesian networks by dynamic programming. Technical Report
CMU-CALD-05-106, Carnegie Mellon University, Center for Automated Learning and Discovery, 2005.

[16] P. Spirtes, C. Glymour, and R. Scheines. Causation, Prediction, and Search. MIT Press, 2001.
[17] M. Teyssier and D. Koller. Ordering-based search: A simple and effective algorithm for learning Bayesian

networks. In UAI, 2005.

8

"
1016,2007,A Bayesian Model of Conditioned Perception,"We propose an extended probabilistic model for human perception. We argue that in many circumstances, human observers simultaneously evaluate sensory evidence under different hypotheses regarding the underlying physical process that might have generated the sensory information. Within this context, inference can be optimal if the observer weighs each hypothesis according to the correct belief in that hypothesis. But if the observer commits to a particular hypothesis, the belief in that hypothesis is converted into subjective certainty, and subsequent perceptual behavior is suboptimal, conditioned only on the chosen hypothesis. We demonstrate that this framework can explain psychophysical data of a recently reported decision-estimation experiment. The model well accounts for the data, predicting the same estimation bias as a consequence of the preceding decision step. The power of the framework is that it has no free parameters except the degree of the observer's uncertainty about its internal sensory representation. All other parameters are defined by the particular experiment which allows us to make quantitative predictions of human perception to two modifications of the original experiment.","A Bayesian Model of Conditioned Perception

Alan A. Stocker

∗

and Eero P. Simoncelli

Howard Hughes Medical Institute,

Center for Neural Science,

and Courant Institute of Mathematical Sciences

New York University

New York, NY-10003, U.S.A.

We argue that in many circumstances, human observers evaluate sensory evidence
simultaneously under multiple hypotheses regarding the physical process that has
generated the sensory information. In such situations, inference can be optimal if
an observer combines the evaluation results under each hypothesis according to
the probability that the associated hypothesis is correct. However, a number of ex-
perimental results reveal suboptimal behavior and may be explained by assuming
that once an observer has committed to a particular hypothesis, subsequent evalu-
ation is based on that hypothesis alone. That is, observers sacriﬁce optimality in
order to ensure self-consistency. We formulate this behavior using a conditional
Bayesian observer model, and demonstrate that it can account for psychophysical
data from a recently reported perceptual experiment in which strong biases in per-
ceptual estimates arise as a consequence of a preceding decision. Not only does
the model provide quantitative predictions of subjective responses in variants of
the original experiment, but it also appears to be consistent with human responses
to cognitive dissonance.

1 Motivation

Is the glass half full or half empty? In different situations, the very same perceptual evidence (e.g. the
perceived level of liquid in a glass) can be interpreted very differently. Our perception is conditioned
on the context within which we judge the evidence. Perhaps we witnessed the process of the glass
being ﬁlled, and thus would more naturally think of it as half full. Maybe it is the only glass on
the table that has liquid remaining, and thus its precious content would be regarded as half full. Or
maybe we simply like the content so much that we cannot have enough, in which case we may view
it as being half empty.

Contextual inﬂuences in low-level human perception are the norm rather than the exception, and
have been widely reported. Perceptual illusions, for example, often exhibit particularly strong con-
textual effects, either in terms of perceptual space (e.g. spatial context affects perceived brightness;
see [1] for impressive examples) or time (prolonged exposure to an adaptor stimulus will affect
subsequent perception, see e.g. the motion after-effect [2]). Data of recent psychophysical exper-
iments suggest that an observer’s previous perceptual decisions provide additional form of context
that can substantially inﬂuence subsequent perception [3, 4]. In particular, the outcome of a categor-
ical decision task can strongly bias a subsequent estimation task that is based on the same stimulus
presentation. Contextual inﬂuences are typically strongest when the sensory evidence is most am-
biguous in terms of its interpretation, as in the example of the half-full (or half-empty) glass.

Bayesian estimators have proven successful in modeling human behavior in a wide variety of low-
level perceptual tasks (for example: cue-integration (see e.g. [5]), color perception (e.g. [6]), visual
motion estimation (e.g. [7, 8])). But they generally do not incorporate contextual dependencies

∗

corresponding author.

beyond a prior distribution (reﬂecting past experience) over the variable of interest. Contextual
dependencies may be incorporated in a Bayesian framework by assuming that human observers,
when performing a perceptual task, test different hypotheses about the underlying structure of the
sensory evidence, and arrive at an estimate by weighting the estimates under each hypothesis ac-
cording to the strength of their belief in that hypothesis. This approach is known as optimal model
evaluation [9], or Bayesian model averaging [10] and has been previously suggested to account for
cognitive reasoning [11]. It further has been suggested that the brain could use different neuro-
modulators to keep track of the probabilities of individual hypotheses [12]. Contextual effects are
reﬂected in the observer’s selection and evaluation of these hypotheses, and thus vary with exper-
imental conditions. For the particular case of cue-integration, Bayesian model averaging has been
proposed and tested against data [13, 14], suggesting that some of the observed non-linearities in
cue integration are the result of the human perceptual system taking into account multiple potential
contextual dependencies.

In contrast to these studies, however, we propose that model averaging behavior is abandoned once
the observer has committed to a particular hypothesis. Speciﬁcally, subsequent perception is condi-
tioned only on the chosen hypothesis, thus sacriﬁcing optimality in order to achieve self-consistency.
We examine this hypothesis in the context of a recent experiment in which subjects were asked to
estimate the direction of motion of random dot patterns after being forced to make a categorical
decision about whether the direction of motion fell on one side or the other of a reference mark [4].
Depending on the different levels of motion coherence, responses on the estimation task were heav-
ily biased by the categorical decision. We demonstrate that a self-consistent conditional Bayesian
model can account for mean behavior, as well as behavior on individual trials [8]. The model has es-
sentially no free parameters, and in addition is able to make precise predictions under a wide variety
of alternative experimental arrangements. We provide two such example predictions.

2 Observer Model

We deﬁne perception as a statistical estimation problem in which an observer tries to infer the value
of some environmental variable s based on sensory evidence m (see Fig. 1). Typically, there are
sources of uncertainty associated with m, including both sensor noise and uncertainty about the
relationship between the sensory evidence and the variable s. We refer to the latter as structural
uncertainty which represents the degree of ambiguity in the observer’s interpretation of the physical
world. In cases where the structural possibilities are discrete, we denote them as a set of hypotheses
H = {h1, ..., hN}. Perceptual inference requires two steps. First, the observer computes their belief

world

measurement

s

property

m

noise!

p(H|m)

h

1

.
.
.

h
n

hypotheses

observer

p(s|m)

estimate

^

s(m)

prior 
knowledge

Figure 1: Perception as conditioned inference problem. Based on noisy sensory measurements
m the observer generates different hypotheses for the generative structure that relates m to the
stimulus variable s. Perception is a two-fold inference problem: Given the measurement and prior
knowledge, the observer generates and evaluates different structural hypotheses h i. Conditioned on
this evaluation, they then infer an estimate ˆs(m) from the measurement m.

in each hypothesis for given sensory evidence m. Using Bayes’ identity, the belief is expressed as

the posterior

p(H|m) = p(m|H)p(H)

.

(1)
Second, for each hypothesis, a conditional posterior is formulated as p(s|m, H = h i), and the full
(non-conditional) posterior is computed by integrating the evidence over all hypotheses, weighted
by the belief in each hypothesis h i:

p(m)

N(cid:1)

p(s|m) =

p(s|m, H = hi)p(H = hi|m) .

(2)

Finally, the observer selects an estimate ˆs that minimizes the expected value (under the posterior)
of an appropriate loss function 1.

i=1

2.1 Decision leads to conditional estimation

In situations where the observer has already made a decision (either explicit or implicit) to select one
hypothesis as being correct, we postulate that subsequent inference will be based on that hypothesis
alone, rather than averaging over the full set of hypotheses. For example, suppose the observer
selects the maximum a posteriori hypothesis hMAP, the hypothesis that is most probable given the
sensory evidence and the prior distribution. We assume that this decision then causes the observer
to reset the posterior probabilities over the hypotheses to

p(H|m) = 1,
= 0,

if H = hMAP
otherwise.

(3)

That is, the decision making process forces the observer to consider the selected hypothesis as
correct, with all other hypotheses rendered impossible. Changing the beliefs over the hypotheses
will obviously affect the estimate ˆs in our model. Applying the new posterior probabilities Eq. (3)
simpliﬁes the inference problem Eq. (2) to

p(s|m) = p(s|m, H = hMAP) .

(4)

We argue that this simpliﬁcation by decision is essential for complex perceptual tasks (see Discus-
sion). By making a decision, the observer frees resources, eliminating the need to continuously
represent probabilities about other hypotheses, and also simpliﬁes the inference problem. The price
to pay is that the subsequent estimate is typically biased and sub-optimal.

3 Example: Conditioned Perception of Visual Motion

We tested our observer model by simulating a recently reported psychophysical experiment [4].
Subjects in this experiment were asked on each trial to decide whether the overall motion direction
of a random dot pattern was to the right or to the left of a reference mark (as seen from the ﬁxation
point). Low levels of motion coherence made the decision task difﬁcult for motion directions close
to the reference mark. In a subset of randomly selected trials subjects were also asked to estimate the
precise angle of motion direction (see Fig. 2). The decision task was always preceding the estimation
task, but at the time of the decision, subjects were unaware whether they would had to perform the
estimation task or not.

3.1 Formulating the observer model

We denote θ as the direction of coherent motion of the random dot pattern, and m the noisy sensory
measurement. Suppose that on a given trial the measurement m indicates a direction of motion to
the right of the reference mark. The observer can consider two hypotheses H = {h 1, h2} about the
actual physical motion of the random dot pattern: Either the true motion is actually to the right and
thus in agreement with the measurement, or it is to the left but noise has disturbed the measurement

1For the purpose of this paper, we assume a standard squared error loss function, in which case the observer

should choose the mean of the posterior distribution.

decision

estimation

reference

?

?

reference

s
l
a
i
r
t

?
?

?

a

b

Figure 2: Decision-estimation experiment.
(a) Jazayeri and Movshon presented moving random
dot patterns to subjects and asked them to decide if the overall motion direction was either to the
right or the left of a reference mark [4]. Random dot patterns could exhibit three different levels of
motion coherence (3, 6, and 12%) and the single coherent motion direction was randomly selected
from a uniform distribution over a symmetric range of angles [−α, α] around the reference mark. (b)
In randomly selected 30% of trials, subjects were also asked, after making the directional decision,
to estimate the exact angle of motion direction by adjusting an arrow to point in the direction of
perceived motion. In a second version of the experiment, motion was either toward the direction of
the reference mark or in the opposite direction.

such that it indicates motion to the right. The observer’s belief in each of the two hypotheses based
on their measurement is given by the posterior distribution according to Eq. (1), and the likelihood

p(m|H) =

p(m|θ, H)p(θ|H)dθ .

(5)

(cid:2) π

−π

The optimal decision is to select the hypothesis hMAP that maximizes the posterior given by Eq. (1).

3.2 Model observer vs. human observer

The subsequent conditioned estimate of motion direction then follows from Eq. (4) which can be
rewritten as

p(θ|m) = p(m|θ, H = hMAP)p(θ|H = hMAP)

.

p(m|H = hMAP)

(6)
is completely characterized by three quantities: The likelihood functions p(m|θ, H),
The model
the prior distributions p(θ|H) of the direction of motion given each hypothesis, and the prior on the
hypotheses p(H) itself (shown in Fig. 3). In the given experimental setup, both prior distributions
were uniform but the width parameter of the motion direction α was not explicitly available to
the subjects and had to be individually learned from training trials. In general, subjects seem to
over-estimate this parameter (up to a factor of two), and adjusting its value in the model accounts
for most of the variability between subjects. The likelihood functions p(m|θ, H) is given by the
uncertainty about the motion direction due to the low motion coherence levels in the stimuli and the
sensory noise characteristics of the observer. We assumed it to be Gaussian with a width that varies
inversely with the coherence level. Values were estimated from the data plots in [4].

Figure 4 compares the prediction of the observer model with human data. Trial data of the model
were generated by ﬁrst sampling a hypothesis h(cid:2)
according to p(H), then drawing a stimulus direc-
tion from p(θ|H = h(cid:2)). then picking a sensory measurement sample m according to the conditional
probability p(m|θ, H = h(cid:2)), and ﬁnally performing inference according to Eqs. (1) and (6). The
model captures the characteristics of human behavior in both the decision and the subsequent es-
timation task. Note the strong inﬂuence of the decision task on the subsequent estimation of the
motion direction, effectively pushing the estimates away from the decision boundary.

We also compared the model with a second version of the experiment, in which the decision task
was to discriminate between motion toward and away from the reference [4]. Coherent motion of
the random dot pattern was uniformly sampled from a range around the reference and from a range

p(m|θ, H)

p(H)

12 %

6 %

−α

3 %

α

0.5

0.5

p(θ|H)

−α

α

θ

Figure 3: Ingredients of the conditional observer model. The sensory signal is assumed to be
corrupted by additive Gaussian noise, with width that varies inversely with the level of motion
coherence. Actual widths were approximated from those reported in [4]. The prior distribution
over the hypotheses p(H) is uniform. The two prior distributions over motion direction given each
hypothesis, p(θ|H = h1,2), are again determined by the experimental setup, and are uniform over
the range [0,±α].

around the direction opposite to the reference, as illustrated by the prior distributions shown in Fig. 5.
Again, note that these distributions are given by the experiment and thus, assuming the same noise
characteristics as in the ﬁrst experiment, the model has no free parameters.

3.3 Predictions

The model framework also allows us to make quantitative predictions of human perceptual behavior
under conditions not yet tested. Figure 6 shows the model observer’s behavior under two modiﬁ-
cations of the original experiment. The ﬁrst is identical to the experiment shown in Fig. 4 but with
unequal prior probability on the two hypotheses. The model predicts that a human subject would
respond to this change by more frequently choosing the more likely hypothesis. However, this hy-
pothesis would also be more likely to be correct, and thus the estimates under this hypothesis would
exhibit less bias than in the original experiment.

The second modiﬁcation is to add a second reference and ask the subject to decide between three
different classes of motion direction (e.g. left, central, right). Again, the model predicts that in such
a case, a human subject’s estimate in the central direction should be biased away from both decision
boundaries, thus leading to an almost constant direction estimate. Estimates following a decision in
favor of the two outer classes show the same repulsive bias as seen in the original experiment.

4 Discussion

We have presented a normative model for human perception that captures the conditioning effects
of decisions on an observer’s subsequent evaluation of sensory evidence. The model is based on
the premise that observers aim for optimal inference (taking into account all sensory evidence and
prior information), but that they exhibit decision-induced biases because they also aim to be self-
consistent, eliminating alternatives that have been decided against. We’ve demonstrated that this
model can account for the experimental results of [4].

Although this strategy is suboptimal (in that it does not minimize expected loss), it provides two
fundamental advantages. First, self-consistency would seem an important requirement for a stable
interpretation of the environment, and adhering to it might outweigh the disadvantages of perceptual
misjudgments. Second, framing perception in terms of optimal statistical estimation implies that the
more information an observer evaluates, the more accurately they should be able to solve a percep-
tual task. But this assumes that the observer can construct and retain full probability distributions
and perform optimal inference calculations on these. Presumably, accumulating more probabilistic
evidence of more complex conditional dependencies has a cost, both in terms of storage, and in terms
of the computational load of performing subsequent inference. Thus, discarding information after
making a decision can help to keep this storage and the computational complexity at a manageable
level, freeing computational resources to perform other tasks.

data

model

data

model

 

e
c
n
e
r
e
f
e
r
 
f
o
 
t
h
g
i
r

 

 

n
o
i
t
o
m
n
o
i
t
c
a
r
f

1

0.5

0

]
g
e
d
[
 
n
o
i
t
c
e
r
i
d
d
e
t
a
m

 

i
t
s
e

20

10

0

-10

-20

coherence level

3 %
6 %
12 %

-20

-10

0

10

20

-20

-10

0

10

20

-20

-10

0

10

20

-20

-10

0

10

20

]
g
e
d
[
 
n
o
i
t
c
e
r
i
d
d
e
t
a
m

 

i
t
s
e

40

20

0

-20

-40

40

20

0

-20

-40

40

20

0

-20

-40

3 %3 %

6 %6 %

-20

-10

0

10

12 %12 %
20

-20

-10

0

10

20

true direction [deg]

true direction [deg]

Figure 4: Comparison of model predictions with data for a single subject. Upper left: The two pan-
els show the percentage of observed motion to the right as a function of the true pattern direction,
for the three coherence levels tested. The model accurately predicts the subject’s behavior, which
exhibits a decrease in the number of false decisions with decreasing noise levels and increasing dis-
tance to the reference. Lower left: Mean estimates of the direction of motion after performing the
decision task. Clearly, the decision has a substantial impact on the subsequent estimate, producing
a strong bias away from the reference. The model response exhibits biases similar to those of the
human subjects, with lower coherence levels producing stronger repulsive effects. Right: Grayscale
images show distributions of estimates across trials for both the human subject and the model ob-
server, for all three coherence levels. All trials are included (correct and incorrect). White dashed
lines represent veridical estimates. Model observer performed 40 trials at each motion direction (in
1.5 degrees increments). Human data are replotted from [4].

An interesting avenue for exploration is the implementation of such an algorithm in neural substrate.
Recent studies propose a means by which population of neurons can represent and multiply proba-
bility distributions [15]. It would be worthwhile to consider how the model presented here could be
implemented with such a neural mechanism. In particular, one might expect that the sudden change
in posterior probabilities over the hypotheses associated with the decision task would be reﬂected in
sudden changes in response pattern in such populations [16].

Questions remain. For the experiment we have modeled, the hypotheses were speciﬁed by the two
alternatives of the decision task, and the subjects were forced to choose one of them. What hap-
pens in more general situations? First, do humans always decompose perceptual inference tasks
into a set of inference problems, each conditioned on a different hypothesis? Data from other,
cue-combination experiments suggest that subjects indeed seem to perform such probabilistic de-
composition [13, 14]. If so, then how do observers generate these hypotheses? In the absence of
explicit instructions, humans may automatically perform implicit comparisons relative to reference
features that are unconsciously selected from the environment. Second, if humans do consider dif-
ferent hypotheses, do they always select a single one on which subsequent percepts are conditioned,
even if not explicitly asked to do so? For example, simply displaying the reference mark in the
experiment of [4] (without asking the observer to report any decision) might be sufﬁcient to trigger
an implicit decision that would result in behaviors similar to those shown in the explicit case.

Finally, although we have only tested it on data of a particular psychophysical experiment, we be-
lieve that our model may have implications beyond low-level sensory perception. For instance, a

data

model

p(H)

0.5

0.5

p(θ|H)
−α

α

θ

]
g
e
d
[
 
 

n
o
i
t
c
e
r
i
d
d
e
t
a
m

 

i
t
s
e

40

20

0

-20

-40

40

20

0

-20

-40

40

20

0

-20

-40

-20

-10

0

3 %3 %

6 %6 %

12 %12 %
20

10
true direction  [deg]

-10

-20

0

10

20

Figure 5: Comparison of model predictions with data for second experiment. Left: Prior distri-
butions for second experiment in [4]. Right: Grayscale images show the trial distributions of the
human subject and the model observer for all three coherence levels. White dashed lines represent
veridical estimates. Note that the human subject does not show any signiﬁcant bias in their estimate.
The trial variance appears to increase with decreasing levels of coherence. Both characteristics are
well predicted by the model. Human data replotted from [4] (supplementary material).

well-studied human attribute is known as cognitive dissonance [17], which causes people to ad-
just their opinions and beliefs to be consistent with their previous statements or behaviors. 2 Thus,
self-consistency may be a principle that governs computations throughout the brain.

Acknowledgments

We thank J. Tenenbaum for referring us to the cognitive dissonance literature, and J. Pillow, N. Daw,
D. Heeger, A. Movshon, and M. Jazayeri for interesting discussions.

References
[1] E.H. Adelson. Perceptual organization and the judgment of brightness. Science, 262:2042–2044, Decem-

ber 1993.

[2] S.P. Thompson. Optical illusions of motion. Brain, 3:289–298, 1880.
[3] S. Baldassi, N. Megna, and D.C. Burr. Visual clutter causes high-magnitude errors. PLoS Biology,

4(3):387ff, March 2006.

[4] M. Jazayeri and J.A. Movshon. A new perceptual illusion reveals mechanisms of sensory decoding.

Nature, 446:912ff, April 2007.

[5] M.O. Ernst and M.S. Banks. Humans integrate visual and haptic information in a statistically optimal

fashion. Nature, 415:429ff, January 2002.

[6] D. Brainard and W. Freeman. Bayesian color constancy. Journal of Optical Society of America A,

14(7):1393–1411, July 1997.

2An example that is directly analogous to the perceptual experiment in [4] is documented in [18]: Subjects
initially rated kitchen appliances for attractiveness, and then were allowed to select one as a gift from amongst
two that they had rated equally. They were subsequently asked to rate the appliances again. The data show a
repulsive bias of the post-decision ratings compared with the pre-decision ratings, such that the rating of the
selected appliance increased, and the rating of the rejected appliance decreased.

p(H)

0.8

0.2

p(θ|H)

−α

α

θ

−β

β

1/3

1/3

1/3

−β

β

−α

α

θ

A

B

40

20

0

-20

-40

40

20

0

]
g
e
d
[
 
 

n
o
i
t
c
e
r
i
d
d
e
t
a
m

 

i
t
s
e

-20

-40

-20

-10

0

trial

mean

40

20

0

-20

-40

40

20

0

-20

-40

20

-10
10
true direction  [deg]

-20

0

10

20

Figure 6: Model predictions for two modiﬁcations of the original experiment. A: We change the
prior probability p(H) to be asymmetric (0.8 vs. 0.2). However, we keep the prior distribution
of motion directions given a particular side p(θ|H) constant within the range [0,±α]. The model
makes two predictions (trials shown for an intermediate coherence level): First, although tested with
an equal number of trials for each motion direction, there is a strong bias induced by the asymmetric
prior. And second, the direction estimates on the left are more veridical than on the right. B: We
present two reference marks instead of one, asking the subjects to make a choice between three
equally likely regions of motion direction. Again, we assume uniform prior distributions of motion
directions within each area. The model predicts bilateral repulsion of the estimates in the central
area, leading to a strong bias that is almost independent of coherence level.

[7] Y. Weiss, E. Simoncelli, and E. Adelson. Motion illusions as optimal percept. Nature Neuroscience,

5(6):598–604, June 2002.

[8] A.A. Stocker and E.P. Simoncelli. Noise characteristics and prior expectations in human visual speed

perception. Nature Neuroscience, pages 578–585, April 2006.

[9] D. Draper. Assessment and propagation of model uncertainty. Journal of the Royal Statistical Society B,

57:45–97, 1995.

[10] J.A. Hoeting, D. Madigan, A.E. Raftery, and C.T. Volinsky. Bayesian model averaging: A tutorial. Sta-

tistical Science, 14(4):382–417, 1999.

[11] T.L. Grifﬁths, C. Kemp, and J. Tenenbaum. Handbook of Computational Cognitive Modeling, chapter

Bayesian models of cognition. Cambridge University Press, to appear.

[12] J.A. Yu and P. Dayan. Uncertainty, neuromodulation, and attention. Neuron, 46:681ff, May 2005.
[13] D. Knill. Robust cue integration: A Bayesian model and evidence from cue-conﬂict studies with stereo-

scopic and ﬁgure cues to slant. Journal of Vision, 7(7):1–24, May 2007.

[14] K. K¨ording and J. Tenenbaum. Causal inference in sensorimotor integration. In B. Sch¨olkopf, J. Platt,

and T. Hoffman, editors, Advances in Neural Information Processing Systems 19. MIT Press, 2007.

[15] W.J. Ma, J.M. Beck, P.E. Latham, and A. Pouget. Bayesian inference with probabilistic population codes.

Nature Neuroscience, 9:1432ff, November 2006.

[16] Roitman J. D. Ditterich J. Mazurek, M. E. and M. N. Shadlen. A role for neural integrators in perceptual

decision-making. Cerebral Cortex, 13:1257–1269, 2003.

[17] L. Festinger. Theory of Cognitive Dissonance. Stanford University Press, Stanford, CA, 1957.
[18] J.W. Brehm. Post-decision changes in the desirability of alternatives. Journal of Abnormal and Social

Psychology, 52(3):384ff., 1956.

"
349,2007,Scan Strategies for Meteorological Radars,"We address the problem of adaptive sensor control in dynamic resource-constrained sensor networks. We focus on a meteorological sensing network comprising radars that can perform sector scanning rather than always scanning 360 degrees. We compare three sector scanning strategies. The sit-and-spin strategy always scans 360 degrees. The limited lookahead strategy additionally uses the expected environmental state K decision epochs in the future, as predicted from Kalman filters, in its decision-making. The full lookahead strategy uses all expected future states by casting the problem as a Markov decision process and using reinforcement learning to estimate the optimal scan strategy. We show that the main benefits of using a lookahead strategy are when there are multiple meteorological phenomena in the environment, and when the maximum radius of any phenomenon is sufficiently smaller than the radius of the radars. We also show that there is a trade-off between the average quality with which a phenomenon is scanned and the number of decision epochs before which a phenomenon is rescanned.","Scan Strategies for Adaptive Meteorological Radars

Victoria Manfredi, Jim Kurose
Department of Computer Science

University of Massachusetts

{vmanfred,kurose}@cs.umass.edu

Amherst, MA USA

Abstract

We address the problem of adaptive sensor control
in dynamic resource-
constrained sensor networks. We focus on a meteorological sensing network com-
prising radars that can perform sector scanning rather than always scanning 360◦
.
We compare three sector scanning strategies. The sit-and-spin strategy always
scans 360◦
. The limited lookahead strategy additionally uses the expected envi-
ronmental state K decision epochs in the future, as predicted from Kalman ﬁlters,
in its decision-making. The full lookahead strategy uses all expected future states
by casting the problem as a Markov decision process and using reinforcement
learning to estimate the optimal scan strategy. We show that the main beneﬁts of
using a lookahead strategy are when there are multiple meteorological phenomena
in the environment, and when the maximum radius of any phenomenon is sufﬁ-
ciently smaller than the radius of the radars. We also show that there is a trade-off
between the average quality with which a phenomenon is scanned and the number
of decision epochs before which a phenomenon is rescanned.

1 Introduction

Traditionally, meteorological radars, such as the National Weather Service NEXRAD system, are
tasked to always scan 360 degrees. In contrast, the Collaborative Adaptive Sensing of the Atmo-
sphere (CASA) Engineering Research Center [5] is developing a new generation of small, low-power
but agile radars that can perform sector scanning, targeting sensing when and where the user needs
are greatest. Since all meteorological phenomena cannot be now all observed all of the time with
the highest degree of ﬁdelity, the radars must decide how best to perform scanning. While we fo-
cus on the problem of how to perform sector scanning in such an adaptive meteorological sensing
network, it is an instance of the larger class of problems of adaptive sensor control in dynamic
resource-constrained sensor networks.

Given the ability of a network of radars to perform sector scanning, how should scanning be adapted
at each decision epoch? Any scan strategy must consider, for each scan action, both the expected
quality with which phenomena would be observed, and the expected number of decision epochs
before which phenomena would be ﬁrst observed (for new phenomena) or rescanned, since not all
regions are scanned every epoch under sectored scanning. Another consideration is whether to opti-
mize myopically only over current and possibly past environmental state, or whether to additionally
optimize over expected future states. In this work we examine three methods for adapting the radar
scan strategy. The methods differ in the information they use to select a scan conﬁguration at a
particular decision epoch. The sit-and-spin strategy of always scanning 360 degrees is indepen-
dent of any external information. The limited lookahead strategies additionally use the expected
environmental state K decision epochs in the future in its decision-making. Finally, the full looka-
head strategy has an inﬁnite horizon: it uses all expected future states by casting the problem as a
Markov decision process and using reinforcement learning to estimate the optimal scan strategy. All
strategies, excluding sit-and-spin, work by optimizing the overall “quality” (a term we will deﬁne

1

precisely shortly) of the sensed information about phenomena in the environment, while restricting
or penalizing long inter-scan intervals.

Our contributions are two-fold. We ﬁrst introduce the meteorological radar control problem and
show how to constrain the problem so that it is amenable to reinforcement learning methods. We
then identify conditions under which the computational cost of an inﬁnite horizon radar scan strategy
such as reinforcement learning is necessary. With respect to the radar meteorological application,
we show that the main beneﬁts of considering expected future states are when there are multiple
meteorological phenomena in the environment, and when the maximum radius of any phenomenon
is sufﬁciently smaller than the radius of the radars. We also show that there is a trade-off between
the average quality with which a phenomenon is scanned and the number of decision epochs before
which a phenomenon is rescanned. Finally, we show that for some environments, a limited looka-
head strategy is sufﬁcient. In contrast to other work on radar control (see Section 5), we focus on
tracking meteorological phenomena and the time frame over which to evaluate control decisions.

The rest of this paper is organized as follows. Section 2 deﬁnes the radar control problem. Section
3 describes the scan strategies we consider. Section 4 describes our evaluation framework and
presents results. Section 5 reviews related work on control and resource allocation in radar and
sensor networks. Finally, Section 6 summarizes this work and outlines future work.

2 Meteorological Radar Control Problem

Meteorological radar sensing characteristics are such that the smaller the sector that a radar scans
(until a minimum sector size is reached), the higher the quality of the data collected, and thus, the
more likely it is that phenomena located within the sector are correctly identiﬁed [2]. The multi-
radar meteorological control problem is then as follows. We have a set of radars, with ﬁxed locations
and possibly overlapping footprints. Each radar has a set of scan actions from which it chooses. In
the simplest case, a radar scan action determines the size of the sector to scan, the start angle, the
end angle, and the angle of elevation. We will not consider elevation angles here. Our goal is
to determine which scan actions to use and when to use them. An effective scanning strategy must
balance scanning small sectors (thus implicitly not scanning other sectors), to ensure that phenomena
are correctly identiﬁed, with scanning a variety of sectors, to ensure that no phenomena are missed.

We will evaluate the performance of different scan strategies based on inter-scan time, quality, and
cost. Inter-scan time is the number of decision epochs before a phenomenon is either ﬁrst observed
or rescanned; we would like this value to be below some threshold. Quality measures how well a
phenomenon is observed, with quality depending on the amount of time a radar spends sampling
a voxel in space, the degree to which a meteorological phenomena is scanned in its (spatial) en-
tirety, and the number of radars observing a phenomenon; higher quality scans are better. Cost is
a meta-metric that combines inter-scan time and quality, and that additionally considers whether a
phenomenon was never scanned. The radar control problem is that of dynamically choosing the scan
strategy of the radars over time to maximize quality while minimizing inter-scan time.

3 Scan Strategies

We deﬁne a radar conﬁguration to be the start and end angles of the sector to be scanned by an
individual radar for a ﬁxed interval of time. We deﬁne a scan action to be a set of radar conﬁgurations
(one conﬁguration for each radar in the meteorological sensing network). We deﬁne a scan strategy
to be an algorithm for choosing scan actions. In Section 3.1 we deﬁne the quality function associated
with different radar conﬁgurations and in Section 3.2 we deﬁne the quality functions associated with
different scan strategies.

3.1 Quality Function

The quality function associated with a given scan action was proposed by radar meteorologists in [5]
and has two components. There is a quality component Up associated with scanning a particular
phenomenon p. There is also a quality component Us associated with scanning a sector, which is
independent of any phenomena in that sector. Let sr be the radar conﬁguration for a single radar r
and let Sr be the scan action under consideration. From [5], we compute the quality Up(p, Sr) of

2

Figure 1: Step functions used by the Up and Us quality functions, from [9]

scanning a phenomenon p using scan action Sr with the following equations,

Up(p, sr) = Fc (c(p, sr)) ×(cid:20)βFd (d(r, p)) + (1 − β)Fw(cid:18) w(sr)
360 (cid:19)(cid:21)

Up(p, Sr) = maxsr∈Sr [Up(p, sr)]

(1)

where

w(sr) = size of sector sr scanned by r
a(r, p) = minimal angle that would allow r to cover p
c(p, sr) = w(sr)
a(r, p)
h(r, p) = distance from r to geometric center of p

= coverage of p by r scanning sr

hmax(r) = range of radar r
d(r, p) = h(r, p)
hmax(r)

= normalized distance from r to p

β = tunable parameter

Up(p, Sr) is the maximum quality obtained for scanning phenomenon p over all possible radars and
their associated radar conﬁgurations sr. Up(p, sr) is the quality obtained for scanning phenomenon
p using a speciﬁc radar r and radar conﬁguration sr. The functions Fc(·), Fw(·), and Fd(·) from [5]
are plotted in Figure 1. Fc captures the effect on quality due to the percentage of the phenomenon
covered; to usefully scan a phenomenon, at least 95% of the phenomenon must be scanned. Fw
captures the effect of radar rotation speed on quality; as rotation speed is reduced, quality increases.
Fd captures the effects of the distance from the radar to the geometrical center of the phenomenon on
quality; the further away the radar center is from the phenomenon being scanned, the more degraded
will be the scan quality due to attenuation. Due to the Fw function, the quality function Up(p, sr)
outputs the same quality for scan angles of 181◦
. The quality Us(ri, sr) for scanning a
subsector i of radar r scanned using conﬁguration sr is,

to 360◦

360 (cid:19)
Us(ri, sr) = Fw(cid:18) w(sr)

(2)

Intuitively, a sector scanning strategy is only preferable when the quality function is such that the
quality gained for scanning a sector is greater than the quality lost for not scanning another sector.

3.2 Scan Strategies

We compare the performance of the following three scan strategies. The strategies differ in whether
they optimize quality over only current or also future expected states. For example, suppose a storm
cell is about to move into a high-quality multi-doppler region (i.e., the area where multiple radar
footprints overlap). By considering future expected states, a lookahead strategy can anticipate this
event and have all radars focused on the storm cell when it enters the multi-doppler region, rather
than expending resources (with little “reward”) to scan the storm cell just before it enters this region.
(i) Sit-and-spin strategy. All radars always scan 360◦
(ii) Limited “lookahead” strategy. We examine both a 1-step and a 2-step look-ahead scan strategy.
Although we do not have an exact model of the dynamics of different phenomena, to perform the

.

3

 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1FccFc Function 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1Fww/360Fw Function 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 1.2FddFd Functionlook-ahead we estimate the future attributes of each phenomenon using a separate Kalman ﬁlter. For
each ﬁlter, the true state x is a vector comprising the (x, y) location and velocity of the phenomenon,
and the measurement y is a vector comprising only the (x, y) location. The Kalman ﬁlter assumes
that the state at time t is a linear function of the state at time t − 1 plus some Gaussian noise, and
that the measurement at time t is a linear function of the state at time t plus some Gaussian noise. In
particular, xt = Axt−1 + N[0, Q] and yt = Bxt + N[0, R].
Following work by [8], we initialize each Kalman ﬁlter as follows. The A matrix reﬂects that storm
cells typically move to the north-east. The B matrix, which when multiplied with xt returns xt,
assumes that the observed state yt is directly the true state xt plus some Gaussian noise. The Q
matrix assumes that there is little noise in the true state dynamics. Finally, the measurement error
covariance matrix R is a function of the quality Up with which phenomenon p was scanned at time
t. We discuss how to compute the σt’s in Section 4. We use the ﬁrst location measurement of a
storm cell y0, augmented with the observed velocity, as the the initial state x0. We assume that our
estimate of x0 has little noise and use .0001 ∗ I for the initial covariance P0.
A =(cid:34) 1

0 (cid:105) , Q =(cid:34) .0001

(cid:35) , B =(cid:104) 1

(cid:35) , R =(cid:104) σt

0

σt (cid:105)

0

.0001

.0001

0
0
0

0
0

0
1

0
0

0

0

0
1
0
0

1
0
1
0

0
1
0
1

0
0
0

0

.0001

We compute the k-step look-ahead quality for different sets of radar conﬁgurations Sr with,

0

0
0

0
0
0

Np(cid:88)

i=1

UK(Sr,1|Tr) =

φk−1

Up(pi,k, Sr,k|Tr)

K(cid:88)

k=1

where Np is the number of phenomena in the environment in the current decision epoch, pi,0 is
the current set of observed attributes for phenomenon i, pi,k is the k-step set of predicted attributes
for phenomenon i, Sr,k is the set of radar conﬁgurations for the kth decision epoch in the future,
and φ is a tunable discount factor between 0 and 1. The optimal set of radar conﬁgurations is
r,1 = argmaxSr,1UK(Sr,1|Tr). To account for the decay of quality for unscanned sectors
∗
then S
and phenomena, and to consider the possibility of new phenomena appearing, we restrict Sr to be
those scan actions that ensure that every sector has been scanned at least once in the last Tr decision
epochs. Tr is a tunable parameter whose purpose is to satisfy the meteorological dictate found in [5],
that all sectors be scanned, for instance by a 360◦
(iii) Full “lookahead” strategy. We formulate the radar control problem as a Markov decision
process (MDP) and use reinforcement learning to obtain a lookahead scan strategy as follows. While
a POMDP (partially observable MDP) could be used to model the environmental uncertainty, due to
the cost of solving a POMDP with a large state space [9], we choose to formulate the radar control
problem as an MDP with quality (or uncertainty) variables as in an augmented MDP [6].

scan, at most every 5 minutes.

S is the observed state of the environment. The state is a function of the observed number of storms,
the observed x, y velocity of each storm, and the observed dimensions of each storm cell given by
x, y center of mass and radius. To model the uncertainty in the environment, we additionally deﬁne
as part of the state quality variables up and us based on the Up and Us quality functions deﬁned
in Equations (1) and (2) in Section 3.1. up is the quality Up(·) with which each storm cell was
observed, and us is the current quality Us(·) of each 90◦
subsector, starting at 0, 90, 180, or 270◦
.
A is the set of actions available to the radars. This is the set of radar conﬁgurations for a given
decision epoch. We restrict each radar to scanning subsectors that are a multiple of 90◦
, starting at
0, 90, 180, or 270◦
The transition function T (S× A× S) → [0, 1] encodes the observed environment dynamics: specif-
ically the appearance, disappearance, and movement of storm cells and their associated attributes.
For meteorological radar control, the next state really is a function of not just the current state but
also the action executed in the current state. For instance, if a radar scans 180 degrees rather than
360 degrees, then any new storm cells that appear in the unscanned areas will not be observed. Thus,
the new storm cells that will be observed will depend on the scanning action of the radar.
The cost function C(S, A, S) → R encodes the goals of the radar sensing network. C is a function
of the error between the true state and the observed state, whether all storms have been observed,

. Thus, with N radars there are 13N possible actions at each decision epoch.

4

and a penalty term for not rescanning a storm within Tr decision epochs. More precisely,

C =

|do

ij

− dij| + (Np − N o

p )Pm +

I(ti)Pr

(3)

N o

p(cid:88)

i=1

Nd(cid:88)

j=1

Np(cid:88)

i=1

p is the observed number of storms, Nd is the number of attributes per storm, do

where N o
ij is the
observed value of attribute j of storm i, dij is the true value of attribute j of storm i, Np is the true
number of storms, Pm is the penalty for missing a storm, ti is the number of decision epochs since
storm i was last scanned, Pr is the penalty for not scanning a storm at least once within Tr decision
epochs, and I(ti) is an indicator function that equals 1 when ti ≥ Tr. The quality with which a
storm is observed determines the difference between the observed and true values of its attributes.

We use linear Sarsa(λ) [15] as the reinforcement learning algorithm to solve the MDP for the radar
control problem. To obtain the basis functions, we use tile coding [13, 14]. Rather than deﬁning
tilings over the entire state space, we deﬁne a separate set of tilings for each of the state variables.

4 Evaluation

4.1 Simulation Environment

We consider radars with both 10 and 30km radii as in [5, 17]. Two overlapping radars are placed
in a 90km × 60km rectangle, one at (30km, 30km) and one at (60km, 30km). A new storm cell
can appear anywhere within the rectangle and a maximum number of cells can be present on any
decision epoch. When the (x, y) center of a storm cell is no longer within range of any radar, the
cell is removed from the environment. Following [5], we use a 30-second decision epoch.
We derive the maximum storm cell radius from [11], which uses 2.83km as “the radius from the cell
−1 of the cell center intensity.” We then permit a
center within which the intensity is greater than e
storm cell’s radius to range from 1 to 4 km. To determine the range of storm cell velocities, we use 39
real storm cell tracks obtained from meteorologists. Each track is a series of (latitude, longitude)
coordinates. We ﬁrst compute the differences in latitude and longitude, and in time, between suc-
cessive pairs of points. We then ﬁt the differences using Gaussian distributions. We obtain, in units
of km/hour, that the latitude (or x) velocity has mean 9.1 km/hr and std. dev. of 35.6 km/hr and that
the longitude (or y) velocity has mean 16.7 km/hr and std. dev. of 28.8 km/hr. To obtain a storm
cell’s (x, y) velocity, we then sample the appropriate Gaussian distribution.
To simulate the environment transitions we use a stochastic model of rainfall in which storm cell
arrivals are modeled using a spatio-temporal Poisson process, see [11, 1]. To determine the number
of new storm cells to add during a decision epoch, we sample a Poisson random variable with rate
ληδaδt with λ = 0.075 storm cells/km2 and η = 0.006 storm cells/minute from [11]. From the
radar setup we have δa = 90 · 60 km2, and from the 30-second decision epoch we have δt = 0.5
minutes. New storm cells are uniformly randomly distributed in the 90km × 60km region and we
uniformly randomly choose new storm cell attributes from their range of values. This simulates the
true state of the environment over time. The following simpliﬁed radar model determines how well
the radars observe the true environmental state under a given set of radar conﬁgurations. If a storm
cell p is scanned using a set of radar conﬁgurations Sr, the location, velocity, and radius attributes
are observed as a function of the Up(p, Sr) quality deﬁned in Section 3.1. Up(p, Sr) returns a value
u between zero and one. Then the observed value of the attribute is the true value of the attribute
plus some Gaussian noise distributed with mean zero and standard deviation (1 − u)V max/ρ where
V max is the largest positive value the attribute can take and ρ is a scaling term that will allow us to
adjust the noise variability. Since u depends on the decision epoch t, for the k-step look-ahead scan
strategy we also use σt = (1 − ut)V max/ρ to compute the measurement error covariance matrix,
R, in our Kalman ﬁlter.
We parameterize the MDP cost function as follows. We assume that any unobserved storm cell has
been observed with quality 0, hence u = 0. Summing over (1 − u)V max/ρ for all attributes with
σ = 0 gives the value Pm = 15.5667, and thus a penalty of 15.5667 is received for each unobserved
storm cell. If a storm cell is not seen within Tr = 4 decision epochs a penalty of Pr = 200 is
given. Using the value 200 ensures that if a storm cell has not been rescanned within the appropriate
amount of time, this part of the cost function will dominate.

5

We distinguish the true environmental state known only to the simulator from the observed environ-
mental state used by the scan strategies for several reasons. Although radars provide measurements
about meteorological phenomena, the true attributes of the phenomena are unknown. Poor over-
lap in a dual-Doppler area, scanning a subsector too quickly or slowly, or being unable to obtain a
sufﬁcient number of elevation scans will degrade the quality of the measurements. Consequently,
models of previously existing phenomena may contain estimation errors such as incorrect velocity,
propagating error into the future predicted locations of the phenomena. Additionally, when a radar
scans a subsector, it obtains more accurate estimates of the phenomena in that subsector than if it
had scanned a full 360◦

, but less accurate estimates of the phenomena outside the subsector.

4.2 Results

In this section we present experimental results obtained using the simulation model of the previous
section and the scan strategies described in Section 3. For the limited lookahead strategy we use
β = 0.5, κp = 0.25, κs = 0.25, and φ = 0.75. For Sarsa(λ), we use a learning rate α = 0.0005,
exploration rate  = 0.01, discount factor γ = 0.9, and eligibility decay λ = 0.3. Additionally,
we use a single tiling for each state variable. For the (x, y) location and radius tilings, we use
a granularity of 1.0; for the (x, y) velocity, phenomenon conﬁdence, and radar sector conﬁdence
tilings, we use a granularity of 0.1. When there are a maximum of four storms, we restrict Sarsa(λ)
to scanning only 180 or 360 degree sectors to reduce the time needed for convergence. Finally, all
strategies are always compared over the same true environmental state.

Figure 2(a) shows an example convergence proﬁle of Sarsa(λ) when there are at most four storms
in the environment. Figure 2(b) shows the average difference in scan quality between the learned
Sarsa(λ) strategy and sit-and-spin and 2-step strategies. When 1/ρ = 0.001 (i.e., little measurement
noise) Sarsa(λ) has the same or higher relative quality than does sit-and-spin, but signiﬁcantly lower
relative quality (0.05 to 0.15) than does the 2-step. This in part reﬂects the difﬁculty of learning
to perform as well as or better than Kalman ﬁltering. Examining the learned strategy showed that
when there was at most one storm with observation noise 1/ρ = 0.001, Sarsa(λ) learned to simply
sit-and-spin, since sector scanning conferred little beneﬁt. As the observation noise increases, the
relative difference increases for sit-and-spin, and decreases for the 2-step. Figure 2(c) shows the
average difference in cost between the learned Sarsa(λ) scan strategy and the sit-and-spin and 2-step
strategies for a 30 km radar radius. Sarsa(λ) has the lowest average cost.

Looking at the Sarsa(λ) inter-scan times, Figure 2 (d) shows that, as a consequence of the penalty for
not scanning a storm within Tr = 4 time-steps, while Sarsa(λ) may rescan fewer storm cells within
1, 2, or 3 decision epochs than do the other scan strategies, it scans almost all storm cells within
4 epochs. Note that for the sit-and-spin CDF, P [X ≤ 1] is not 1; due to noise, for example, the
measured location of a storm cell may be (expected) outside any radar footprint and consequently
the storm cell will not be observed. Thus the 2-step has more inter-scan times greater than Tr = 4
than does Sarsa(λ). Together with Figure 2(b) and (c), this implies that there is a trade-off between
inter-scan time and scan quality. We hypothesize that this trade-off occurs because increasing the
size of the scan sectors ensures that inter-scan time is minimized, but decreases the scan quality.

Other results (not shown, see [7]) examine the average difference in quality between the 1-step and 2-
step strategies for 10 km and 30 km radar radii. With a 10 km radius, the 1-step quality is essentially
the same as the 2-step quality. We hypothesize that this is a consequence of the maximum storm cell
radius, 4 km, relative to the 10 km radar radius. With a 30 km radius and at most eight storm cells,
the 2-step quality is about 0.005 better than the 1-step and about 0.07 better than sit-and-spin (recall
that quality is a value between 0 and 1). Now recall that Figure 2(b) shows that with a 30 km radius
and at most four storm cells, the 2-step quality is as much as 0.12 than sit-and-spin. This indicates
that there may be some maximum number of storms above which it is best to sit-and-spin.

Overall, depending on the environment in which the radars are deployed, there are decreasing
marginal returns for considering more than 1 or 2 future expected states. Instead, the primary value
of reinforcement learning for the radar control problem is balancing multiple conﬂicting goals, i.e.,
maximizing scan quality while minimizing inter-scan time. Implementing the learned reinforcement
learning scan strategy in a real meteorological radar network requires addressing the differences be-
tween the ofﬂine environment in which the learned strategy is trained, and the online environment
in which the strategy is deployed. Given the slow convergence time for Sarsa(λ) (on the order of

6

(a)

(c)

(b)

(d)

Figure 2: Comparing the scan strategies based on quality, cost, and inter-scan time. Recall that ρ is
a scaling term used to determine measurement noise, see Section 4.1.

days), training solely online is likely infeasible, although the time complexity could be mitigated
by using hierarchical reinforcement learning methods and semi-Markov decision process. Some
online training could be achieved by treating 360◦
scans as the true environment state. Then when
unknown states are entered, learning could be performed, alternating between 360◦
scans to gauge
the true state of the environment and exploratory scans by the reinforcement learning algorithm.

5 Related Work

Other reinforcement learning applications in large state spaces include robot soccer [12] and heli-
copter control [10]. With respect to radar control, [4] examines the problem of using agile radars
on airplanes to detect and track ground targets. They show that lookahead scan strategies for radar
tracking of a ground target outperform myopic strategies. In comparison, we consider the problem of
tracking meteorological phenomena using ground radars. [4] uses an information theoretic measure
to deﬁne the reward metric and proposes both an approximate solution to solving the MDP Bellman
equations as well as a Q-learning reinforcement learning-based solution. [16] examines where to
target radar beams and which waveform to use for electronically steered phased array radars. They
maintain a set of error covariance matrices and dynamical models for existing targets, as well as

7

0123456x 10414161820222426EpisodeAverage Cost Per Episode of 1000 StepsRadar Radius = 30km, Max 4 Storms  sit−and−spinsarsa00.010.020.030.040.050.060.070.080.090.1−0.2−0.15−0.1−0.0500.050.10.151/rAverage Difference in Scan Quality (250,000 steps)Radar Radius = 30km  2step − sarsa, max 1 storm2step − sarsa, max 4 stormssitandspin− sarsa, max 1 stormsitandspin − sarsa, max 4 storms00.010.020.030.040.050.060.070.080.090.1−0.500.511.522.533.544.51/rAverage Difference in Cost (250,000 steps)Radar Radius = 30km  2step − sarsa, max 1 storm2step − sarsa, max 4 stormssitandspin− sarsa, max 1 stormsitandspin − sarsa, max 4 storms0123456789100.80.820.840.860.880.90.920.940.960.981x = # of decision epochs between storm scansP[X <= x]Max # of Storms = 4, Radar Radius = 30km  sit−and−spin,  1/s=0.11step,  1/s=0.12step,  1/s=0.1sarsa,  1/s=0.1track existence probability density functions to model the probability that targets appear. They then
choose the scan mode for each target that has both the longest revisit time for scanning a target and
error covariance below a threshold. They do this for control 1-step and 2-steps ahead and show
that considering the environment two decision epochs ahead outperforms a 1-step look-ahead for
tracking of multiple targets.

6 Conclusions and Future Work

In this work we compared the performance of myopic and lookahead scan strategies in the context
of the meteorological radar control problem. We showed that the main beneﬁts of using a lookahead
strategy are when there are multiple meteorological phenomena in the environment, and when the
maximum radius of any phenomenon is sufﬁciently smaller than the radius of the radars. We also
showed that there is a trade-off between the average quality with which a phenomenon is scanned
and the number of decision epochs before which a phenomenon is rescanned. Overall, considering
only scan quality, a simple lookahead strategy is sufﬁcient. To additionally consider inter-scan time
(or optimize over multiple metrics of interest), a reinforcement learning strategy is useful. For future
work, rather than identifying a policy that chooses the best action to execute in a state for a single
decision epoch, it may be useful to consider actions that cover multiple epochs, as in semi-Markov
decision processes or to use controllers from robotics [3]. We would also like to incorporate more
radar and meteorological information into the transition, measurement, and cost functions.

Acknowledgments

The authors thank Don Towsley for his input. This work was supported in part by the National Sci-
ence Foundation under the Engineering Research Centers Program, award number EEC-0313747.
Any opinions, ﬁndings and conclusions or recommendations expressed in this material are those of
the author(s) and do not necessarily reﬂect those of the National Science Foundation.

References
[1] D. Cox and V. Isham. A simple spatial-temporal model of rainfall. Proceedings of the Royal Society of London. Series A, Mathematical

and Physical Sciences, 415:1849:317–328, 1988.

[2] B. Donovan and D. J. McLaughlin. Improved radar sensitivity through limited sector scanning: The DCAS approach. In Proceedings of

AMS Radar Meteorology, 2005.

[3] M. Huber and R. Grupen. A feedback control structure for on-line learning tasks. Robotics and Autonomous Systems, 22(3-4):303–315,

1997.

[4] C. Kreucher and A. O. H. III. Non-myopic approaches to scheduling agile sensors for multistage detection, tracking and identiﬁcation.

In Proceedings of ICASSP, pages 885–888, 2005.

[5] J. Kurose, E. Lyons, D. McLaughlin, D. Pepyne, B. Phillips, D. Westbrook, and M. Zink. An end-user-responsive sensor network

architecture for hazardous weather detection, prediction and response. AINTEC, 2006.

[6] C. Kwok and D. Fox. Reinforcement learning for sensing strategies. In IROS, 2004.

[7] V. Manfredi and J. Kurose. Comparison of myopic and lookahead scan strategies for meteorological radars. Technical Report U of

Massachusetts Amherst, 2006-62, 2006.

[8] V. Manfredi, S. Mahadevan, and J. Kurose. Switching kalman ﬁlters for prediction and tracking in an adaptive meteorological sensing

network. In IEEE SECON, 2005.

[9] K. Murphy. A survey of POMDP solution techniques. Technical Report U.C. Berkeley, 2000.

[10] A. Ng, A. Coates, M. Diel, V. Ganapathi, J. Schulte, B. Tse, E. Berger, and E. Liang.

Inverted autonomous helicopter ﬂight via

reinforcement learning. In International Symposium on Experimental Robotics, 2004.

[11]

I. Rodrigues-Iturbe and P. Eagleson. Mathematical models of rainstorm events in space and time. Water Resources Research, 23:1:181–
190, 1987.

[12] P. Stone, R. Sutton, and G. Kuhlmann. Reinforcement learning for robocup-soccer keepaway. Adaptive Behavior, 3, 2005.

[13] R. Sutton. Tile coding software. http://rlai.cs.ualberta.ca/RLAI/RLtoolkit/tiles.html.

[14] R. Sutton. Generalization in reinforcement learning: Successful examples using sparse coarse coding. In NIPS, 1996.

[15] R. Sutton and A. Barto. Reinforcement Learning: An Introduction. MIT Press, Cambridge, Massachusetts, 1998.

[16] S. Suvorova, D. Musicki, B. Moran, S. Howard, and B. L. Scala. Multi step ahead beam and waveform scheduling for tracking of

manoeuvering targets in clutter. In Proceedings of ICASSP, 2005.

[17] J. M. Trabal, B. C. Donovan, M. Vega, V. Marrero, D. J. McLaughlin, and J. G. Colom. Puerto Rico student test bed applications and

system requirements document development. In Proceedings of the 9th International Conference on Engineering Education, 2006.

8

"
726,2007,The Tradeoffs of Large Scale Learning,This contribution develops a theoretical framework that takes into account the effect of approximate optimization on learning algorithms. The analysis shows distinct tradeoffs for the case of small-scale and large-scale learning problems. Small-scale learning problems are subject to the usual approximation--estimation tradeoff. Large-scale learning problems are subject to a qualitatively different tradeoff involving the computational complexity of the underlying optimization algorithms in non-trivial ways.,"The Tradeoffs of Large Scale Learning

L´eon Bottou

NEC laboratories of America
Princeton, NJ 08540, USA

Olivier Bousquet

Google Z¨urich

8002 Zurich, Switzerland

leon@bottou.org

olivier.bousquet@m4x.org

Abstract

This contribution develops a theoretical framework that takes into account the
effect of approximate optimization on learning algorithms. The analysis shows
distinct tradeoffs for the case of small-scale and large-scale learning problems.
Small-scale learning problems are subject to the usual approximation–estimation
tradeoff. Large-scale learning problems are subject to a qualitatively different
tradeoff involving the computational complexity of the underlying optimization
algorithms in non-trivial ways.

1 Motivation

The computational complexity of learning algorithms has seldom been taken into account by the
learning theory. Valiant [1] states that a problem is “learnable” when there exists a probably approx-
imatively correct learning algorithm with polynomial complexity. Whereas much progress has been
made on the statistical aspect (e.g., [2, 3, 4]), very little has been told about the complexity side of
this proposal (e.g., [5].)

Computational complexity becomes the limiting factor when one envisions large amounts of training
data. Two important examples come to mind:

• Data mining exists because competitive advantages can be achieved by analyzing the
masses of data that describe the life of our computerized society. Since virtually every
computer generates data, the data volume is proportional to the available computing power.
Therefore one needs learning algorithms that scale roughly linearly with the total volume
of data.

• Artiﬁcial intelligence attempts to emulate the cognitive capabilities of human beings. Our
biological brains can learn quite efﬁciently from the continuous streams of perceptual data
generated by our six senses, using limited amounts of sugar as a source of power. This
observation suggests that there are learning algorithms whose computing time requirements
scale roughly linearly with the total volume of data.

This contribution ﬁnds its source in the idea that approximate optimization algorithms might be
sufﬁcient for learning purposes. The ﬁrst part proposes new decomposition of the test error where
an additional term represents the impact of approximate optimization. In the case of small-scale
learning problems, this decomposition reduces to the well known tradeoff between approximation
error and estimation error. In the case of large-scale learning problems, the tradeoff is more com-
plex because it involves the computational complexity of the learning algorithm. The second part
explores the asymptotic properties of the large-scale learning tradeoff for various prototypical learn-
ing algorithms under various assumptions regarding the statistical estimation rates associated with
the chosen objective functions. This part clearly shows that the best optimization algorithms are not
necessarily the best learning algorithms. Maybe more surprisingly, certain algorithms perform well
regardless of the assumed rate for the statistical estimation error.

2 Approximate Optimization

2.1 Setup

Following [6, 2], we consider a space of input-output pairs (x, y) ∈ X × Y endowed with a proba-
bility distribution P (x, y). The conditional distribution P (y|x) represents the unknown relationship
between inputs and outputs. The discrepancy between the predicted output ˆy and the real output
y is measured with a loss function ℓ(ˆy, y). Our benchmark is the function f ∗ that minimizes the
expected risk

that is,

E(f ) =Z ℓ(f (x), y) dP (x, y) = E [ℓ(f (x), y)],

f ∗(x) = arg min

E [ ℓ(ˆy, y)| x].

ˆy

Although the distribution P (x, y) is unknown, we are given a sample S of n independently drawn
training examples (xi, yi), i = 1 . . . n. We deﬁne the empirical risk

En(f ) =

1
n

n

Xi=1

ℓ(f (xi), yi) = En[ℓ(f (x), y)].

Our ﬁrst learning principle consists in choosing a family F of candidate prediction functions and
ﬁnding the function fn = arg minf ∈F En(f ) that minimizes the empirical risk. Well known com-
binatorial results (e.g., [2]) support this approach provided that the chosen family F is sufﬁciently
restrictive. Since the optimal function f ∗ is unlikely to belong to the family F, we also deﬁne
F = arg minf ∈F E(f ). For simplicity, we assume that f ∗, f ∗
F and fn are well deﬁned and unique.
f ∗
We can then decompose the excess error as

E [E(fn) − E(f ∗)] = E [E(f ∗

F ) − E(f ∗)] + E [E(fn) − E(f ∗

(1)
where the expectation is taken with respect to the random choice of training set. The approximation
error Eapp measures how closely functions in F can approximate the optimal solution f ∗. The
estimation error Eest measures the effect of minimizing the empirical risk En(f ) instead of the
expected risk E(f ). The estimation error is determined by the number of training examples and by
the capacity of the family of functions [2]. Large families1 of functions have smaller approximation
errors but lead to higher estimation errors. This tradeoff has been extensively discussed in the
literature [2, 3] and lead to excess error that scale between the inverse and the inverse square root of
the number of examples [7, 8].

F )] = Eapp + Eest ,

2.2 Optimization Error

Finding fn by minimizing the empirical risk En(f ) is often a computationally expensive operation.
Since the empirical risk En(f ) is already an approximation of the expected risk E(f ), it should
not be necessary to carry out this minimization with great accuracy. For instance, we could stop an
iterative optimization algorithm long before its convergence.
Let us assume that our minimization algorithm returns an approximate solution ˜fn such that

En( ˜fn) < En(fn) + ρ

where ρ ≥ 0 is a predeﬁned tolerance. An additional term Eopt = E(cid:2)E( ˜fn) − E(fn)(cid:3) then appears
in the decomposition of the excess error E = E(cid:2)E( ˜fn) − E(f ∗)(cid:3):

F ) − E(f ∗)] + E [E(fn) − E(f ∗

E = E [E(f ∗

= Eapp + Eest + Eopt.

(2)
We call this additional term optimization error. It reﬂects the impact of the approximate optimization
on the generalization performance. Its magnitude is comparable to ρ (see section 3.1.)

F )] + E(cid:2)E( ˜fn) − E(fn)(cid:3)

1We often consider nested families of functions of the form Fc = {f ∈ H, Ω(f ) ≤ c}. Then, for each
value of c, function fn is obtained by minimizing the regularized empirical risk En(f ) + λΩ(f ) for a suitable
choice of the Lagrange coefﬁcient λ. We can then control the estimation-approximation tradeoff by choosing
λ instead of c.

2.3 The Approximation–Estimation–Optimization Tradeoff

This decomposition leads to a more complicated compromise. It involves three variables and two
constraints. The constraints are the maximal number of available training example and the maximal
computation time. The variables are the size of the family of functions F, the optimization accuracy
ρ, and the number of examples n. This is formalized by the following optimization problem.

min
F ,ρ,n

E = Eapp + Eest + Eopt

The number n of training examples is a variable because we could choose to use only a subset of
the available training examples in order to complete the optimization within the alloted time. This
happens often in practice. Table 1 summarizes the typical evolution of the quantities of interest with
the three variables F, n, and ρ increase.

subject to (cid:26)

n ≤ nmax
T (F, ρ, n) ≤ Tmax

(3)

Table 1: Typical variations when F, n, and ρ increase.

n

ρ

F
(approximation error) ց
(estimation error)
(optimization error)
(computation time)

ր ց
· · ·
· · · ր
ր ր ց

Eapp
Eest
Eopt
T

The solution of the optimization program (3) depends critically of which budget constraint is active:
constraint n < nmax on the number of examples, or constraint T < Tmax on the training time.

• We speak of small-scale learning problem when (3) is constrained by the maximal number
of examples nmax. Since the computing time is not limited, we can reduce the optimization
error Eopt to insigniﬁcant levels by choosing ρ arbitrarily small. The excess error is then
dominated by the approximation and estimation errors, Eapp and Eest. Taking n = nmax,
we recover the approximation-estimation tradeoff that is the object of abundant literature.
• We speak of large-scale learning problem when (3) is constrained by the maximal com-
puting time Tmax. Approximate optimization, that is choosing ρ > 0, possibly can achieve
better generalization because more training examples can be processed during the allowed
time. The speciﬁcs depend on the computational properties of the chosen optimization
algorithm through the expression of the computing time T (F, ρ, n).

3 The Asymptotics of Large-scale Learning

In the previous section, we have extended the classical approximation-estimation tradeoff by taking
into account the optimization error. We have given an objective criterion to distiguish small-scale
and large-scale learning problems. In the small-scale case, we recover the classical tradeoff between
approximation and estimation. The large-scale case is substantially different because it involves
the computational complexity of the learning algorithm. In order to clarify the large-scale learning
tradeoff with sufﬁcient generality, this section makes several simpliﬁcations:

• We are studying upper bounds of the approximation, estimation, and optimization er-
rors (2).
It is often accepted that these upper bounds give a realistic idea of the actual
convergence rates [9, 10, 11, 12]. Another way to ﬁnd comfort in this approach is to say
that we study guaranteed convergence rates instead of the possibly pathological special
cases.

• We are studying the asymptotic properties of the tradeoff when the problem size increases.
Instead of carefully balancing the three terms, we write E = O(Eapp) + O(Eest) + O(Eopt)
and only need to ensure that the three terms decrease with the same asymptotic rate.

• We are considering a ﬁxed family of functions F and therefore avoid taking into account
the approximation error Eapp. This part of the tradeoff covers a wide spectrum of practical
realities such as choosing models and choosing features. In the context of this work, we do

not believe we can meaningfully address this without discussing, for instance, the thorny
issue of feature selection. Instead we focus on the choice of optimization algorithm.

• Finally, in order to keep this paper short, we consider that the family of functions F is
linearly parametrized by a vector w ∈ Rd. We also assume that x, y and w are bounded,
ensuring that there is a constant B such that 0 ≤ ℓ(fw(x), y) ≤ B and ℓ(·, y) is Lipschitz.

We ﬁrst explain how the uniform convergence bounds provide convergence rates that take the op-
timization error into account. Then we discuss and compare the asymptotic learning properties of
several optimization algorithms.

3.1 Convergence of the Estimation and Optimization Errors

The optimization error Eopt depends directly on the optimization accuracy ρ. However, the accuracy
ρ involves the empirical quantity En( ˜fn) − En(fn), whereas the optimization error Eopt involves
its expected counterpart E( ˜fn) − E(fn). This section discusses the impact on the optimization
error Eopt and of the optimization accuracy ρ on generalization bounds that leverage the uniform
convergence concepts pioneered by Vapnik and Chervonenkis (e.g., [2].)
In this discussion, we use the letter c to refer to any positive constant. Multiple occurences of the
letter c do not necessarily imply that the constants have identical values.

3.1.1 Simple Uniform Convergence Bounds

Recall that we assume that F is linearly parametrized by w ∈ Rd. Elementary uniform convergence
results then state that

E»sup

f ∈F

|E(f ) − En(f )|– ≤ cr d

n

,

where the expectation is taken with respect to the random choice of the training set.2 This result
immediately provides a bound on the estimation error:

Eest = Eˆ`E(fn) − En(fn)´ +`En(fn) − En(f ∗

F )´ +`En(f ∗

≤ 2 E» sup

f ∈F

|E(f ) − En(f )|– ≤ cr d

n

.

F ) − E(f ∗

F )´˜

This same result also provides a combined bound for the estimation and optimization errors:

Eest + Eopt = EˆE( ˜fn) − En( ˜fn)˜ + EˆEn( ˜fn) − En(fn)˜
= c ρ +r d
n! .

+ ρ + 0 + cr d

≤ cr d

+ E [En(fn) − En(f ∗

F )] + E [En(f ∗

F ) − E(f ∗

n

n

F )]

Unfortunately, this convergence rate is known to be pessimistic in many important cases. More
sophisticated bounds are required.

3.1.2 Faster Rates in the Realizable Case

When the loss functions ℓ(ˆy, y) is positive, with probability 1 − e−τ for any τ > 0, relative uniform
convergence bounds state that

E(f ) − En(f )

sup
f ∈F

pE(f )

≤ cr d

n

log

n
d

+

τ
n

.

This result is very useful because it provides faster convergence rates O(log n/n) in the realizable
case, that is when ℓ(fn(xi), yi) = 0 for all training examples (xi, yi). We have then En(fn) = 0,
En( ˜fn) ≤ ρ, and we can write

E( ˜fn) − ρ ≤ cqE( ˜fn)r d

n

log

n
d

+

τ
n

.

2Although the original Vapnik-Chervonenkis bounds have the form cq d

be eliminated using the “chaining” technique (e.g., [10].)

n log n

d , the logarithmic term can

Viewing this as a second degree polynomial inequality in variableqE( ˜fn), we obtain

E( ˜fn) ≤ c„ρ +

d
n

log

+

n
d

τ

n« .

Integrating this inequality using a standard technique (see, e.g., [13]), we obtain a better convergence
rate of the combined estimation and optimization error:

Eest + Eopt = EhE( ˜fn) − E(f ∗

F )i ≤ EhE( ˜fn)i = c„ρ +

d
n

log

n

d« .

3.1.3 Fast Rate Bounds

Many authors (e.g., [10, 4, 12]) obtain fast statistical estimation rates in more general conditions.
These bounds have the general form

Eapp + Eest ≤ c(cid:18) Eapp +(cid:18) d

n

log

n

d(cid:19)α(cid:19) for

1
2

≤ α ≤ 1 .

(4)

This result holds when one can establish the following variance condition:

∀f ∈ F Eh(cid:0)ℓ(f (X), Y ) − ℓ(f ∗

F (X), Y )(cid:1)2i ≤ c (cid:18) E(f ) − E(f ∗

The convergence rate of (4) is described by the exponent α which is determined by the quality of
the variance bound (5). Works on fast statistical estimation identify two main ways to establish such
a variance condition.

.

(5)

α

F )(cid:19)2− 1

• Exploiting the strict convexity of certain loss functions [12, theorem 12]. For instance, Lee

et al. [14] establish a O(log n/n) rate using the squared loss ℓ(ˆy, y) = (ˆy − y)2.

• Making assumptions on the data distribution. In the case of pattern recognition problems,
for instance, the “Tsybakov condition” indicates how cleanly the posterior distributions
P (y|x) cross near the optimal decision boundary [11, 12]. The realizable case discussed in
section 3.1.2 can be viewed as an extreme case of this.

Despite their much greater complexity, fast rate estimation results can accomodate the optimization
accuracy ρ using essentially the methods illustrated in sections 3.1.1 and 3.1.2. We then obtain a
bound of the form

E = Eapp + Eest + Eopt = EhE( ˜fn) − E(f ∗)i ≤ c(cid:18) Eapp +(cid:18) d

n

log

n

d(cid:19)α

+ ρ(cid:19) .

(6)

For instance, a general result with α = 1 is provided by Massart [13, theorem 4.2]. Combining this
result with standard bounds on the complexity of classes of linear functions (e.g., [10]) yields the
following result:

n
d
See also [15, 4] for more bounds taking into account the optimization accuracy.

E = Eapp + Eest + Eopt = EhE( ˜fn) − E(f ∗)i ≤ c(cid:18) Eapp +

d
n

log

+ ρ(cid:19) .

(7)

3.2 Gradient Optimization Algorithms

We now discuss and compare the asymptotic learning properties of four gradient optimization algo-
rithms. Recall that the family of function F is linearly parametrized by w ∈ Rd. Let w∗
F and wn
correspond to the functions f ∗
F and fn deﬁned in section 2.1. In this section, we assume that the
functions w 7→ ℓ(fw(x), y) are convex and twice differentiable with continuous second derivatives.
Convexity ensures that the empirical const function C(w) = En(fw) has a single minimum.
Two matrices play an important role in the analysis: the Hessian matrix H and the gradient covari-
ance matrix G, both measured at the empirical optimum wn.

H =

∂2C

∂w2 (wn) = En(cid:20) ∂2ℓ(fwn (x), y)

G = En""(cid:18) ∂ℓ(fwn (x), y)

∂w

∂w2

(cid:21) ,
(cid:19)(cid:18) ∂ℓ(fwn (x), y)

∂w

(8)

(9)

(cid:19)′# .

The relation between these two matrices depends on the chosen loss function. In order to summarize
them, we assume that there are constants λmax ≥ λmin > 0 and ν > 0 such that, for any η > 0,
we can choose the number of examples n large enough to ensure that the following assertion is true
with probability greater than 1 − η :

tr(G H −1) ≤ ν

and

EigenSpectrum(H) ⊂ [ λmin , λmax ]

(10)

The condition number κ = λmax/λmin is a good indicator of the difﬁculty of the optimization [16].
The condition λmin > 0 avoids complications with stochastic gradient algorithms. Note that this
condition only implies strict convexity around the optimum. For instance, consider the loss func-
tion ℓ is obtained by smoothing the well known hinge loss ℓ(z, y) = max{0, 1 − yz} in a small
neighborhood of its non-differentiable points. Function C(w) is then piecewise linear with smoothed
edges and vertices. It is not strictly convex. However its minimum is likely to be on a smoothed
vertex with a non singular Hessian. When we have strict convexity, the argument of [12, theorem 12]
yields fast estimation rates α ≈ 1 in (4) and (6). This is not necessarily the case here.
The four algorithm considered in this paper use information about the gradient of the cost function
to iteratively update their current estimate w(t) of the parameter vector.

• Gradient Descent (GD) iterates

w(t + 1) = w(t) − η

∂C
∂w

(w(t)) = w(t) − η

1
n

∂
∂w

n

Xi=1

ℓ(cid:0)fw(t)(xi), yi(cid:1)

where η > 0 is a small enough gain. GD is an algorithm with linear convergence [16].
When η = 1/λmax, this algorithm requires O(κ log(1/ρ)) iterations to reach accuracy ρ.
The exact number of iterations depends on the choice of the initial parameter vector.

• Second Order Gradient Descent (2GD) iterates

w(t + 1) = w(t) − H −1 ∂C
∂w

(w(t)) = w(t) −

1
n

H −1

∂
∂w

n

Xi=1

ℓ(cid:0)fw(t)(xi), yi(cid:1)

where matrix H −1 is the inverse of the Hessian matrix (8). This is more favorable than
Newton’s algorithm because we do not evaluate the local Hessian at each iteration but
simply assume that we know in advance the Hessian at the optimum. 2GD is a superlinear
optimization algorithm with quadratic convergence [16]. When the cost is quadratic, a
single iteration is sufﬁcient. In the general case, O(log log(1/ρ)) iterations are required to
reach accuracy ρ.

• Stochastic Gradient Descent (SGD) picks a random training example (xt, yt) at each

iteration and updates the parameter w on the basis of this example only,

w(t + 1) = w(t) −

η
t

∂
∂w

ℓ(cid:0)fw(t)(xt), yt(cid:1).

Murata [17, section 2.2], characterizes the mean ES[w(t)] and variance VarS[w(t)] with
respect to the distribution implied by the random examples drawn from the training set S at
each iteration. Applying this result to the discrete training set distribution for η = 1/λmin,
we have δw(t)2 = O(1/t) where δw(t) is a shorthand notation for w(t) − wn.
We can then write

≤ tr(GH)

ES[ C(w(t)) − inf C ] = ESˆtr`H δw(t) δw(t)′´˜ + o` 1
t´
t + o` 1
t´ .

= tr` H ES[δw(t)] ES[δw(t)]′ + H VarS[w(t)]´ + o` 1
t´

Therefore the SGD algorithm reaches accuracy ρ after less than νκ2/ρ + o(1/ρ) iterations
on average. The SGD convergence is essentially limited by the stochastic noise induced
by the random choice of one example at each iteration. Neither the initial value of the
parameter vector w nor the total number of examples n appear in the dominant term of this
bound! When the training set is large, one could reach the desired accuracy ρ measured on
the whole training set without even visiting all the training examples. This is in fact a kind
of generalization bound.

t + o` 1

t´ ≤ νκ2

(11)

Table 2: Asymptotic results for gradient algorithms (with probability 1). Compare the second
last column (time to optimize) with the last column (time to reach the excess test error ǫ).
Legend: n number of examples; d parameter dimension; κ, ν see equation (10).

Algorithm Cost of one
iteration

Iterations
to reach ρ

Time to reach

accuracy ρ

Time to reach

E ≤ c (Eapp + ε)

GD

2GD

SGD

2SGD

O(nd)

O(cid:16)κ log 1
O(cid:16)ndκ log 1
ρ(cid:17)
ρ(cid:17)
O(cid:0)d2 + nd(cid:1) O(cid:16)log log 1
ρ(cid:17) O(cid:16)(cid:0)d2 + nd(cid:1) log log 1
O(cid:16) dνκ2
ρ (cid:17)
ρ + o(cid:16) 1
ρ(cid:17)
O(cid:16) d2ν
ρ + o(cid:16) 1
ρ (cid:17)
ρ(cid:17)
O(cid:0)d2(cid:1)

O(d)

νκ2

ν

ρ(cid:17) O(cid:16) d2

ε log log 1

ε(cid:17)
ε1/α log2 1

ε1/α log 1

O(cid:16) d2 κ
O(cid:16) d ν κ2
ε (cid:17)
O(cid:16) d2 ν
ε (cid:17)

ε(cid:17)

• Second Order Stochastic Gradient Descent (2SGD) replaces the gain η by the inverse of

the Hessian matrix H:

w(t + 1) = w(t) −

1
t

H −1 ∂
∂w

ℓ(cid:0)fw(t)(xt), yt(cid:1).

Unlike standard gradient algorithms, using the second order information does not change
the inﬂuence of ρ on the convergence rate but improves the constants. Using again [17,
theorem 4], accuracy ρ is reached after ν/ρ + o(1/ρ) iterations.

For each of the four gradient algorithms, the ﬁrst three columns of table 2 report the time for a single
iteration, the number of iterations needed to reach a predeﬁned accuracy ρ, and their product, the
time needed to reach accuracy ρ. These asymptotic results are valid with probability 1, since the
probability of their complement is smaller than η for any η > 0.
The fourth column bounds the time necessary to reduce the excess error E below c (Eapp +ε) where c

is the constant from (6). This is computed by observing that choosing ρ ∼` d

the fastest rate for ε, with minimal computation time. We can then use the asymptotic equivalences
ρ ∼ ε and n ∼ d
ε . Setting the fourth column expressions to Tmax and solving for ǫ yields
the best excess error achieved by each algorithm within the limited time Tmax . This provides the
asymptotic solution of the Estimation–Optimization tradeoff (3) for large scale problems satisfying
our assumptions.

d´α in (6) achieves

ε1/α log 1

n log n

These results clearly show that the generalization performance of large-scale learning systems de-
pends on both the statistical properties of the estimation procedure and the computational properties
of the chosen optimization algorithm. Their combination leads to surprising consequences:

• The SGD and 2SGD results do not depend on the estimation rate α. When the estimation
rate is poor, there is less need to optimize accurately. That leaves time to process more
examples. A potentially more useful interpretation leverages the fact that (11) is already a
kind of generalization bound: its fast rate trumps the slower rate assumed for the estimation
error.

• Second order algorithms bring little asymptotical improvements in ε. Although the super-
linear 2GD algorithm improves the logarithmic term, all four algorithms are dominated by
the polynomial term in (1/ε). However, there are important variations in the inﬂuence of
the constants d, κ and ν. These constants are very important in practice.

• Stochastic algorithms (SGD, 2SGD) yield the best generalization performance despite be-
ing the worst optimization algorithms. This had been described before [18] and observed
in experiments.

In contrast, since the optimization error Eopt of small-scale learning systems can be reduced to
insigniﬁcant levels, their generalization performance is solely determined by the statistical properties
of their estimation procedure.

4 Conclusion
Taking in account budget constraints on both the number of examples and the computation time,
we ﬁnd qualitative differences between the generalization performance of small-scale learning sys-
tems and large-scale learning systems. The generalization properties of large-scale learning systems
depend on both the statistical properties of the estimation procedure and the computational proper-
ties of the optimization algorithm. We illustrate this fact with some asymptotic results on gradient
algorithms.

Considerable reﬁnements of this framework can be expected. Extending the analysis to regular-
ized risk formulations would make results on the complexity of primal and dual optimization algo-
rithms [19, 20] directly exploitable. The choice of surrogate loss function [7, 12] could also have a
non-trivial impact in the large-scale case.

Acknowledgments Part of this work was funded by NSF grant CCR-0325463.

References
[1] Leslie G. Valiant. A theory of learnable. Proc. of the 1984 STOC, pages 436–445, 1984.
[2] Vladimir N. Vapnik. Estimation of Dependences Based on Empirical Data. Springer Series in Statistics.

Springer-Verlag, Berlin, 1982.

[3] St´ephane Boucheron, Olivier Bousquet, and G´abor Lugosi. Theory of classiﬁcation: a survey of recent

advances. ESAIM: Probability and Statistics, 9:323–375, 2005.

[4] Peter L. Bartlett and Shahar Mendelson. Empirical minimization. Probability Theory and Related Fields,

135(3):311–334, 2006.

[5] J. Stephen Judd. On the complexity of loading shallow neural networks. Journal of Complexity, 4(3):177–

192, 1988.

[6] Richard O. Duda and Peter E. Hart. Pattern Classiﬁcation And Scene Analysis. Wiley and Son, 1973.
[7] Tong Zhang. Statistical behavior and consistency of classiﬁcation methods based on convex risk mini-

mization. The Annals of Statistics, 32:56–85, 2004.

[8] Clint Scovel and Ingo Steinwart. Fast rates for support vector machines. In Peter Auer and Ron Meir,
editors, Proceedings of the 18th Conference on Learning Theory (COLT 2005), volume 3559 of Lecture
Notes in Computer Science, pages 279–294, Bertinoro, Italy, June 2005. Springer-Verlag.

[9] Vladimir N. Vapnik, Esther Levin, and Yann LeCun. Measuring the VC-dimension of a learning machine.

Neural Computation, 6(5):851–876, 1994.

[10] Olivier Bousquet. Concentration Inequalities and Empirical Processes Theory Applied to the Analysis of

Learning Algorithms. PhD thesis, Ecole Polytechnique, 2002.

[11] Alexandre B. Tsybakov. Optimal aggregation of classiﬁers in statistical learning. Annals of Statististics,

32(1), 2004.

[12] Peter L. Bartlett, Michael I. Jordan, and Jon D. McAuliffe. Convexity, classiﬁcation and risk bounds.

Journal of the American Statistical Association, 101(473):138–156, March 2006.

[13] Pascal Massart. Some applications of concentration inequalities to statistics. Annales de la Facult´e des

Sciences de Toulouse, series 6, 9(2):245–303, 2000.

[14] Wee S. Lee, Peter L. Bartlett, and Robert C. Williamson. The importance of convexity in learning with

squared loss. IEEE Transactions on Information Theory, 44(5):1974–1980, 1998.

[15] Shahar Mendelson. A few notes on statistical learning theory. In Shahar Mendelson and Alexander J.
Smola, editors, Advanced Lectures in Machine Learning, volume 2600 of Lecture Notes in Computer
Science, pages 1–40. Springer-Verlag, Berlin, 2003.

[16] John E. Dennis, Jr. and Robert B. Schnabel. Numerical Methods For Unconstrained Optimization and

Nonlinear Equations. Prentice-Hall, Inc., Englewood Cliffs, New Jersey, 1983.

[17] Noboru Murata. A statistical study of on-line learning. In David Saad, editor, Online Learning and Neural

Networks. Cambridge University Press, Cambridge, UK, 1998.
[18] L´eon Bottou and Yann Le Cun. Large scale online learning.

In Sebastian Thrun, Lawrence K. Saul,
and Bernhard Sch¨olkopf, editors, Advances in Neural Information Processing Systems 16. MIT Press,
Cambridge, MA, 2004.

[19] Thorsten Joachims. Training linear SVMs in linear time. In Proceedings of KDD’06, Philadelphia, PA,

USA, August 20-23 2006. ACM.

[20] Don Hush, Patrick Kelly, Clint Scovel, and Ingo Steinwart. QP algorithms with guaranteed accuracy and

run time for support vector machines. Journal of Machine Learning Research, 7:733–769, 2006.

"
674,2007,A learning framework for nearest neighbor search,Can we leverage learning techniques to build a fast nearest-neighbor (NN) retrieval data structure? We present a general learning framework for the NN problem in which sample queries are used to learn the parameters of a data structure that minimize the retrieval time and/or the miss rate. We explore the potential of this novel framework through two popular NN data structures: KD-trees and the rectilinear structures employed by locality sensitive hashing. We derive a generalization theory for these data structure classes and present simple learning algorithms for both. Experimental results reveal that learning often improves on the already strong performance of these data structures.,"A Learning Framework for Nearest Neighbor Search

Lawrence Cayton

Department of Computer Science
University of California, San Diego

lcayton@cs.ucsd.edu

Sanjoy Dasgupta

Department of Computer Science
University of California, San Diego

dasgupta@cs.ucsd.edu

Abstract

Can we leverage learning techniques to build a fast nearest-neighbor (ANN) re-
trieval data structure? We present a general learning framework for the NN prob-
lem in which sample queries are used to learn the parameters of a data structure
that minimize the retrieval time and/or the miss rate. We explore the potential of
this novel framework through two popular NN data structures: KD-trees and the
rectilinear structures employed by locality sensitive hashing. We derive a gener-
alization theory for these data structure classes and present simple learning algo-
rithms for both. Experimental results reveal that learning often improves on the
already strong performance of these data structures.

1 Introduction

Nearest neighbor (NN) searching is a fundamental operation in machine learning, databases, signal
processing, and a variety of other disciplines. We have a database of points X = {x1, . . . , xn}, and
on an input query q, we hope to return the nearest (or approximately nearest, or k-nearest) point(s)
to q in X using some similarity measure.
A tremendous amount of research has been devoted to designing data structures for fast NN retrieval.
Most of these structures are based on some clever partitioning of the space and a few have bounds
(typically worst-case) on the number of distance calculations necessary to query it.
In this work, we propose a novel approach to building an efﬁcient NN data structure based on
learning. In contrast to the various data structures built using geometric intuitions, this learning
framework allows one to construct a data structure by directly minimizing the cost of querying it.
In our framework, a sample query set guides the construction of the data structure containing the
database. In the absence of a sample query set, the database itself may be used as a reasonable prior.
The problem of building a NN data structure can then be cast as a learning problem:

Learn a data structure that yields efﬁcient retrieval times on the sample queries
and is simple enough to generalize well.

A major beneﬁt of this framework is that one can seamlessly handle situations where the query
distribution is substantially different from the distribution of the database.
We consider two different function classes that have performed well in NN searching: KD-trees
and the cell structures employed by locality sensitive hashing. The known algorithms for these
data structures do not, of course, use learning to choose the parameters. Nevertheless, we can
examine the generalization properties of a data structure learned from one of these classes. We
derive generalization bounds for both of these classes in this paper.
Can the framework be practically applied? We present very simple learning algorithms for both of
these data structure classes that exhibit improved performance over their standard counterparts.

1

2 Related work

There is a voluminous literature on data structures for nearest neighbor search, spanning several
academic communities. Work on efﬁcient NN data structures can be classiﬁed according to two
criteria: whether they return exact or approximate answers to queries; and whether they merely
assume the distance function is a metric or make a stronger assumption (usually that the data are
Euclidean). The framework we describe in this paper applies to all these methods, though we focus
in particular on data structures for RD.
Perhaps the most popular data structure for nearest neighbor search in RD is the simple and con-
venient KD-tree [1], which has enjoyed success in a vast range of applications. Its main downside
is that its performance is widely believed to degrade rapidly with increasing dimension. Variants
of the data structure have been developed to ameliorate this and other problems [2], though high-
dimensional databases continue to be challenging. One recent line of work suggests randomly pro-
jecting points in the database down to a low-dimensional space, and then using KD-trees [3, 4].
Locality sensitive hashing (LSH) has emerged as a promising option for high-dimensional NN search
in RD [5]. It has strong theoretical guarantees for databases of arbitrary dimensionality, though they
are for approximate NN search. We review both KD-trees and LSH in detail later.
For data in metric spaces, there are several schemes based on repeatedly applying the triangle in-
equality to eliminate portions of the space from consideration; these include Orchard’s algorithm
[6] and AESA [7]. Metric trees [8] and the recently suggested spill trees [3] are based on similar
ideas and are related to KD-trees. A recent trend is to look for data structures that are attuned to the
intrinsic dimension, e.g. [9]. See the excellent survey [10] for more information.
There has been some work on building a data structure for a particular query distribution [11];
this line of work is perhaps most similar to ours. Indeed, we discovered at the time of press that the
algorithm for KD-trees we describe appeared previously in [12]. Nevertheless, the learning theoretic
approach in this paper is novel; the study of NN data structures through the lens of generalization
ability provides a fundamentally different theoretical basis for NN search with important practical
implications.

3 Learning framework

In this section we formalize a learning framework for NN search. This framework is quite general
and will hopefully be of use to algorithmic developments in NN searching beyond those presented
in this paper.
Let X = {x1, . . . , xn} denote the database and Q the space from which queries are drawn. A
typical example is X ⊂ RD and Q = RD. We take a nearest neighbor data structure to be a
mapping f : Q → 2X; the interpretation is we compute distances only to f(q), not all of X. For
example, the structure underlying LSH partitions RD into cells and a query is assigned to the subset
of X that falls into the same cell.
What quantities are we interested in optimizing? We want to only compute distances to a small
fraction of the database on a query; and, in the case of probabilistic algorithms, we want a high
probability of success. More precisely, we hope to minimize the following two quantities for a data
structure f:

• The fraction of X that we need to compute distances to:
sizef (q) ≡ |f(q)|

.

n

• The fraction of a query’s k nearest neighbors that are missed:
missf (q) ≡ |Γk(q) \ f(q)|

k

(Γk(q) denotes the k nearest neighbors of q in X).

2

In -approximate NN search, we only require a point x such that d(q, x) ≤ (1 + )d(q, X), so we
instead use an approximate miss rate:

missf (q) ≡ 1 [(cid:64)x ∈ f(q) such that d(q, x) ≤ (1 + )d(q, X)] .

None of the previously discussed data structures are built by explicitly minimizing these quantities,
though there are known bounds for some. Why not? One reason is that research has typically
focused on worst-case sizef and missf rates, which require minimizing these functions over all
q ∈ Q. Q is typically inﬁnite of course.
In this work, we instead focus on average-case sizef and missf rates—i.e. we assume q is a draw
from some unknown distribution D on Q and hope to minimize

Eq∼D [sizef (q)]

and Eq∼D [missf (q)] .

To do so, we assume that we are given a sample query set Q = {q1, . . . , qm} drawn iid from D. We
attempt to build f minimizing the empirical size and miss rates, then resort to generalization bounds
to relate these rates to the true ones.

4 Learning algorithms

We propose two learning algorithms in this section. The ﬁrst is based on a splitting rule for KD-trees
designed to minimize a greedy surrogate for the empirical sizef function. The second is a algorithm
that determines the boundary locations of the cell structure used in LSH that minimize a tradeoff of
the empirical sizef and missf functions.

4.1 KD-trees
KD-trees are a popular cell partitioning scheme for RD based on the binary search paradigm. The
data structure is built by picking a dimension, splitting the database along the median value in that
dimension, and then recursing on both halves.
procedure BUILDTREE(S)
if |S| < MinSize, return leaf.
else:

Pick an axis i.
Let median = median(si : s ∈ S).
LeftTree = BUILDTREE({s ∈ S : si ≤ median}).
RightTree= BUILDTREE({s ∈ S : si > median}).
return [LeftTree, RightTree, median, i].

To ﬁnd a NN for a query q, one ﬁrst computes distances to all points in the same cell, then traverses
up the tree. At each parent node, the minimum distance between q and points already explored is
compared to the distance to the split. If the latter is smaller, then the other child must be explored.

Explore right subtree:

Do not explore:

Typically the cells contain only a few points; a query is expensive because it lies close to many of
the cell boundaries and much of the tree must be explored.

Learning method

Rather than picking the median split at each level, we use the training queries qi to pick a split that
greedily minimizes the expected cost. A split s divides the sample queries (that are in the cell being
split) into three sets: Qtc, those q that are “too close” to s—i.e. nearer to s than d(q, X); Qr, those
on the right of s but not in Qtc; and Ql, those on the left of s but not in Qtc. Queries in Qtc will
require exploring both sides of the split. The split also divides the database points (that are in the
cell being split) into Xl and Xr. The cost of split s is then deﬁned to be

cost(s) ≡ |Ql| · |Xl| + |Qr| · |Xr| + |Qtc| · |X|.

3

cost(s) is a greedy surrogate for (cid:80)

i sizef (qi); evaluating the true average size would require a
potentially costly recursion. In contrast, minimizing cost(s) can be done painlessly since it takes on
at most 2m + n possible values and each can be evaluated quickly. Using a sample set led us to a
very simple, natural cost function that can be used to pick splits in a principled manner.

4.2 Locality sensitive hashing

LSH was a tremendous breakthrough in NN search as it led to data structures with provably sublinear
(in the database size) retrieval time for approximate NN searching. More impressive still, the bounds
on retrieval are independent of the dimensionality of the database. We focus on the LSH scheme for
the (cid:107) · (cid:107)p norm (p ∈ (0, 2]), which we refer to as LSHp. It is built on an extremely simple space
partitioning scheme which we refer to as a rectilinear cell structure (RCS).
procedure BUILDRCS(X ⊂ RD)
Let R ∈ RO(log n)×d with Rij iid draws from a p-stable distribution.1
Project database down to O(log n) dimensions: xi (cid:55)→ Rxi.
Uniformly grid the space with B bins per direction.

See ﬁgure 3, left panel, for an example. On query q, one simply ﬁnds the cell that q belongs to, and
returns the nearest x in that cell.
In general, LSHp requires many RCSs, used in parallel, to achieve a constant probability of success;
in many situations one may sufﬁce [13]. Note that LSHp only works for distances at a single scale
R: the speciﬁc guarantee is that LSHp will return a point x ∈ X within distance (1 + )R of q as
long as d(q, X) < R. To solve the standard  approximate NN problem, one must build O(log(n/))
LSHp structures.

Learning method

We apply our learning framework directly to the class of RCSs since they are the core structural
component of LSHp. We consider a slightly wider class of RCSs where the bin widths are allowed
to vary. Doing so potentially allows a single RCS to work at multiple scales if the bin positions are
chosen appropriately. We give a simple procedure that selects the bin boundary locations.

We wish to select boundary locations minimizing the cost(cid:80)

i missf (qi) + λsizef (qi), where λ is a
tradeoff parameter (alternatively, one could ﬁx a miss rate that is reasonable, say 5%, and minimize
the size). The optimization is performed along one dimension at a time. Fortunately, the optimal
binning along a dimension can be found by dynamic programming. There are at most m+n possible
boundary locations; order them from left to right. The cost of placing the boundaries at p1, p2, pB+1
can be decomposed as c[p1, p2] + ··· + c[pB, pB+1], where

missf (q) + λ

q∈[pi,pi+1]

q∈[pi,pi+1]

|{x ∈ [pi, pi+1]}| .

c[pi, pi+1] = (cid:88)

(cid:88)

Let D be our dynamic programming table where D[p, i] is deﬁned as the cost of putting the ith
boundary at position p and the remaining B + 1 − i to the right. Then D[p, i] = minp(cid:48)≥p c[p, p(cid:48)] +
D[p(cid:48), i − 1].

5 Generalization theory2

In our framework, a nearest neighbor data structure is learned by speciﬁcally designing it to per-
form well on a set of sample queries. Under what conditions will this search structure have good
performance on future queries?
Recall the setting: there is a database X = {x1, . . . , xn}, sample queries Q = {q1, . . . , qm} drawn
iid from some distribution D on Q, and we wish to learn a data structure f : Q → 2X drawn from a
1Dp is p-stable if for any v ∈ Rd and Z, X1, . . . , Xd drawn iid from Dp, (cid:104)v, X(cid:105) d= (cid:107)v(cid:107)pZ. For example,

N (0, 1) is 2-stable.

2See the full version of this paper for any missing proofs.

4

and missf (q) ≡
, both of which have range [0, 1] (missf (q) can be substituted for missf (q) throughout

function class F. We are interested in the generalization of sizef (q) ≡ |f (q)|
n ,
|Γk(q)\f (q)|
this section).
Suppose a data structure f is chosen from some class F, so as to have low empirical cost

k

m(cid:88)

i=1

1
m

sizef (qi) and

1
m

missf (qi).

m(cid:88)

i=1

Can we then conclude that data structure f will continue to perform well for subsequent queries
drawn from the underlying distribution on Q? In other words, are the empirical estimates above
necessarily close to the true expected values Eq∼Dsizef (q) and Eq∼Dmissf (q) ?
There is a wide range of uniform convergence results which relate the difference between empirical
and true expectations to the number of samples seen (in our case, m) and some measure of the
complexity of the two classes {sizef : f ∈ F} and {missf : f ∈ F}. The following is particularly
convenient to use, and is well-known [14, theorem 3.2].
Theorem 1. Let G be a set of functions from a set Z to [0, 1]. Suppose a sample z1, . . . , zm is drawn
from some underlying distribution on Z. Let Gm denote the restriction of G to these samples, that is,

Gm = {(g(z1), g(z2), . . . , g(zm)) : g ∈ G}.
Then for any δ > 0, the following holds with probability at least 1 − δ:

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)Eg − 1

m

m(cid:88)

i=1

sup
g∈G

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤ 2

(cid:114)2 log |Gm|

+

m

(cid:114)log(2/δ)

.

m

g(zi)

This can be applied immediately to the kind of data structure used by LSH.
Deﬁnition 2. A (u1, . . . , ud, B)-rectilinear cell structure (RCS) in RD is a partition of RD into Bd
cells given by
where each hi : R → {1, . . . , B} is a partition of the real line into B intervals.
Theorem 3. Fix any vectors u1, . . . , ud ∈ RD, and, for some positive integer B, let the set of data
structures F consist of all (u1, . . . , ud, B)-rectilinear cell structures in RD. Fix any database of n
points X ⊂ RD. Suppose there is an underlying distribution over queries in RD, from which m
sample queries q1, . . . , qm are drawn. Then

x (cid:55)→ (h1(x · u1), . . . , hd(x · ud)),

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤ 2
(cid:114)2d(B − 1) log(m + n)

m

(cid:114)log(2/δ)

m

+

missf (qi)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)E[missf ] − 1

m

m(cid:88)

i=1

sup
f∈F

and likewise for sizef .
Proof. Fix any X = {x1, . . . , xn} and any q1, . . . , qm. In how many ways can these points be
assigned to cells by the class of all (u1, . . . , ud, B)-rectilinear data structures? Along each axis ui
there are B − 1 boundaries to be chosen and only m + n distinct locations for each of these (as far
as partitioning of the xi’s and qi’s is concerned). Therefore there are at most (m + n)d(B−1) ways to
carve up the points. Thus the functions {missf : f ∈ F} (or likewise, {sizef : f ∈ F}) collapse to
a set of size just (m + n)d(B−1) when restricted to m queries; the rest follows from theorem 1.

This is good generalization performance because it depends only on the projected dimension, not
the original dimension. It holds when the projection directions u1, . . . , ud are chosen randomly, but,
more remarkably, even if they are chosen based on X (for instance, by running PCA on X). If we
learn the projections as well (instead of using random ones) the bound degrades substantially.
that now F ranges over
Theorem 4. Consider the same setting as Theorem 3, except
(u1, . . . , ud, B)-rectilinear cell structures for all choices of u1, . . . , ud ∈ RD. Then with proba-
bility at least 1 − δ,

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤ 2
(cid:114)2 + 2d(D + B − 2) log(m + n)

m

(cid:114)log(2/δ)

m

+

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)E[missf ] − 1

m

m(cid:88)

i=1

missf (qi)

sup
f∈F

and likewise for sizef .

5

Figure 1: Left: Outer ring is the database; inner cluster of points are the queries. Center: KD-tree
with standard median splits. Right: KD-tree with learned splits.

KD-trees are slightly different than RCSs: the directions ui are simply the coordinate axes, and the
number of partitions per direction varies (e.g. one direction may have 10 partitions, another only 1).
Theorem 5. Let F be the set of all depth η KD-trees in RD and X ⊂ RD be a database of points.
Suppose there is an underlying distribution over queries in RD from which q1, . . . qm are drawn.
Then with probability at least 1 − δ,

(cid:114)(2η+1 − 2) log (D(3m + n))

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)E[missf ] − 1

m

m(cid:88)

i=1

sup
f∈F

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤ 2

missf (qi)

(cid:114)log (2/δ)

m

+

m

A KD-tree utilizing median splits has depth η ≤ log n. The depth of a KD-tree with learned splits
can be higher, though we found empirically that the depth was always much less than 2 log n (and
can of course be restricted manually). KD-trees require signiﬁcantly more samples than RCSs to
generalize; the class of KD-trees is much more complex than that of RCSs.

6 Experiments3

6.1 KD-trees

First let us look at a simple example comparing the learned splits to median splits. Figure 1 shows
a 2-dimensional dataset and the cell partitions produced by the learned splits and the median splits.
The KD-tree constructed with the median splitting rule places nearly all of the boundaries running
right through the queries. As a result, nearly the entire database will have to be searched for queries
drawn from the center cluster distribution. The KD-tree with the learned splits places most of the
boundaries right around the actual database points, ensuring that fewer leaves will need to be exam-
ined for each query.
We now show results on several datasets from the UCI repository and 2004 KDD cup competition.
We restrict attention to relatively low-dimensional datasets (D < 100) since that is the domain
in which KD-trees are typically applied. These experiments were all conducted using a modiﬁed
version of Mount and Arya’s excellent KD-tree software [15]. For this set of experiments, we used
a randomly selected subset of the dataset as the database and a separate small subset as the test
queries. For the sample queries, we used the database itself—i.e. no additional data was used to
build the learned KD-tree.
The following table shows the results. We compare performance in terms of the average number of
database points we have to compute distances to on a test set.

# distance calculations

median split

learned split

improvement

data set

DB size

test pts

dim

Corel (UCI)

Covertype (UCI)

Letter (UCI)

Pen digits (UCI)

Bio (KDD)

Physics (KDD)

32k
100k
18k
9k
100k
100k

5k
10k
2k
1k
10k
10k

32
55
16
16
74
78

1035.7
20.8
470.1
168.9
1409.8
1676.6

403.7
18.4
353.8
114.9
1310.8
404.0

%

61.0
11.4
27.4
31.9
7.0
75.9

The learned method outperforms the standard method on all of the datasets, showing a very large im-
provement on several of them. Note also that even the standard method exhibits good performance,

3Additional experiments appear in the full version of this paper.

6

ann2fig dumpSTDann2fig dumpLBears

N. American animals

All animals

Everything

standard KD-tree

learned KD-tree

0.8

0.6

0.4

0.2

0

0

0.8

0.6

0.4

0.2

0

0.8

0.6

0.4

0.2

0

.1

.2

.3

0

.1

.2

.3

0

.1

.2

.3

0.8

0.6

0.4

0.2

0

0

.1

.2

.3

Figure 2: Percentage of DB examined as a function of (cid:31)(the approximation factor) for various query
distributions.

Random boundaries

Random boundaries

Tuned boundaries

Tuned boundaries

Figure 3: Example RCSs. Left: Standard RCS. Right: Learned RCS

often requiring distance calculations to less than one percent of the database. We are showing strong
improvements on what are already quite good results.
We additionally experimented with the ‘Corel50’ image dataset. It is divided into 50 classes (e.g.
air shows, bears, tigers, Fiji) containing 100 images each. We used the 371-dimensional “semantic
space” representation of the images recently developed in a series of image retrieval papers (see e.g.
[16]). This dataset allows us to explore the effect of differing query and database distributions in
a natural setting. It also demonstrates that KD-trees with learned parameters can perform well on
high-dimensional data.
Figure 2 shows the results of running KD-trees using median and learned splits. In each case, 4000
images were chosen for the database (from across all the classes) and images from select classes
were chosen for the queries. The “All” queries were chosen from all classes; the “Animals” were
chosen from the 11 animal classes; the “N. American animals” were chosen from 5 of the animal
classes; and the “Bears” were chosen from the two bear classes. Standard KD-trees are performing
somewhat better than brute force in these experiments; the learned KD-trees yield much faster re-
trieval times across a range of approximation errors. Note also that the performance of the learned
KD-tree seems to improve as the query distribution becomes simpler whereas the performance for
the standard KD-tree actually degrades.

6.2 RCS/LSH

Figure 3 shows a sample run of the learning algorithm. The queries and DB are drawn from the
same distribution. The learning algorithm adjusts the bin boundaries to the regions of density.
Experimenting with RCS structures is somewhat challenging since there are two parameters to set
(number of projections and boundaries), an approximation factor (cid:31), and two quantities to compare
(size and miss). We swept over the two parameters to get results for the standard RCSs. Results for
learned RCSs were obtained using only a single (essentially unoptimized) parameter setting. Rather
than minimizing a tradeoff between sizef and missf , we constrained the miss rate and optimized the
sizef . The constraint was varied between runs (2%, 4%, etc.) to get comparable results.
Figure 4 shows the comparison on databases of 10k points drawn from the MNIST and Physics
datasets (2.5k points were used as sample queries). We see a marked improvement for the Physics
dataset and a small improvement for the MNIST dataset. We suspect that the learning algorithm
helps substantially for the physics data because the one-dimensional projections are highly non-
uniform whereas the MNIST one-dimensional projections are much more uniform.

7

Figure 4: Left: Physics dataset. Right: MNIST dataset.

7 Conclusion

The primary contribution of this paper is demonstrating that building a NN search structure can be
fruitfully viewed as a learning problem. We used this framework to develop algorithms that learn
RCSs and KD-trees optimized for a query distribution. Possible future work includes applying the
learning framework to other data structures, though we expect that even stronger results may be
obtained by using this framework to develop a novel data structure from the ground up. On the
theoretical side, margin-based generalization bounds may allow the use of richer classes of data
structures.

Acknowledgments
We are grateful to the NSF for support under grants IIS-0347646 and IIS-0713540. Thanks to Nikhil
Rasiwasia, Sunhyoung Han, and Nuno Vasconcelos for providing the Corel50 data.

References
[1] J. H. Friedman, J. L. Bentley, and R. A. Finkel. An algorithm for ﬁnding best matches in logarithmic

expected time. ACM Transactions on Mathematical Software, 3(3):209–226, 1977.

[2] S. Arya, D. M. Mount, N. S. Netanyahu, R. Silverman, and A. Wu. An optimal algorithm for approximate

nearest neighbor searching. Journal of the ACM, 45(6):891–923, 1998.

[3] T. Liu, A. W. Moore, A. Gray, and K. Yang. An investigation of practical approximate neighbor algo-

rithms. In Neural Information Processing Systems (NIPS), 2004.

[4] S. Dasgupta and Y. Freund. Random projection trees and low dimensional manifolds. Technical report,

UCSD, 2007.

[5] P. Indyk. Nearest neighbors in high dimensional spaces. In J. E. Goodman and J. O’Rourke, editors,

Handbook of Discrete and Computational Geometry. CRC Press, 2006.

[6] M. T. Orchard. A fast nearest-neighbor search algorithm. In ICASSP, pages 2297–3000, 1991.
[7] E. Vidal. An algorithm for ﬁnding nearest neighbours in (approximately) constant average time. Pattern

Recognition Letters, 4:145–157, 1986.

[8] S. Omohundro. Five balltree construction algorithms. Technical report, ICSI, 1989.
[9] A. Beygelzimer, S. Kakade, and J. Langford. Cover trees for nearest neighbor. In ICML, 2006.
[10] K. L. Clarkson. Nearest-neighbor searching and metric space dimensions. In Nearest-Neighbor Methods

for Learning and Vision: Theory and Practice, pages 15–59. MIT Press, 2006.

[11] S. Maneewongvatana and D. Mount. The analysis of a probabilistic approach to nearest neighbor search-

ing. In Workshop on Algorithms and Data Structures, 2001.

[12] S. Maneewongvatana and D. Mount. Analysis of approximate nearest neighbor searching with clustered

point sets. In Workshop on Algorithm Engineering and Experimentation (ALENEX), 1999.

[13] Mayur Datar, Nicole Immorlica, Piotr Indyk, and Vahab S. Mirrokni. Locality-sensitive hashing scheme

based on p-stable distributions. In SCG 2004, pages 253–262, New York, NY, USA, 2004. ACM Press.

Probability and Statistics, 9:323–375, 2004.

[14] O. Bousquet, S. Boucheron, and G. Lugosi. Theory of classiﬁcation: a survey of recent advances. ESAIM:
[15] D. Mount and S. Arya. ANN library. http://www.cs.umd.edu/∼mount/ANN/.
[16] N. Rasiwasia, P. Moreno, and N. Vasconcelos. Bridging the gap: query by semantic example.

Transactions on Multimedia, 2007.

IEEE

8

00.050.10.150.200.050.10.150.2miss ratesize rate (fraction of DB)Physics00.050.10.150.200.050.10.150.20.25MNISTmiss ratesize rate (fraction of DB)StandardTuned"
353,2007,Theoretical Analysis of Heuristic Search Methods for Online POMDPs,"Planning in partially observable environments remains a challenging problem, despite significant recent advances in offline approximation techniques. A few online methods have also been proposed recently, and proven to be remarkably scalable, but without the theoretical guarantees of their offline counterparts. Thus it seems natural to try to unify offline and online techniques, preserving the theoretical properties of the former, and exploiting the scalability of the latter. In this paper, we provide theoretical guarantees on an anytime algorithm for POMDPs which aims to reduce the error made by approximate offline value iteration algorithms through the use of an efficient online searching procedure. The algorithm uses search heuristics based on an error analysis of lookahead search, to guide the online search towards reachable beliefs with the most potential to reduce error. We provide a general theorem showing that these search heuristics are admissible, and lead to complete and epsilon-optimal algorithms. This is, to the best of our knowledge, the strongest theoretical result available for online POMDP solution methods. We also provide empirical evidence showing that our approach is also practical, and can find (provably) near-optimal solutions in reasonable time.","Theoretical Analysis of Heuristic Search Methods for

Online POMDPs

St´ephane Ross
McGill University

Montr´eal, Qc, Canada

Joelle Pineau

McGill University

Montr´eal, Qc, Canada

Brahim Chaib-draa

Laval University

Qu´ebec, Qc, Canada

sross12@cs.mcgill.ca

jpineau@cs.mcgill.ca

chaib@ift.ulaval.ca

Abstract

Planning in partially observable environments remains a challenging problem, de-
spite signiﬁcant recent advances in ofﬂine approximation techniques. A few on-
line methods have also been proposed recently, and proven to be remarkably scal-
able, but without the theoretical guarantees of their ofﬂine counterparts. Thus it
seems natural to try to unify ofﬂine and online techniques, preserving the theo-
retical properties of the former, and exploiting the scalability of the latter. In this
paper, we provide theoretical guarantees on an anytime algorithm for POMDPs
which aims to reduce the error made by approximate ofﬂine value iteration algo-
rithms through the use of an efﬁcient online searching procedure. The algorithm
uses search heuristics based on an error analysis of lookahead search, to guide the
online search towards reachable beliefs with the most potential to reduce error. We
provide a general theorem showing that these search heuristics are admissible, and
lead to complete and ǫ-optimal algorithms. This is, to the best of our knowledge,
the strongest theoretical result available for online POMDP solution methods. We
also provide empirical evidence showing that our approach is also practical, and
can ﬁnd (provably) near-optimal solutions in reasonable time.

1 Introduction

Partially Observable Markov Decision Processes (POMDPs) provide a powerful model for sequen-
tial decision making under state uncertainty. However exact solutions are intractable in most do-
mains featuring more than a few dozen actions and observations. Signiﬁcant efforts have been
devoted to developing approximate ofﬂine algorithms for larger POMDPs [1, 2, 3, 4]. Most of these
methods compute a policy over the entire belief space. This is both an advantage and a liability.
On the one hand, it allows good generalization to unseen beliefs, and this has been key to solving
relatively large domains. Yet it makes these methods impractical for problems where the state space
is too large to enumerate. A number of compression techniques have been proposed, which han-
dle large state spaces by projecting into a sub-dimensional representation [5, 6]. Alternately online
methods are also available [7, 8, 9, 10, 11]. These achieve scalability by planning only at execution
time, thus allowing the agent to only consider belief states that can be reached over some (small)
ﬁnite planning horizon. However despite good empirical performance, both classes of approaches
lack theoretical guarantees on the approximation. So it would seem we are constrained to either
solving small to mid-size problems (near-)optimally, or solving large problems possibly badly.

This paper suggests otherwise, arguing that by combining ofﬂine and online techniques, we can
preserve the theoretical properties of the former, while exploiting the scalability of the latter. In
previous work [11], we introduced an anytime algorithm for POMDPs which aims to reduce the
error made by approximate ofﬂine value iteration algorithms through the use of an efﬁcient online
searching procedure. The algorithm uses search heuristics based on an error analysis of lookahead
search, to guide the online search towards reachable beliefs with the most potential to reduce error. In

this paper, we derive formally the heuristics from our error minimization point of view and provide
theoretical results showing that these search heuristics are admissible, and lead to complete and ǫ-
optimal algorithms. This is, to the best of our knowledge, the strongest theoretical result available
for online POMDP solution methods. Furthermore the approach works well with factored state
representations, thus further enhancing scalability, as suggested by earlier work [2]. We also provide
empirical evidence showing that our approach is computationally practical, and can ﬁnd (provably)
near-optimal solutions within a smaller overall time than previous online methods.

2 Background: POMDP

A POMDP is deﬁned by a tuple (S, A, Ω, T, R, O, γ) where S is the state space, A is the action
set, Ω is the observation set, T : S × A × S → [0, 1] is the state-to-state transition function,
R : S × A → R is the reward function, O : Ω × A × S → [0, 1] is the observation function,
and γ is the discount factor. In a POMDP, the agent often does not know the current state with full
certainty, since observations provide only a partial indicator of state. To deal with this uncertainty,
the agent maintains a belief state b(s), which expresses the probability that the agent is in each state
at a given time step. After each step, the belief state b is updated using Bayes rule. We denote the

belief update function b′ = τ (b, a, o), deﬁned as b′(s′) = ηO(o, a, s′)Ps∈S T (s, a, s′)b(s), where
η is a normalization constant ensuringPs∈S b′(s) = 1.

Solving a POMDP consists in ﬁnding an optimal policy, π∗ : ∆S → A, which speciﬁes the best
action a to do in every belief state b, that maximizes the expected return (i.e., expected sum of
discounted rewards over the planning horizon) of the agent. We can ﬁnd the optimal policy by
computing the optimal value of a belief state over the planning horizon. For the inﬁnite horizon, the

value Q∗(b, a) of a particular action a in belief state b, as the return we will obtain if we perform a in

where R(b, a) represents the expected immediate reward of doing action a in belief state b and
P (o|b, a) is the probability of observing o after doing action a in belief state b. This probability can

optimal value function is deﬁned as V ∗(b) = maxa∈A[R(b, a) + γPo∈Ω P (o|b, a)V ∗(τ (b, a, o))],
be computed according to P (o|b, a) =Ps′∈S O(o, a, s′)Ps∈S T (s, a, s′)b(s). We also denote the
b and then follow the optimal policy Q∗(b, a) = R(b, a) + γPo∈Ω P (o|b, a)V ∗(τ (b, a, o)). Using

this, we can deﬁne the optimal policy π∗(b) = argmaxa∈A Q∗(b, a).
While any POMDP problem has inﬁnitely many belief states, it has been shown that the optimal
value function of a ﬁnite-horizon POMDP is piecewise linear and convex. Thus we can deﬁne the
optimal value function and policy of a ﬁnite-horizon POMDP using a ﬁnite set of |S|-dimensional
hyper plans, called α-vectors, over the belief state space. As a result, exact ofﬂine value iteration
algorithms are able to compute V ∗ in a ﬁnite amount of time, but the complexity can be very high.
Most approximate ofﬂine value iteration algorithms achieve computational tractability by selecting
a small subset of belief states, and keeping only those α-vectors which are maximal at the selected
belief states [1, 3, 4]. The precision of these algorithms depend on the number of belief points and
their location in the space of beliefs.

3 Online Search in POMDPs

Contrary to ofﬂine approaches, which compute a complete policy determining an action for every
belief state, an online algorithm takes as input the current belief state and returns the single action
which is the best for this particular belief state. The advantage of such an approach is that it only
needs to consider belief states that are reachable from the current belief state. This naturally provides
a small set of beliefs, which could be exploited as in ofﬂine methods. But in addition, since online
planning is done at every step (and thus generalization between beliefs is not required), it is sufﬁcient
to calculate only the maximal value for the current belief state, not the full optimal α-vector. A
lookahead search algorithm can compute this value in two simple steps.

First we build a tree of reachable belief states from the current belief state. The current belief is the
top node in the tree. Subsequent belief states (as calculated by the τ (b, a, o) function) are represented
using OR-nodes (at which we must choose an action) and actions are included in between each layer
of belief nodes using AND-nodes (at which we must consider all possible observations). Note that
in general the belief MDP could have a graph structure with cycles. Our algorithm simply handle

such structure by unrolling the graph into a tree. Hence, if we reach a belief that is already elsewhere
in the tree, it will be duplicated.1
Second, we estimate the value of the current belief state by propagating value estimates up from the
fringe nodes, to their ancestors, all the way to the root. An approximate value function is generally
used at the fringe of the tree to approximate the inﬁnite-horizon value. We are particularly interested
in the case where a lower bound and an upper bound on the value of the fringe belief states is
available, as this allows us to get a bound on the error at any speciﬁc node. The lower and upper
bounds can be propagated to parent nodes according to:

if b is a leaf in T ,
otherwise;

P (o|b, a)LT (τ (b, a, o));

(1)

(2)

(3)

(4)

maxa∈A UT (b, a)

if b is a leaf in T ,
otherwise;

P (o|b, a)UT (τ (b, a, o));

UT (b) =(cid:26) U (b)
UT (b, a) = RB(b, a) + γXo∈Ω
LT (b) =(cid:26) L(b)
LT (b, a) = RB(b, a) + γXo∈Ω

maxa∈A LT (b, a)

where UT (b) and LT (b) represent the upper and lower bounds on V ∗(b) associated to belief state
b in the tree T , UT (b, a) and LT (b, a) represent corresponding bounds on Q∗(b, a), and L(b) and
U (b) are the bounds on fringe nodes, typically computed ofﬂine.
Performing a complete k-step lookahead search multiplies the error bound on the approximate value
function used at the fringe by γk ([13]), and thus ensures better value estimates. However, it has
complexity exponential in k, and may explore belief states that have very small probabilities of oc-
curring (and an equally small impact on the value function) as well as exploring suboptimal actions
(which have no impact on the value function). We would evidently prefer to have a more efﬁcient
online algorithm, which can guarantee equivalent or better error bounds. In particular, we believe
that the best way to achieve this is to have a search algorithm which uses estimates of error reduction
as a criteria to guide the search over the reachable beliefs.

4 Anytime Error Minimization Search

In this section, we review the Anytime Error Minimization Search (AEMS) algorithm we had ﬁrst
introduced in [11] and present a novel mathematical derivation of the heuristics that we had sug-
gested. We also provide new theoretical results describing sufﬁcient conditions under which the
heuristics are guaranteed to yield ǫ-optimal solutions.
Our approach uses a best-ﬁrst search of the belief reachability tree, where error minimization (at the
root node) is used as the search criteria to select which fringe nodes to expand next. Thus we need a
way to express the error on the current belief (i.e. root node) as a function of the error at the fringe
nodes. This is provided in Theorem 1. Let us denote (i) F(T ), the set of fringe nodes of a tree T ; (ii)
eT (b) = V ∗(b) − LT (b), the error function for node b in the tree T ; (iii) e(b) = V ∗(b) − L(b), the
error at a fringe node b ∈ F(T ); (iv) hb0,b
T , the unique action/observation sequence that leads from
the root b0 to belief b in tree T ; (v) d(h), the depth of an action/observation sequence h (number of
a), the probability of executing
the action/observation sequence h if we follow the optimal policy π∗ from the root node b0 (where
a and hi
o refers to the ith action and observation in the sequence h, and bhi is the belief obtained
hi
after taking the i ﬁrst actions and observations from belief b. π∗(b, a) is the probability that the
optimal policy chooses action a in belief b).
By abuse of notation, we will use b to represent both a belief node in the tree and its associated
belief2.

actions); and (vi) P (h|b0, π∗) =Qd(h)

a)π∗(bhi−1 , hi

i=1 P (hi

, hi

o|bhi−1

0

1We are considering using a technique proposed in the LAO* algorithm [12] to handle cycle, but we have

not investigated this fully, especially in terms of how it affects the heuristic value presented below.

2e.g. Pb∈F (T ) should be interpreted as a sum over all fringe nodes in the tree, while e(b) to be the error

associated to the belief in fringe node b.

Proof. Consider an arbitrary parent node b in tree T and let’s denote ˆaT
have eT (b) = V ∗(b) − LT (b).
On the other hand, when ˆaT
b

Theorem 1. In any tree T , eT (b0) ≤Pb∈F (T ) γd(h
eT (b) ≤ γPo∈Ω P (o|b, π∗(b))e(τ (b, π∗(b), o)). Consequently, we have the following:

b = π∗(b), then eT (b) = γPo∈Ω P (o|b, π∗(b))e(τ (b, π∗(b), o)).

6= π∗(b), then we know that LT (b, π∗(b)) ≤ LT (b, ˆaT

b = argmaxa∈A LT (b, a). We

T |b0, π∗)e(b).

b ) and therefore

)P (hb0,b

If ˆaT

b0 ,b
T

P (o|b, π∗(b))eT (τ (b, π∗(b), o))

if b ∈ F (T )
otherwise

eT (b) ≤( e(b)
γPo∈Ω

Then eT (b0) ≤Pb∈F (T ) γd(h

4.1 Search Heuristics

b0 ,b
T

)P (hb0,b

T

|b0, π∗)e(b) can be easily shown by induction.

b0 ,b
T

b0 ,b
T

)P (hb0,b

)P (hb0,b

From Theorem 1, we see that the contribution of each fringe node to the error in b0 is simply
the term γd(h
T |b0, π∗)e(b). Consequently, if we want to minimize eT (b0) as quickly as
possible, we should expand fringe nodes reached by the optimal policy π∗ that maximize the term
T |b0, π∗)e(b) as they offer the greatest potential to reduce eT (b0). This suggests us
γd(h
a sound heuristic to explore the tree in a best-ﬁrst-search way. Unfortunately we do not know V ∗
nor π∗, which are required to compute the terms e(b) and P (hb0,b
T |b0, π∗); nevertheless, we can
approximate them. First, the term e(b) can be estimated by the difference between the lower and
upper bound. We deﬁne ˆe(b) = U (b) − L(b) as an estimate of the error introduced by our bounds at
fringe node b. Clearly, ˆe(b) ≥ e(b) since U (b) ≥ V ∗(b).

To approximate P (hb0,b
T |b0, π∗), we can view the term π∗(b, a) as the probability that action a
is optimal in belief b. Thus, we consider an approximate policy ˆπT that represents the proba-
bility that action a is optimal in belief state b given the bounds LT (b, a) and UT (b, a) that we
have on Q∗(b, a) in tree T . More precisely, to compute ˆπT (b, a), we consider Q∗(b, a) as a
random variable and make some assumptions about its underlying probability distribution. Once
cumulative distribution functions F b,a
T (x) = P (Q∗(b, a) ≤ x), and their associated
density functions f b,a
are determined for each (b, a) in tree T , we can compute the probability
(x)dx. Computing this
T . We

ˆπT (b, a) = P (Q∗(b, a′) ≤ Q∗(b, a)∀a′ 6= a) =R ∞

integral may not be computationally efﬁcient depending on how we deﬁne the functions f b,a
consider two approximations.

T (x)Qa′6=a F b,a′

T , s.t. F b,a

−∞ f b,a

T

T

One possible approximation is to simply compute the probability that the Q-value of a given action
is higher than its parent belief state value (instead of all actions’ Q-value). In this case, we get
T is the cumulative distribution function for V ∗(b),
given bounds LT (b) and UT (b) in tree T . Hence by considering both Q∗(b, a) and V ∗(b) as random
variables with uniform distributions between their respective lower and upper bounds, we get:

ˆπT (b, a) = R ∞

T (x)dx, where F b

−∞ f b,a

T (x)F b

ˆπT (b, a) =( η (UT (b,a)−LT (b))2

UT (b,a)−LT (b,a)

0

if UT (b, a) > LT (b),
otherwise.

(5)

where η is a normalization constant such thatPa∈A ˆπT (b, a) = 1. Notice that if the density function

is 0 outside the interval between the lower and upper bound, then ˆπT (b, a) = 0 for dominated
actions, thus they are implicitly pruned from the search tree by this method.

A second practical approximation is:

ˆπT (b, a) =(cid:26) 1 if a = argmaxa′∈A UT (b, a′),

0 otherwise.

(6)

which simply selects the action that maximizes the upper bound. This restricts exploration of the
search tree to those fringe nodes that are reached by sequence of actions that maximize the upper
bound of their parent belief state, as done in the AO∗ algorithm [14]. The nice property of this
approximation is that these fringe nodes are the only nodes that can potentially reduce the upper
bound in b0.

Using either of these two approximations for ˆπT , we can estimate the error contribution ˆeT (b0, b) of
a fringe node b on the value of root belief b0 in tree T , as: ˆeT (b0, b) = γd(h
T |b0, ˆπT )ˆe(b).

)P (hb0,b

b0 ,b
T

Using this as a heuristic, the next fringe nodeeb(T ) to expand in tree T is deﬁned aseb(T ) =

T |b0, ˆπT )ˆe(b). We use AEMS13 to denote the heuristic that uses ˆπT
argmaxb∈F (T ) γd(h
as deﬁned in Equation 5, and AEMS24 to denote the heuristic that uses ˆπT as deﬁned in Equation 6.

)P (hb0,b

b0 ,b
T

4.2 Algorithm

Algorithm 1 presents the anytime error minimization search. Since the objective is to provide a
near-optimal action within a ﬁnite allowed online planning time, the algorithm accepts two input
parameters: t, the online search time allowed per action, and ǫ, the desired precision on the value
function.

Algorithm 1 AEMS: Anytime Error Minimization Search

Function SEARCH(t, ǫ)
Static : T : an AND-OR tree representing the current search tree.
t0 ← TIME()
while TIME() − t0 ≤ t and not SOLVED(ROOT(T ), ǫ) do

b∗ ←eb(T )

EXPAND(b∗)
UPDATEANCESTORS(b∗)

end while
return argmaxa∈A LT (ROOT(T ), a)

The EXPAND function expands the tree one level under the node b∗ by adding the next action and
belief nodes to the tree T and computing their lower and upper bounds according to Equations 1-
4. After a node is expanded, the UPDATEANCESTORS function simply recomputes the bounds of
its ancestors according to Equations determining b′(s′), V ∗(b), P (o|b, a) and Q∗(b, a), as outlined
in Section 2. It also recomputes the probabilities ˆπT (b, a) and the best actions for each ancestor
node. To ﬁnd quickly the node that maximizes the heuristic in the whole tree, each node in the tree
contains a reference to the best node to expand in their subtree. These references are updated by
the UPDATEANCESTORS function without adding more complexity, such that when this function
terminates, we always know immediatly which node to expand next, as its reference is stored in the
root node. The search terminates whenever there is no more time available, or we have found an ǫ-
optimal solution (veriﬁed by the SOLVED function). After an action is executed in the environment,
the tree T is updated such that our new current belief state becomes the root of T ; all nodes under
this new root can be reused at the next time step.

4.3 Completeness and Optimality

We now provide some sufﬁcient conditions under which our heuristic search is guaranteed to con-
verge to an ǫ-optimal policy after a ﬁnite number of expansions. We show that the heuristics pro-
posed in Section 4.1 satisfy those conditions, and therefore are admissible. Before we present the
main theorems, we provide some useful preliminary lemmas.
Lemma 1. In any tree T , the approximate error contribution ˆeT (b0, bd) of a belief node bd at depth
d is bounded by ˆeT (b0, bd) ≤ γd supb ˆe(b).

Proof. P (hb0,b

T

|b0, ˆπT ) ≤ 1 and ˆe(b) ≤ supb′ ˆe(b′) for all b. Thus ˆeT (b0, bd) ≤ γd supb ˆe(b).

a) the
probability of observing the sequence of observations ho in some action/observation sequence h,

For the following lemma and theorem, we will denote P (ho|b0, ha) = Qd(h)
given that the sequence of actions ha in h is performed from current belief b0, and bF(T ) ⊆ F(T )

T |b0, ˆπT ) > 0, for ˆπT deﬁned as in Equation 6 (i.e.

the set of all fringe nodes in T such that P (hb0,b

i=1 P (hi

o|bhi−1

, hi

0

3This heuristic is slightly different from the AEMS1 heuristic we had introduced in [11].
4This is the same as the AEMS2 heuristic we had introduced in [11].

the set of fringe nodes reached by a sequence of actions in which each action maximizes UT (b, a)
in its respective belief state.)

Lemma 2. For any tree T , ǫ > 0, and D such that γD supb ˆe(b) ≤ ǫ, if for all b ∈ bF(T ), either

T ) ≥ D or there exists an ancestor b′ of b such that ˆeT (b′) ≤ ǫ, then ˆeT (b0) ≤ ǫ.

d(hb0,b

ˆe(b)
ǫ

b )ˆeT (τ (b, ˆaT

b0 ,b
T

b0 ,b
T

T

T

T ,o |b0, hb0,b′

)P (hb0,b

T ,o |b0, hb0,b

P (o|b, ˆaT

b )ˆeT (τ (b, ˆaT

b , o))

)P (hb0,b

T ,o |b0, hb0,b

T ,a )ˆe(b) +

T ,o |b0, hb0,b′

T ,a ) = ǫ.

if b ∈ F (T )
if ˆeT (b) ≤ ǫ
otherwise

ˆeT (b) ≤8><>:

that for all b in A(T ), d(hb0,b
T ,o |b0, hb0,b′

Proof. Let’s denote ˆaT
b )−LT (b, ˆaT
UT (b)−LT (b) ≤ UT (b, ˆaT
recurrence is an upper bound on ˆeT (b):

b = argmaxa∈A UT (b, a). Notice that for any tree T , and parent belief b ∈ T , ˆeT (b) =
b , o)). Consequently, the following

b ) = γPo∈Ω P (o|b, ˆaT
γPo∈Ω
By unfolding the recurrence for b0, we get ˆeT (b0) ≤ Pb∈A(T ) γd(h
ǫPb∈B(T ) γd(h
T ,a ), where B(T ) is the set of parent nodes b′ having a descendant in bF (T )
such that ˆeT (b′) ≤ ǫ and A(T ) is the set of fringe nodes b′′ in bF (T ) not having an ancestor in B(T ). Hence
if for all b ∈ bF (T ), d(hb0,b
ǫPb′∈B(T ) P (hb0,b′
b = argmaxa∈A UT (b, a), then Algorithm 1 usingeb(T ) is complete and ǫ-optimal.

) ≥ D, and therefore, ˆeT (b0) ≤ γD supb ˆe(b)Pb′∈A(T ) P (hb0,b′
T ,a ) ≤ ǫPb′∈A(T )∪B(T ) P (hb0,b′

Theorem 2. For any tree T and ǫ > 0, if ˆπT is deﬁned such that inf b,T |ˆeT (b)>ǫ ˆπT (b, ˆaT
ˆaT

) ≥ D or there exists an ancestor b′ of b such that ˆeT (b′) ≤ ǫ, then this means
T ,a ) +

Proof. If γ = 0, then the proof is immediate. Consider now the case where γ ∈ (0, 1). Clearly, since U
is bounded above and L is bounded below, then ˆe is bounded above. Now using γ ∈ (0, 1), we can ﬁnd a
positive integer D such that γD supb ˆe(b) ≤ ǫ. Let’s denote AT
b the set of ancestor belief states of b in the
tree T , and given a ﬁnite set A of belief nodes, let’s deﬁne ˆemin
(A) = minb∈A ˆeT (b). Now let’s deﬁne Tb =
) ≤ D}.
b ) > 0, then B contains all belief states b within depth

Clearly, by the assumption that inf b,T |ˆeT (b)>ǫ ˆπT (b, ˆaT
D such that ˆe(b) > 0, P (hb0,b
b′ of b have ˆeT (b′) > ǫ. Furthermore, B is ﬁnite since there are only ﬁnitely many belief states within depth
D. Hence there exist a Emin = minb∈B γd(h
|b0, ˆπT ). Clearly, Emin > 0 and we

T ,a ) > 0 and there exists a ﬁnite tree T where b ∈ bF (T ) and all ancestors
know that for any tree T , all beliefs b in B ∩ bF (T ) have an approximate error contribution ˆeT (b0, b) ≥ Emin.
a tree T where B ∩ bF (T ) = ∅. Because there are only ﬁnitely many nodes within depth D′, then it is clear
B ∩ bF (T ) = ∅, we have that for all beliefs b ∈ bF (T ), either d(hb0,b

that Algorithm 1 will reach such tree T after a ﬁnite number of expansions. Furthermore, for this tree T , since
b ) ≤ ǫ. Hence by
Lemma 2, this implies that ˆeT (b0) ≤ ǫ, and consequently Algorithm 1 will terminate after a ﬁnite number of
expansions (SOLVED(b0, ǫ) will evaluate to true) with an ǫ-optimal solution (since eT (b0) ≤ ˆeT (b0)).

Since Emin > 0 and γ ∈ (0, 1), there exist a positive integer D′ such that γD′
supb ˆe(b) < Emin. Hence
by Lemma 1, this means that Algorithm 1 cannot expand any node at depth D′ or beyond before expanding

{T |T f inite, b ∈ bF (T ), ˆemin

b ) > ǫ} and B = {b|ˆe(b) inf T ∈Tb P (hb0,b

)ˆe(b) inf T ∈Tb P (hb0,b

|b0, ˆπT ) > 0, d(hb0,b

) ≥ D or ˆemin

(AT

T

b ) > 0 for

T

T

(AT

T

T ,o |b0, hb0,b

T

T

T

b0 ,b
T

From this last theorem, we notice that we can potentially develop many different admissible
the main sufﬁcient condition being that ˆπT (b, a) > 0 for a =
heuristics for Algorithm 1;
argmaxa′∈A UT (b, a′). It also follows from this theorem that the two heuristics described above,
AEMS1 and AEMS2, are admissible. The following corollaries prove this:

Proof. Immediate by Theorem 2 and the fact that ˆπT (b, ˆaT

Corollary 1. Algorithm 1, usingeb(T ), with ˆπT as deﬁned in Equation 6 is complete and ǫ-optimal.
Corollary 2. Algorithm 1, usingeb(T ), with ˆπT as deﬁned in Equation 5 is complete and ǫ-optimal.

Proof. We ﬁrst notice that (UT (b, a) − LT (b))2/(UT (b, a) − LT (b, a)) ≤ ˆeT (b, a), since LT (b) ≥
LT (b, a) for all a. Furthermore, ˆeT (b, a) ≤ supb′ ˆe(b′). Therefore the normalization constant
b ) = UT (b), and there-
η ≥ (|A| supb ˆe(b))−1. For ˆaT
fore UT (b, ˆaT
b ) ≥

b ) − LT (b) = ˆeT (b). Hence this means that ˆπT (b, ˆaT

b = argmaxa∈A UT (b, a), we have UT (b, ˆaT

b ) = η(ˆeT (b))2/ˆeT (b, ˆaT

b ) = 1 for all b, T .

(|A|(supb′ ˆe(b′))2)−1(ˆeT (b))2 for all T , b. Hence, for any ǫ > 0, inf b,T |ˆeT (b)>ǫ ˆπT (b, ˆaT
(|A|(supb ˆe(b))2)−1ǫ2 > 0. Hence, corrolary follows from Theorem 2.

b ) ≥

5 Experiments

In this section we present a brief experimental evaluation of Algorithm 1, showing that in addition to
its useful theoretical properties, the empirical performance matches, and in some cases exceeds, that
of other online approaches. The algorithm is evaluated in three large POMDP environments: Tag
[1], RockSample [3] and FieldVisionRockSample (FVRS) [11]; all are implemented using a factored
In each environments we compute the Blind policy5 to get a lower bound
state representation.
and the FIB algorithm [15] to get an upper bound. We then compare performance of Algorithm 1
with both heuristics (AEMS1 and AEMS2) to the performance achieved by other online approaches
(Satia [7], BI-POMDP [8], RTBSS [10]). For all approaches we impose a real-time constraint of
1 sec/action, and measure the following metrics: average return, average error bound reduction6
(EBR), average lower bound improvement7 (LBI), number of belief nodes explored at each time
step, percentage of belief nodes reused in the next time step, and the average online time per action
(< 1s means the algorithm found an ǫ-optimal action)8. Satia, BI-POMDP, AEMS1 and AEMS2
were all implemented using the same algorithm since they differ only in their choice of search
heuristic used to guide the search. RTBSS served as a base line for a complete k-step lookahead
search using branch & bound pruning. All results were obtained on a Xeon 2.4 Ghz with 4Gb of
RAM; but the processes were limited to use a max of 1Gb of RAM.

Table 1 shows the average value (over 1000+ runs) of the different statistics. As we can see from
these results, AEMS2 provides the best average return, average error bound reduction and average
lower bound improvement in all considered environments. The higher error bound reduction and
lower bound improvement obtained by AEMS2 indicates that it can guarantee performance closer
to the optimal. We can also observe that AEMS2 has the best average reuse percentage, which
indicates that AEMS2 is able to guide the search toward the most probable nodes and allows it to
generally maintain a higher number of belief nodes in the tree. Notice that AEMS1 did not perform
very well, except in FVRS[5,7]. This could be explained by the fact that our assumption that the
values of the actions are uniformly distributed between the lower and upper bounds is not valid in
the considered environments.

Finally, we also examined how fast the lower and upper bounds converge if we let the algorithm run
up to 1000 seconds on the initial belief state. This gives an indication of which heuristic would be
the best if we extended online planning time past 1sec. Results for RockSample[7,8] are presented
in Figure 2, showing that the bounds converge much more quickly for the AEMS2 heuristic.

6 Conclusion

In this paper we examined theoretical properties of online heuristic search algorithms for POMDPs.
To this end, we described a general online search framework, and examined two admissible heuris-
tics to guide the search. The ﬁrst assumes that Q∗(b, a) is distributed uniformly at random be-
tween the bounds (Heuristic AEMS1), the second favors an optimistic point of view, and assume
the Q∗(b, a) is equal to the upper bound (Heuristic AEMS2). We provided a general theorem that
shows that AEMS1 and AEMS2 are admissible and lead to complete and ǫ-optimal algorithms. Our
experimental work supports the theoretical analysis, showing that AEMS2 is able to outperform on-
line approaches. Yet it is equally interesting to note that AEMS1 did not perform nearly as well.
This highlights the fact that not all admissible heuristics are equally useful. Thus it will be interest-
ing in the future to develop further guidelines and theoretical results describing which subclasses of
heuristics are most appropriate.

5The policy obtained by taking the combination of the |A| α-vectors that each represents the value of a

policy performing the same action in every belief state.

6The error bound reduction is deﬁned as 1 − UT (b0)−LT (b0)
, when the search process terminates on b0
U (b0)−L(b0)
7The lower bound improvement is deﬁned as LT (b0) − L(b0), when the search process terminates on b0
8For RTBSS, the maximum search depth under the 1sec time constraint is show in parenthesis.

Figure 1: Comparison of different online search al-
gorithm in different environments.

Heuristic /
Algorithm

Return
± 0.01

EBR (%)

± 0.1

LBI

± 0.01

Belief Reuse Time
(ms)
Nodes
±1

(%)
±0.1

-

0

22.3
22.9
49.0
76.2
76.3

-10.30
-8.35
-6.73
-6.22
-6.19

Tag (|S| = 870, |A| = 5, |Ω| = 30)
45067
36908
43693
79508
80250

10.0
25.1
54.6
54.8
RockSample[7,8] (|S| = 12545, |A| = 13, |Ω| = 2)
8.9
5.3
0

3.03
2.47
3.92
7.81
7.81

FVRS[5,7] (|S| = 3201, |A| = 5, |Ω| = 128)

RTBSS(5)
Satia & Lave

AEMS1

BI-POMDP

AEMS2

Satia & Lave

AEMS1
RTBSS(2)
BI-POMDP

AEMS2

RTBSS(1)
BI-POMDP
Satia & Lave

AEMS1
AEMS2

7.35
10.30
10.30
18.43
20.75

20.57
22.75
22.79
23.31
23.39

3.6
9.5
9.7
33.3
52.4

7.7
11.1
11.1
12.4
13.3

0

0.90
1.00
4.33
5.30

2.07
2.08
2.05
2.24
2.35

509
579
439
2152
3145

516
4457
3683
3856
4070

29.9
36.4

0
0.4
0.4
1.4
1.6

580
856
814
622
623

900
916
896
953
859

254
923
947
942
944

)

0

b
(
V

30

25

20

15

10

5
10−2

AEMS2
AEMS1
BI−POMDP
Satia

10−1

100

Time (s)

101

102

103

Figure 2: Evolution of the upper / lower bounds on
the initial belief state in RockSample[7,8].

Acknowledgments

This research was supported by the Natural Sciences and Engineering Research Council of Canada
(NSERC) and the Fonds Qu´eb´ecois de la Recherche sur la Nature et les Technologies (FQRNT).

References

[1] J. Pineau. Tractable planning under uncertainty: exploiting structure. PhD thesis, Carnegie Mellon

University, Pittsburgh, PA, 2004.

[2] P. Poupart. Exploiting structure to efﬁciently solve large scale partially observable Markov decision

processes. PhD thesis, University of Toronto, 2005.

[3] T. Smith and R. Simmons. Point-based POMDP algorithms: improved analysis and implementation. In

UAI, 2005.

[4] M. T. J. Spaan and N. Vlassis. Perseus: randomized point-based value iteration for POMDPs. JAIR,

24:195–220, 2005.

[5] N. Roy and G. Gordon. Exponential family PCA for belief compression in POMDPs. In NIPS, 2003.
[6] P. Poupart and C. Boutilier. Value-directed compression of POMDPs. In NIPS, 2003.
[7] J. K. Satia and R. E. Lave. Markovian decision processes with probabilistic observation of states. Man-

agement Science, 20(1):1–13, 1973.

[8] R. Washington. BI-POMDP: bounded, incremental partially observable Markov model planning. In 4th

Eur. Conf. on Planning, pages 440–451, 1997.

[9] D. McAllester and S. Singh. Approximate Planning for Factored POMDPs using Belief State Simpliﬁca-

tion. In UAI, 1999.

[10] S. Paquet, L. Tobin, and B. Chaib-draa. An online POMDP algorithm for complex multiagent environ-

ments. In AAMAS, 2005.

[11] S. Ross and B. Chaib-draa. AEMS: an anytime online search algorithm for approximate policy reﬁnement

in large POMDPs. In IJCAI, 2007.

[12] E. A. Hansen and S. Zilberstein. LAO * : A heuristic search algorithm that ﬁnds solutions with loops.

Artiﬁcial Intelligence, 129(1-2):35–62, 2001.

[13] M. L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John Wiley &

Sons, Inc., New York, NY, USA, 1994.

[14] N.J. Nilsson. Principles of Artiﬁcial Intelligence. Tioga Publishing, 1980.
[15] M. Hauskrecht. Value-function approximations for POMDPs. JAIR, 13:33–94, 2000.

"
170,2007,An online Hebbian learning rule that performs Independent Component Analysis,"Independent component analysis (ICA) is a powerful method to decouple signals. Most of the algorithms performing ICA do not consider the temporal correlations of the signal, but only higher moments of its amplitude distribution. Moreover, they require some preprocessing of the data (whitening) so as to remove second order correlations. In this paper, we are interested in understanding the neural mechanism responsible for solving ICA. We present an online learning rule that exploits delayed correlations in the input. This rule performs ICA by detecting joint variations in the firing rates of pre- and postsynaptic neurons, similar to a local rate-based Hebbian learning rule.","An online Hebbian learning rule that performs

Independent Component Analysis

Claudia Clopath

School of Computer Science and Brain Mind Institute

Ecole polytechnique federale de Lausanne

1015 Lausanne EPFL

claudia.clopath@epfl.ch

Andre Longtin

Center for Neural Dynamics

University of Ottawa

150 Louis Pasteur, Ottawa
alongtin@uottawa.ca

Wulfram Gerstner

School of Computer Science and Brain Mind Institute

Ecole polytechnique federale de Lausanne

1015 Lausanne EPFL

wulfram.gerstner@epfl.ch

Abstract

Independent component analysis (ICA) is a powerful method to decouple signals.
Most of the algorithms performing ICA do not consider the temporal correlations
of the signal, but only higher moments of its amplitude distribution. Moreover,
they require some preprocessing of the data (whitening) so as to remove second
order correlations. In this paper, we are interested in understanding the neural
mechanism responsible for solving ICA. We present an online learning rule that
exploits delayed correlations in the input. This rule performs ICA by detecting
joint variations in the ﬁring rates of pre- and postsynaptic neurons, similar to a
local rate-based Hebbian learning rule.

1 Introduction

The so-called cocktail party problem refers to a situation where several sound sources are simul-
taneously active, e.g. persons talking at the same time. The goal is to recover the initial sound
sources from the measurement of the mixed signals. A standard method of solving the cocktail
party problem is independent component analysis (ICA), which can be performed by a class of pow-
erful algorithms. However, classical algorithms based on higher moments of the signal distribution
[1] do not consider temporal correlations, i.e. data points corresponding to different time slices could
be shufﬂed without a change in the results. But time order is important since most natural signal
sources have intrinsic temporal correlations that could potentially be exploited. Therefore, some
algorithms have been developed to take into account those temporal correlations, e.g. algorithms
based on delayed correlations [2, 3, 4, 5] potentially combined with higher-order statistics [6], based
on innovation processes [7], or complexity pursuit [8]. However, those methods are rather algorith-
mic and most of them are difﬁcult to interpret biologically, e.g. they are not online or not local or
require a preprocessing of the data.
Biological learning algorithms are usually implemented as an online Hebbian learning rule that trig-
gers changes of synaptic efﬁcacy based on the correlations between pre- and postsynaptic neurons.
A Hebbian learning rule, like Oja’s learning rule [9], combined with a linear neuron model, has been
shown to perform principal component analysis (PCA). Simply using a nonlinear neuron combined
with Oja’s learning rule allows one to compute higher moments of the distributions which yields
ICA if the signals have been preprocessed (whitening) at an earlier stage [1]. In this paper, we are

1

Figure 1: The sources s are mixed with a matrix
C, x = Cs, x are the presynaptic signals. Us-
ing a linear neuron y = W x, we want to ﬁnd the
matrix W which allows the postsynaptic signals
y to recover the sources, y = P s, where P is
a permutation matrix with different multiplicative
constants.

interested in exploiting the correlation of the signals at different time delays, i.e. a generalization of
the theory of Molgedey and Schuster [4]. We will show that a linear neuron model combined with a
Hebbian learning rule based on the joint ﬁring rates of the pre- and postsynaptic neurons of different
time delays performs ICA by exploiting the temporal correlations of the presynaptic inputs.

2 Mathematical derivation of the learning rule

2.1 The problem

We assume statistically independent autocorrelated source signals si with mean < si >= 0 (<>
means averaging over time) and correlations < si(t)sj(t(cid:48)) >= Ki(|t − t(cid:48)|)δij. The sources s are
mixed by a matrix C
(1)

x = Cs,

where x are the mixed signals recorded by a ﬁnite number of receptors (bold notation refers to a
vector). We think of the receptors as presynaptic neurons that are connected via a weight matrix W
to postsynaptic neurons. We consider linear neurons [9], so that the postsynaptic signals y can be
written

(2)
The aim is to ﬁnd a learning rule that adjusts the appropriate weight matrix W to W ∗ (* denotes the
value at the solution) so that the postsynaptic signals y recover the independent sources s (Fig 1),
i.e. y = P s where P is a permutation matrix with different multiplicative constants (the sources are
recovered in a different order up to a multiplicative constant), which means that, neglecting P ,

y = W x.

W ∗ = C−1.

(3)

To solve this problem we extend the theory of Molgedey and Schuster [4] in order to derive an online
biological hebbian rule.

2.2 Theory of Molgedey and Schuster and generalization

The paper of Molgedey and Schuster [4] focuses on the instantaneous correlation matrix but also the
time delayed correlations Mij =< xi(t)xj(t + τ) > of the incoming signals. Since the correlation
matrix Mij is symmetric, it has up to n(n + 1)/2 independent elements. However, the unknown
mixing matrix C has potentially n2 elements (for n sources and n detectors). Therefore, we need to
evaluate two delayed correlation matrices M and ¯M with two different time delays deﬁned as

Mij =< xi(t)xj(t + τ2) >

¯Mij =< xi(t)xj(t + τ1) >

(4)

to get enough information about the mixing process [10].
l CilCjl ¯Λll
From equation 1, we obtain the relation Mij =
where Λij = δijKi(τ2) and ¯Λij = δijKi(τ1) are diagonal matrices. Since M = CΛC T and
¯M = C ¯ΛC T , we have

l CilCjlΛll and similarly ¯Mij =

(cid:80)

(cid:80)

(M ¯M−1)C = C(Λ¯Λ−1).

(5)

2

sxyCWMixingICAHebbian LearningIt follows that C can be found from an eigenvalue problem. Since C is the mixing matrix, a simple
algorithmic inversion allows Molgedey and Schuster to recover the original sources [4].

2.3 Our learning rule

In order to understand the putative neural mechanism performing ICA derived from the formalism
developed above, we need to ﬁnd an online learning rule describing changes of the synapses as
a function of pre- and postsynaptic activity. Taking the inverse of (5), we have C−1 ¯M M−1 =
¯ΛΛ−1C−1. Therefore, for weights that solve the ICA problem we expect because of (3) that

W ∗ ¯M = ¯ΛΛ−1W ∗M,

(6)

which deﬁnes the weight matrix W ∗ at the solution.
For the sake of simplicity, consider only one linear postsynaptic neuron. The generalization to many
postsynaptic neurons is straightforward (see section 4). The output signal y of the neuron can be
written as y = w∗Tx, where w∗T is a row of the matrix W ∗. Then equation 6 can be written as

w∗T ¯M = λw∗TM,

(7)

where λ is one element of the diagonal matrix ¯ΛΛ−1.
In order to solve this equation, we can use the following iterative update rule with update parameter
γ.

˙w = γ[wT ¯M − λwTM].

(8)
The ﬁxed point of this update rule is giving by (7), i.e. w = w∗. Furthermore, multiplication of (7)
with w yields λ = wT ¯M w
wTM w .
If we insert the deﬁnition of M from (2), we obtain the following rule

˙w = γ[< y(t)x(t + τ1) > −λ < y(t)x(t + τ2) >],

(9)

with a parameter λ given by

λ = < y(t)y(t + τ1) >
< y(t)y(t + τ2) >

.

It is possible to show that ˙w is orthogonal to w. This implies that to ﬁrst order (in | ˙w/w|), w will
keep the same norm during iterations of (9).
The rule 9 we derived is a batch-rule, i.e. it averages over all sample signals. We convert this rule
into an online learning rule by taking a small learning rate γ and using an online estimate of λ.

˙w = γ[y(t)x(t + τ1) − λ1
λ2
˙λ1 = −λ1 + y(t)y(t + τ1)
˙λ2 = −λ2 + y(t)y(t + τ2).

τλ

τλ

y(t)x(t + τ2)]

(10)

Note that the rule deﬁned in (10) uses information on the correlated activity xy of pre- and postsy-
naptic neurons as well as an estimate of the autocorrelation < yy > of the postsynaptic neuron. τλ
is taken sufﬁciently long so as to average over a representative sample of the signals and |γ| (cid:191) 1 is
a small learning rate. Stability properties of updates under rule (10) are discussed in section 4.

3 Performances of the learning rule

A simple example of a cocktail party problem is shown in Fig 2 where two signals, a sinus and a
ramp (saw-tooth signal), have been mixed. The learning rule converges to a correct set of synaptic

3

A

C

B

D

Figure 2: A. Two periodic source signals, a sinus (thick solid line) and a ramp (thin solid line), are
mixed into the presynaptic signals (dotted lines). B. The autocorrelation functions of the two source
signals are shown (the sinus in thick solid line and the ramp in thin solid line). The sources are
normalized so that Λ(0) = 1 for both. C. The learning rule with τ1 = 3 and τ2 = 0 extracts the
sinusoidal output signal (dashed) composed to the two input signals. In agreement with the calcula-
tion of stability, γ > 0 , the output is recovering the sinus source because Λsin(3) > Λramp(3). D.
The learning rule with τ1 = 10, τ2 = 0, converges to the other signal (dashed line), i.e. the ramp,
because Λramp(10) > Λsin(10). Note that the signals have been rescalled since the learning rule
recovers the signals up to a multiplicative factor.

weights so that the postsynaptic signal recovers correctly one of the sources. Postsynaptic neurons
with different combinations of τ1 and τ2 are able to recover different signals (see the section 4 on
Stability). In the simulations, we ﬁnd that the convergence is fast and the performance is very accu-
rate and stable. Here we show only a two-sources problem for the sake of visual clarity. However,
the rule can easily recover several mixed sources that have different temporal characteristics.
Fig 3 shows an ICA problem with sources s(t) generated by an Ornstein-Uhlenbeck process of the
form τsi ˙si = −si + ξ, where ξ is some gaussian noise. The different sources are characterized
by different time constants. The learning rule is able to decouple these colored noise signals with
gaussian amplitude distribution since they have different temporal correlations.
Finally, Fig 4 shows an application with nine different sounds. We used 60 postsynaptic neurons
with time delays τ1 chosen uniformly in an interval [1,30ms] and τ2 = 0 . Globally 52 of the 60
neurons recovered exactly 1 source (A, B) and the remaining 8 recovered mixtures of 2 sources (E).
One postsynaptic neuron is recovering one of the sources depending on the source’s autocorrelation
at time τ1 and τ2 (.i.e. the source with the biggest autocorrelation at time τ1 since τ2 = 0 for all
neurons, see section Stability). A histogram (C) shows how many postsynaptic neurons recover
each source. However, as it will become clear from the stability analysis below, a few speciﬁc
postsynaptic neurons tuned to time delays, where the autocorrelation functions intersect (D, at time
τ1 = 3ms and τ2 = 0), cannot recover one of the sources precisely (E).

4

timesignalstimeautocorrelation Ki(t-t’)0510-5-10timesignalstimesignalsA

B

Figure 3: A. The 3 source signals (solid lines generated with the equation τsi ˙si = −si + ξ with
different time constants, where ξ is some gaussian noise) are plotted together with the output signal
(dashed). The learning rule is converging to one of the sources. B. Same as before, but only the one
signal (solid) that was recovered is shown together with the neuronal output (dashed).

B

D

A

C

E

Figure 4: Nine different sound sources from [11] were mixed with a random matrix. 60 postsynaptic
neurons tuned to different τ1 and τ2 were used in order to recover the sources, i.e. τ1 varies from 1ms
to 30ms by steps of 0.5ms and τ2 = 0 for all neurons. A. One source signal (below) is recovered
by one of the postsynaptic neurons (above, for clarity reason, the output is shifted upward). B.
Zoom on one source (solid line) and one output (dashed line). C. Histogram of the number of
postsynaptic neurons recovering each sources. D. Autocorrelation of the different sources. There
are several sources with the biggest autocorrelation at time 3ms. E. The postsynaptic neuron tuned
to a τ1 = 3ms and τ2 = 0 (above) is not able to recover properly one of the sources even though it
still performs well except for the low amplitude parts of the signal (below).

5

timesingalstimesingals12345time [s]signalstimesignals 10 ms123456789051015sources ## of ouput−4−2024time [ms]autocorrelation12345time [s]signals4 Stability of the learning rule

In principle our online learning rule (10) could lead to several solutions corresponding to different
ﬁxed points of the dynamics. Fixed points will be denoted by w∗ = ek, which are by construction
the row vectors of the decoupling matrix W ∗ (see (5) and (7)). The rule 10 has two parameters, i.e.
the delays τ1 and τ2 (the τλ is considered ﬁxed). We assume that in our architecture, these delays
characterize different properties of the postsynaptic neuron. Neurons with different choices of τ1
and τ2 will potentially recover different signals from the same mixture. The stability analysis will
show which ﬁxed point is stable depending on the autocorrelation functions of the signals and the
delays τ1 and τ2.
We analyze the stability, assuming small perturbation of the weights, i.e. w = ei + ej where {ek},
the basis of the matrix C−1, are the ﬁxed points. We obtain the expression (see Appendix for
calculation details)

˙ = γ

Λjj(τ1)Λii(τ2) − Λii(τ1)Λjj(τ2)

Λii(τ2)

,

(11)

where Λ(τ)ij =< si(t)sj(t + τ) > is the diagonal correlation matrix.
To illustrate the stability equation (11), let us take τ1 = 0 and assume that Λii(0) = Λjj(0), i.e. all
signals have the same zero-time-lag autocorrelation. In this case (11) reduces to ˙ = γ[Λjj(τ1) −
Λii(τ1)]. That is the solution ei is stable if Λjj(τ1) < Λii(τ1) for all directions ej (with biggest
autocorrelation at time τ1) for γ > 0. If γ < 0, the solution ei is stable for Λjj(τ1) > Λii(τ1).
This stability relation is veriﬁed in the simulations. Fig 2 shows two signals with different autocor-
relation functions. In this example, we chose τ1 = 0 and Λ(0) = I, i.e. the signals are normalized.
The learning rule is recovering the signal with the biggest autocorrelation at time τ1, Λkk(τ1), for a
positive learning rate.

5 Comparison between Spatial ICA and Temporal ICA

One of the algorithms most used to solve ICA is FastICA [1].
It is based on an approximation
of negentropy and is purely spatial, i.e. it takes into account only the amplitude distribution of the
signal, but not it’s temporal structure. Therefore we show an example (Fig. 5), where three signals
generated by Ornstein-Uhlenbeck processes have the same spatial distribution but different time
constants of the autocorrelation. With a spatial algorithm data points corresponding to different time
slices can be shufﬂed without any change in the results. Therefore, it cannot solve this example. We
tested our example with FastICA downloaded from [11] and it failed to recover the original sources
(Fig. 5). However, to our surprise, FastICA could for very few trial solve this problem even though
the convergence was not stable. Indeed, since FastICA algorithm is an iterative online algorithm, it
takes the signals in the temporal order in which they arrive. Therefore temporal correlations can in
some cases be taken into account even though this is not part of the theory of FastICA.

6 Discussions and conclusions

We presented a powerful online learning rule that performs ICA by computing joint variations in
the ﬁring rates of pre- and postsynaptic neurons at different time delays. This is very similar to a
standard Hebbian rule with exception of an additional factor λ which is an online estimate of the
output correlations at different time delays. The different delay times τ1, τ2 are necessary to recover
different sources. Therefore properties varying between one postsynaptic neuron and the next could
lead to different time delays used in the learning rule. We could assume that the time delays are
intrinsic properties of each postsynaptic neuron due to for example the distance on the dendrites
where the synapse is formed [12], i.e. due to different signal propagation time. The calculation of
stability shows that a postsynaptic neuron will recover the signal with the biggest autocorrelation at
the considered delay time or the smallest depending of the sign of the learning rates. We assume that
for biological signals autocorrelation functions cross so that it’s possible with different postsynaptic
neurons to recover all the signals.

6

A

C

B

D

Figure 5: Two signals generated by an Ornstein-Uhlenbeck process are mixed. A. The signals have
the same spatial distributions. B. The time constants of the autocorrelations are different. C. Our
learning rule converges to an output (dashed line) recovering one of the signals source (solid line).
D. FastICA (dashed line) doesn’t succeed to recover the sources (solid line).

The algorithm assumes centered signals. However for a complete mapping of those signals
to neural rates, we have to consider positive signals. Nevertheless we can easily compute an
online estimate of the mean ﬁring rate and remove this mean from the original rates. This way the
algorithm still holds taking neural rates as input.

Hyvaerinen proposed an ICA algorithm [8] based on complexity pursuit.
It uses the non-
gaussianity of the residuals once the part of the signals that is predictable from the temporal
correlations has been removed. The update step of this algorithm has some similarities with our
learning rule even though the approach is completely different since we want to exploit temporal
correlations directly rather than formally removing them by a ”predictor”. We also do not assume
pre-whitened data and are not considering nongaussianity.

Our learning rule considers smooth signals that are assumed to be rates. However, it is com-
monly accepted that synaptic plasticity takes into account spike trains of pre- and postsynaptic
neurons looking at the precise timing of the spikes, i.e. Spike Timing Dependent Plasticity (STDP)
[13, 14, 15]. Therefore a spike-based description of our algorithm is currently under study.

Appendix: Stability calculation
By construction, the row vectors {ek, k = 1,..,n} of W ∗ = C−1, the inverse of the mixing matrix,
are solutions of the batch learning rule 9 (n is the number of sources). Assume one of these row
vectors eT
i , (i.e. a ﬁxed point of the dynamic), and consider w = ei + ej a small perturbation in
j . Note that {ek} is a basis because det(C) (cid:54)= 0 (the matrix must be invertible). The rule
direction eT
(9) becomes:

7

signalsdistributiontime delayautocorrelationtimesignalstimesignals˙ei =γ[< x(t + τ1)(ei + ej)T x(t) >

(12)

− < (ei + ej)T x(t)(ei + ej)T x(t + τ1) >

< (ei + ej)T x(t)(ei + ej)T x(t + τ2 >) < x(t + τ2 >)(ei + ej)T x(t) >].

We can expand the terms on the righthand side to ﬁrst order in . Multiplying the stability expres-
sion by eT
j ej = 1 since the recovering of the sources are up to a
j
multiplicative constant), we ﬁnd:

(here we can assume that eT

[eT
j CΛ(τ1)C T ej][eT

i CΛ(τ2)C T ei] − [eT

˙ =γ

i CΛ(τ1)C T ei][eT

j CΛ(τ2)C T ej]

(13)

eT
i CΛ(τ2)C T ei

− 

4[eT

i CΛ(τ1)C T ej][eT

j CΛ(τ2)C T ei]

eT
i CΛ(τ2)C T ei

.

where Λ(τ)ij =< si(t)sj(t + τ) > is the diagonal matrix.
This expression can be simpliﬁed because eT
i C is the unit vector
i
of the form (0,0,...,1,0,...) where the position of the ”1” indicates the solution number 0. Therefore,
i CΛ(τ)C T ek = Λ(τ)ik.
we have eT
The expression of stability becomes

is a row of W ∗ = C−1, so that eT

˙ = γ

Λjj(τ1)Λii(τ2) − Λii(τ1)Λjj(τ2)

Λii(τ2)

(14)

References
[1] A. Hyvaerinen, J. Karhunen, and E. Oja. Independent Component Analysis. Wiley-Interscience, 2001.
[2] L Tong, R Liu, VC Soon, and YF Huang. Indeterminacy and identiﬁability of blind identiﬁcation. IEEE

Trans. on Circuits and Systems, 1991.

[3] A. Belouchrani, KA. Meraim, JF. Cardoso, and E. Moulines. A blind source separation technique based

on second order statistics. IEEE Trans. on Sig. Proc., 1997.

[4] L. Molgedey and H.G. Schuster. Separation of a mixture of independent signals using time delayed

correlations. Phys. Rev. Lett., 72:3634–37, 1994.

[5] A. Ziehe and K. Muller. Tdsep – an efﬁcient algorithm for blind separation using time structure.
[6] KR. Mueller, P. Philips, and A. Ziehe. Jade td : Combining higher-order statistics and temporal informa-

tion for blind source separation (with noise). Proc. Int. Workshop on ICA, 1999.

[7] A. Hyvaerinen. Independent component analysis for time-dependent stochastic processes. Proc. Int. Conf.

on Art. Neur. Net., 1998.

[8] A. Hyvaerinen. Complexity pursuit: Separating interesting components from time-series. Neural Com-

putation, 13:883–898, 2001.

[9] E. Oja. A simpliﬁed neuron model as principal component analyzer. J. Math. Biol., 15:267 –273, 1982.
[10] J.J. Hopﬁeld. Olfactory computation and object perception. PNAS, 88:6462–6466, 1991.
[11] H. Gavert,

Fastica and cocktail party demo.

and A. Hyvarinen.

J. Sarela,

J. Hurri,

http://www.cis.hut.ﬁ/projects/ica/.

[12] R. C. Froemke, M. Poo, and Y. Dan. Spike-timing dependent synaptic plasticity depends on dentritic

location. Nature, 434:221–225, 2005.

[13] G. Bi and M. Poo. Synaptic modiﬁcation by correlated activity: Hebb’s postulate revisited. Annual

Review of Neuroscience, 2001.

[14] H. Markram, J. L¨ubke, M. Frotscher, and B. Sakmann. Regulation of synaptic efﬁcacy by coincidence of

postsynaptic APs and EPSPs. Science, 275:213–215, 1997.

[15] W. Gerstner, R. Kempter, JL. van Hemmen, and H. Wagner. A neuronal learning rule for sub-millisecond

temporal coding. Nature, 383:76–78, 1996.

8

"
681,2007,Modeling Natural Sounds with Modulation Cascade Processes,"Natural sounds are structured on many time-scales. A typical segment of speech, for example, contains features that span four orders of magnitude: Sentences (~1s); phonemes (~0.1s); glottal pulses (~0.01s); and formants (<0.001s). The auditory system uses information from each of these time-scales to solve complicated tasks such as auditory scene analysis. One route toward understanding how auditory processing accomplishes this analysis is to build neuroscience-inspired algorithms which solve similar tasks and to compare the properties of these algorithms with properties of auditory processing. There is however a discord: Current machine-audition algorithms largely concentrate on the shorter time-scale structures in sounds, and the longer structures are ignored. The reason for this is two-fold. Firstly, it is a difficult technical problem to construct an algorithm that utilises both sorts of information. Secondly, it is computationally demanding to simultaneously process data both at high resolution (to extract short temporal information) and for long duration (to extract long temporal information). The contribution of this work is to develop a new statistical model for natural sounds that captures structure across a wide range of time-scales, and to provide efficient learning and inference algorithms. We demonstrate the success of this approach on a missing data task.","Modeling Natural Sounds

with Modulation Cascade Processes

Richard E. Turner and Maneesh Sahani
Gatsby Computational Neuroscience Unit

17 Alexandra House, Queen Square, London, WC1N 3AR, London

Abstract

Natural sounds are structured on many time-scales. A typical segment of speech,
for example, contains features that span four orders of magnitude: Sentences
(∼ 1 s); phonemes (∼ 10−1 s); glottal pulses (∼ 10−2 s); and formants (. 10−3 s).
The auditory system uses information from each of these time-scales to solve com-
plicated tasks such as auditory scene analysis [1]. One route toward understand-
ing how auditory processing accomplishes this analysis is to build neuroscience-
inspired algorithms which solve similar tasks and to compare the properties of
these algorithms with properties of auditory processing. There is however a dis-
cord: Current machine-audition algorithms largely concentrate on the shorter
time-scale structures in sounds, and the longer structures are ignored. The rea-
son for this is two-fold. Firstly, it is a difﬁcult technical problem to construct
an algorithm that utilises both sorts of information. Secondly, it is computation-
ally demanding to simultaneously process data both at high resolution (to extract
short temporal information) and for long duration (to extract long temporal infor-
mation). The contribution of this work is to develop a new statistical model for
natural sounds that captures structure across a wide range of time-scales, and to
provide efﬁcient learning and inference algorithms. We demonstrate the success
of this approach on a missing data task.

1 Introduction

Computational models for sensory processing are still in their infancy, but one promising approach
has been to compare aspects of sensory processing with aspects of machine-learning algorithms
crafted to solve the same putative task. A particularly fruitful approach in this vein uses the genera-
tive modeling framework to derive these learning algorithms. For example, Independent Component
Analysis (ICA) and Sparse Coding (SC), Slow Feature Analysis (SFA), and Gaussian Scale Mix-
ture Models (GSMMs) are examples of algorithms corresponding to generative models that show
similarities with visual processing [3]. In contrast, there has been much less success in the auditory
domain, and this is due in part to the paucity of ﬂexible models with an explicit temporal dimension
(although see [2]). The purpose of this paper is to address this imbalance.

This paper has three parts. In the ﬁrst we review models for the short-time structure of sound and
argue that a probabilistic time-frequency model has several distinct beneﬁts over traditional time-
frequency representations for auditory modeling. In the second we review a model for the long-time
structure in sounds, called probabilistic amplitude demodulation.
In the third section these two
models are combined with the notion of auditory features to produce a full generative model for
sounds called the Modulation Cascade Process (MCP). We then show how to carry out learning and
inference in such a complex hierarchical model, and provide results on speech for complete and
missing data tasks.

1

2 Probabilistic Time-Frequency Representations
Most representations of sound focus on the short temporal structures. Short segments (<10−1 s) are
frequently periodic and can often be efﬁciently represented in a Fourier basis as the weighted sum of
a few sinusoids. Of course, the spectral content of natural sounds changes slowly over time. This is
handled by time-frequency representations, such as the Short-Time Fourier Transform (STFT) and
spectrogram, which indicate the spectral content of a local, windowed section of the sound. More
speciﬁcally, the STFT (xd,t) and spectrogram (sd,t) of a discretised sound (yt0) are given by,

xd,t =

rt−t0 yt0 exp (−iωdt0) ,

sd,t = log |xd,t|.

(1)

T 0X

t0=1

The (possibly frequency dependent) duration of the window (rt−t0) must be chosen carefully, as it
controls whether features are represented in the spectrum or in the time-variation of the spectra. For
example, the window for speech is typically chosen to last for several pitch periods, so that both
pitch and formant information is represented spectrally.

The ﬁrst stage of the auditory pathway derives a time-frequency-like representation mechanically at
the basilar membrane. Subsequent stages extract progressively more complex auditory features, with
structure extending over more time. Thus, computational models of auditory processing often begin
with a time-frequency (or auditory-ﬁlter bank) decomposition, deriving new representations from
the time-frequency coefﬁcients [4]. Machine-learning algorithms also typically operate on the time-
frequency coefﬁcients, and not directly on the waveform. The potential advantage lies in the ease
with which auditory features may be extracted from the STFT representation. There are, however,
associated problems. For example, time-frequency representations tend to be over-complete (e.g.
the number of STFT coefﬁcients tends to be larger than the number of samples of the original sound
T × D > T 0). This means that realisable sounds live on a manifold in the time-frequency space (for
the STFT this manifold is a hyper-plane). Algorithms that solve tasks like ﬁlling-in missing data
or denoising must ensure that the new coefﬁcients lie on the manifold. Typically this is achieved in
an ad hoc manner by projecting time-frequency coefﬁcients back onto the manifold according to an
arbitrary metric [5]. For generative models of time-frequency coefﬁcients, it is difﬁcult to force the
model to generate only on the realisable manifold. An alternative is to base a probabilistic model of
the waveform on the same heuristics that led to the original time-frequency representation. Not only
does this side-step the generation problem, but it also allows parameters of the representation, like
the “window”, to be chosen automatically.

The heuristic behind the STFT – that sound comprises sinusoids in slowly-varying linear superpo-
sition – led Qi et al [6] to propose a probabilistic algorithm called Bayesian Spectrum Estimation
(BSE), in which the sinusoid coefﬁcients (xd,t) are latent variables. The forward model is,

p(xd,t|xd,t−1) = Norm(cid:0)λdxd,t−1, σ2

(cid:1) ,

d

!

(2)

(3)

p(yt|xt) = Norm

xd,t sin (ωdt + φd) , σ2
y

.

d

d. Thus, as λd → 1 and σ2

The prior distribution over the coefﬁcients is Gaussian and auto-regressive, evolving at a rate con-
d → 0 the processes become
trolled by the dynamical parameters λd and σ2
very slow, and as λd → 0 and σ2
d → ∞ they become very fast. More precisely, the length-scale of
the coefﬁcients is given by λd = − log(λd). The observations are generated by a weighted sum of
sinusoids, plus Gaussian noise. This model is essentially a Linear Gaussian State Space System with
time varying weights deﬁned by the sinusoids. Thus, inference is simple, proceeding via the Kalman
Smoother recursions with time-varying weights. In effect, these recursions dynamically adjust the
window used to derive the coefﬁcients, based on the past history of the stimulus. BSE is a model for
the short-time structure of sounds and it will essentially form the bottom level of the MCP. In the
next section we turn our attention to a model of the longer-time structure.

3 Probabilistic Demodulation Cascade

A salient property of the long-time statistics of sounds is the persistence of strong amplitude mod-
ulation [7]. Speech, for example, contains power in isolated regions corresponding to phonemes.

2

 X

p

z(m)
0

= Norm (0, 1) ,

z(m)
t

|z(m)
t−1

= Norm

λmz(m)

t−1, σ2
m

x(m)
t = fa(m)

z(m)
t

x(1)
t = Norm (0, 1) ,

yt =

(cid:17) ∀t > 0,
MY

x(m)
t

.

m=1

(4)

(5)

(6)

The phonemes themselves are localised into words, and then into sentences. Motivated by these
observations, Anonymous Authors [8] have proposed a model for the long-time structures in sounds
using a demodulation cascade. The basic idea of the demodulation cascade is to represent a sound as
a product of processes drawn from a hierarchy, or cascade, of progressively longer time-scale mod-
ulators. For speech this might involve three processes: representing sentences on top, phonemes in
the middle, and pitch and formants at the bottom (e.g. ﬁg. 1A and B). To construct such a repre-
sentation, one might start with a traditional amplitude demodulation algorithm, which decomposes
a signal into a quickly-varying carrier and more slowly-varying envelope. The cascade could then
be built by applying the same algorithm to the (possibly transformed) envelope, and then to the en-
velope that results from this, and so on. This procedure is only stable, however, if both the carrier
and the envelope found by the demodulation algorithm are well-behaved. Unfortunately, traditional
methods (like the Hilbert Transform, or low-pass ﬁltering a non-linear transformation of the stimu-
lus) return a suitable carrier or envelope, but not both. A new approach to amplitude demodulation
is thus called for.

In a nutshell, the new approach is to view amplitude demodulation as a task of probabilistic in-
ference. This is natural, as demodulation is fundamentally ill-posed — there are inﬁnitely many
decompositions of a signal into a positive envelope and real valued carrier — and so prior infor-
mation must always be leveraged to realise such a decomposition. The generative model approach
makes this information explicit. Furthermore, it not necessary to use the recursive procedure (just
described) to derive a modulation cascade: the whole hierarchy can be estimated at once using a
single generative model. The generative model for Probabilistic Amplitude Demodulation (PAD) is

(cid:17)

(cid:16)

(cid:16)

(cid:17)

(cid:16)
(cid:17) ∀m > 1,

p

(cid:16)

A set of modulators (X2:M ) are drawn in a two stage process: First a set of slowly varying processes
(Z2:M ) are drawn from a one-step linear Gaussian prior (identical to Eq. 2). The effective length-
scales of these processes, inherited by the modulators, are ordered such that λm > λm−1. Second
the modulators are formed by passing these variables through a point-wise non-linearity to enforce
positivity. A typical choice might be

(cid:0)z(m)

t

(cid:1) = log(cid:0) exp(cid:0)z(m)

t + a(m)(cid:1) + 1(cid:1),

fa(m)

t

which is logarithmic for large negative values of z(m)
, and linear for large positive values. This
transforms the Gaussian distribution over z(m)
into a sparse, non-negative, distribution, which is a
good match to the marginal distributions of natural envelopes. The parameter a(m) controls exactly
where the transition from log to linear occurs, and consequently alters the degree of sparsity. These
positive signals modulate a Gaussian white-noise carrier, to yield observations y1:T by a simple
point-wise product. A typical draw from this generative model can be seen in Fig. 1C. This model
is a fairly crude one for natural sounds. For example, as described in the previous section, we
expect that the carrier process will be structured and yet it is modelled as Gaussian white noise. The
surprising observation is that this very simple model is excellent at demodulation.

t

Inference in this model typically proceeds by a zero-temperature EM-like procedure. Firstly the
carrier (x(1)
) is integrated out and then the modulators are found by maximum a posteriori (MAP).
Slower, more Bayesian algorithms that integrate over the modulators using MCMC indicate that this
approximation is not too severe, and the results are compelling.

t

4 Modulation Cascade Processes

We have reviewed two contrasting models: The ﬁrst captures the local harmonic structure of sounds,
but has no long-time structure; The second captures long-time amplitude modulations, but models
the short-time structure as white noise. The goal of this section is to synthesise both to form a new
model. We are guided by the observation that the auditory system might implement a similar syn-
thesis. In the well-known psychophysical phenomenon of comodulation masking release (see [9] for
a review), a tone masked by noise with a bandwidth greater than an auditory ﬁlter becomes audible

3

Figure 1: An example of a modulation-cascade representation of speech (A and B) and typical sam-
ples from generative models used to derive that representation (C). A) The spoken-speech waveform
(black) is represented as the product of a carrier (blue), a phoneme modulator (red) and a sentence
modulator (magenta). B) A close up of the ﬁrst sentence (2 s) additionally showing the derived enve-
lope (x(2)
) superposed onto the speech (red, bottom panel). C) The generative model (M = 3)
with a carrier (blue), a phoneme modulator (red) and a sentence modulator (magenta).

t x(3)

t

if the noise masker is amplitude modulated. This suggests that long-time envelope information is
processed and analysed across (short-time) frequency channels in the auditory system.

A simple way to combine the two models would be to express each ﬁlter coefﬁcient of the time-
frequency model as a product of processes (e.g. xd,t = x(1)
d,t ). However, power across even
widely seperated channels of natural sounds can be strongly correlated [7]. Furthermore, comodu-
lation masking release suggests that amplitude-modulation is processed across frequency channels
and not independently in each channel. Presumably this reﬂects the collective modulation of wide-
band (or harmonic) sounds, with features that span many frequencies. Thus, a synthesis of BSE and
PAD should incorporate the notion of auditory features.

d,t x(2)

The forward model. The Modulation Cascade Process (MCP) is given by

(cid:16)

(cid:16)
(cid:16)
(cid:17)

(cid:17)
(cid:17)
km,t|z(m)
z(m)
= Norm(cid:0)µyt, σ2
z(m)
km,0

km,t−1, θ

y

= Norm (0, 1) ,

p

p

yt|x(m)

t

, θ

(cid:16)

p

= Norm

λ(m)z(m)

km,t−1, σ2

x(m)
km,t = f(z(m)

(cid:1) , µyt = X

(m)

m = 1 : 3, t > 0,
km,t, a(m)) m = 1 : 3, t ≥ 0,
gd,k1,k2x(1)

k2,tx(3)

k1,tx(2)

t

sin (ωdt + φd) .

(7)

(8)

(9)

(cid:17)

d,k1,k2

Once again, latent variables are arranged in a hierarchy according to their time-scales (which de-
pend on m). At the top of the hierarchy is a long-time process which models slow structures, like
the sentences of speech. The next level models more quickly varying structure (like phonemes).
Finally, the bottom level of the hierarchy captures short-time variability (intra-phoneme variability
for instance). Unlike in PAD, the middle and lower levels now contain multiple process. So, for
example if K1 = 4 and K2 = 2, there would be four quickly varying modulators in the lower level,
two modulators in the middle level, and one slowly varying modulator at the top (see ﬁg. 2A).

individual spectral features (given byP

The idea is that the modulators in the ﬁrst level independently control the presence or absence of
d gd,k1,k2 sin (ωdt + φd)). For example, in speech a typical
phoneme might be periodic, but this periodicity might change systematically as the speaker alters
their pitch. This change in pitch might be modeled using two spectral features: one for the start of the
phoneme and one for the end, with a region of coactivation in the middle. Indeed it is because speech

4

x(1)1:Tx(2)1:Tx(3)1:T246y1:Ttime /s0.511.5time /sA.B.C. Figure 2: A. Schematic representation of the MCP forward model in the simple case when K1 = 4,
K2 = 2 and D = 6. The hierarchy of latent variables moves from the slowest modulator at the
top (magenta) to the fastest (blue) with an intermediate modulator between (red). The outer-product
of the modulators multiplies the generative weights (black and white, only 4 of the 8 shown). In
turn, these modulate sinusoids (top right) which are summed to produce the observations (bottom
right). B. A draw from the forward model using parameters learned from a spoken-sentence (see
the results section for more details of the model). The grey bars on the top four panels indicate the
region depicted in the bottom four panels.

and other natural sounds are not precisely stationary even over short time-scales that we require the
lowest layer of the hierarchy. The role of the modulators in the second level is to simultaneously
turn on groups of similar features. For example, one modulator might control the presence of all the
harmonic features and the other the broad-band features. Finally the top level modulator gates all
the auditory features at once. Fig. 2B shows a draw from the forward model for a more complicated
example. Promisingly the samples share many features of natural sounds.

Relationship to other models. This model has an interesting relationship to previous statistical
models and in particular to the GSMMs. It is well known that when ICA is applied to data from
natural scenes the inferred ﬁlter coefﬁcients tend not to be independent (see [3, 10]), with coefﬁcients
corresponding to similar ﬁlters sharing power. GSMMs model dependencies using a hierarchical
framework, in which the distribution over the coefﬁcients depends on a set of latent variables that
introduce correlations between their powers. The MCP is similar, in that the higher level latent
variables alter the power of similar auditory features. Indeed, we suggest that the correlations in the
power of ICA coefﬁcients are a sign that AM is prevalent in natural scenes. The MCP can be seen
as a generalisation of the GSMMs to include time-varying latent variables, a deeper hierarchy and a
probabilistic time-frequency representation.

Inference and learning algorithms. Any type of learning in the MCP is computationally demand-
ing. Motivated by speed, and encouraged by the results from PAD, the aim will therefore be to ﬁnd
a joint MAP estimate of the latent variables and the weights, that is
log p(X, Y, G|θ).

X, G = arg max

(10)

X,G

5

x(3)tx(2)tx(1)t00.511.52ytx(3)tx(2)tx(1)t0.840.860.880.90.920.94yttime /s··x(3)tx(2)tx(1)t········+++·=ytA.B.Note that we have introduced a prior over the generative tensor. This prevents an undesirable feature
of combined MAP and ML inference in such models: namely that the weights grow without bound,
enabling the modal values of latent variables to shrink towards zero, increasing their density under
the prior. The resulting cost function is,

log p(X, Y, G|θ) =

log p(yt|x(1)

, x(2)

t ) +

log p(z(m)

km,t|z(m)

km,t−1)

TX

t=1

+

TX

t=0

log

t

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) dz(m)

km,t
dx(m)
km,t

(cid:18) TX
3X
X
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) + log p(z(m)
(cid:19)
+ X

km,0)

m=1

t=1

km

k1,k2,d

log p(gd,k1,k2)

(11)

We would like to optimise this objective-function with respect to the latent variables (x(m)
km,t) and the
generative tensor (gd,k1,k2). There are, however, two main obstacles. The ﬁrst is that there are a
large number of latent variables to estimate (T × (K1 + K2)), making inference slow. The second
is that the generative tensor contains a large number of elements D × K1 × K2, making learning
slow too. The solution is to ﬁnd a good initialisation procedure, and then to ﬁne-tune using a slow
EM-like algorithm that iterates between updating the latents and the weights. First we outline the
initialisation procedure.

The key to learning complicated hierarchical models is to initialise well, and so the procedure devel-
oped for the MCP will be explained in some detail. The main idea is to learn the model one layer at
a time. This is achieved by clamping the upper layers of the hierarchy that are not being learned to
unity. In the ﬁrst stage of the initialisation, for example, the top and middle levels of the hierarchy
are clamped and the mean of the emission distribution becomes

γd,k1x(1)

k1,t sin (ωdt + φd) ,

(12)

µyt = X

d,k1

where γd,k1 =P

k2 gd,k1,k2. Learning and inference then proceed by gradient based optimisation of
the cost-function (log p(X, Y, G|θ)) with respect to the un-clamped latents (x(1)
k1,t) and the contracted
generative weights (γd,k1). This is much faster than the full optimisation as there are both fewer
latents and fewer parameters to estimate. When this process is complete, the second layer of latent
variables is un-clamped, and learning of these variables commences. This requires the full generative
tensor, which must be initialised from the contracted generative weights learned at the previous
stage. One choice is to set the individual weights to their averages gd,k1,k2 = 1
γd,k1 and this
K2
works well, but empirically slows learning. An alternative is to use small chunks of sounds to
learn the lower level weights. These chunks are chosen to be relatively stationary segments that
have a time-scale similar to the second-level modulators. This allows us to make the simplifying
assumption that just one second-level modulator was active during the chunk. The generative tensor
can be therefore be initialised using gd,k1,k2 = γd,k1δk2,j. Typically this method causes the second
stage of learning to converge faster, and to a similar solution.

In contrast to the initialisation, the ﬁne tuning algorithm is simple. In the E-Step the latent variables
are updated simultaneously using gradient based optimisation of Eq. 11. In the M-Step, the gen-
erative tensor is updated using co-ordinate ascent. That is to say that we sequentially update each
gk1,k2 using gradient based optimisation of Eq. 11 and iterate over k1 and k2. In principle, joint
optimisation of the generative tensor and latent variables is possible, but the memory requirements
are prohibitive. This is also why co-ordinate ascent is used to learn the generative tensor (rather than
using the usual linear regression solution which involves a prohibitive matrix inverse).

5 Results

The MCP was trained on a spoken sentence, lasting 2s and sampled at 8000Hz, using the algorithm
outlined in the previous section. The time-scales of the modulators were chosen to be {20 ms,
200 ms, 2 s}. The time-frequency representation had D/2 = 100 sines and D/2 = 100 cosines
spaced logarithmically from 100 − 4000Hz. The model was given K1 = 18 latent variables in the
ﬁrst level of the hierarchy, and K2 = 6 in the second. Learning took 16hrs to run on a 2.2 GHz
Opteron with 2Gb of memory.

6

The learned spectral features (pg2

Figure 3: Application of the MCP to speech. Left panels: The inferred latent variable hierarchy.
At top is the sentence modulator (magenta). Next are the phoneme modulators, followed by the
intra-phoneme modulators. These are coloured according to which of the phoneme modulators they
interact most strongly with. The speech waveform is shown in the bottom panel. Right panels:
cos) coloured according to phoneme modulator. For ex-
ample, the top panel show the spectra from gk1=1:18,k2=1. Spectra corresponding to one phoneme
modulator look similar and offer the features only differ in their phase.

sin + g2

[t]

The results can be seen in Fig. 3. The MCP recovers a sentence modulator, phoneme modulators,
and intra-phoneme modulators. Typically a pair of features are used to model a phoneme, and often
they have similar spectra as expected. The spectra fall into distinct classes: those which are har-
monic (modelling voiced features) and those which are broad-band (modelling unvoiced features).
One way of assessing which features of speech the model captures is to sample from the forward
model using the learned parameters. This can be seen in Fig. 2B. The conclusion is that the model
is capturing structure across a wide range of time-scales: formants and pitch structure, phoneme
structure, and sentence structure.

There are, however, two noticeable differences between the real and generated data. The ﬁrst is that
the generated data contain fewer transients and noise segments than natural speech, and more vowel-
like components. The reason for this is that at the sampling rates used, many of the noisy segments
are indistinguishable from white-noise and are explained using observation noise. These problems
are alleviated by moving to higher sampling rates, but the algorithm is then markedly slower. The
second difference concerns the inferred and generated latent variables in that the former are much
sparser than the latter. The reason is that learned generative tensor contains many gk1,k2 which are
nearly zero. In generation, this means that signiﬁcant contributions to the output are only made when
particular pairs of phoneme and intra-phoneme modulators are active. So although many modulators
are active at one time, only one or two make sizeable contributions. Conversely, in inference, we
can only get information about the value of a modulator when it is part of a contributing pair. If this
is not the case, the inference goes to the maximum of the prior which is zero. In effect there are
large error-bars on the non-contributing components’ estimates.

Finally, to indicate the improvement of the MCP over PAD and BSE, we compare the algorithms
abilities to ﬁll in missing sections of a spoken sentence. The average root-mean-squared (RMS)
error per sample is used as a metric to compare the algorithms. In order to use the MCP to ﬁll in the
missing data, it is ﬁrst necessary to learn a set of auditory features. The MCP was therefore trained
on a different spoken sentence from the same speaker, before inference was carried out on the test
data. To make the comparison fair, BSE is given an identical set of sinusoidal basis functions as
MCP, and the associated smoothness priors were learned on the same training data.

Typical results can be seen in ﬁg. 4. On average the RMS errors for MCP, BSE and PAD were:
{0.10, 0.30, 0.41}. As PAD models the carrier as white noise it predicts zeros in the missing regions

7

x(3)tx(2)t,k2x(1)t,k100.511.5time /sy01000200030004000frequency /HZFigure 4: A selection of typical missing data results for three phonemes (columns). The top row
shows the original speech segement with the missing regions shown in red. The middle row shows
the predictions made by the MCP and the bottom row those made by BSE.

and therefore it merely serves as a baseline in these experiments. Both MCP and BSE smoothly
interpolate their latent variables over the missing region. However, whereas BSE smoothly inter-
polates each sinusoidal component independently, MCP interpolates the set of learned auditory fea-
tures in a complex manner determined by the interaction of the modulators. It is for this reason that
it improves over BSE by such a large margin.

6 Conclusion

We have introduced a neuroscience-inspired generative model for natural sounds that is capable of
capturing structure spanning a wide range of temporal scales. The model is a marriage between a
probabilistic time-frequency representation (that captures the short-time structure) and a probabilis-
tic demodulation cascade (that captures the long-time structure). When the model is trained on a
spoken sentence, the ﬁrst level of the hierarchy learns auditory features (weighted sets of sinusoids)
that capture structures like different voiced sections of speech. The upper levels comprise a tem-
porally ordered set of modulators are used to represent sentence structure, phoneme structure and
intra-phoneme variability. The superiority of the new model over its parents was demonstrated in
a missing data experiment where it out-performed the Bayesian time-frequency analysis by a large
margin.

References

[1] Bregman, A.S. (1990) Auditory Scene Analysis. MIT Press.
[2] Smith E. & Lewicki, M.S. (2006) Efﬁcient Auditory Coding. Nature 439 (7079).
[3] Simoncelli, E.P. (2003) Vision and the statistics of the visual environment. Curr Opin Neurobi 13(2):144-9.
[4] Patterson, R.D. (2000) Auditory images: How complex sounds are represented in the auditory system. J

Acoust Soc Japan (E) 21(4):183-190.

[5] Grifﬁn, D. & Lim J. (1984) Signal estimation from modiﬁed short-time Fourier transform. IEEE Trans. on

ASSP 32(2):236-243.

[6] Qi, Y., Minka, T. & Picard, R.W. (2002) Bayesian Spectrum Estimation of Unevenly Sampled Nonstation-

ary Data. MIT Media Lab Technical Report Vismod-TR-556.

[7] Attias, H. & Schreiner, C.E. (1997) Low-Order Temporal Statistics of Natural Sounds. Adv in Neural Info

Processing Sys 9. MIT Press.

[8] Anonymous Authors (2007) Probabilistic Amplitude Demodulation. ICA 2007 Conference Proceedings.

Springer, in press.

[9] Moore, B.C.J. (2003) An Introduction to the Psychology of Hearing. Academic Press.
[10] Karklin, Y. & Lewicki, M.S. (2005) A hierarchical Bayesian model for learning nonlinear statistical reg-

ularities in nonstationary natural signals. Neural Comput 17(2):397-423.

8

originalMCP0.080.10.120.140.160.18time /sBSE0.080.10.120.140.16time /s0.150.20.250.3time /s"
863,2007,Fast and Scalable Training of Semi-Supervised CRFs with Application to Activity Recognition,"We present a new and efficient semi-supervised training method for parameter estimation and feature selection in conditional random fields (CRFs). In real-world applications such as activity recognition, unlabeled sensor traces are relatively easy to obtain whereas labeled examples are expensive and tedious to collect. Furthermore, the ability to automatically select a small subset of discriminatory features from a large pool can be advantageous in terms of computational speed as well as accuracy. In this paper, we introduce the semi-supervised virtual evidence boosting (sVEB) algorithm for training CRFs -- a semi-supervised extension to the recently developed virtual evidence boosting (VEB) method for feature selection and parameter learning. Semi-supervised VEB takes advantage of the unlabeled data via minimum entropy regularization -- the objective function combines the unlabeled conditional entropy with labeled conditional pseudo-likelihood. The sVEB algorithm reduces the overall system cost as well as the human labeling cost required during training, which are both important considerations in building real world inference systems. In a set of experiments on synthetic data and real activity traces collected from wearable sensors, we illustrate that our algorithm benefits from both the use of unlabeled data and automatic feature selection, and outperforms other semi-supervised training approaches.","Fast and Scalable Training of Semi-Supervised CRFs

with Application to Activity Recognition

Maryam Mahdaviani

Computer Science Department
University of British Columbia

Vancouver, BC, Canada

Tanzeem Choudhury

Intel Research

1100 NE 45th Street

Seattle, WA 98105,USA

Abstract

We present a new and efﬁcient semi-supervised training method for parameter es-
timation and feature selection in conditional random ﬁelds (CRFs). In real-world
applications such as activity recognition, unlabeled sensor traces are relatively
easy to obtain whereas labeled examples are expensive and tedious to collect.
Furthermore, the ability to automatically select a small subset of discriminatory
features from a large pool can be advantageous in terms of computational speed as
well as accuracy. In this paper, we introduce the semi-supervised virtual evidence
boosting (sVEB) algorithm for training CRFs – a semi-supervised extension to the
recently developed virtual evidence boosting (VEB) method for feature selection
and parameter learning. The objective function of sVEB combines the unlabeled
conditional entropy with labeled conditional pseudo-likelihood.
It reduces the
overall system cost as well as the human labeling cost required during training,
which are both important considerations in building real-world inference systems.
Experiments on synthetic data and real activity traces collected from wearable
sensors, illustrate that sVEB beneﬁts from both the use of unlabeled data and au-
tomatic feature selection, and outperforms other semi-supervised approaches.

1 Introduction

Conditional random ﬁelds (CRFs) are undirected graphical models that have been successfully ap-
plied to the classiﬁcation of relational and temporal data [1]. Training complex CRF models with
large numbers of input features is slow, and exact inference is often intractable. The ability to select
the most informative features as needed can reduce the training time and the risk of over-ﬁtting of
parameters. Furthermore, in complex modeling tasks, obtaining the large amount of labeled data
necessary for training can be impractical. On the other hand, large unlabeled datasets are often easy
to obtain, making semi-supervised learning methods appealing in various real-world applications.
The goal of our work is to build an activity recognition system that is not only accurate but also scal-
able, efﬁcient, and easy to train and deploy. An important application domain for activity recognition
technologies is in health-care, especially in supporting elder care, managing cognitive disabilities,
and monitoring long-term health. Activity recognition systems will also be useful in smart environ-
ments, surveillance, emergency and military missions. Some of the key challenges faced by current
activity inference systems are the amount of human effort spent in labeling and feature engineering
and the computational complexity and cost associated with training. Data labeling also has privacy
implications because it often requires human observers or recording of video. In this paper, we intro-
duce a fast and scalable semi-supervised training algorithm for CRFs and evaluate its classiﬁcation
performance on extensive real world activity traces gathered using wearable sensors. In addition
to being computationally efﬁcient, our proposed method reduces the amount of labeling required
during training, which makes it appealing for use in real world applications.

1

Several supervised techniques have been proposed for feature selection in CRFs. For discrete fea-
tures, McCallum [2] suggested an efﬁcient method for feature induction by iteratively increasing
conditional log-likelihood. Dietterich [3] applied gradient tree boosting to select features in CRFs
by combining boosting with parameter estimation for 1D linear-chain models. Boosted random
ﬁelds (BRFs) [4] combine boosting and belief propagation for feature selection and parameter esti-
mation for densely connected graphs that have weak pairwise connections. Recently, Liao et.al. [5]
developed a more general version of BRFs, called virtual evidence boosting (VEB) that does not
make any assumptions about graph connectivity or the strength of pairwise connections. The ob-
jective function in VEB is a soft version of maximum pseudo-likelihood (MPL), where the goal is
to maximize the sum of local log-likelihoods given soft evidence from its neighbors. This objective
function is similar to that used in boosting, which makes it suitable for uniﬁed feature selection and
parameter estimation. This approximation applies to any CRF structures and leads to a signiﬁcant
reduction in training complexity and time. Semi-supervised training techniques have been exten-
sively explored in the case of generative models and naturally ﬁt under the expectation maximization
framework [6]. However, it is not straight forward to incorporate unlabeled data in discriminative
models using the traditional conditional likelihood criteria. A few semi-supervised training meth-
ods for CRFs have been proposed that introduce dependencies between nearby data points [7, 8].
More recently, Grandvalet and Bengio [9] proposed a minimum entropy regularization framework
for incorporating unlabeled data. Jiao et.al. [10] used this framework and proposed an objective
function that combines the conditional likelihood of the labeled data with the conditional entropy of
the unlabeled data to train 1D CRFs, which was extended to 2D lattice structures by Lee et.al. [11].
In our work, we combine the minimum entropy regularization framework for incorporating unla-
beled data with VEB for training CRFs. The contributions of our work are: (i) semi-supervised
virtual evidence boosting (sVEB) - an efﬁcient technique for simultaneous feature selection and
semi-supervised training of CRFs, which to the best of our knowledge is the ﬁrst method of its
kind, (ii) experimental results that demonstrate the strength of sVEB, which consistently outper-
forms other training techniques on synthetic data and real-world activity classiﬁcation tasks, and
(iii) analysis of the time and complexity requirements of our algorithm, and comparison with other
existing techniques that highlight the signiﬁcant computational advantages of our approach. The
sVEB algorithm is fast and easy to implement and has the potential of being broadly applicable.

2 Approaches to training of Conditional Random Fields

Maximum likelihood parameter estimation in CRFs involves maximizing the overall conditional
log-likelihood, where x is the observation sequence and y is the hidden state sequence:

L(θ) = log(p(y|x, θ)) − (cid:107)θ(cid:107)/2 = log

− (cid:107)θ(cid:107)/2

(1)

K(cid:80)
K(cid:80)

k=1

k=1

exp(

θkfk(x, y))

exp(

θkfk(x, y(cid:48)))

(cid:80)

y(cid:48)

The conditional distribution is deﬁned by a log-linear combination of k features functions fk asso-
ciated with weight θk. A regularizer on θ is used to keep the weights from getting too large and
to avoid overﬁtting1. For large CRFs exact inference is often intractable and approximate methods
such as mean ﬁeld approximation or loopy belief propagation [12, 13] are used.
An alternative to approximating the conditional likelihood is to change the objective function.
MPL [14] and VEB [5] are such techniques. For MPL the CRF is cut into a set of independent
patches; each patch consists of a hidden node or class label yi, the true value of its direct neighbors
and the observations, i.e., the Markov Blanket(M Byi) of the node. The parameter estimation then
becomes maximizing the pseudo log-likelihood:

Lpseudo(θ) =

log(p(yi|M Byi , θ)) =

log

N(cid:80)

i=1

exp(

θkfk(M Byi ,yi))

exp(

θkfk(M By(cid:48)

i

,y(cid:48)

i))

K2k=1
K2k=1

2y(cid:48)

i

N(cid:80)

i=1

MPL has been known to over-estimate the dependency parameters in some cases and there is no
general guideline on when it can be safely used [15].

1When a prior is used in the maximum likelihood objective function as a regularizer – the second term in eq. (1), the method is in fact

called maximum a posteriori.

2

2.1 Virtual evidence boosting
By extending the standard LogitBoost algorithm [16], VEB integrates boosting based feature se-
lection into CRF training. The objective function used in VEB is very similar to MPL, except that
VEB uses the messages from the neighboring nodes as virtual evidence instead of using the true
labels of neighbors. The use of virtual evidence helps to reduce over-estimation of neighborhood
dependencies. We brieﬂy explain the approach here but please refer to [5] for more detail.
VEB incorporates two types of observations nodes: (i) hard evidence corresponding to the observa-
tions ve(xi), which are indicator functions at the observation values and (ii) soft evidence, corre-
sponding to the messages from neighboring nodes ve(n(yi)), which are discrete distributions over
the hidden states. Let vei (cid:44) {ve(xi), ve(n(yi))}. The objective function of VEB is as follows:

(cid:80)
(cid:80)
(cid:80)

vei

y(cid:48)

i

vei

K(cid:80)
K(cid:80)

k=1

k=1

vei exp(

θkfk(vei, yi))

vei exp(

(2)

θkfk(vei, y(cid:48)
i))

N(cid:88)

i=1

LV EB(θ) =

log(p(yi|vei, θ)), where p(yi|vei, θ) =

VEB learns a set weak learners fts iteratively and estimates the combined feature Ft = Ft−1 + ft
by solving the following weighted least square error(WLSE) problem:

N(cid:88)
where wi = p(yi|vei)(1 − p(yi|vei)), zi = yi − 0.5
p(yi|vei)

wiE(f(vei) − zi)2 = arg min

(cid:88)

N(cid:88)

i=1

i=1

vei

[

f

wip(yi|vei)(f(vei) − zi)2]

(3)

(4)

ft(vei) = arg min

f

The wi and zi in equation 4 are the boosting weight and working response respectively for the ith
data point, exactly as in LogitBoost. However, the least square problem for VEB (eq.3) involves
N X points because of virtual evidence as opposed to N points in LogitBoost. Although eq. 4 is
given for the binary case (i.e. yi ∈ {0, 1}), it is easily extendible to the multi-class case and we have
done that in our experiments. At each iteration, vei is updated as messages from n(yi) changes with
the addition of new features. We run belief propagation (BP) to obtain the virtual evidence before
each iteration. The CRF feature weights, θ’s are computed by solving the WLSE problem, where
the local features, nki is the count of feature k in data instance i and the compatibility features, nki
is the virtual evidence from the neighbors.: θk =

wizinki/

N(cid:80)

N(cid:80)

winki.

i=1

i=1

2.2 Semi-supervised training
For semi-supervised training of CRFs, Jiao et.al. [10] have proposed an algorithm that utilizes unla-
beled data via entropy regularization – an extension of the approach proposed by [9] to structured
CRF models. The objective function that is maximized during semi-supervised training of CRFs is
given below, where (xl, yl) and (xu, yu) represent the labeled and unlabeled data respectively:

LSS(θ) = log p(yl|xl, θ) + α

p(yu|xu, θ)log p(yu|xu, θ) − (cid:107)θ(cid:107)/2

(cid:80)

yu

By minimizing the conditional entropy of the unlabeled data, the algorithm will generally ﬁnd la-
beling of the unlabeled data that mutually reinforces the supervised labels. One drawback of this
objective function is that it is no longer concave and in general there will be local maxima. The
authors [10] showed that this method is still effective in improving an initial supervised model.

3 Semi-supervised virtual evidence boosting

In this work, we develop semi-supervised virtual evidence boosting (sVEB) that combines feature
selection with semi-supervised training of CRFs. sVEB extends the VEB framework to take advan-
tage of unlabeled data via minimum entropy regularization similar to [9, 10, 11]. The new objective
function LsV EB we propose is as follows, where (i = 1··· N) are labeled and (i = N + 1··· M)
are unlabled examples:

M(cid:88)

(cid:88)

LsV EB =

log p(yi|vei) + α

p(y(cid:48)

i|vei) log p(y(cid:48)

i|vei)

(5)

N(cid:88)

i=1

i=N +1

y(cid:48)

i

3

The sVEB aglorithm, similar to VEB, maximizes the conditional soft pseudo-likelihood of the la-
beled data but in addition minimizes the conditional entropy over unlabeled data. The α is a tuning
parameter for controlling how much inﬂuence the unlabeled data will have.
By considering the soft pseudo-likelihood in LsV EB and using BP to estimate p(yi|vei), sVEB can
use boosting to learn the parameters of CRFs. The virtual evidence from the neighboring nodes
captures the label dependencies. There are three different types of feature functions fs that’s used:
for continuous observations f1(xi) is a linear combination of decision stumps, for discrete obser-
vations the learner f2(xi) is expressed as indicator functions, and for virtual evidences the weak
learner f3(xi) is the weighted sum of two indicator functions (for binary case). These functions are
computed as follows, where δ is an indicator function, h is a threshold for the decision stump, and
D is the number of dimensions of the observations:
f1(xi) = θ1δ(xi ≥ h) + θ2δ(xi < h), f2(xi) =

θkδ(xi = d), f3(yi) =

θkδ(yi = k) (6)

1(cid:88)

D(cid:88)

k=1

k=0

Similar to LogitBoost and VEB, the sVEB algorithm estimates a combined feature function F that
maximizes the objective by sequentially learning a set of weak learners, ft’s (i.e. iteratively selecting
features). In other words, sVEB solves the following weighted least-square error (WLSE) problem
to learn fts:

N(cid:88)

(cid:88)

(cid:88)

(cid:88)

M(cid:88)

wip(yi|vei)(f(xi) − zi)2 +

wip(y(cid:48)

i|vei)(f(xi) − zi)2]

(7)

ft = arg min

[

f

i=1

vei

i=N +1

y(cid:48)

i

vei

For labeled data (ﬁrst term in eq.7), boosting weights, wi’s, and working responses, zi’s, are com-
puted as described in equation 4. But for the case of unlabeled data the expression for wi and zi
becomes more complicated because of the entropy term. We present the equations for wi and zi
below, please refer to the Appendix for the derivations:

wi = α2(1 − p(yi|vei))[p(yi|vei)(1 − p(yi|vei)) + log p(yi|vei)]

zi =

(yi − 0.5)p(yi|vei)(1 − log p(yi|vei))

α[p(yi|vei)(1 − p(yi|vei)) + log p(yi|vei)]

(8)

M(cid:80)

(cid:80)

M(cid:80)

(cid:80)

The soft evidence corresponding to messages from the neighboring nodes is obtained by running BP
on the entire training dataset (labeled and unlabeled). The CRF feature weights θks are computed
by solving the WLSE problem (e.q.(7)), θk =

wizinki/

winki

i=1

yi

i=1

yi

Algorithm 1 gives the pseudo-code for sVEB. The main difference between VEB and sVEB are
steps 7 − 10, where we compute wi’s and zi’s for all possible values of yi based on the virtual
evidence and observations of unlabeled training cases. The boosting weights and working responses
are computed using equation (8). The weighted least-square error (WLSE) equation (eq. 7) in step
10 of sVEB is different from that of VEB and the solution results in slightly different CRF feature
weights, θ’s. One of the major advantages of VEB and sVEB over ML and sML is that the parameter
estimation is done by mainly performing feature counting. Unlike ML and sML, we do not need to
use an optimizer to learn the model parameters which results in a huge reduction in the time required
to train the CRF models. Please refer to the complexity analysis section for details.

4 Experiments

We conduct two sets of experiments to evaluate the performance of the sVEB method for training
CRFs and the advantage of performing feature selection as part of semi-supervised training.
In
the ﬁrst set of experiments, we analyze how much the complexity of the underlying CRF and the
tuning parameter α effect the performance using synthetic data. In the second set of experiments, we
evaluate the beneﬁt of feature selection and using unlabeled data on two real-world activity datasets.
We compare the performance of the semi-supervised virtual evidence boosting(sVEB) presented in
this paper to the semi-supervised maximum likelihood (sML) method [10]. In addition, for the ac-
tivity datasets, we also evaluate an alternative approach (sML+Boost), where a subset of features
is selected in advance using boosting. To benchmark the performance of the semi-supervised tech-
niques, we also evaluate three different supervised training approaches, namely maximum likelihood

4

for t = 1, 2,··· , T do

Algorithm 1: Training CRFs using semi-supervised VEB
inputs : structure of CRF and training data (xi, yi), with yi ∈ {0, 1}, 1 ≤ i ≤ M, and F0 = 0
output: Learned FT and their corresponding weights, θ
1
2
3
4
5
6
7
8
9
10
11
12

end
Obtain “best” weak learner ft according to equation (7) and update Ft = Ft−1 + ft ;

Compute likelihood p(yi|vei);
Compute wi and zi using equation (8)

Run BP using Ft to get virtual evidences vei;
for i = 1, 2,··· , N do

Compute likelihood p(yi|vei);
Compute wi and zi using equation (4)

end
for i = N + 1, ..., M and yi = 0, 1 do

end

Figure 1: Accuracy of sML and sVEB for different number of states, local features and different values of α.

method using all observed features(ML), (ML+Boost) using a subset of features selected in advance,
and virtual evidence boosting (VEB). All the learned models are tested using standard maximum a
posteriori(MAP) estimate and belief propagation. We used a l2-norm shrinkage prior as a regularizer
for the ML and sML methods.

4.1 Synthetic data
The synthetic data is generated using a ﬁrst-order Markov Chain with self-transition probabilities
set to 0.9. For each model, we generate ﬁve sequences of length 4,000 and divide each trace into
sequences of length 200. We randomly choose 50% of them as the labeled and the other 50% as un-
labeled training data. We perform leave-one-out cross-validation and report the average accuracies.
To measure how the complexity of the CRFs affects the performance of the different semi-supervised
methods, we vary the number of local features and the number of states. First, we compare the per-
formance of sVEB and sML on CRFs with increasing the number of features. The number of states
is set to 10 and the number of observation features is varied from 20 to 400 observations. Figure
(1a) shows the average accuracy for the two semi-supervised training methods and their conﬁdence
intervals. The experimental results demonstrate that sVEB outperforms sML as we increase the di-
mension of observations (i.e. the number of local features). In the second experiment, we increase
the number of classes and keep the dimension of observations ﬁxed to 100. Figure (1b) demonstrates
that sVEB again outperforms sML as we increase the number of states. Given the same amount of
training data, sVEB is less likely to overﬁt because of the feature selection step. In both these ex-
periments we set the value of tuning parameter, α, to 1.5. To explore the effect of tuning parameter
α, we vary the value of α from 0.1 to 10 , while setting the number of states to 10 and the number
of dimensions to 100. Figure (1c) shows that the performance of both sML and sVEB depends on
the value of α but the accuracy decreases for large α’s similar to the sML results presented in [10].

5

0102030400.60.650.70.750.80.850.9Number of statesAccuracysMLsVEB(b)01002003004005000.50.60.70.80.9Dimension of ObservationsAccuracysMLsVEB(a)02468100.70.750.80.850.90.951Values of αsMLsVEBAccuracy(c)ML+all obs ML+Boost VEB

ML+all obs ML+Boost VEB

Figure 2: An example of a sensor trace and a classiﬁcation trace

Labeled Average Accuracy (%) - Dataset 1
60% 62.7 ± 6.6 69.4 ± 3.9 82.6 ± 7.3
80% 73.0 ± 4.2 81.8 ± 4.7 90.3 ± 4.7
100% 77.8 ± 3.4 87.0 ± 2.3 91.5 ± 3.8

Labeled Average Accuracy (%) - Dataset 2
60% 74.3 ± 3.7 75.8 ± 3.3 88.5 ± 5.1
80% 80.6 ± 2.9 84.8 ± 2.9 93.4 ± 3.8
100% 86.2 ± 3.1 87.5 ± 3.1 93.8 ± 4.6
Table 1: Accuracy ± 95% conﬁdence interval of the supervised algorithms on activity datasets 1 and 2

4.2 Activity dataset
We collected two activity datasets using wearable sensors, which include audio, acceleration, light,
temperature, pressure, and humidity. The ﬁrst dataset contains instances of 8 basic physical activities
(e.g. walking, running, going up/down stairs, going up/down elevator, sitting, standing, and brushing
teeth) from 7 different users. There is on average 30 minutes of data per user and a total of 3.5 hours
of data that is manually labeled for training and testing purposes. The data is segmented into 0.25s
chunks resulting in a total of 49613 data points. For each chunk, we compute 651 features, which
include signal energy in log and linear frequency bands, autocorrelation, different entropy measures,
mean, variances etc. The features are chosen based on what is used in existing activity recognition
literature and a few additional ones that we felt could be useful. During training, the data from
each person is divided into sequences of length 200 and fed into linear chain CRFs as observations.
The second dataset contains instances of 5 different indoor activities (e.g. computer usage, meal,
meeting, watching TV and sleeping) from a single user. We recorded 15 hours of sensor traces over
12 days. As this set contains longer time-scale activities, the data is segmented into 1 minute chunks
and 321 different features are computed, similar to the ﬁrst dataset. There are a total of 907 data
points. These features are fed into CRFs as observations, one linear chain CRF is created per day.
We evaluate the performance of supervised and semi-supervised training algorithms on these two
datasets. For the semi-supervised case, we randomly select 40% of the sequences for a given person
or a given day as labeled and a different subset as the unlabeled training data. We compare the
performance of sML and sVEB as we incorporate more unlabeled data (20%, 40% and 60%) into
the training process. We also compare the supervised techniques, ML, ML+Boost, and VEB, with
increasing amount of labeled data. For all the experiments, the tuning parameter α is set to 1.5. We
perform leave-one-person-out cross-validation on dataset 1 and leave-one-day-out cross-validation
on dataset 2 and report the average the accuracies. The number of features chosen (i. e. through
the boosting iterations) is set to 50 for both datasets – including more features did not signiﬁcantly
improve the classiﬁcation performance.
For both datasets, incorporating more unlabeled data improves accuracy. The sML estimate of the
CRF parameters performs the worst. Even with the shrinkage prior, the high dimensionality can still
cause over-ﬁtting and lower the accuracy. Whereas parameter estimation and feature selection via
sVEB consistently results in the highest accuracy. The (sML+Boost) method performs better than
sML but does not perform as well as when feature selection and parameter estimation is done within
a uniﬁed framework as in sVEB. Table 2 summarize our results. The results of supervised learn-

Average Accuracy (%) - Dataset 1
Un-
sVEB
labeled sML+all obs sML+Boost
20% 60.8 ± 5.4 66.4 ± 4.2 72.6 ± 2.3
40% 68.1 ± 4.8 76.8 ± 3.4 78.5 ± 3.4
60% 74.9 ± 3.1 81.3 ± 3.9 85.3 ± 4.1

Average Accuracy (%) - Dataset 2
Un-
sVEB
labeled sML+all obs sML+Boost
20% 71.4 ± 3.2 70.5 ± 5.3 79.9 ± 4.2
40% 73.5 ± 5.8 74.1 ± 4.6 83.5 ± 6.3
60% 75.6 ± 3.9 77.8 ± 3.2 87.4 ± 4.7

Table 2: Accuracy ± 95% conﬁdence interval of semi-supervised algorithms on activity datasets 1 and 2

6

TimeClasses1000200030004000500012345678Sensor TracesTimeGround truthInferenceML+all obs ML+Boost VEB

Labeled Average Accuracy (%) - Dataset 2
5% 59.2 ± 6.5 65.7 ± 8.3 71.2 ± 5.7
20% 66.9 ± 5.9 67.3 ± 8.5 77.4 ± 3.6

Labeled Average Accuracy (%) - Dataset 2
5% 71.2 ± 4.1 68.3 ± 6.7 79.7 ± 7.9
20% 71.4 ± 6.3 73.8 ± 5.2 83.1 ± 6.4
Table 3: Accuracy ± 95% conﬁdence interval of semi-supervised algorithms on activity datasets 1 and 2

ML+all obs ML+Boost VEB

ing algorithms are presented in Table 1. Similar to the semi-supervised results, the VEB method
performs the best, the ML is the worst performer, and the accuracy numbers for the (ML+Boost)
method is in between. The accuracy increases if we incorporate more labeled data during training.
To evaluate sVEB when a small amount of labeled data is available, we performed another set of
experiments on datasets 1 and 2, where only 5% and 20% of the training data is labeled respec-
tively. We used all the available unlabeled data during training. The results are shown in table 3.
These experiments clearly demonstrate that although adding more unlabeled data is not as helpful
as incorporating more labeled data, the use of cheap unlabeled data along with feature selection can
signiﬁcantly boost the performance of the models.
4.3 Complexity Analysis
The sVEB and VEB algorithm are signiﬁcantly faster than ML and sML because they do not need
to use optimizers such as quasi-newton methods to learn the weight parameters. For each training
iteration in sML the cost of running BP is O(clns2 + cun2s3) [10] whereas the cost of each boosting
iteration in sVEB is O((cl + cu)ns2). An efﬁcient entropy gradient computation is proposed in [17],
which reduces the cost of sML to O((cl + cu)ns2) but still requires an optimizer to maximize the
log-likelihood. Moreover, the number of training iterations needed is usually much higher than the
number of boosting iterations because optimizers such as L-BFGS require many more iterations to
reach convergence in high dimensional spaces. For example, for dataset 1, we needed about 1000
iterations for sML to converge but we ran sVEB for only 50 iterations. Table 4 shows the time for
performing the experiments on activity datasets (as described in the previous section) 2. On the
other hand the space complexity of sVEB is linearly smaller than sML and ML. Similar to ML, sML
has the space complexity of O(ns2D) in the best case [10]. VEB and sVEB have a lower space
cost of O(ns2Db), because of the feature selection step Db (cid:191) D usually. Therefore, the difference
becomes signiﬁcant when we are dealing with high dimensional data, particularly if they include a
large number of redundant features.

Time (hours)

ML ML+Boost VEB sML sML+Boost sVEB

Dataset 1 34
Dataset 2 7.5

18
4.25

5 Conclusion

2.5 96
0.4 10.5
Table 4: Training time for the different algorithms.

4
0.6

D, Db

48
8

n
cl
cu
s

length of training sequence
number of labeled training sequences
number of unlabeled training sequences
number of states
dimension of observations

We presented sVEB, a new semi-supervised training method for CRFs, that can simultaneously
select discriminative features via modiﬁed LogitBoost and utilize unlabeled data via minimum-
entropy regularization. Our experimental results demonstrate the sVEB signiﬁcantly outperforms
other training techniques in real-world activity recognition problems. The uniﬁed framework for
feature selection and semi-supervised training presented in this paper reduces the computational and
human labeling costs, which are often the major bottlenecks in building large classiﬁcation systems.

Acknowledgments
The authors would like to thank Nando de Freitas and Lin Liao for many helpful discussions. This work was
supported by the NSF under grant number IIS 0433637 and NSERC Canada Graduate Scholarship.

References
[1] J. Lafferty, A. McCallum, and F. Pereira. Conditional random ﬁelds: Probabilistic models for segmenting
and labeling sequence data. In Proc. of the International Conference on Machine Learning (ICML), 2001.

2The experiments were run in Matlab environment and as a result they took longer.

7

[2] Andrew McCallum. Efﬁciently inducing features or conditional random ﬁelds. In Proc. of the Conference

on Uncertainty in Artiﬁcial Intelligence (UAI), 2003.

[3] T. Dietterich, A. Ashenfelter, and Y. Bulatov. Training conditional random ﬁelds via gradient tree boost-

ing. In Proc. of the International Conference on Machine Learning (ICML), 2004.

[4] A. Torralba, K. P. Murphy, and W. T. Freeman. Contextual models for object detection using boosted

random ﬁelds. In Advances in Neural Information Processing Systems (NIPS), 2004.

[5] L. Liao, T. Choudhury, D. Fox, and H Kautz. Training conditional random ﬁelds using virtual evidence

boosting. In Proc. of the International Joint Conference on Artiﬁcial Intelligence (IJCAI), 2007.

[6] K. Nigam, A. McCallum, A. Thrun, and T. Mitchell. Text classiﬁcation from labeled and unlabeled

documents using em. Machine learning, 2000.

[7] A. Zhu, Z. Ghahramani, and J. Lafferty. Semi-supervised learning using gaussian ﬁelds and harmonic

functions. In Proc. of the International Conference on Machine Learning (ICML), 2003.

[8] W. Li and M. Andrew. Semi-supervised sequence modeling with syntactic topic models. In Proc. of the

National Conference on Artiﬁcial Intelligence (AAAI), 2005.

[9] Y. Grandvalet and Y. Bengio. Semi-supervised learning by entropy minimization. In Advances in Neural

Information Processing Systems (NIPS), 2004.

[10] F. Jiao, W. Wang, C. H. Lee, R. Greiner, and D. Schuurmans. Semi-supervised conditional random
ﬁelds for improved sequence segmentation and labeling. In International Committee on Computational
Linguistics and the Association for Computational Linguistics, 2006.

[11] C. Lee, S. Wang, F. Jiao, Schuurmans D., and R. Greiner. Learning to Model Spatial Dependency: Semi-

Supervised Discriminative Random Fields. In NIPS, 2006.

[12] J.S. Yedidia, W.T. Freeman, and Y. Weiss. Constructing free-energy approximations and generalized

belief propagation algorithms. IEEE Transactions on Information Theory, 51(7):2282–2312, 2005.

[13] Y. Weiss. Comparing mean ﬁeld method and belief propagation for approximate inference in mrfs. 2001.
[14] J. Besag. Statistical analysis of non-lattice data. The Statistician, 24, 1975.
[15] C. J. Geyer and E. A. Thompson. Constrained Monte Carlo Maximum Likelihood for dependent data.

Journal of Royal Statistical Society, 1992.

[16] Jerome Friedman, Trevor Hastie, and Robert Tibshirani. Additive logistic regression: a statistical view of

boosting. The Annals of Statistics, 38(2):337–374, 2000.

[17] G. Mann and A. McCullum. Efﬁcient computation of entropy gradient for semi-supervised conditional

random ﬁelds. In Human Language Technologies, 2007.

6 Appendix

In this section, we show how we derived the equations for wi and zi (eq. 8):
LF = LsV EB = LV EB − αHemp =
p(y(cid:48)

log p(yi|vei) + α

i|vei) log p(y(cid:48)

i|vei)

As in LogitBoost, the likelihood function LF is maximized by learning an ensemble of weak learners. We start
with an empty ensemble F = 0 and iteratively add the next best weak learner, ft, by computing the Newton
update s
F (vei, yi)) ← F (vei, yi) − s

H , where s and H are the ﬁrst and second derivative respectively of LF with respect to f (vei, yi).

|f =0 and H = ∂2LF +f

H , where s = ∂LF +f

∂f

2(2yi − 1)(1 − p(yi|vei)) + α

[2(2y(cid:48)

i − 1)(1 − p(y(cid:48)

i|vei)(1 − log p(y(cid:48)

i|vei))]

∂f 2

|f =0
i|vei))p(y(cid:48)

M2i=N +12y(cid:48)

i

N2i=1

M2i=N +12y(cid:48)

i

i|vei)) + log p(y(cid:48)

s =

p(y(cid:48)

N2i=1
H = − N2i=1
N2i=1
N2i=1

F ← F +

ziwi+

ziwi

i|vei)]
M2i=N+12y(cid:48)
M2i=N+12y(cid:48)
α2(1 − p(y(cid:48)

wi+

i

i

wi

and wi = p(yi|vei)(1 − p(yi|vei))

i|vei))[p(y(cid:48)

4p(yi|vei)(1 − p(yi|vei))(2yi − 1)2 + α2

4(2y(cid:48)

i − 1)2(1 − p(y(cid:48)

i|vei))[p(y(cid:48)

i|vei)(1 −

M2i=N +12y(cid:48)

i

where zi = yi−0.5

p(yi|vei)
(y(cid:48)
α[p(y(cid:48)

i−0.5)p(y(cid:48)
i|vei)(1−p(y(cid:48)

i|vei)(1−log p(y(cid:48)

i|vei))+log p(y(cid:48)

i|vei))

i|vei)]

if 1 ≤ i ≤ N
eq. (4)
if N < i ≤ M eq. (8)

i|vei)(1 − p(y(cid:48)

i|vei)) + log p(y(cid:48)

i|vei)]

if 1 ≤ i ≤ N eq. (4)
if N < i ≤ M eq. (8)

At iteration t we get the best weak learner, ft, by solving the WLSE problem in eq. 7.

8

"
1100,2007,Random Projections for Manifold Learning,"We propose a novel method for {\em linear} dimensionality reduction of manifold modeled data. First, we show that with a small number $M$ of {\em random projections} of sample points in $\reals^N$ belonging to an unknown $K$-dimensional Euclidean manifold, the intrinsic dimension (ID) of the sample set can be estimated to high accuracy. Second, we rigorously prove that using only this set of random projections, we can estimate the structure of the underlying manifold. In both cases, the number random projections required is linear in $K$ and logarithmic in $N$, meaning that $K<M\ll N$. To handle practical situations, we develop a greedy algorithm to estimate the smallest size of the projection space required to perform manifold learning. Our method is particularly relevant in distributed sensing systems and leads to significant potential savings in data acquisition, storage and transmission costs.","Random Projections for Manifold Learning

Chinmay Hegde
ECE Department
Rice University
ch3@rice.edu

Michael B. Wakin
EECS Department

University of Michigan

wakin@eecs.umich.edu

Richard G. Baraniuk

ECE Department
Rice University

richb@rice.edu

Abstract

We propose a novel method for linear dimensionality reduction of manifold mod-
eled data. First, we show that with a small number M of random projections of
sample points in RN belonging to an unknown K-dimensional Euclidean mani-
fold, the intrinsic dimension (ID) of the sample set can be estimated to high accu-
racy. Second, we rigorously prove that using only this set of random projections,
we can estimate the structure of the underlying manifold. In both cases, the num-
ber of random projections required is linear in K and logarithmic in N , meaning
that K < M ≪ N . To handle practical situations, we develop a greedy algorithm
to estimate the smallest size of the projection space required to perform manifold
learning. Our method is particularly relevant in distributed sensing systems and
leads to signiﬁcant potential savings in data acquisition, storage and transmission
costs.

1 Introduction

Recently, we have witnessed a tremendous increase in the sizes of data sets generated and processed
by acquisition and computing systems. As the volume of the data increases, memory and processing
requirements need to correspondingly increase at the same rapid pace, and this is often prohibitively
expensive. Consequently, there has been considerable interest in the task of effective modeling of
high-dimensional observed data and information; such models must capture the structure of the
information content in a concise manner.

A powerful data model for many applications is the geometric notion of a low-dimensional man-
ifold. Data that possesses merely K “intrinsic” degrees of freedom can be assumed to lie on a
K-dimensional manifold in the high-dimensional ambient space. Once the manifold model is iden-
tiﬁed, any point on it can be represented using essentially K pieces of information. Thus, algorithms
in this vein of dimensionality reduction attempt to learn the structure of the manifold given high-
dimensional training data.

While most conventional manifold learning algorithms are adaptive (i.e., data dependent) and non-
linear (i.e., involve construction of a nonlinear mapping), a linear, nonadaptive manifold dimen-
sionality reduction technique has recently been introduced that employs random projections [1].
Consider a K-dimensional manifold M in the ambient space RN and its projection onto a random
subspace of dimension M = CK log(N ); note that K < M ≪ N . The result of [1] is that the
pairwise metric structure of sample points from M is preserved with high accuracy under projection
from RN to RM .

(a)

(b)

(c)

(d)

Figure 1: Manifoldlearningusing randomprojections. (a)Inputdataconsistingof1000imagesofashifted
disk,eachofsizeN = 64×64 = 4096. (b)Trueθ1 andθ2 valuesofthesampleddata. (c,d)Isomapembedding
learnedfrom(c)originaldatain RN,and(d)arandomlyprojectedversionofthedatainto RM withM = 15.

This result has far reaching implications. Prototypical devices that directly and inexpensively ac-
quire random projections of certain types of data (signals, images, etc.) have been developed [2, 3];
these devices are hardware realizations of the mathematical tools developed in the emerging area of
Compressed Sensing (CS) [4, 5]. The theory of [1] suggests that a wide variety of signal processing
tasks can be performed directly on the random projections acquired by these devices, thus saving
valuable sensing, storage and processing costs.

The advantages of random projections extend even to cases where the original data is available in
the ambient space RN . For example, consider a wireless network of cameras observing a scene. To
perform joint image analysis, the following steps might be executed:

1. Collate: Each camera node transmits its respective captured image (of size N ) to a central

processing unit.

2. Preprocess: The central processor estimates the intrinsic dimension K of the underlying

image manifold.

3. Learn: The central processor performs a nonlinear embedding of the data points – for
instance, using Isomap [6] – into a K-dimensional Euclidean space, using the estimate of
K from the previous step.

In situations where N is large and communication bandwidth is limited, the dominating costs will be
in the ﬁrst transmission/collation step. On the one hand, to reduce the communication needs one may
perform nonlinear image compression (such as JPEG) at each node before transmitting to the central
processing. But this requires a good deal of processing power at each sensor, and the compression
would have to be undone during the learning step, thus adding to overall computational costs. On the
other hand, every camera could encode its image by computing (either directly or indirectly) a small
number of random projections to communicate to the central processor. These random projections
are obtained by linear operations on the data, and thus are cheaply computed. Clearly, in many
situations it will be less expensive to store, transmit, and process such randomly projected versions
of the sensed images. The question now becomes: how much information about the manifold is
conveyed by these random projections, and is any advantage in analyzing such measurements from
a manifold learning perspective?

In this paper, we provide theoretical and experimental evidence that reliable learning of a K-
dimensional manifold can be performed not just in the high-dimensional ambient space RN but also
in an intermediate, much lower-dimensional random projection space RM , where M = CK log(N ).
See, for example, the toy example of Figure 1. Our contributions are as follows. First, we present a
theoretical bound on the minimum number of measurements per sample point required to estimate
the intrinsic dimension (ID) of the underlying manifold, up to an accuracy level comparable to that
of the Grassberger-Procaccia algorithm [7, 8], a widely used geometric approach for dimensionality
estimation. Second, we present a similar bound on the number of measurements M required for
Isomap [6] – a popular manifold learning algorithm – to be “reliably” used to discover the nonlinear
structure of the manifold. In both cases, M is shown to be linear in K and logarithmic in N . Third,
we formulate a procedure to determine, in practical settings, this minimum value of M with no a
priori information about the data points. This paves the way for a weakly adaptive, linear algorithm
(ML-RP) for dimensionality reduction and manifold learning.

The rest of the paper is organized as follows. Section 2 recaps the manifold learning approaches we
utilize. In Section 3 presents our main theoretical contributions, namely, the bounds on M required
to perform reliable dimensionality estimation and manifold learning from random projections. Sec-

tion 4 describes a new adaptive algorithm that estimates the minimum value of M required to provide
a faithful representation of the data so that manifold learning can be performed. Experimental re-
sults on a variety of real and simulated data are provided in Section 5. Section 6 concludes with
discussion of potential applications and future work.

2 Background

An important input parameter for all manifold learning algorithms is the intrinsic dimension (ID) of
a point cloud. We aim to embed the data points in as low-dimensional a space as possible in order to
avoid the curse of dimensionality. However, if the embedding dimension is too small, then distinct
data points might be collapsed onto the same embedded point. Hence a natural question to ask is:
given a point cloud in N -dimensional Euclidean space, what is the dimension of the manifold that
best captures the structure of this data set? This problem has received considerable attention in the
literature and remains an active area of research [7, 9, 10].

For the purposes of this paper, we focus our attention on the Grassberger-Procaccia (GP) [7] algo-
rithm for ID estimation. This is a widely used geometric technique that takes as input the set of
pairwise distances between sample points. It then computes the scale-dependent correlation dimen-
sion of the data, deﬁned as follows.

Deﬁnition 2.1 Suppose X = (x1, x2, ..., xn) is a ﬁnite dataset of underlying dimension K. Deﬁne

Cn(r) =

1

n(n − 1)Xi6=j

Ikxi−xj k<r,

where I is the indicator function. The scale-dependent correlation dimension of X is deﬁned as

bDcorr(r1, r2) =

log Cn(r1) − log Cn(r2)

log r1 − log r2

.

The best possible approximation to K (call this bK) is obtained by ﬁxing r1 and r2 to the biggest

range over which the plot is linear and the calculating Dcorr in that range. There are a number of
practical issues involved with this approach; indeed, it has been shown that geometric ID estimation
algorithms based on ﬁnite sampling yield biased estimates of intrinsic dimension [10, 11]. In our
theoretical derivations, we do not attempt to take into account this bias; instead, we prove that
the effect of running the GP algorithm on a sufﬁcient number of random projections produces a
dimension estimate that well-approximates the GP estimate obtained from analyzing the original
point cloud.

Isomap [6], Locally Linear Embedding (LLE) [12], and Hessian Eigenmaps [13], among many

The estimate bK of the ID of the point cloud is used by nonlinear manifold learning algorithms (e.g.,
others) to generate a bK-dimensional coordinate representation of the input data points. Our main

analysis will be centered around Isomap. Isomap attempts to preserve the metric structure of the
manifold, i.e., the set of pairwise geodesic distances of any given point cloud sampled from the
manifold. In essence, Isomap approximates the geodesic distances using a suitably deﬁned graph
and performs classical multidimensional scaling (MDS) to obtain a reduced K-dimensional repre-
sentation of the data [6]. A key parameter in the Isomap algorithm is the residual variance, which is
equivalent to the stress function encountered in classical MDS. The residual variance is a measure
of how well the given dataset can be embedded into a Euclidean space of dimension K. In the next
section, we prescribe a speciﬁc number of measurements per data point so that performing Isomap
on the randomly projected data yields a residual variance that is arbitrarily close to the variance
produced by Isomap on the original dataset.

We conclude this section by revisiting the results derived in [1], which form the basis for our de-
velopment. Consider the effect of projecting a smooth K-dimensional manifold residing in RN
onto a random M -dimensional subspace (isomorphic to RM ). If M is sufﬁciently large, a stable
near-isometric embedding of the manifold in the lower-dimensional subspace is ensured. The key
advantage is that M needs only to be linear in the intrinsic dimension of the manifold K. In addition,
M depends only logarithmically on other properties of the manifold, such as its volume, curvature,
etc. The result can be summarized in the following theorem.

Theorem 2.2 [1] Let M be a compact K-dimensional manifold in RN having volume V and
condition number 1/τ. Fix 0 < ǫ < 1 and 0 < ρ < 1. Let Φ be a random orthoprojector1 from RN
to RM and

M ≥ O(cid:18) K log(N V τ −1) log(ρ−1)

ǫ2

(cid:19) .

(1)

Suppose M < N . Then, with probability exceeding 1 − ρ, the following statement holds: For every
pair of points x, y ∈ M, and i ∈ {1, 2},

(1 − ǫ)r M

N

≤

di(Φx, Φy)

di(x, y)

≤ (1 + ǫ)r M

N

.

(2)

where d1(x, y) (respectively, d2(x, y)) stands for the geodesic (respectively, ℓ2) distance between
points x and y.

The condition number τ controls the local, as well as global, curvature of the manifold – the smaller
the τ, the less well-conditioned the manifold with higher “twistedness” [1]. Theorem 2.2 has been
proved by ﬁrst specifying a ﬁnite high-resolution sampling on the manifold, the nature of which
depends on its intrinsic properties; for instance, a planar manifold can be sampled coarsely. Then the
Johnson-Lindenstrauss Lemma [14] is applied to these points to guarantee the so-called “isometry
constant” ǫ, which is nothing but (2).

3 Bounds on the performance of ID estimation and manifold learning

algorithms under random projection

We saw above that random projections essentially ensure that the metric structure of a high-
dimensional input point cloud (i.e., the set of all pairwise distances between points belonging to the
dataset) is preserved up to a distortion that depends on ǫ. This immediately suggests that geometry-
based ID estimation and manifold learning algorithms could be applied to the lower-dimensional,
randomly projected version of the dataset.

The ﬁrst of our main results establishes a sufﬁcient dimension of random projection M required to
maintain the ﬁdelity of the estimated correlation dimension using the GP algorithm. The proof of
the following is detailed in [15].

Theorem 3.1 Let M be a compact K-dimensional manifold in RN having volume V and condi-
tion number 1/τ. Let X = {x1, x2, ...} be a sequence of samples drawn from a uniform density

supported on M. Let bK be the dimension estimate of the GP algorithm on X over the range

(rmin, rmax). Let β = ln(rmax/rmin) . Fix 0 < δ < 1 and 0 < ρ < 1. Suppose the following
condition holds:

Let Φ be a random orthoprojector from RN to RM with M < N and

rmax < τ /2

(3)

M ≥ O(cid:18) K log(N V τ −1) log(ρ−1)

β2δ2

(cid:19) .

(4)

Let bKΦ be the estimated correlation dimension on ΦX in the projected space over the range
(rminpM/N , rmaxpM/N ). Then, bKΦ is bounded by:

(5)

with probability exceeding 1 − ρ.

(1 − δ)bK ≤ bKΦ ≤ (1 + δ)bK

Theorem 3.1 is a worst-case bound and serves as a sufﬁcient condition for stable ID estimation using
random projections. Thus, if we choose a sufﬁciently small value for δ and ρ, we are guaranteed
estimation accuracy levels as close as desired to those obtained with ID estimation in the original

signal space. Note that the bound on bKΦ is multiplicative. This implies that in the worst case, the

1Such a matrix is formed by orthogonalizing M vectors of length N having, for example, i.i.d. Gaussian or

Bernoulli distributed entries.

number of projections required to estimate bKΦ very close to bK (say, within integer roundoff error)

becomes higher with increasing manifold dimension K.
The second of our main results prescribes the minimum dimension of random projections required
to maintain the residual variance produced by Isomap in the projected domain within an arbitrary
additive constant of that produced by Isomap with the full data in the ambient space. This proof of
this theorem [15] relies on the proof technique used in [16].

Theorem 3.2 Let M be a compact K-dimensional manifold in RN having volume V and condition
number 1/τ. Let X = {x1, x2, ..., xn} be a ﬁnite set of samples drawn from a sufﬁciently ﬁne
density supported on M. Let Φ be a random orthoprojector from RN to RM with M < N . Fix
0 < ǫ < 1 and 0 < ρ < 1. Suppose

M ≥ O(cid:18) K log(N V τ −1) log(ρ−1)

ǫ2

(cid:19) .

Deﬁne the diameter Γ of the dataset as follows:

Γ = max

1≤i,j≤n

diso(xi, xj)

where diso(x, y) stands for the Isomap estimate of the geodesic distance between points x and y.
Deﬁne R and RΦ to be the residual variances obtained when Isomap generates a K-dimensional
embedding of the original dataset X and projected dataset ΦX respectively. Under suitable con-
structions of the Isomap connectivity graphs, RΦ is bounded by:

RΦ < R + CΓ2ǫ

with probability exceeding 1 − ρ. C is a function only on the number of sample points n.

Since the choice of ǫ is arbitrary, we can choose a large enough M (which is still only logarithmic
in N ) such that the residual variance yielded by Isomap on the randomly projected version of the
dataset is arbitrarily close to the variance produced with the data in the ambient space. Again,
this result is derived from a worst-case analysis. Note that Γ acts as a measure of the scale of the
dataset. In practice, we may enforce the condition that the data is normalized (i.e., every pairwise
distance calculated by Isomap is divided by Γ). This ensures that the K-dimensional embedded
representation is contained within a ball of unit norm centered at the origin.

Thus, we have proved that with only an M -dimensional projection of the data (with M ≪ N )
we can perform ID estimation and subsequently learn the structure of a K-dimensional manifold,
up to accuracy levels obtained by conventional methods. In Section 4, we utilize these sufﬁciency
results to motivate an algorithm for performing practical manifold structure estimation using random
projections.

4 How many random projections are enough?

In practice, it is hard to know or estimate the parameters V and τ of the underlying manifold. Also,

since we have no a priori information regarding the data, it is impossible to ﬁx bK and R, the outputs

of GP and Isomap on the point cloud in the ambient space. Thus, often, we may not be able ﬁx a
deﬁnitive value for M . To circumvent this problem we develop the following empirical procedure
that we dub it ML-RP for manifold learning using random projections.

We initialize M to a small number, and compute M random projections of the data set X =
{x1, x2, ..., xn} (here n denotes the number of points in the point cloud). Using the set ΦX =

{Φx : x ∈ X}, we estimate the intrinsic dimension using the GP algorithm. This estimate, say bK,
is used by the Isomap algorithm to produce an embedding into bK-dimensional space. The resid-

ual variance produced by this operation is recorded. We then increment M by 1 and repeat the
entire process. The algorithm terminates when the residual variance obtained is smaller than some
tolerance parameter δ. A full length description is provided in Algorithm 1.
The essence of ML-RP is as follows. A sufﬁcient number M of random projections is determined by
a nonlinear procedure (i.e., sequential computation of Isomap residual variance) so that conventional

Algorithm 1 ML-RP

M ← 1
Φ ← Random orthoprojector of size M × N .
while residual variance ≥ δ do

Run the GP algorithm on ΦX.

Use ID estimate (bK) to perform Isomap on ΦX.

Calculate residual variance.
M ← M + 1
Add one row to Φ

end while
return M

return bK

(a)

(b)

Figure 2: PerformanceofIDestimationusingGPasafunctionofrandomprojections. Samplesizen=1000,
ambient dimension N = 150. (a) Estimated intrinsic dimension for underlying hyperspherical manifolds of
increasing dimension. The solidlineindicates thevalue of theIDestimateobtained by GP performed on the
originaldata. (b)MinimumnumberofprojectionsrequiredforGPtoworkwith90%accuracyascomparedto
GPonnativedata.

manifold learning does almost as well on the projected dataset as the original. On the other hand,
the random linear projections provide a faithful representation of the data in the geodesic sense.
In this manner, ML-RP helps determine the number of rows that Φ requires in order to act as an
operator that preserves metric structure. Therefore, ML-RP can be viewed as an adaptive method
for linear reduction of data dimensionality. It is only weakly adaptive in the sense that only the
stopping criterion for ML-RP is determined by monitoring the nature of the projected data.

The results derived in Section 3 can be viewed as convergence proofs for ML-RP. The existence of
a certain minimum number of measurements for any chosen error value δ ensures that eventually,
M in the ML-RP algorithm is going to become high enough to ensure “good” Isomap performance.
Also, due to the built-in parsimonious nature of ML-RP, we are ensured to not “overmeasure” the
manifold, i.e., just the requisite numbers of projections of points are obtained.

5 Experimental results

This section details the results of simulations of ID estimation and subsequent manifold learning on
real and synthetic datasets. First, we examine the performance of the GP algorithm on random pro-
jections of K-dimensional dimensional hyperspheres embedded in an ambient space of dimension
N = 150. Figure 2(a) shows the variation of the dimension estimate produced by GP as a function
of the number of projections M . The sampled dataset in each of the cases is obtained from drawing
n = 1000 samples from a uniform distribution supported on a hypersphere of corresponding dimen-
sion. Figure 2(b) displays the minimum number of projections per sample point required to estimate
the scale-dependent correlation dimension directly from the random projections, up to 10% error,
when compared to GP estimation on the original data.

We observe that the ID estimate stabilizes quickly with increasing number of projections, and indeed
converges to the estimate obtained by running the GP algorithm on the original data. Figure 2(b)
illustrates the variation of the minimum required projection dimension M vs. K, the intrinsic dimen-

Figure 3: Standarddatabases. AmbientdimensionforthefacedatabaseN=4096;ambientdimensionforthe
handrotationdatabasesN=3840.

Figure 4: Performance of ML-RP on the above databases. (left)ML-RP on the face database (N = 4096).
Goodapproximatesareobtainedfor M > 50. (right)ML-RPonthehandrotationdatabase(N = 3840). For
M > 60,theIsomapvarianceisindistinguishablefromthevarianceobtainedintheambientspace.

sion of the underlying manifold. We plot the intrinsic dimension of the dataset against the minimum

number of projections required such that bKΦ is within 10% of the conventional GP estimate bK (this

is equivalent to choosing δ = 0.1 in Theorem 3.1). We observe the predicted linearity (Theorem 3.1)
in the variation of M vs K.
Finally, we turn our attention to two common datasets (Figure 3) found in the literature on dimension
estimation – the face database2 [6], and the hand rotation database [17].3 The face database is a
collection of 698 artiﬁcial snapshots of a face (N = 64 × 64 = 4096) varying under 3 degrees of
freedom: 2 angles for pose and 1 for lighting dimension. The signals are therefore believed to reside
on a 3D manifold in an ambient space of dimension 4096. The hand rotation database is a set of
90 images (N = 64 × 60 = 3840) of rotations of a hand holding an object. Although the image
appearance manifold is ostensibly one-dimensional, estimators in the literature always overestimate
its ID [11].

Random projections of each sample in the databases were obtained by computing the inner product
of the image samples with an increasing number of rows of the random orthoprojector Φ. We
note that in the case of the face database, for M > 60, the Isomap variance on the randomly
projected points closely approximates the variance obtained with full image data. This behavior of
convergence of the variance to the best possible value is even more sharply observed in the hand
rotation database, in which the two variance curves are indistinguishable for M > 60. These results
are particularly encouraging and demonstrate the validity of the claims made in Section 3.

6 Discussion

Our main theoretical contributions in this paper are the explicit values for the lower bounds on the
minimum number of random projections required to perform ID estimation and subsequent manifold
learning using Isomap, with high guaranteed accuracy levels. We also developed an empirical greedy
algorithm (ML-RP) for practical situations. Experiments on simple cases, such as uniformly gener-
ated hyperspheres of varying dimension, and more complex situations, such as the image databases
displayed in Figure 3, provide sufﬁcient evidence of the nature of the bounds described above.

2http://isomap.stanford.edu
3http://vasc.ri.cmu.edu//idb/html/motion/hand/index.html. Note that we use a subsampled version of the

database used in the literature, both in terms of resolution of the image and sampling of the manifold.

The method of random projections is thus a powerful tool for ensuring the stable embedding of low-
dimensional manifolds into an intermediate space of reasonable size. The motivation for developing
results and algorithms that involve random measurements of high-dimensional data is signiﬁcant,
particularly due to the increasing attention that Compressive Sensing (CS) has received recently. It
is now possible to think of settings involving a huge number of low-power devices that inexpen-
sively capture, store, and transmit a very small number of measurements of high-dimensional data.
ML-RP is applicable in all such situations. In situations where the bottleneck lies in the transmission
of the data to the central processing node, ML-RP provides a simple solution to the manifold learn-
ing problem and ensures that with minimum transmitted amount of information, effective manifold
learning can be performed. The metric structure of the projected dataset upon termination of ML-
RP closely resembles that of the original dataset with high probability; thus, ML-RP can be viewed
as a novel adaptive algorithm for ﬁnding an efﬁcient, reduced representation of data of very large
dimension.

References

[1] R. G. Baraniuk and M. B. Wakin. Random projections of smooth manifolds. 2007. To appear

in Foundations of Computational Mathematics.

[2] M. B. Wakin, J. N. Laska, M. F. Duarte, D. Baron, S. Sarvotham, D. Takhar, K. F. Kelly, and
R. G. Baraniuk. An architecture for compressive imaging. In IEEE International Conference
on Image Processing (ICIP), pages 1273–1276, Oct. 2006.

[3] S. Kirolos, J.N. Laska, M.B. Wakin, M.F. Duarte, D.Baron, T. Ragheb, Y. Massoud, and R.G.
Baraniuk. Analog-to-information conversion via random demodulation. In Proc. IEEE Dallas
Circuits and Systems Workshop (DCAS), 2006.

[4] E. J. Cand`es, J. Romberg, and T. Tao. Robust uncertainty principles: Exact signal reconstruc-
tion from highly incomplete frequency information. IEEE Trans. Info. Theory, 52(2):489–509,
Feb. 2006.

[5] D. L. Donoho. Compressed sensing. IEEE Trans. Info. Theory, 52(4):1289–1306, September

2006.

[6] J. B. Tenenbaum, V.de Silva, and J. C. Landford. A global geometric framework for nonlinear

dimensionality reduction. Science, 290:2319–2323, 2000.

[7] P. Grassberger and I. Procaccia. Measuring the strangeness of strange attractors. Physica D

Nonlinear Phenomena, 9:189–208, 1983.

[8] J. Theiler. Statistical precision of dimension estimators. Physical Review A, 41(6):3038–3051,

1990.

[9] F. Camastra. Data dimensionality estimation methods: a survey. Pattern Recognition, 36:2945–

2954, 2003.

[10] J. A. Costa and A. O. Hero. Geodesic entropic graphs for dimension and entropy estimation in

manifold learning. IEEE Trans. Signal Processing, 52(8):2210–2221, August 2004.

[11] E. Levina and P. J. Bickel. Maximum likelihood estimation of intrinsic dimension. In Advances

in NIPS, volume 17. MIT Press, 2005.

[12] S. Roweis and L. Saul. Nonlinear dimensionality reduction by locally linear embedding. Sci-

ence, 290:2323–2326, 2000.

[13] D. Donoho and C. Grimes. Hessian eigenmaps: locally linear embedding techniques for high

dimensional data. Proc. of National Academy of Sciences, 100(10):5591–5596, 2003.

[14] Sanjoy Dasgupta and Anupam Gupta. An elementary proof of the JL lemma. Technical Report

TR-99-006, University of California, Berkeley, 1999.

[15] C. Hegde, M. B. Wakin, and R. G. Baraniuk. Random projections for manifold learning -

proofs and analysis. Technical Report TREE 0710, Rice University, 2007.

[16] M. Bernstein, V. de Silva, J. Langford, and J. Tenenbaum. Graph approximations to geodesics

on embedded manifolds, 2000. Technical report, Stanford University.

[17] B. K´egl. Intrinsic dimension estimation using packing numbers. In Advances in NIPS, vol-

ume 14. MIT Press, 2002.

"
664,2007,The Noisy-Logical Distribution and its Application to Causal Inference,"We describe a novel noisy-logical distribution for representing the distribution of a binary output variable conditioned on multiple binary input variables. The distribution is represented in terms of noisy-or's and noisy-and-not's of causal features which are conjunctions of the binary inputs. The standard noisy-or and noisy-and-not models, used in causal reasoning and artificial intelligence, are special cases of the noisy-logical distribution. We prove that the noisy-logical distribution is complete in the sense that it can represent all conditional distributions provided a sufficient number of causal factors are used. We illustrate the noisy-logical distribution by showing that it can account for new experimental findings on how humans perform causal reasoning in more complex contexts. Finally, we speculate on the use of the noisy-logical distribution for causal reasoning and artificial intelligence.","The Noisy-Logical Distribution and its Application to

Causal Inference

University of California at Los Angeles

University of California at Los Angeles

Hongjing Lu

Department of Psychology

Los Angeles, CA 90095
hongjing@ucla.edu

Alan Yuille

Department of Statistics

Los Angeles, CA 90095

yuille@stat.ucla.edu

Abstract

We describe a novel noisy-logical distribution for representing the distribution of
a binary output variable conditioned on multiple binary input variables. The distri-
bution is represented in terms of noisy-or’s and noisy-and-not’s of causal features
which are conjunctions of the binary inputs. The standard noisy-or and noisy-and-
not models, used in causal reasoning and artiﬁcial intelligence, are special cases
of the noisy-logical distribution. We prove that the noisy-logical distribution is
complete in the sense that it can represent all conditional distributions provided a
sufﬁcient number of causal factors are used. We illustrate the noisy-logical dis-
tribution by showing that it can account for new experimental ﬁndings on how
humans perform causal reasoning in complex contexts. We speculate on the use
of the noisy-logical distribution for causal reasoning and artiﬁcial intelligence.

1 Introduction

The noisy-or and noisy-and-not conditional probability distributions are frequently studied in cog-
nitive science for modeling causal reasoning [1], [2],[3] and are also used as probabilistic models
for artiﬁcial intelligence [4]. It has been shown, for example, that human judgments of the power of
causal cues in experiments involving two cues [1] can be interpreted in terms of maximum likelihood
estimation and model selection using these types of models [3].
But the noisy-or and noisy-and-not distributions are limited in the sense that they can only represent
a restricted set of all possible conditional distributions. This restriction is sometimes an advantage
because there may not be sufﬁcient data to determine the full conditional distribution. Nevertheless it
would be better to have a representation that can expand to represent the full conditional distribution,
if sufﬁcient data is available, but can be reduced to simpler forms (e.g. standard noisy-or) if there is
only limited data.
This motivates us to deﬁne the noisy-logical distribution. This is deﬁned in terms of noisy-or’s
and noisy-and-not’s of causal features which are conjunctions of the basic input variables (inspired
by the use of conjunctive features in [2] and the extensions in [5]). By restricting the choice of
causal features we can obtain the standard noisy-or and noisy-and-not models. We prove that the
noisy-logical distribution is complete in the sense that it can represent any conditional distribution
provided we use all the causal features. Overall, it gives a distribution whose complexity can be
adjusted by restricting the number of causal features.
To illustrate the noisy-logical distribution we apply it to modeling some recent human experiments
on causal reasoning in complex environments [6]. We show that noisy-logical distributions involv-
ing causal factors are able to account for human performance. By contrast, an alternative linear
model gives predictions which are the opposite of the observed trends in human causal judgments.
Section (2) presents the noisy-logical distribution for the case with two input causes (the case com-
monly studied in causal reasoning). In section (3) we specify the full noisy-logical distribution and

1

we prove its completeness in section (4). Section (5) illustrates the noisy-logical distribution by
showing that it accounts for recent experimental ﬁndings in causal reasoning.

2 The Case with N = 2 causes

In this section we study the simple case when the binary output effect E depends only on two binary-
valued causes C1, C2. This covers most of the work reported in the cognitive science literature
[1],[3]. In this case, the probability distribution is speciﬁed by the four numbers P (E = 1|C1, C2),
for C1 ∈ {0, 1}, C2 ∈ {0, 1}.
To deﬁne the noisy-logical distribution over two variables P (E = 1|C1, C2), we introduce three
concepts. Firstly, we deﬁne four binary-valued causal features Ψ0(.), Ψ1(.), Ψ2(.), Ψ3(.) which are
functions of the input state (cid:126)C = (C1, C2). They are deﬁned by Ψ0( (cid:126)C) = 1, Ψ1( (cid:126)C) = C1, Ψ2( (cid:126)C) =
C2, Ψ3( (cid:126)C) = C1∧C2, where ∧ denotes logical-and operation(i.e. C1∧C2 = 1 if C1 = C2 = 1 and
C1 ∧ C2 = 0 otherwise). Ψ3( (cid:126)C) is the conjunction of C1 and C2. Secondly, we introduce binary-
valued hidden states E0, E1, E2, E3 which are caused by the corresponding features Ψ0, Ψ1, Ψ2, Ψ3.
We deﬁne P (Ei = 1|Ψi; ωi) = ωiΨi with ωi ∈ [0, 1], for i = 1, ..., 4 with (cid:126)ω = (ω1, ω2, ω3, ω4).
Thirdly, we deﬁne the output effect E to be a logical combination of the states E0, E1, E2, E3
which we write in form δE,f (E0,E1,E2,E3), where f(., ., ., .) is a logic function which is formed by a
combination of three logic operations AN D, OR, N OT . This induces the noisy-logical distribution
Pnl(E| (cid:126)C; (cid:126)ω) =
The noisy-logical distribution is characterized by the parameters ω0, ..., ω3 and the choice of the
logic function f(., ., ., .). We can represent the distribution by a circuit diagram where the output E
is a logical function of the hidden states E0, ..., E3 and each state is caused probabilistically by the
corresponding causal features Ψ0, ..., Ψ3, as shown in Figure (1).

(cid:81)3
i=0 P (Ei|Ψi( (cid:126)C); ωi).

E0,...,E3 δE,f (E0,E1,E2,E3)

(cid:80)

Figure 1: Circuit diagram in the case with N = 2 causes.

(cid:88)

E1,E2

The noisy-logical distribution includes the commonly known distributions, noisy-or and noisy-and-
not, as special cases. To obtain the noisy-or, we set E = E1 ∨ E2 (i.e. E1 ∨ E2 = 0 if E1 = E2 = 0
and E1 ∨ E2 = 1 otherwise). A simple calculation shows that the noisy-logical distribution reduces
to the noisy-or Pnor(E|C1, C2; ω1, ω2) [4], [1]:

Pnl(E = 1|C1, C2; ω1, ω2) =

δ1,E1∨E2P (E1|Ψ1( (cid:126)C); ω1)P (E2|Ψ2( (cid:126)C); ω2)
= ω1C1(1 − ω2C2) + (1 − ω1C1)ω2C2 + ω1ω2C1C2
= ω1C1 + ω2C2 − ω1ω2C1C2 = Pnor(E = 1|C1, C2; ω1, ω2)(1)
To obtain the noisy-and-not, we set E = E1 ∧ ¬E2 (i.e. E1 ∧ ¬E2 = 1 if E1 = 1, E2 = 0
and E1 ∧ ¬E2 = 0 otherwise). The noisy-logical distribution reduces to the noisy-and-not
Pn−and−not(E|C1, C2; ω1, ω2) [4],[?]:

(cid:88)

Pnl(E = 1|C1, C2; ω1, ω2) =

δ1,E1∧¬E2P (E1|Ψ1( (cid:126)C); ω1)P (E2|Ψ2( (cid:126)C); ω2)

E1,E2

= ω1C1{1 − ω2C2} = Pn−and−not(E = 1|C1, C2; ω1, ω2) (2)

2

We claim that noisy-logical distributions of this form can represent any conditional distribution
P (E| (cid:126)C). The logical function f(E0, E1, E2, E3) will be expressed as a combination of logic oper-
ations AND-NOT, OR. The parameters of the distribution are given by ω0, ω1, ω2, ω3.
The proof of this claim will be given for the general case in the next section. To get some insight,
we consider the special case where we only know the values P (E|C1 = 1, C2 = 0) and P (E|C1 =
1, C2 = 1). This situation is studied in cognitive science where C1 is considered to be a background
cause which always takes value 1, see [1] [3]. In this case, the only causal features are considered,
Ψ1( (cid:126)C) = C1 and Ψ2( (cid:126)C) = C2.
Result. The noisy-or and the noisy-and-not models, given by equations (1,2) are sufﬁcient to ﬁt any
values of P (E = 1|1, 0) and P (E = 1|1, 1). (In this section we use P (E = 1|1, 0) to denote
P (E = 1|C1 = 1, C2 = 0) and use P (E = 1|1, 1) to denote P (E = 1|C1 = 1, C2 = 1).)
The noisy-or and noisy-and-not ﬁt the cases when P (E = 1|1, 1) ≥ P (E = 1|1, 0) and P (E =
1|1, 1) ≤ P (E = 1|1, 0) respectively. In Cheng’s terminology [1] C2 is respectively a generative or
preventative cause).
Proof. We can ﬁt both the noisy-or and noisy-and-not models to P (E|1, 0) by setting ω1 = P (E =
1|1, 0), so it remains to ﬁt the models to P (E|1, 1). There are three cases to consider: (i) P (E =
1|1, 1) > P (E = 1|1, 0), (ii) P (E = 1|1, 1) < P (E = 1|1, 0), and (iii) P (E = 1|1, 1) =
P (E = 1|1, 0).
It follows directly from equations (1,2) that Pnor(E = 1|1, 1) ≥ Pnor(E =
1|1, 0) and Pn−and−not(E = 1|1, 1) ≤ Pn−and−not(E = 1|1, 0) with equality only if P (E =
1|1, 1) = P (E = 1|1, 0). Hence we must ﬁt a noisy-or and a noisy-and-not model to cases (i)
and (ii) respectively. For case (i), this requires solving P (E = 1|1, 1) = ω1 + ω2 − ω1ω2 to
obtain ω2 = {P (E = 1|1, 1) − P (E = 1|1, 0)}/{1 − P (E = 1|1, 0)} (note that the condition
P (E = 1|1, 1) > P (E = 1|1, 0) ensures that ω2 ∈ [0, 1]). For case (ii), we must solve P (E =
1|1, 1) = ω1 − ω1ω2 which gives ω2 = {P (E = 1|1, 0) − P (E = 1|1, 1)}/P (E = 1|1, 0) (the
condition P (E = 1|1, 1) < P (E = 1|1, 0) ensures that ω2 ∈ [0, 1]). For case (iii), we can ﬁt either
model by setting ω2 = 0.

3 The Noisy-Logical Distribution for N causes

We next consider representing probability distributions of form P (E| (cid:126)C), where E ∈ {0, 1} and
(cid:126)C = (C1, ..., CN ) where Ci ∈ {0, 1}, ∀i = 1, .., N. These distributions can be characterized by
the values of P (E = 1| (cid:126)C) for all possible 2N values of (cid:126)C.
We deﬁne the set of 2N binary-valued causal features {Ψi( (cid:126)C) : i = 0, ..., 2N − 1}. These features
are ordered so that Ψ0( (cid:126)C) = 1, Ψi( (cid:126)C) = Ci : i = 1, .., N, ΨN +1( (cid:126)C) = C1 ∧ C2 is the conjunction
of C1 and C2, and so on. The feature Ψ( (cid:126)C) = Ca ∧ Cb ∧ ... ∧ Cg will take value 1 if Ca = Cb =
... = Cg = 1 and value 0 otherwise.
We deﬁne binary variables {Ei : i = 0, ..., 2N − 1} which are related to the causal features {Ψi :
i = 0, ..., 2N − 1} by distributions P (Ei = 1|Ψi; ωi) = ωiΨi, speciﬁed by parameters {ωi : i =
0, ..., 2N − 1}.
Then we deﬁne the output variable E to be a logical (i.e. deterministic) function of the {Ei
:
i = 0, ..., 2N − 1}. This can be thought of as a circuit diagram. In particular, we deﬁne E =
f(E0, ..., E2N−1) = (((((E1 ⊗ E2) ⊗ E3) ⊗ E4....) where E1 ⊗ E2 can be E1 ∨ E2 or E1 ∧ ¬E2
(where ¬E means logical negation). This gives the general noisy-logical distribution, as shown in
Figure (2).

(cid:88)

2N−1(cid:89)

P (E = 1| (cid:126)C; (cid:126)ω) =

δE,f (E0,...,E2N −1)

P (Ei = 1|Ψi; ωi).

(3)

(cid:126)E

i=0

4 The Completeness Result

This section proves that the noisy-logical distribution is capable of representing any conditional
distribution. This is the main theoretical result of this paper.

3

Figure 2: Circuit diagram in the case with N causes. All conditional distributions can be represented
in this form if we use all possible 2N causal features Ψ, choose the correct parameters ω, and select
the correct logical combinations ⊗.

Result We can represent any conditional distribution P (E| (cid:126)C) deﬁned on binary variables in terms
of a noisy logical distribution given by equation (3).
Proof. The proof is constructive. We show that any distribution P (E| (cid:126)C) can be expressed as a
noisy-logical distribution.
We order the states (cid:126)C0, ..., (cid:126)C2N−1. This ordering must obey Ψi( (cid:126)Ci) = 1 and Ψi( (cid:126)Cj) = 0, ∀j < i.
This ordering can be obtained by setting (cid:126)C0 = (0, ..., 0), then selecting the terms with a single
conjunction (i.e. only one Ci is non-zero), then those with two conjunctions (i.e.
two Ci’s are
non-zero), then with three conjunctions, and so on.
The strategy is to use induction to build a noisy-logical distribution which agrees with P (E| (cid:126)C)
for all values of (cid:126)C. We loop over the states and incrementally construct the logical function
f(E0, ..., E2N−1) and estimate the parameters ω0, ..., ω2N−1. It is convenient to recursively de-
ﬁne a variable Ei+1 = Ei ⊗ Ei, so that f(E0, ..., E2N−1) = E2N−1.
We start the induction using feature Ψ0( (cid:126)C) = 1. Set E0 = E0 and ω0 = P (E|0, ..., 0). Then
P (E0| (cid:126)C0; ω0) = P (E| (cid:126)C0), so the noisy-logical distribution ﬁts the data for input (cid:126)C0.
Now proceed by induction to determine EM +1 and ωM +1, assuming that we have determined EM
and ω0, ..., ωM such that P (EM = 1| (cid:126)Ci; ω0, ..., ωM ) = P (E = 1| (cid:126)Ci), for i = 0, ..., M. There are
three cases to consider which are analogous to the cases considered in the section with two causes.
Case 1. If P (E = 1| (cid:126)CM +1) > P (EM = 1| (cid:126)CM +1; ω0, ..., ωM ) we need ΨM +1( (cid:126)C) to be a genera-
tive feature. Set EM +1 = EM ∨ EM +1 with P (EM +1 = 1|ΨM +1; ωM +1) = ωM +1ΨM +1. Then
we obtain:
P (EM +1 = 1| (cid:126)CM +1; ω0, ., ωM +1) = P (EM = 1| (cid:126)CM +1; ω0, ., ωM )+P (EM +1|ΨM +1( (cid:126)C); ωM +1)

−P (EM = 1| (cid:126)CM +1; ω0, ., ωM )P (EM +1 = 1|ΨM +1( (cid:126)C); ωM +1) =

P (EM = 1| (cid:126)CM +1; ω0, ., ωM )+ωM +1ΨM +1( (cid:126)C)−P (EM = 1| (cid:126)CM +1; ω0, ., ωM )ωM +1ΨM +1( (cid:126)C)
In particular, we see that P (EM +1 = 1| (cid:126)Ci; ω0, ..., ωM +1) = P (EM = 1| (cid:126)Ci; ω0, ..., ωM ) =
P (E = 1| (cid:126)Ci) for i < M + 1 (using ΨM +1( (cid:126)Ci) = 0, ∀i < M + 1). To determine the value
of ωM +1, we must solve P (E = 1| (cid:126)CM +1) = P (EM = 1| (cid:126)CM +1; ω0, ..., ωM ) + ωM +1 − P (EM =
1| (cid:126)CM +1; ω0, ..., ωM )ωM +1 (using ΨM +1( (cid:126)CM +1) = 1). This gives ωM +1 = {P (E = 1| (cid:126)CM +1) −
P (EM = 1| (cid:126)CM +1; ω0, ..., ωM )}/{1 − P (EM = 1| (cid:126)CM +1; ω0, ..., ωM +1)} (the conditions ensure
that ωM +1 ∈ [0, 1]).
Case 2. If P (E = 1| (cid:126)CM +1) < P (EM = 1| (cid:126)CM +1; ω0, ..., ωM ) we need ΨM +1( (cid:126)C) to be a preven-
tative feature. Set EM +1 = EM ∧ ¬EM +1 with P (EM +1 = 1|ΨM +1; ωM +1) = ωM +1ΨM +1.
Then we obtain:
P (EM +1 = 1| (cid:126)CM +1; ω0, ..., ωM +1) = P (EM = 1| (cid:126)CM +1; ω0, ..., ωM ){1 − ωM +1ΨM +1( (cid:126)C)}.
(4)

4

As for the ﬁrst case, P (EM +1 = 1| (cid:126)Ci; ω0, ..., ωM +1) = P (EM = 1| (cid:126)Ci; ω0, ..., ωM ) = P (E =
1| (cid:126)Ci) for i < M + 1 (because ΨM +1( (cid:126)Ci) = 0, ∀i < M + 1). To determine the value of
ωM +1 we must solve P (E = 1| (cid:126)CM +1) = P (EM = 1| (cid:126)CM +1; ω0, ..., ωM ){1 − ωM +1} (us-
ing ΨM +1( (cid:126)CM +1) = 1). This gives ωM +1 = {P (EM = 1| (cid:126)CM +1; ω0, ..., ωM ) − P (E =
1| (cid:126)CM +1)}/P (EM = 1| (cid:126)CM +1; ω0, ..., ωM ) (the conditions ensure that ωM +1 ∈ [0, 1]).
Case 3. If P (E = 1| (cid:126)CM +1) = P (EM = 1| (cid:126)CM +1; ω0, ..., ωM ), then we do nothing.

5 Cognitive Science Human Experiments

We illustrate noisy-logical distributions by applying them to model two recent cognitive science
experiments by Liljeholm and Cheng which involve causal reasoning in complex environments [6].
In these experiments, the participants are asked questions about the causal structure of the data.
But the participants are not given enough data to determine the full distribution (i.e. not enough
to determine the causal structure with certainty). Instead the experimental design forces them to
choose between two different causal structures.
We formulate this as a model selection problem [3].
Formally, we specify distributions
P (D|(cid:126)ω, Graph) for generating the data D from a causal model speciﬁed by Graph and parameter-
ized by (cid:126)ω. These distributions will be of simple noisy-logical form. We set the prior distributions
P ((cid:126)ω|Graph) on the parameter values to be the uniform distribution. The evidence for the causal
model is given by:

(cid:90)

P (D|Graph) =

d(cid:126)ωP (D|(cid:126)ω, Graph)P ((cid:126)ω|Graph).

(5)

We then evaluate the log-likelihood ratio log P (D|Graph1)
P (D|Graph2) between two causal models Graph1
Graph2, called the causal support [3] and use this to predict the performance of the participants.
This gives good ﬁts to the experimental results.
As an alternative theoretical model, we consider the possibility that the participants use the same
causal structures, speciﬁed by Graph1 and Graph2, but use a linear model to combine cues.
Formally, this corresponds to a model P (E = 1|C1, ..., CN ) = ω1C1 + ... + ωN CN (with
ωi ≥ 0, ∀i = 1, ..., N and ω1 + ... + ωN ≤ 1). This model corresponds [1, 3] to the classic
Rescorla-Wagner learning model [8].
It cannot be expressed in simple noisy-logical form. Our
simulations show that this model does not account for human participant performance .
We note that previous attempts to model experiments with multiple causes and conjunctions by
Novick and Cheng [2] can be interpreted as performing maximum likelihood estimation of the pa-
rameters of noisy-logical distributions (their paper helped inspire our work). Those experiments,
however, were simpler than those described here and model selection was not used. The extensive
literatures on two cases [1, 3] can also be interpreted in terms of noisy-logical models.

5.1 Experiment I: Multiple Causes

In Experiment 1 of [6], the cover story involves a set of allergy patients who either did or did not
have a headache, and either had or had not received allergy medicines A and B. The experimental
participants were informed that two independent studies had been conducted in different labs us-
ing different patient groups. In the ﬁrst study, patients were administered medicine A, whereas in
the second study patients were administered both medicines A and B. A simultaneous presenta-
tion format [7] was used to display the speciﬁc contingency conditions used in both studies to the
experimental subjects. The participants were then asked whether medicine B caused the headache.
We represent this experiment as follows using binary-valued variables E, B1, B2, C1, C2. The vari-
able E indicates whether a headache has occurred (E = 1) or not (E = 0). B1 = 1 and B2 = 1 no-
tate background causes for the two studies (which are always present). C1 and C2 indicate whether
medicine A and B are present respectively (e.g. C1 = 1 if A is present, C1 = 0 otherwise). The
data D shown to the subjects can be expressed as D = (D1, D2) where D1 is the contingency table
Pd(E = 1|B1 = 1, C1 = 0, C2 = 0), Pd(E = 1|B1 = 1, C1 = 1, C2 = 0) for the ﬁrst study

5

and D2 is the contingency table Pd(E = 1|B2 = 1, C1 = 0, C2 = 0), Pd(E = 1|B2 = 1, C1 =
1, C2 = 1) for the second study.
The experimental design forces the participants to choose between the two causal models shown
on the left of ﬁgure (3).
These causal models differ by whether C2 (i.e. medicine B)
can have an effect or not. We set P (D|(cid:126)ω, Graph) = P (D1|(cid:126)ω1, Graph)P (D2|(cid:126)ω2, Graph),
i )} (for i = 1, 2) is the contingency data. We express these distribu-
where Di = {(Eµ, (cid:126)C µ
tions in form P (Di|(cid:126)ωi, Graph) =
i , Graph). For Graph1, P1(.) and P2(.)
are P (E|B1, C1, ωB1, ωC1) and P (E|B2, C1, ωB2, ωC1).
For Graph2, P1(.) and P2(.) are
P (E|B1, C1, ωB1, ωC1) and P (E|B2, C1, C2, ωB2, ωC1, ωC2). All these P (E|.) are noisy-or dis-
tributions.
For Experiment 1 there are two conditions [6], see table (1). In the ﬁrst power-constant condition
[6], the data is consistent with the causal structure for Graph1 (i.e. C2 has no effect) using noisy-or
distributions. In the second ∆P-constant condition [6], the data is consistent with the causal structure
for Graph1 but with noisy-or replaced by the linear distributions (e.g. P (E = 1|C1, ..., Cn) =
ω1C1 + ... + ωnCn)).

µ Pi(Eµ| (cid:126)C µ

(cid:81)

i , (cid:126)ωµ

Table 1: Experimental conditions (1) and (2) for Experiment 1
(1) Pd(E = 1|B1 = 1, C1 = 0, C2 = 0), Pd(E = 1|B1 = 1, C1 = 1, C2 = 0)
Pd(E = 1|B2 = 1, C1 = 0, C2 = 0), Pd(E = 1|B2 = 1, C1 = 1, C2 = 1)
(2) Pd(E = 1|B1 = 1, C1 = 0, C2 = 0), Pd(E = 1|B1 = 1, C1 = 1, C2 = 0)
Pd(E = 1|B2 = 1, C1 = 0, C2 = 0), Pd(E = 1|B2 = 1, C1 = 1, C2 = 1)

16/24, 22/24
0/24,18/24
0/24, 6/24
16/24,22/24

5.2 Experiment I: Results

We compare Liljeholm and Cheng’s experimental results with our theoretical simulations. These
comparisons are shown on the right-hand-side of ﬁgure (3). The left panel shows the proportion
of participants who decide that medicine B causes a headache for the two conditions. The right
panel shows the predictions of our model (labeled ”noisy-logical”) together with predictions of a
model that replaces the noisy-logical distributions by a linear model (labeled ”linear”). The simu-
lations show that the noisy-logical model correctly predicts that participants (on average) judge that
medicine B has no effect in the ﬁrst experimental condition, but B does have an effect in the second
condition. By contrast, the linear model makes the opposite (wrong) prediction. In summary, model
selection comparing two noisy-logical models gives a good prediction of participant performance.

Figure 3: Causal model and results for Experiment I. Left panel: two alternative causal models for
the two studies. Right panel: the experimental results (proportion of patients who think medicine
B causes headaches)) for the Power-constant and ∆P-constant conditions [6]. Far right, the causal
support for the noisy-logic and linear models.

6

5.3 Experiment II: Causal Interaction

Liljeholm and Cheng [6] also investigated causal interactions. The experimental design was identical
to that used in Experiment 1, except that participants were presented with three studies in which only
one medicine (A) was tested. Participants were asked to judge whether medicine A interacts with
background causes that vary across the three studies. We deﬁne the background causes as B1,B2,B3
for the three studies, and C1 for medicine A. This experiment was also run under two different
conditions, see table (2). The ﬁrst power-constant condition [6] was consistent with a noisy-logical
model, but the second power-varying condition [6] was not.

Table 2: Experimental conditions (1) and (2) for Experiment 2
(1) P (E = 1|B1 = 1, C1 = 0), P (E = 1|B1 = 1, C1 = 1)
P (E = 1|B2 = 1, C1 = 0), P (E = 1|B2 = 1, C1 = 1)
P (E = 1|B3 = 1, C1 = 0), P (E = 1|B3 = 1, C1 = 1)
(2) P (E = 1|B1 = 1, C1 = 0), P (E = 1|B1 = 1, C1 = 1)
P (E = 1|B2 = 1, C1 = 0), P (E = 1|B2 = 1, C1 = 1)
P (E = 1|B3 = 1, C1 = 0), P (E = 1|B3 = 1, C1 = 1)

16/24, 22/24
8/24,20/24
0/24,18/24
0/24, 6/24
0/24,12/24
0/24,18/24

The experimental design caused participants to choose between two causal models shown on the
left panel of ﬁgure (4). The probability of generating the data is given by P (D|(cid:126)ω, Graph) =
P (D1|(cid:126)ω1, Graph)P (D2|(cid:126)ω2, Graph)P (D3|(cid:126)ω3, Graph).
the P (Di|.) are noisy-
or distributions P (E|B1, C1, ωB1, ωC1), P (E|B2, C1, ωB2, ωC1), P (E|B3, C1, ωB3, ωC1).
For
the P (Di|.) are P (E|B1, C1, ωB1, ωC1), P (E|B2, C1, B2C1, ωB2, ωC1, ωB2C1) and
Graph2,
P (E|B3, C1, B3C1, ωB3, ωC1, ωB3C1).
All the distributions are noisy-or on the unary causal features (e.g. B, C1), but the nature of the
conjunctive cause B ∧ C1 is unknown (i.e. not speciﬁed by the experimental design). Hence our
theory considers the possibilities that it is a noisy-or (e.g. can produce headaches) or noisy-and-not
(e.g. can prevent headaches), see graph 2 of Figure (4).

For Graph1,

5.4 Results of Experiment II

Figure (4) shows human and model performance for the two experimental conditions. Our noisy-
logical model is in agreement with human performance – i.e. there is no interaction between causes
in the power-constant condition, but there is interaction in the power-varying condition. By contrast,
the linear model predicts interaction in both conditions and hence fails to model human performance.

Figure 4: Causal model and results for Experiment II. Left panel: two alternative causal models (one
involving conjunctions) for the three studies . Right panel: the proportion of participants who think
that there is an interaction (conjunction) between medicine A and the background for the power-
constant and power-varying conditions [6]. Far right, the causal support for the noisy-logical and
linear models.

7

6 Summary

The noisy-logical distribution gives a new way to represent conditional probability distributions
deﬁned over binary variables. The complexity of the distribution can be adjusted by restricting
the set of causal factors. If all the causal factors are allowed, then the distribution can represent
any conditional distribution. But by restricting the set of causal factors we can obtain standard
distributions such as the noisy-or and noisy-and-not.
We illustrated the noisy-logical distribution by modeling experimental ﬁndings on causal reasoning.
Our results showed that this distribution ﬁtted the experimental data and, in particular, accounted for
the major trends (unlike the linear model). This is consistent with the success of noisy-or and noisy-
and-not models for accounting for experiments involving two causes [1], [2],[3]. This suggests that
humans may make use of noisy-logical representations for causal reasoning.
One attraction of the noisy-logical representation is that it helps clarify the relationship between
logic and probabilities. Standard logical relationships between causes and effects arise in the limit
as the ωi take values 0 or 1. We can, for example, bias the data towards a logical form by using
a prior on the (cid:126)ω. This may be useful, for example, when modeling human cognition – evidence
suggests that humans ﬁrst learn logical relationships and, only later, move to probabilities.
In summary, the noisy-logical distribution is a novel way to represent conditional probability distri-
butions deﬁned on binary variables. We hope this class of distributions will be useful for modeling
cognitive phenomena and for applications to artiﬁcial intelligence.

Acknowledgements

We thank Mimi Liljeholm, Patricia Cheng, Adnan Darwiche, Keith Holyoak, Iasonas Kokkinos, and
YingNian Wu for helpful discussions. Mimi and Patricia kindly gave us access to their experimental
data. We acknowledge funding support from the W.M. Keck foundation and from NSF 0413214.

References
[1] P. W. Cheng. From covariation to causation: A causal power theory. Psychological Review,

104, 367405. 1997.

[2] L.R. Novick and P.W. Cheng. Assessing interactive causal inﬂuence. Psychological Review,

111, 455-485. 2004.

[3] T. L. Grifﬁths, and J. B. Tenenbaum. Structure and strength in causal induction. Cognitive

Psychology, 51, 334-384, 2005.

[4] J. Pearl, Probabilistic Reasoning in Intelligent Systems. Morgan-Kauffman, 1988.
[5] C.N. Glymour. The Mind’s Arrow: Bayes Nets and Graphical Causal Models in Psychology.

MIT Press. 2001.

[6] M. Liljeholm and P. W. Cheng. When is a Cause the ”Same”? Coherent Generalization across

Contexts. Psychological Science, in press. 2007.

[7] M. J. Buehner, P. W. Cheng, and D. Clifford. From covariation to causation: A test of the
assumption of causal power. Journal of Experimental Psychology: Learning, Memory, and
Cognition, 29, 1119-1140, 2003.

[8] R. A. Rescorla, and A. R. Wagner. A theory of Pavlovian conditioning: Variations in the effec-
tiveness of reinforcement and nonreinforcement. In A. H. Black and W. F. Prokasy (Eds.), Clas-
sical conditioning II: Current theory and research (pp. 64-99). New York: Appleton-Century
Crofts. 1972.

8

"
870,2007,DIFFRAC: a discriminative and flexible framework for clustering,"We present a novel linear clustering framework (Diffrac) which relies on a linear discriminative cost function and a convex relaxation of a combinatorial optimization problem. The large convex optimization problem is solved through a sequence of lower dimensional singular value decompositions. This framework has several attractive properties: (1) although apparently similar to K-means, it exhibits superior clustering performance than K-means, in particular in terms of robustness to noise. (2) It can be readily extended to non linear clustering if the discriminative cost function is based on positive definite kernels, and can then be seen as an alternative to spectral clustering. (3) Prior information on the partition is easily incorporated, leading to state-of-the-art performance for semi-supervised learning, for clustering or classification. We present empirical evaluations of our algorithms on synthetic and real medium-scale datasets.","DIFFRAC : a discriminative and flexible

framework for clustering

Francis R. Bach

INRIA - Willow Project
´Ecole Normale Sup´erieure

45, rue d’Ulm, 75230 Paris, France
francis.bach@mines.org

Za¨ıd Harchaoui

LTCI, TELECOM ParisTech and CNRS

46, rue Barrault

75634 Paris cedex 13, France

zaid.harchaoui@enst.fr

Abstract

We present a novel linear clustering framework (DIFFRAC) which relies on a lin-
ear discriminative cost function and a convex relaxation of a combinatorial op-
timization problem. The large convex optimization problem is solved through a
sequence of lower dimensional singular value decompositions. This framework
has several attractive properties: (1) although apparently similar to K-means, it
exhibits superior clustering performance than K-means, in particular in terms of
robustness to noise. (2) It can be readily extended to non linear clustering if the
discriminative cost function is based on positive deﬁnite kernels, and can then be
seen as an alternative to spectral clustering. (3) Prior information on the partition
is easily incorporated, leading to state-of-the-art performance for semi-supervised
learning, for clustering or classiﬁcation. We present empirical evaluations of our
algorithms on synthetic and real medium-scale datasets.

1 Introduction

Many clustering frameworks have already been proposed, with numerous applications in machine
learning, exploratory data analysis, computer vision and speech processing. However, these un-
supervised learning techniques have not reached the level of sophistication of supervised learning
techniques, that is, for all methods, there are still a signiﬁcant number of explicit or implicit param-
eters to tune for successful clustering, most generally, the number of clusters and the metric or the
similarity structure over the space of conﬁgurations.
In this paper, we present a discriminative and flexible framework for clustering (DIFFRAC), which
is aimed at alleviating some of those practical annoyances. Our framework is based on a recent
set of works [1, 2] that have used the support vector machine (SVM) cost function used for linear
classiﬁcation as a clustering criterion, with the intuitive goal of looking for clusters which are most
linearly separable. This line of work has led to promising results; however, the large convex opti-
mization problems that have to be solved prevent application to datasets larger than few hundreds
data points.1 In this paper, we consider the maximum value of the regularized linear regression on
indicator matrices. By choosing a square loss (instead of the hinge loss), we obtain a simple cost
function which can be simply expressed in closed form and is amenable to speciﬁc efﬁcient convex
optimization algorithms, that can deal with large datasets of size 10,000 to 50,000 data points. Our
cost function turns out to be a linear function of the “equivalence matrix” M , which is a square
{0, 1}-matrix indexed by the data points, with value one for all pairs of data points that belong to
the same clusters, and zero otherwise. In order to minimize this cost function with respect to M , we
follow [1] and [2] by using convex outer approximations of the set of equivalence matrices, with a
novel constraint on the minimum number of elements per cluster, which is based on the eigenvalues
of M , and essential to the success of our approach.

1Recent work [3] has looked at more efﬁcient formulations.

In Section 2, we present a derivation of our cost function and of the convex relaxations. In Section 3,
we show how the convex relaxed problem can be solved efﬁciently through a sequence of lower
dimensional singular value decompositions, while in Section 4, we show how a priori knowledge
can be incorporated into our framework. Finally, in Section 5, we present simulations comparing
our new set of algorithms to other competing approaches.

2 Discriminative clustering framework

In this section, we ﬁrst assume that we are given n points x1, . . . , xn in Rd, represented in a matrix
X ∈ Rn×d. We represent the various partitions of {1, . . . , n} into k > 1 clusters by indicator
matrices y ∈ {0, 1}n×k such that y1k = 1n, where 1k and 1n denote the constant vectors of all
ones, of dimensions k and n. We let denote Ik the set of k-class indicator matrices.

2.1 Discriminative clustering cost

Given y, we consider the regularized linear regression problem of y given X, which takes the form:

min

w∈Rd×k, b∈R1×k

1
n ky − Xw − 1nbk2

F + κ tr w⊤w,

(1)

where the Frobenius norm is deﬁned for any vector or rectangular matrix as kAk2
F = trAA⊤ =
trA⊤A. Denoting f (x) = w⊤x + b ∈ Rk, this corresponds to a multi-label classiﬁcation problem
with square loss functions [4, 5]. The main advantage of this cost function is the possibility of (a)
minimizing the regularized cost in closed form and (b) including a bias term by simply centering
the data; namely, the global optimum is attained at w∗ = (X ⊤ΠnX + nκIn)−1X ⊤Πny and b∗ =
1
n is the usual centering projection matrix. The optimal
n 1⊤
value is then equal to

n (y − Xw∗), where Πn = In − 1

n 1n1⊤

where the n × n matrix A(X, κ) is deﬁned as:

J(y, X, κ) = tr yy⊤A(X, κ),

A(X, κ) = 1

n Πn(In −X(X ⊤ΠnX + nκI)−1X ⊤)Πn.

(2)

(3)

The matrix A(X, κ) is positive semi-deﬁnite, i.e., for all u ∈ Rn, u⊤A(X, κ)u > 0, and 1n is a
singular vector of A(X, κ), i.e., A(X, κ)1n = 0.
Following [1] and [2], we are thus looking for a k-class indicator matrix y such that tr yy⊤A(X, κ)
is minimal, i.e., for a partition such that the clusters are most linearly separated, where the sepa-
rability of clusters is measured through the minimum of the discriminative cost with respect to all
linear classiﬁers. This combinatorial optimization is NP-hard in general [6], but efﬁcient convex
relaxations may be obtained, as presented in the next section.

2.2 Indicator and equivalence matrices
The cost function deﬁned in Eq. (2) only involves the matrix M = yy⊤ ∈ Rn×n. We let denote Ek
the set of “k-class equivalence matrices”, i.e., the set of matrices M such that there exists a k-class
indicator matrix y with M = yy⊤.
There are many outer convex approximations of the discrete sets Ek, based on different properties of
matrices in Ek, that were used in different contexts, such as maximum cut problems [6] or correlation
clustering [7]. We have the following usual properties of equivalence matrices (independent of k):
if M ∈ Ek, then (a) M is positive semideﬁnite (denoted as M < 0), (b) M has nonnegative values
(denoted as M > 0) , and (c) the diagonal of M is equal to 1n (denoted as diag(M ) = 1n).
Moreover, if M corresponds to at most k clusters, we have M < 1
n , which is a consequence to
the convex outer approximation of [6] for the maximum k-cut problem. We thus use the following
convex outer approximation:

k 1n1⊤

Ck = {M ∈ Rn×n, M = M ⊤, diag(M ) = 1n, M > 0, M < 1

k 1n1⊤

n } ⊃ Ek.

Note that when k = 2, the constraints M > 0 (pointwise nonnegativity) is implied by the other
constraints.

2.3 Minimum cluster sizes
Given the discriminative nature of our cost function (and in particular that A(X, κ)1n = 0), the
minimum value 0 is always obtained with M = 1n1⊤
n , a matrix of rank one, equivalent to a single
cluster. Given the number of desired clusters, we thus need to add some prior knowledge regarding
the size of those clusters. Following [1], we impose a minimum size λ0 for each cluster, through
row sums and eigenvalues:

Row sums
smaller than (n − (k − 1)λ0) if they are all larger than λ0)–this is the same constraint as in [1].

If M ∈ Ek, then M 1n > λ01n and M 1n 6 (n − (k − 1)λ0)1n (the cluster must be

Eigenvalues When M ∈ Ek, the sizes of the clusters are exactly the k largest eigenvalues of M .
Thus, for a matrix in Ek, the minimum cluster size constraint is equivalent to Pn
i=1 1λi(M)>λ0 >
k, where λ1(M ), . . . , λn(M ) are the n eigenvalues of M . Functions of the form Φ(M ) =
Pn
i=1 φ(λi(M )) are referred to as spectral functions and are particularly interesting in machine
learning and optimization, since Φ inherits from φ many of its properties, such as differentiability
and convexity [8]. The previous constraint can be seen as Φ(M ) > k, with φ(λ) = 1λ>λ0, which
is not concave and thus does not lead to a convex constraint. In this paper we propose to use the
concave upper envelope of this function, namely φλ0 (λ) = min{λ/λ0, 1}, thus leading to a novel
additional constraint.
Our ﬁnal convex relaxation is thus of minimizing trA(X, κ)M with respect to M ∈ Ck and
such that Φλ0 (M ) > k, M 1n > λ01n and M 1n 6 (n − (k − 1)λ0)1n, where Φλ0 (M ) =
Pn
i=1 min{λi(M )/λ0, 1}. The clustering results are empirically robust to the value of λ0. In all

our simulations we use λ0 = ⌊n/2k⌋.

2.4 Comparison with K-means

Our method bears some resemblance with the usual K-means algorithm. Indeed, in the unregular-
ized case (κ = 0), we aim to minimize

Results from [9] show that K-means aims at minimizing the following criterion with respect to y:

tr Πn(In − X(X ⊤ΠnX)−1X ⊤)Πnyy⊤.

min

µ∈Rk×d

kX − yµk2

F = tr(In − y(y⊤y)−1y⊤)(ΠnX)(ΠnX)⊤.

The main differences between the two cost functions are that (1) we require an additional parameter,
namely the minimum number of elements per cluster and (2) our cost function normalizes the data,
while the K-means distortion measure normalizes the labels. This apparently little difference has
a signiﬁcant impact on the performance, as our method is invariant by afﬁne scaling of the data,
while K-means is only invariant by translation, isometries and isotropic scaling, and is very much
dependent on how the data are presented (in particular the marginal scaling of the variables). In
Figure 1, we compare the two algorithms on a simple synthetic task with noisy dimensions, showing
that ours is more robust to noisy features. Note that using a discriminative criterion based on the
square loss may lead to the masking problem [4], which can be dealt with in the usual way by using
second-order polynomials or, equivalently, a polynomial kernel.

2.5 Kernels

The matrix A(X, κ) in Eq. (3) can be expressed only in terms of the Gram matrix K = XX ⊤.
Indeed, using the matrix inversion lemma, we get:

where eK = ΠnKΠn is the “centered Gram matrix” of the points X. We can thus apply our

framework with any positive deﬁnite kernel [5].

A(K, κ) = κΠn(eK + nκIn)−1Πn,

(4)

2.6 Additional relaxations

Our convex optimization problem can be further relaxed. An interesting relaxation is obtained by
(1) relaxing the constraints M < 1
n into M < 0, (2) relaxing diag(M ) = 1n into trM = n,

k 1n1⊤

r
o
r
r
e
 
g
n
i
r
e
t
s
u
c

l

1

0.8

0.6

0.4

0.2

0

−0.2
 
0

 

K−means
diffrac

10

20

30

noise dimension

Figure 1: Comparison with K-means, on a two-dimensional dataset composed of two linearly sep-
arable bumps (100 data points, plotted in the left panel), with additional random independent noise
dimensions (with normal distributions with same marginal variances as the 2D data). The clustering
performance is plotted against the number of irrelevant dimensions, for regular K-means and our
DIFFRAC approach (right panel, averaged over 50 replications with the standard deviation in dotted
lines) . The clustering performance is measured by a metric between partitions deﬁned in Section 5,
which is always between 0 and 1.

shows that this relaxation leads to an eigenvalue problem: let A = Pn
relaxed convex optimization problem is attained at M = Pj
i=1 uiu⊤

and (3) removing the constraint M > 0 and the constraints on the row sums. A short calculation
i be an eigenvalue
decomposition of A, where a1 6 · · · 6 an are the sorted eigenvalues. The minimal value of the
j+1, with
j = ⌊n/λ0⌋. This additional relaxation into an eigenvalue problem is the basis of our efﬁcient
optimization algorithm in Section 3.

i + (n − λ0j)uj+1u⊤

i=1 aiuiu⊤

In the kernel formulation, since the smallest eigenvectors of A = 1

n Πn(eK + nκIn)−1Πn are the
same as the largest eigenvectors of eK, the relaxed problem is thus equivalent to kernel principal

component analysis [10, 5] in the kernel setting, and in the linear setting to regular PCA (followed by
our rounding procedure presented in Section 3.3). In the linear setting, since PCA has no clustering
effects in general2, it is clear that the constraints that were removed are essential to the clustering
performance. In the kernel setting, experiments have shown that the most important constraint to
keep in order to achieve the best embedding and clustering is the constraint diag(M ) = 1n.

3 Optimization

Since φλ0 (λ) = 1
(λ + λ0 − |λ − λ0|), and the sum of singular values can be represented as a
2λ0
semideﬁnite program (SDP), our problem is an SDP. It can thus be solved to any given accuracy in
polynomial time by general purpose interior-point methods [12]. However, the number of variables
is O(n2) and thus the complexity of general purpose algorithms will be at least O(n7); this remains
much too slow for medium scale problems, where the number of data points is between 1,000 and
10,000. We now present an efﬁcient approximate method that uses the speciﬁcity of the problem to
reduce the computational load.

3.1 Optimization by partial dualization

We saw earlier that by relaxing some of the constraints, we get back an eigenvalue problem. Eigen-
value decompositions are among the most important tools in numerical algebra and algorithms and
codes are heavily optimized for these, and it is thus advantageous to rely on a sequence of eigenvalue
decompositions for large scale algorithms.

We can dualize some constraints while keeping others; this leads to the following proposition:

2Recent results show however that it does have an effect when clusters are spherical Gaussians [11].

Proposition 1 The solution of the convex optimization problem deﬁned in Section 2.3 can be ob-
tained my maximizing F (β) = minM <0,trM=n,Φλ0 (M)>k trB(β)M − b(β) with respect to β, where

B(β) = A + Diag(β1) −

1
2

(β2 − β3)1⊤ −

1
2

1(β2 − β3)⊤ − β4 +

1
2

β5β⊤
5
β6

b(β) = β⊤

1 1 − (n − (k − 1)λ0)β⊤

2 1 + λ0β⊤

3 1 + kβ6/2 + β⊤

5 1,

and β1 ∈ Rn, β2 ∈ Rn

+, β3 ∈ Rn

+, β4 ∈ Rn×n

+ ,β5 ∈ Rn, β6 ∈ R+.

1n1⊤

The variables β1, β2, β3, β4, (β5, β6) correspond to the respective dualizations of the constraints
diag(M ) = 1n, M 1n 6 (n − (k − 1)λ0)1n, M 1n > λ01n, M > 0, and M <
The function J(B) = minM <0,trM=n,Φλ0 (M)>k trBM is a spectral convex function and may be
computed in closed form through an eigenvalue decomposition. Moreover, a subgradient may be
easily computed, readily leading to a numerically efﬁcient subgradient method in fewer dimensions
than n2. Indeed, if we subsample the pointwise positivity constraint N > 0 (so that β4 has only a
size smaller than n1/2 × n1/2), then the set of dual variables β we are trying to maximize has linear
size in n (instead of the primal variable M being quadratic in n).
More reﬁned optimization schemes, based on smoothing of the spectral function J(B) by
minM <0,trM=n,Φλ0 (M)>k[trBM + εtrM 2] are also used to speed up convergence (steepest descent
of a smoothed function is generally faster than subgradient iterations) [13].

k

n

.

3.2 Computational complexity

The running time complexity can be splitted into initialization procedures and per iteration com-
plexity. The per iteration complexity depends directly on the cost of our eigenvalue problems, which
themselves are linear in the matrix-vector operation with the matrix A (we only require a ﬁxed small
number of eigenvalues). In all situations, we manage to keep a linear complexity in the number n
of data points. Note, however, that the number of descent iterations cannot be bounded a priori; in
simulations we limit the number of those iterations to 200.

For linear kernels with dimension d, the complexity of initialization is O(d2n), while the complexity
of each iteration is proportional to the cost of performing a matrix-vector operation with A, that is,
O(dn). For general kernels, the complexity of initialization is O(n3), while the complexity of each
iteration is O(n2). However, using an incomplete Cholesky decomposition [5] makes all costs linear
in n.

3.3 Rounding
After the convex optimization, we obtain a low-rank matrix M ∈ Ck which is pointwise nonnegative
with unit diagonal, of the form U U ⊤ where U ∈ Rn×m. We need to project it back to the discrete
Ek. We have explored several possibilities, all with similar results. We propose the following pro-
cedure: we ﬁrst project M back to the set of matrices of rank k and unit diagonal, by computing
an eigendecomposition, rescaling the ﬁrst k eigenvectors to unit norms and then perform K-means,
which is equivalent to performing the spectral clustering algorithm of [14] on the matrix M .

4 Semi-supervised learning

Working with equivalence matrices M allows to easily include prior knowledge on the clus-
ters [2, 15, 16], namely, “must-link” constraints (also referred to a positive constraints) for which we
constrain an element of M to be one, and “must-not-link” constraints (also referred to as negative
constraints), for which we constrain an element of M to be zero. Those two constraints are linear in
M and can thus easily be included in our convex formulation.
We assume throughout this section that we have a set of “must-link” pairs P+ and a set of “must-
not-link” pairs P−. Moreover, we assume that the set of positive constraints is closed, i.e., that if
there is a path of positive constraints between two data points, then these two data points are already
forming a pair in P+. If the set of positive pairs does not satisfy this assumption, a larger set of pairs
can be obtained by transitive closure.

r
o
r
r
e
 
g
n
i
r
e
t
s
u
c

l

20 % x n

40 % x n

1

0.5

0

 

1

K−means
diffrac

0.5

0

 
0
noise dimension

40

20

20

0
noise dimension

40

Figure 2: Comparison with K-means in the semi-supervised setting, with data taken from Figure 1:
clustering performance (averaged over 50 replications, with standard deviations in dotted) vs. num-
ber of irrelevant dimensions, with 20% × n and 40% × n random matching pairs used for semi-
supervision.

Positive constraints Given our closure assumption on P+, we get a partition of {1, . . . , n} into
p “chunks” of size greater or equal to 1. The singletons in this partition correspond to data points
that are not involved in any positive constraints, while other subsets corresponds to chunks of data
points that must occur together in the ﬁnal partition. We let Cj, j = 1, . . . , p denote those groups,
and let P denote the n × p {0, 1}-matrix deﬁned such that each column (indexed by j) is equal to
one for rows in Cj and zero otherwise. Forcing those groups is equivalent to considering M of the
form M = P MP P ⊤, where MP is an equivalence matrix of size p. Note that the positive constraint
Mij = 1 is in fact turned into the equality of columns (and thus rows by symmetry) i and j of M ,
which is equivalent when M ∈ Ek, but much stronger for M ∈ Ck.
In our linear clustering framework, this is in fact equivalent to (a) replacing each chunk by its
mean, (b) adding a weight equal to the number of elements in the group into the discriminative cost
function and (c) modifying the regularization matrix to take into account the inner variance within
each chunk. Positive constraints can be similarly included into K-means, to form a reduced weighted
K-means problem, which is simpler than other approaches to deal with positive constraints [17].

In Figure 2, we compare constrained K-means and the DIFFRAC framework under the same setting
as in Figure 1, with different numbers of randomly selected positive constraints.

Negative constraints After the chunks corresponding to positive constraints have been collapsed
to one point, we extend the set of negative constraints to those collapsed points (if the constraints
were originally consistent, the negative constraints can be uniquely extended). In our optimization
framework, we simply add a penalty function of the form 1
ij. The K-means
rounding procedure also has to be constrained, e.g., using the procedure of [17].

ε|P−| P(i,j)∈P− M 2

5 Simulations

1 ∪ · · · ∪ B′

(cid:16)k + k′ − 2Pi,i′

In this section, we apply the DIFFRAC framework to various clustering problems and situa-
In all our simulations, we use the following distance between partitions B = B1 ∪
tions.
· · · ∪ Bk and B′ = B′
k′ into k and k′ disjoints subsets of {1, . . . , n}: d(B, B′) =
i′ )(cid:17)1/2
. d(B, B′) deﬁnes a distance over the set of partitions [9]
which is always between 0 and (k + k′ − 2)1/2. When comparing partitions, we use the squared
distance 1
2 − 1 (and between 0 and k − 1, if the two
partitions have the same number of clusters).

2 d(B, B′)2, which is always between 0 and k+k′

Card(Bi∩B′

i′ )2
Card(Bi)Card(B′

5.1 Clustering classiﬁcation datasets

We looked at the Isolet dataset (26 classes, 5,200 data points) from the UCI repository and the
MNIST datasets of handwritten digits (10 classes, 5,000 data points). For each of those datasets,
we compare the performances of K-means, RCA [18] and DIFFRAC, for linear and Gaussian ker-
nels (referred to as “rbf”), for ﬁxed value of the regularization parameter, with different levels of
supervision. Results are presented in Table 1: on unsupervised problems, K-means and DIFFRAC

K-means

RCA

5.6 ± 0.2 4.9 ± 0.2

3.6 ± 0.3 3.0 ± 0.2
2.2 ± 0.2 1.8 ± 0.4

Dataset
DIFFRAC
Mnist-linear 0% 5.6 ± 0.1 6.0 ± 0.4
Mnist-linear 20% 4.5 ± 0.3
Mnist-linear 40% 2.9 ± 0.3
Mnist-RBF 0%
Mnist-RBF 20% 4.6 ± 0.0 1.8 ± 0.4 4.1 ± 0.2
Mnist-RBF 40% 4.9 ± 0.0 0.9 ± 0.1 2.9 ± 0.1
Isolet-linear 0% 12.1 ± 0.6 12.3 ± 0.3
Isolet-linear 20% 10.5 ± 0.2 7.8 ± 0.8 9.5 ± 0.4
Isolet-linear 40% 9.2 ± 0.5 3.7 ± 0.2 7.0 ± 0.4
Isolet-RBF 0%
Isolet-RBF 20% 10.6 ± 0.0 7.5 ± 0.5 7.8 ± 0.5
Isolet-RBF 40% 10.0 ± 0.0 3.7 ± 1.0 6.9 ± 0.6

11.4 ± 0.4 11.0 ± 0.3

Table 1: Comparison of K-means, RCA and linear DIFFRAC, using the clustering metric deﬁned in
Section 5 (averaged over 10 replications), for linear and “rbf” kernels and various levels of supervi-
sion.

have similar performance, while on semi-supervised problems, and in particular for nonlinear ker-
nels, DIFFRAC outperforms both K-means and RCA. Note that all algorithms work on the same
data representation (linear or kernelized) and that differences are due to the underlying clustering
frameworks.

5.2 Semi-supervised classiﬁcation

To demonstrate the effectiveness of our method in a semi-supervised learning (SSL) context, we
performed experiments on some benchmarks datasets for SSL described in [19]. We considered the
following datasets: COIL, BCI and Text. We carried out the experiments in a transductive setting,
i.e., the test set coincides with the set of unlabelled samples. This allowed us to conduct a fair
comparison with the low density separation (LDS) algorithm of [19], which is an enhanced version
of the so-called Transductive SVM. However, deriving “out-of-sample” extensions for our method
is straightforward.

A primary goal in semi-supervised learning is to take into account a large number of labelled points
in order to dramatically reduce the number of labelled points required to achieve a competitive
classiﬁcation accuracy. Henceforth, our experimental setting consists in observing how fast the
classiﬁcation accuracy collapses as the number of labelled points increases. The less labelled points
a method needs to achieve decent classiﬁcation accuracy, the more it is relevant for semi-supervised
learning tasks. As shown in Figure 3, our method yields competitive classiﬁcation accuracy with
very few labelled points on the three datasets. Moreover, DIFFRAC reaches unexpectedly good re-
sults on the Text dataset, where most semi-supervised learning methods usually show disappointing
performance. One explanation might be that DIFFRAC acts as an “augmented”-clustering algorithm,
whereas most semi-supervised learning algorithms are built as “augmented”-versions of traditional
supervised learning algorithms such as LDS which is built on SVMs for instance. Hence, for datasets
exhibiting multi-class structure such as Text, DIFFRAC is more able to utilize unlabelled points
since it based on a multi-class clustering algorithm rather than algorithms based on binary SVMs,
where multi-class extensions are currently unclear. Thus, our experiments support the fact that semi-
supervised learning algorithms built on clustering algorithms augmented with labelled data acting
as hints on clusters are worth for investigation and further research.

6 Conclusion

We have presented a discriminative framework for clustering based on the square loss and penaliza-
tion through spectral functions of equivalence matrices. Our formulation enables the easy incorpora-
tion of semi-supervised constraints, which leads to state-of-the-art performance in semi-supervised
learning. Moreover, our discriminative framework should allow to use existing methods for learn-
ing the kernel matrix from data [20]. Finally, we are currently investigating the use of DIFFRAC in
semi-supervised image segmentation. In particular, early experiments on estimating the number of
clusters using variation rates of our discriminative costs are very promising.

Learning curve on Coil100

Learning curve on BCI

Learning curve on Text

0.8

0.6

0.4

0.2

r
o
r
r
e
 
t
s
e
T

 

DIFFRAC
LDS

0.5

0.45

0.4

0.35

0.3

r
o
r
r
e
 
t
s
e
T

DIFFRAC
LDS

 

0.35

0.3

0.25

0.2

0.15

r
o
r
r
e
 
t
s
e
T

DIFFRAC
LDS

 

0
 
0
200
Number of labelled training points

100

50

150

0.25
 
0
150
Number of labelled training points

100

50

0.1
 
0
200
Number of labelled training points

100

150

50

Figure 3: Semi-supervised classiﬁcation.

References

[1] L. Xu, J. Neufeld, B. Larson, and D. Schuurmans. Maximum margin clustering. In Adv. NIPS, 2004.
[2] T. De Bie and N. Cristianini. Fast SDP relaxations of graph cut clustering, transduction, and other com-

binatorial problems. J. Mac. Learn. Res., 7:1409–1436, 2006.

[3] K. Zhang, I. W. Tsang, and J. T. Kwok. Maximum margin clustering made practical. In Proc. ICML,

2007.

[4] T. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical Learning. Springer-Verlag, 2001.
[5] J. Shawe-Taylor and N. Cristianini. Kernel Methods for Pattern Analysis. Camb. Univ. Press, 2004.
[6] A. Frieze and M. Jerrum. Improved approximation algorithms for MAX k-CUT and MAX BISECTION.

In Integer Programming and Combinatorial Optimization, volume 920, pages 1–13. Springer, 1995.

[7] C. Swamy. Correlation clustering: maximizing agreements via semideﬁnite programming. In ACM-SIAM

Symp. Discrete algorithms, 2004.

[8] A. S. Lewis and H. S. Sendov. Twice differentiable spectral functions. SIAM J. Mat. Anal. App.,

23(2):368–386, 2002.

[9] F R. Bach and M I. Jordan. Learning spectral clustering, with application to speech separation. J. Mac.

Learn. Res., 7:1963–2001, 2006.

[10] B. Sch¨olkopf, A. J. Smola, and K.-R. M¨uller. Nonlinear component analysis as a kernel eigenvalue

problem. Neural Comp., 10(3):1299–1319, 1998.

[11] N. Srebro, G. Shakhnarovich, and S. Roweis. An investigation of computational and informational limits

in gaussian mixture clustering. In Proc. ICML, 2006.

[12] S. Boyd and L. Vandenberghe. Convex Optimization. Camb. Univ. Press, 2003.
[13] J. F. Bonnans, J. C. Gilbert, C. Lemar´echal, and C. A. Sagastizbal. Numerical Optimization Theoretical

and Practical Aspects. Springer, 2003.

[14] A. Y. Ng, M. I. Jordan, and Y. Weiss. On spectral clustering: analysis and an algorithm. In Adv. NIPS,

2002.

[15] L. Xu and D. Schuurmans. Unsupervised and semi-supervised multi-class support vector machines. In

Proc. AAAI, 2005.

[16] M. Heiler, J. Keuchel, and C. Schn¨orr. Semideﬁnite clustering for image segmentation with a-priori

knowledge. In Pattern Recognition, Proc. DAGM, 2005.

[17] K. Wagstaff, C. Cardie, S. Rogers, and S. Schr¨odl. Constrained K-means clustering with background

knowledge. In Proc. ICML, 2001.

[18] A. Bar-Hillel, T. Hertz, N. Shental, and D. Weinshall. Learning distance functions using equivalence

relations. In Proc. ICML, 2003.

[19] O. Chapelle and A. Zien. Semi-supervised classiﬁcation by low density separation. In Proc. AISTATS,

2004.

[20] F. R. Bach, G. R. G. Lanckriet, and M. I. Jordan. Multiple kernel learning, conic duality, and the SMO

algorithm. In Proc. ICML, 2004.

"
234,2007,Receptive Fields without Spike-Triggering,"Stimulus selectivity of sensory neurons is often characterized by estimating their receptive field properties such as orientation selectivity. Receptive fields are usually derived from the mean (or covariance) of the spike-triggered stimulus ensemble. This approach treats each spike as an independent message but does not take into account that information might be conveyed through patterns of neural activity that are distributed across space or time. Can we find a concise description for the processing of a whole population of neurons analogous to the receptive field for single neurons? Here, we present a generalization of the linear receptive field which is not bound to be triggered on individual spikes but can be meaningfully linked to distributed response patterns. More precisely, we seek to identify those stimulus features and the corresponding patterns of neural activity that are most reliably coupled. We use an extension of reverse-correlation methods based on canonical correlation analysis. The resulting population receptive fields span the subspace of stimuli that is most informative about the population response. We evaluate our approach using both neuronal models and multi-electrode recordings from rabbit retinal ganglion cells. We show how the model can be extended to capture nonlinear stimulus-response relationships using kernel canonical correlation analysis, which makes it possible to test different coding mechanisms. Our technique can also be used to calculate receptive fields from multi-dimensional neural measurements such as those obtained from dynamic imaging methods.","Receptive Fields without Spike- Triggering

Jakob H Macke

j a k o b@ t u e bi n g e n . mpg . de

G ¨unther Zeck

z e c k @ n e u r o . mpg . de

Max Planck Ins titute for B iological Cybernetics

Max Planck Ins titute of Neurobiology

S pemanns tras s e 41

72076 T ¨ubingen, Germany

Am Klopfers pitze 1 8

8 21 5 2 Martins ried, Germany

Matthias Bethge

mbe t h g e @ t u e bi n g e n . mp g . de

Max Planck Ins titute for B iological Cybernetics

S pemanns tras s e 41

72076 T ¨ubingen, Germany

Abstract

S timulus s electivity of s ens ory neurons is often characterized by es timating their
receptive ﬁeld properties s uch as orientation s electivity. Receptive ﬁelds are us u-
ally derived from the mean (or covariance) of the s pike-triggered s timulus ens em-
ble. This approach treats each s pike as an independent mes s age but does not take
into account that information might be conveyed through patterns of neural activ-
ity that are dis tributed acros s s pace or time. Can we ﬁnd a concis e des cription for
the proces s ing of a whole population of neurons analogous to the receptive ﬁeld
for s ingle neurons ? Here, we pres ent a generalization of the linear receptive ﬁeld
which is not bound to be triggered on individual s pikes but can be meaningfully
linked to dis tributed res pons e patterns . More precis ely, we s eek to identify thos e
s timulus features and the corres ponding patterns of neural activity that are mos t
reliably coupled. We us e an extens ion of revers e-correlation methods bas ed on
canonical correlation analys is . The res ulting population receptive ﬁelds s pan the
s ubs pace of s timuli that is mos t informative about the population res pons e. We
evaluate our approach us ing both neuronal models and multi-electrode recordings
from rabbit retinal ganglion cells . We s how how the model can be extended to
capture nonlinear s timulus -res pons e relations hips us ing kernel canonical correla-
tion analys is , which makes it pos s ible to tes t different coding mechanis ms . Our
technique can als o be us ed to calculate receptive ﬁelds from multi-dimens ional
neural meas urements s uch as thos e obtained from dynamic imaging methods .

1

Introduction

Vis ual input to the retina cons is ts of complex light intens ity patterns . The interpretation of thes e
patterns cons titutes a challenging problem: for computational tas ks like object recognition, it is not
clear what information about the image s hould be extracted and in which format it s hould be repre-
s ented. S imilarly, it is difﬁcult to as s es s what information is conveyed by the multitude of neurons
in the vis ual pathway. Right from the ﬁrs t s ynaps e, the information of an individual photoreceptor
is s ignaled to many different cells with different temporal ﬁltering properties , each of which is only
a s mall unit within a complex neural network [ 20] . Even if we leave the difﬁculties impos ed by
nonlinearities and feedback as ide, it is hard to judge what the contribution of any particular neuron
is to the information trans mitted.

1

The prevalent tool for characterizing the behavior of s ens ory neurons , the s pike triggered average,
is bas ed on a quas i-linear model of neural res pons es [ 1 5 ] . For the s ake of clarity, we cons ider an
idealized model of the s ignaling channel

.

.

.

.

.

.

y = W x + ξ ,

(1 )
, yN ) T denotes the vector of neural res pons es , x the s timulus parameters , W =
where y = ( y1 ,
, wN ) T the ﬁlter matrix with row ‘ k ’ containing the receptive ﬁeld wk of neuron k , and ξ
( w1 ,
is the nois e. The s pike-triggered average only allows des cription of the s timulus -res pons e function
(i. e.
In order to unders tand the collective behavior of a
neuronal population, we rather have to unders tand the behavior of the matrix W, and the s tructure
of the nois e correlations Σ ξ : B oth of them inﬂuence the feature s electivity of the population.

the wk ) of one s ingle neuron at a time.

Can we ﬁnd a compact des cription of the features that a neural ens emble is mos t s ens itive to? In
the cas e of a s ingle cell, the receptive ﬁeld provides s uch a des cription: It can be interpreted as the
“favorite s timulus ” of the neuron, in the s ens e that the more s imilar an input is to the receptive ﬁeld,
the higher is the s piking probability, and thus the ﬁring rate of the neuron. In addition, the receptive
ﬁeld can eas ily be es timated us ing a s pike-triggered average, which, under certain as s umptions ,
yields the optimal es timate of the receptive ﬁeld in a linear-nonlinear cas cade model [ 1 1 ] .

If we are cons idering an ens emble of neurons rather than a s ingle neuron, it is not obvious what to
trigger on: This requires as s umptions about what patterns of s pikes or modulations in ﬁring rates
acros s the population carry information about the s timulus . Rather than addres s ing the ques tion
“what features of the s timulus are correlated with the occurence of s pikes ”, the ques tion now is :
“What s timulus features are correlated with what patterns of s piking activity? ” [ 1 4] . Phras ed in the
language of information theory, we are s earching for the s ubs pace that contains mos t of the mu-
tual information between s ens ory inputs and neuronal res pons es . B y this dimens ionality reduction
technique, we can ﬁnd a compact des cription of the proces s ing of the population.

As an efﬁcient implementation of this s trategy, we pres ent an extens ion of revers e-correlation meth-
ods bas ed on canonical correlation analys is . The res ulting population receptive ﬁelds (PRFs ) are not
bound to be triggered on individual s pikes but are linked to res pons e patterns that are s imultaneous ly
determined by the algorithm.

We calculate the PRF for a population cons is ting of uniformly s paced cells with center-s urround
receptive ﬁelds and nois e correlations , and es timate the PRF of a population of rabbit retinal ganglion
cells from multi-electrode recordings .
In addition, we s how how our method can be extended to
explore different hypothes es about the neural code, s uch as s pike latencies or interval coding, which
require nonlinear read out mechanis ms .

2 From reverse correlation to canonical correlation

m .
We regard the s timulus at time t as a random variable Xt ∈ R
For s implicity, we as s ume that the s timulus cons is ts of Gaus s ian white nois e, i. e. E( X) = 0 and
Cov( X) = I.

n , and the neural res pons e as Yt ∈ R

The s pike-triggered average a of a neuron can be motivated by the fact that it is the direction in
s timulus -s pace maximizing the correlation-coefﬁcient

ρ =

�

Cov( aT X, Y1 )
Var( aT X) Var( Y1 )

.

(2)

between the ﬁltered s timulus aT X and a univariate neural res pons e Y1 .
In the cas e of a neural
population, we are not only looking for the s timulus feature a, but als o need to determine what
pattern of s piking activity b it is coupled with. The natural extens ion is to s earch for thos e vectors
a1 and b 1 that maximize

.

(3 )

�

ρ1 =

Cov( aT
Var( aT

1 X, b T
1 Y)
1 X) Var( b T

1 Y)

We interpret a1 as the s timulus ﬁlter whos e output is maximally correlated with the output of the
“res pons e ﬁlter” b 1 . Thus , we are s imultaneous ly s earching for features of the s timulus that the
neural s ys tem is s elective for, and the patterns of activity that it us es to s ignal the pres ence or abs ence

2

of this feature. We refer to the vector a1 as the (ﬁrs t) population receptive ﬁeld of the population,
and b 1 is the response feature corres ponding to a1 . If a hypothetical neuron receives input from the
population, and wants to decode the pres ence of the s timulus a1 , the weights of the optimal linear
readout [ 1 6] could be derived from b 1 .

Canonical Correlation Analys is (CCA) [ 9]
that
maximize (3 ): We denote the covariances of X and Y by Σ x , Σ y , the cros s -covariance by Σ x y , and
the whitened cros s -covariance by

is an algorithm that ﬁnds the vectors a1 and b 1

C = Σ ( − 1 / 2 )

x

Σ x y Σ ( − 1 / 2 )

y

.

(4)

Let C = UDV T denote the s ingular value decompos ition of C, where the entries of the diagonal
matrix D are non-negative and decreas ing along the diagonal. Then, the k -th pair of canonical
variables is given by ak = Σ
vk , where uk and vk are the k -th column
the k -th diagonal
vectors of U and V , res pectively. Furthermore, the k -th s ingular value of C, i. e.
entry of D is the correlation-coefﬁcient ρk of aT
j X
are uncorrelated for i �= j.

k Y. The random variables aT

uk and b k = Σ

i X and aT

k Xand b T

( − 1 / 2 )
x

( − 1 / 2 )
y

Importantly, the s olution for the optimization problem in CCA is unique and can be computed ef-
ﬁciently via a s ingle eigenvalue problem. The population receptive ﬁelds and the characteris tic
patterns are found by a joint optimization in s timulus and res pons e s pace. Therefore, one does
not need to know—or as s ume—a priori what features the population is s ens itive to, or what s pike
patterns convey the information.

The ﬁrs t K PRFs form a bas is for the s ubs pace of s timuli that the neural population is mos t s ens itive
to, and the individual bas is vectors ak are s orted according to their “informativenes s ” [ 1 3 , 1 7] .

The mutual information between two one-dimens ional Gaus s ian Variables with correlation ρ is given
by MIG a us s = − 1
2 log( 1 − ρ2 ) , s o maximizing correlation coefﬁcients is equivalent to maximizing
mutual information [ 3 ] . As s uming the neural res pons e Y to be Gaus s ian, the s ubs pace s panned by
the ﬁrs t K vectors BK = ( b 1 ,
, b K ) is als o the K-s ubs pace of s timuli that contains the maximal
amount of mutual information between s timuli and neural res pons e. That is

.

.

.

`

´

“

“

det

T

B

Σ y B

”

”

BK = argmax

B ∈ R n × k

det

B T

Σ y − Σ T

x y Σ

( − 1 )
x

Σ x y

B

.

(5 )

Thus ,
in terms of dimens ionality reduction, CCA optimizes the s ame objective as oriented PCA
[ 5 ] .
In contras t to oriented PCA, however, CCA does not require one to know explicitly how the
res pons e covariance Σ y = Σ s + Σ ξ s plits into s ignal Σ s and nois e Σ ξ covariance. Ins tead, it us es the
cros s -covariance Σ x y which is directly available from revers e correlation experiments . In addition,
CCA not only returns the mos t predictable res pons e features b 1 ,
. b K but als o the mos t predictive
s timulus components AK = ( a1 ,
For general Y and for s timuli X with elliptically contoured dis tribution, MIG a us s − J( AT X) pro-
vides a lower bound to the mutual information between AT X and B T Y, where

. aK ) .

.

.

.

.

T

J( A

X) =

1
2

log( det( 2 πeA

T

Σ x A) ) − h( A

T

X)

(6)

is the Negentropy of AT X, and h( AT X)
its differential entropy. S ince for elliptically contoured
dis tributions J( AT X) does not depend on A, the PRFs can be s een as the s olution of a variational
approach, maximizing a lower bound to the mutual information. Maximizing mutual information
directly is hard, requires extens ive amounts of data, and us ually multiple repetitions of the s ame
s timulus s equence.

3 The receptive ﬁeld of a population of neurons

3.1 The effect of tuning functions and noise correlations

To illus trate the relations hip between the tuning-functions of individual neurons and the PRFs [ 22] ,
we calculate the ﬁrs t PRF of a s imple one-dimens ional population model cons is ting of center-

3

s urround neurons . Each tuning function is modeled by a “Difference of Gaus s ians ” (DOG)

„

“

«

”

 

„

!

«

f ( x) = exp

−

1
2

x − c

σ

2

− A exp

−

2

1
2

x − c

η

(7)

whos e centers c are uniformly dis tributed over the real axis . The width η of the negative Gaus s ian is
s et to be twice as large as the width σ of the pos itive Gaus s ian. If the area of both Gaus s ians is the
s ame ( A = 1 ) , the DC component of the DOG-ﬁllter is zero, i. e.
the neuron is not s ens itive to the
mean luminance of the s timulus . If the ratio between both areas becomes s ubs tantially unbalanced,
the DC component will become the larges t s ignal ( A ≈ 0) .

In addition to the parameter A, we will s tudy the length s cale of nois e correlations λ [ 1 8 ] . S peciﬁ-
cally, we as s ume exponentially decaying nois e correlation with Σ ξ ( s ) = exp( − | s | / λ) .

As this model is invariant under s patial s hifts , the ﬁrs t PRF can be calculated by ﬁnding the s patial
frequency at which the S NR is maximal. That is , the ﬁrs t PRF can be us ed to es timate the pas s band
of the population trans fer function. The S NR is given by

„

„

««

S NR( ω) =

2

ω

2

1 + λ
2 λ

− ω 2 σ 2

e

+ A

2

e

− η 2 ω 2

− 2 Ae

σ 2 + η 2

2

ω 2

−

2

.

(8 )

The pas s band of the ﬁrs t population ﬁlter moves as a function of both parameters A and λ. It equals
large imbalance) and s mall λ (i. e. s hort correlation length). In
the DC component for s mall A (i. e.
this cas e, the mean intens ity is the s timulus property that is mos t faithfully s ignaled by the ens emble.

A

1

0.8

0.6

0.4

0.2

 

 

0.5

1
λ

1.5

2

1

0.8

0.6

0.4

0.2

0

Figure 1 : S patial frequency of the ﬁrs t PRF for the model des cribed above. λ is the length-s cale of
the nois e correlations , A is the weight of the negative Gaus s ian in the DOG-model. The region in
the bottom left corner (bounded by the white line) is the part of the parameter-s pace in which the
PRF equals the DC component.

3.2 The receptive ﬁeld of an ensemble of retinal ganglion cells

We mapped the population receptive ﬁelds of rabbit retinal ganglion cells recorded with a whole-
mount preparation. We are not primarily interes ted in prediction performance [ 1 2] , but rather in
dimens ionality reduction: We want to characterize the ﬁltering properties of the population.

The neurons were s timulated with a 1 6 × 1 6 checkerboard cons is ting of binary white nois e which
was updated every 20ms . The experimental procedures are des cribed in detail in [ 21 ] . After s pike-
s orting, s pike trains from 32 neurons were binned at 2 0ms res olution, and the res pons e of a neuron
to a s timulus at time t was deﬁned to cons is t of the the s pike-counts in the 1 0 bins between 40ms
and 240ms after t. Thus , each population res pons e Yt is a 3 20 dimens ional vector.

Figure 3 . 2 A) dis plays the ﬁrs t 6 PRFs , the corres ponding patterns of neural activity (B ) and their
correlation coefﬁcients ρk (which were calculated us ing a cros s -validation procedure). It can be s een
that the PRFs look very different to the us ual center-s urrond s tructure of retinal ganglion. However,
one s hould keep in mind that it is really the s pace s panned by the PRFs that is relevant, and thus be
careful when interpreting the actual ﬁlter s hapes [ 1 5 ] .

For comparis on, we als o plotted the s ingle-cell receptive ﬁelds in Figure 3 . 2 C), and their projections
into the s paced s panned by the ﬁrs t 6 PRFs . Thes e plots s ugges t that a s mall number of PRFs might

4

�

be s ufﬁcient to approximate each of the receptive ﬁelds . To determine the dimens ionality of the
relevant s ubs pace, we analyzed the correlation-coefﬁcients ρk . The Gaus s ian Mutual Information
MIG a us s = − 1
is an es timate of the information contained in the s ubs pace
2
s panned by the ﬁrs t K PRFs . B as ed on this meas ure, a 1 2 dimens ional s ubs pace accounts for 90%
of the total information.

K
k = 1 log( 1 − ρ2
k )

In order to link the empirically es timated PRFs with the theoretical analys is in s ection 3 . 1 , we
calculated the s pectral properties of the ﬁrs t PRF. Our analys is revealed that mos t of the power is in
the low frequencies , s ugges ting that the population is in the parameter-regime where the s ingle-cell
receptive ﬁelds have power in the DC-component and the nois e-correlations have s hort range, which
is certainly reas onable for retinal ganglion cells [ 4] .

0.51

0.44

0.38

0.35

0.29

0.27

A)

B )

x
e
d
n

i
 

n
o
r
u
e
N

5
10
15
20
25
30

5
10
15
20
25
30

5
10
15
20
25
30

5
10
15
20
25
30

5
10
15
20
25
30

 

0.2

0

−0.2

160

220

5
10
15
20
25
30
220

 
40

40

160

Time →

220

40

160

220

40

160

220

40

160

220

40

160

 
 
 
 
 

C)
F
R

 
 
 
 
 

F
R

 
.
j
o
r
P

 
 
 
 
 

F
R

 
 
 
 
 

F
R

 
.
j

o
r
P

Figure 2: The population receptive ﬁelds of a group of 32 retinal ganglion cells : A) the ﬁrs t 6 PRFs ,
as s orted by the correlation coefﬁcient ρk B ) the res pons e features bk coupled with the PRFs . Each
row of each image corres ponds to one neuron, and each column to one time-bin. B lue color denotes
enhanced activity, red s uppres s ed. It can be s een that only a s ubs et of neurons contributed to the ﬁrs t
6 PRFs . C) The s ingle-cell receptive ﬁelds of 2 4 neurons from our population, and their projections
into the s pace s panned by the 6 PRFs .

5

A)

k

ρ
 
s
t

i

n
e
c
i
f
f

e
o
c
 
s
n
o

i
t

l

a
e
r
r
o
C

0.6
0.5
0.4
0.3
0.2
0.1
0
−0.1

1

5

10 15 20

30
PRF index

40

50

B )

I

M

 
f

 

o
e
g
a

t

n
e
c
r
e
P

100
90
80

60

40

20

0

1

5

10

15

20

Dimensionality of subspace

30

40

50

Figure 3 : A) Correlation coefﬁcients ρk for the PRFs . Es timates and error-bars are calculated us ing
a cros s -validation procedure. B ) Gaus s ian-MI of the s ubs pace s panned by the ﬁrs t K PRFs .

4 Nonlinear extensions using Kernel Canonical Correlation Analysis

Thus far, our model is completely linear: We as s ume that the s timulus is linearly related to the
neural res pons es , and we als o as s ume a linear readout of the res pons e.
In this s ection, we will
explore generalizations of the CCA model us ing Kernel CCA: B y embedding the s timulus -s pace
nonlinearly in a feature s pace, nonlinear codes can be des cribed.

Kernel methods provide a framework for extending linear algorithms to the nonlinear cas e [ 8 ] . After
projecting the data into a feature s pace via a feature maps φ and ψ, a s olution is found us ing linear
In the cas e of Kernel CCA [ 1 , 1 0, 2, 7] one s eeks to ﬁnd a linear
methods in the feature s pace.
ˆX = φ( X) and ˆY = ψ( Y) , rather than between X and
relations hip between the random variables
Y. If an algorithm is purely deﬁned in terms of dot-products , and if the dot-product in feature s pace
k( s , t) = � ψ( s ) , ψ( t) � can be computed efﬁciently, then the algorithm does not require explicit
calculation of the feature maps φ and ψ. This “kernel-trick” makes it pos s ible to work in high-
It is worth mentioning that the s pace of patterns Y its elf
(or inﬁnite)-dimens ional feature s paces .
does not have to be a vector s pace. Given a data-s et x 1 .
. x n , it s ufﬁces to know the dot-products
between any pair of training points , Ki j : = � ψ( yi ) , ψ( yj ) � .

.

The kernel function k( s , t) can be s een as a s imiliarity meas ure.
It incorporates our as s umptions
about which s pike-patterns s hould be regarded as s imilar “mes s ages ”. Therefore, the choice of the
kernel-function is clos ely related to s peciﬁng what the s earch-s pace of potential neural codes is . A
number of dis tance- and kernel-functions [ 6, 1 9] have been propos ed to compute dis tances between
s pike-trains . They can be des igned to take into account precis ely timed pattern of s pikes , or to be
invariant to certain trans formations s uch as temporal jitter.

We illus trate the concept on s imulated data: We will us e a s imilarity meas ure bas ed on the metric
D interval
[ 1 9] to es timate the receptive ﬁeld of a neuron which does not us e its ﬁring rate, but rather
the occurrence of s peciﬁc inters pike intervals to convey information about the s timulus . The metric
D interval
between two s pike-trains is es s entially the cos t of matching their intervals by s hifting, adding
or deleting s pikes .
In theory, this function is not guaranteed
to be pos itive deﬁnite, which could lead to numerical problems , but we did not encounter any in
our s imulation. ) If we cons ider coding-s chemes that are bas ed on patterns of s pikes , the methods
des cribed here become us eful even for the analys is of s ingle neurons . We will here concentrate on a
s ingle neuron, but the analys is can be extended to patterns dis tributed acros s s everal neurons .

(We s et k( s , t) = exp( − D( s , t) .

Our hypothetical neuron encodes information in a pattern cons is ting of three s pikes : The relative
timing of the s econd s pike is informative about the s timulus : The bigger the correlation between
receptive ﬁeld and s timulus � r, s t � , the s horter is the interval. If the receptive ﬁeld is very dis s imilar
to the s timulus , the interval is long. While the timing of the s pikes relative to each other is precis e,
there is jitter in the timing of the pattern relative to the s timulus . Figure 4 A) is a ras ter plot of
s imulated s pike-trains from this model, ordered by � r, s t � . We als o included nois e s pikes at random
times .

6

A)

C)

D)

B )

i

s
n
a
r
t
 

i

e
k
p
S

0

50

100

Time →

150

200

Figure 4: Coding by s pike patterns : A) Receptive ﬁeld of neuron des cribed in S ection 4. B ) A
s ubs et of the s imulated s pike-trains , s orted with res pect to the s imilarity between the s hown s timulus
and the receptive ﬁeld of the model. The interval between the ﬁrs t two informative s pikes in each
trial is highlighted in red. C) Receptive ﬁeld recovered by Kernel CCA, the correlation coefﬁcient
between real and es timated receptive ﬁeld is 0. 93 . D) Receptive ﬁeld derived us ing linear decoding,
correlation coefﬁcient is 0. 02 .

Us ing thes e s pike-trains , we tried to recover the receptive ﬁeld r without telling the algorithm what
the indicating pattern was . Each s timulus was s hown only once, and therefore, that every s pike-
pattern occurred only once. We s imulated 5 000 s timulus pres entations for this model, and applied
Kernel CCA with a linear kernel on the s timuli, and the alignment-s core on the s pike-trains . B y
us ing incomplete Choles ky decompos itions [ 2] , one can compute Kernel CCA without having to
calculate the full kernel matrix. As many kernels on s pike trains are computationally expens ive, this
trick can res ult in s ubs tantial s peed-ups of the computation. The receptive ﬁeld was recovered (s ee
Figure 4), des pite the highly nonlinear encoding mechanis m of the neuron. For comparis on, we als o
s how what receptive ﬁeld would be obtained us ing linear decoding on the indicated bins .

Although this neuron model may s eem s lightly contrived,
in
principle, receptive ﬁelds can be es timated even if the ﬁring rate gives no information at all about
the s timulus , and the encoding is highly nonlinear. Our algorithm does not only look at patterns that
occur more often than expected by chance, but als o takes into account to what extent their occurrence
is correlated to the s ens ory input.

it is a good proof of concept that,

5 Conclusions

We s et out to ﬁnd a us eful des cription of the s timulus -res pons e relations hip of an ens emble of
neurons akin to the concept of receptive ﬁeld for s ingle neurons . The population receptive ﬁelds are
found by a joint optimization over s timuli and s pike-patterns , and are thus not bound to be triggered
by s ingle s pikes .

We es timated the PRFs of a group of retinal ganglion cells , and found that the ﬁrs t PRF had mos t
s pectral power in the low-frequency bands , cons is tent with our theoretical analys is . The s timulus
we us ed was a white-nois e s equence—it will be interes ting to s ee how the informative s ubs pace and
its s pectral properties change for different s timuli s uch as colored nois e. The ganglion cell layer of
the retina is a s ys tem that is relatively well unders tood at the level of s ingle neurons . Therefore,
our res ults can readily be compared and connected to thos e obtained us ing conventional analys is
techniques . However, our approach has the potential to be es pecially us eful in s ys tems in which the
functional s igniﬁcance of s ingle cell receptive ﬁelds is difﬁcult to interpret.

7

We us ually as s umed that each dimens ion of the res pons e vector Y repres ents an electrode-recording
from a s ingle neuron. However, the vector Y could als o repres ent any other multi-dimens ional mea-
s urement of brain activity: For example, imaging modalities s uch as voltage-s ens itive dye imaging
yield meas urements at multiple pixels s imultaneous ly. Data from electro-phys iological data, e. g. lo-
cal ﬁeld potentials , are often analyzed in frequency s pace, i. e. by looking at the energy of the s ignal
in different frequency bands . This als o res ults in a multi-dimens ional repres entation of the s ignal.
Us ing CCA, receptive ﬁelds can readily be es timated from thes e kinds of repres entations without
limiting attention to s ingle channels or extracting neural events .

Acknowledgments

We would like to thank A Gretton and J Eichhorn for us eful dis cus s ions , and F J¨akel, J B utler and S Liebe for
comments on the manus cript.

References

[ 1 ] S . Akaho. A kernel method for canonical correlation analys is . In International Meeting ofPsychometric

Society, Osaka, 2001 .

[ 2] F. R. B ach and M. I. Jordan. Kernel independent component analys is . Journal ofMachine Learning

Research, 3 : 1 : 48 , 2002.

[ 3 ] G. Chechik, A. Globers on, N. Tis hby, and Y. Weis s . Information B ottleneck for Gaus s ian Variables . The

Journal ofMachine Learning Research, 6: 1 65 –1 8 8 , 2005 .

[ 4] S . Devries and D. B aylor. Mos aic Arrangement of Ganglion Cell Receptive Fields in Rabbit Retina.

Journal ofNeurophysiology, 78 (4): 2048 –2060, 1 997.

[ 5 ] K. Diamantaras and S . Kung. Cros s -correlation neural network models . Signal Processing, IEEE Trans-

actions on, 42(1 1 ): 3 21 8 –3 223 , 1 994.

[ 6] J. Eichhorn, A. Tolias , A. Zien, M. Kus s , C. E. Ras mus s en, J. Wes ton, N. Logothetis , and B . S ch ¨olkopf.
Prediction on s pike data us ing kernel algorithms . In S . Thrun, L. S aul, and B . S ch ¨olkopf, editors , Advances
in Neural Information Processing Systems 1 6. MIT Pres s , Cambridge, MA, 2004.

[ 7] K. Fukumizu, F. R. B ach, and A. Gretton. S tatis tical cons is tency of kernel canonical correlation analys is .

Journal ofMachine Learning Research, 2007.

[ 8 ] T. Hofmann, B . S ch ¨olkopf, and A. S mola. Kernel methods in machine learning. Annals ofStatistics (in

press), 2007.

[ 9] H. Hotelling. Relations between two s ets of variates . Biometrika, 28 : 3 21 –3 77, 1 93 6.

[ 1 0] T. Melzer, M. Reiter, and H. B is chof. Nonlinear feature extraction us ing generalized canonical correlation
analys is . In Proc. ofInternational Conference on Artiﬁcial Neural Networks (ICANN), pages 3 5 3 –3 60, 8
2001 .

[ 1 1 ] L. Panins ki. Convergence properties of three s pike-triggered analys is techniques . Network, 1 4(3 ): 43 7–64,

Aug 2003 .

[ 1 2] J. W. Pillow, L. Panins ki, V. J. Uzzell, E. P. S imoncelli, and E. J. Chichilnis ky. Prediction and decoding
of retinal ganglion cell res pons es with a probabilis tic s piking model. J Neurosci, 25 (47): 1 1 003 –1 3 , 2005 .
[ 1 3 ] J. W. Pillow and E. P. S imoncelli. Dimens ionality reduction in neural models : an information-theoretic

generalization of s pike-triggered average and covariance analys is . J Vis, 6(4): 41 4–28 , 2006.

[ 1 4] M. J. S chnitzer and M. Meis ter. Multineuronal ﬁring patterns in the s ignal from eye to brain. Neuron,

3 7(3 ): 499–5 1 1 , 2003 .

[ 1 5 ] O. S chwartz, J. W. Pillow, N. C. Rus t, and E. P. S imoncelli. S pike-triggered neural characterization. J

Vis, 6(4): 48 4–5 07, 2006.

[ 1 6] H. S . S eung and H. S ompolins ky. S imple models for reading neuronal population codes . Proc Natl Acad

Sci U S A, 90(22): 1 0749–5 3 , 1 993 .

[ 1 7] T. S harpee, N. Rus t, and W. B ialek. Analyzing neural res pons es to natural s ignals : maximally informative

dimens ions . Neural Comput, 1 6(2): 223 –5 0, 2004.

[ 1 8 ] H. S ompolins ky, H. Yoon, K. Kang, and M. S hamir. Population coding in neuronal s ys tems with corre-

lated nois e. Phys Rev E Stat Nonlin Soft Matter Phys, 64(5 Pt 1 ): 05 1 904, 2001 .

[ 1 9] J. Victor. S pike train metrics . Curr Opin Neurobiol, 1 5 (5 ): 5 8 5 –92, 2005 .
[ 20] H. W¨as s le. Parallel proces s ing in the mammalian retina. Nat Rev Neurosci, 5 (1 0): 747–5 7, 2004.
[ 21 ] G. M. Zeck, Q. Xiao, and R. H. Mas land. The s patial ﬁltering properties of local edge detectors and

bris k-s us tained retinal ganglion cells . Eur J Neurosci, 22(8 ): 201 6–26, 2005 .

[ 22] K. Zhang and T. S ejnows ki. Neuronal Tuning: To S harpen or B roaden? , 1 999.

8

"
978,2007,Robust Regression with Twinned Gaussian Processes,"We propose a Gaussian process (GP) framework for robust inference in which a GP prior on the mixing weights of a two-component noise model augments the standard process over latent function values. This approach is a generalization of the mixture likelihood used in traditional robust GP regression, and a specialization of the GP mixture models suggested by Tresp (2000) and Rasmussen and Ghahramani (2002). The value of this restriction is in its tractable expectation propagation updates, which allow for faster inference and model selection, and better convergence than the standard mixture. An additional benefit over the latter method lies in our ability to incorporate knowledge of the noise domain to influence predictions, and to recover with the predictive distribution information about the outlier distribution via the gating process. The model has asymptotic complexity equal to that of conventional robust methods, but yields more confident predictions on benchmark problems than classical heavy-tailed models and exhibits improved stability for data with clustered corruptions, for which they fail altogether. We show further how our approach can be used without adjustment for more smoothly heteroscedastic data, and suggest how it could be extended to more general noise models. We also address similarities with the work of Goldberg et al. (1998), and the more recent contributions of Tresp, and Rasmussen and Ghahramani.","Robust Regression with Twinned Gaussian Processes

Andrew Naish-Guzman & Sean Holden

Cambridge, CB3 0FD. United Kingdom
{agpn2,sbh11}@cl.cam.ac.uk

Computer Laboratory

University of Cambridge

Abstract

We propose a Gaussian process (GP) framework for robust inference in which a
GP prior on the mixing weights of a two-component noise model augments the
standard process over latent function values. This approach is a generalization of
the mixture likelihood used in traditional robust GP regression, and a specializa-
tion of the GP mixture models suggested by Tresp [1] and Rasmussen and Ghahra-
mani [2]. The value of this restriction is in its tractable expectation propagation
updates, which allow for faster inference and model selection, and better conver-
gence than the standard mixture. An additional beneﬁt over the latter method lies
in our ability to incorporate knowledge of the noise domain to inﬂuence predic-
tions, and to recover with the predictive distribution information about the outlier
distribution via the gating process. The model has asymptotic complexity equal
to that of conventional robust methods, but yields more conﬁdent predictions on
benchmark problems than classical heavy-tailed models and exhibits improved
stability for data with clustered corruptions, for which they fail altogether. We
show further how our approach can be used without adjustment for more smoothly
heteroscedastic data, and suggest how it could be extended to more general noise
models. We also address similarities with the work of Goldberg et al. [3].

1 Introduction

Regression data are often modelled as noisy observations of an underlying process. The simplest
assumption is that all noise is independent and identically distributed (i.i.d.) zero-mean Gaussian,
such that a typical set of samples appears as a cloud around the latent function. The Bayesian frame-
work of Gaussian processes [4] is well-suited to these conditions, for which all computations remain
tractable (see ﬁgure 1a). Furthermore, the Gaussian noise model enjoys the theoretical justiﬁcation
of the central limit theorem, which states that the sum of sufﬁciently many i.i.d. random variables of
ﬁnite variance will be distributed normally. However, only rarely can perturbations affecting data in
the real world be argued to have originated in the addition of many i.i.d. sources. The random com-
ponent in the signal may be caused by human or measurement error, or it may be the manifestation
of systematic variation invisible to a simpliﬁed model. In any case, if ever there is the possibility of
encountering small quantities of highly implausible data, we require robustness, i.e. a model whose
predictions are not greatly affected by outliers.

Such demands render the standard GP inappropriate: the light tails of the Gaussian distribution
cannot explain large non-Gaussian deviations, which either skew the mean interpolant away from
the majority of the data, or force us to infer an unreasonably large (global) noise variance (see
ﬁgure 1b). Robust methods use a heavy-tailed likelihood to allow the interpolant effectively to
favour smoothness and ignore such erroneous data. Figure 1c shows how this can be achieved using
a two-component noise model

p(yn|fn) = (1 − ǫ)N(cid:0)yn ; fn , σ2

R(cid:1) + ǫN(cid:0)yn ; fn , σ2
O(cid:1) ,

1

(1)

(a)

(b)

(c)

(d)

Figure 1: Black dots show noisy samples from the sinc function. In panels (a) and (b), the be-
haviour of a GP with a Gaussian noise assumption is illustrated; the shaded region shows 95%
conﬁdence intervals. The presence of a single outlier is highly inﬂuential in this model, but the
heavy-tailed likelihood (1) in panel (c) is more resilient. Unfortunately, even this model fails for
the cluster of outliers in panel (d). Here, grey lines show ten repeated runs of the EP inference
algorithm, while the black line and shaded region are their averaged mean and conﬁdence intervals
respectively—grossly at odds with those of the latent generative model.

in which observations yn are Gaussian corruptions of fn, being drawn with probability ǫ from a
large variance outlier distribution (σ2
R). Inference in this model is tractable, but impractical
for all but the smallest problems due to the exponential explosion of terms in products of (1).

O ≫ σ2

In this paper, we address the more fundamental GP assumption of i.i.d. noise. Our research is mo-
tivated by observing how the predictive distribution suffers for heavy-tailed models when outliers
appear in bursts: ﬁgure 1d replicates ﬁgure 1c, but introduces an additional three outliers. All param-
eters were taken from the optimal solution to (c), but even without the challenge of hyperparameter
optimization there is now considerable uncertainty in the posterior since the competing interpreta-
tions of the cluster as signal or noise have similar posterior mass. Viewed another way, the tails of
the effective log likelihood of four clustered observations have approximately one-quarter the weight
of a single outlier, so the magnitude of the posterior peak associated with the robust solution is com-
parably reduced. One simple remedy is to make the tails of the likelihood heavier. However, since
the noise model is global, this has ramiﬁcations across the entire data space, potentially causing
underﬁtting elsewhere when real data are relegated to the tails. We can establish an optimal choice
for the parameters by gradient ascent on the marginal likelihood, but it is entirely possible that no
single setting will be universally satisfactory.

The model introduced in this paper, which we call the twinned Gaussian process (TGP), generalizes
the noise model (1) by using a GP gating function to choose between the “real” and “outlier dis-
tributions”: in regions of conﬁdence, the tails can be made very light, encouraging the interpolant
to hug the data points tightly; more dubious observations can be treated appropriately by broaden-
ing the noise distribution in their vicinity. Our model is also a specialization of the GP mixtures
proposed by Tresp [1] and Rasmussen and Ghahramani [2]; indeed, the latter automatically infers
the correct number of components to use. One may therefore wonder what can possibly be gained
by restricting ourselves to a comparatively simple architecture. The answer is in the computational
overhead required for the different approaches, since these more general models require inference
by Monte Carlo methods. We argue that the two-component mixture is often a sensible distribution
for modelling real data, with a natural interpretation and the heavy tails required for robustness;
its weaknesses are exposed primarily when the noise distribution is not homoscedastic. The TGP
largely solves this problem, and allows inference by an efﬁcient expectation propagation (EP) [5]
procedure (rather than resorting to more heavy duty Monte Carlo methods). Hence, provided a two-
component mixture is likely to reﬂect adequately the noise on our data, the TGP will give similar
results to the generalized mixtures mentioned above, but at a fraction of the cost.

Goldberg et al. [3] suggest an approach to input-dependent noise in the spirit of the TGP, in which
the log variance on observations is itself modelled as a GP (the logarithm since noise variance is
a non-negative property).
Inference is again analytically intractable, so Gibbs sampling is used
to generate noise vectors from the posterior distribution by alternately ﬁtting the signal process
and ﬁtting the noise process. A further stage of Gibbs sampling is required at each test point to
estimate the predictive variance, making testing rather slow. Model selection is even slower, and the
Metropolis-Hastings algorithm is suggested for updating hyperparameters.

2

2 Twinned Gaussian processes

Given a domain X and covariance function K(·, ·) ∈ X × X → R, a Gaussian process (GP) over
the space of real-valued functions of X speciﬁes the joint distribution at any ﬁnite set X ⊂ X :

p(f |X) = N (f ; 0 , Kf ) ,

where the f = {fn}N
n=1 are (latent) values associated with each xn ∈ X, and Kf is the Gram
matrix, the evaluation of the covariance function at all pairs (xi, xj). We apply Bayes’ rule to obtain
the posterior distribution over the f, given the observed X and y, which with the assumption of
i.i.d. Gaussian corrupted observations is also normally distributed. Predictions at X⋆ are made by
marginalizing over f in the (Gaussian) joint p(f , f⋆|X, y, X⋆). See [6] for a thorough introduction.
Robust GP regression is achieved by using a leptokurtic likelihood distribution, i.e. one whose tails
have more mass than the Gaussian. Common choices are the Laplace (or double exponential) distri-
bution, Student’s t distribution, and the mixture model (1). In product with the prior, a heavy-tailed
likelihood over an outlying observation does not exert the strong pull on the posterior witnessed
with a light-tailed noise model. Kuss [7] describes how inference can be performed for all these
likelihoods, and establishes that in many cases their performance is broadly comparable. Since it
bears closest resemblance to the twinned GP, we are particularly interested in the mixture; however,
in section 4, we include results for the Laplace model: it is the heaviest-tailed log concave distri-
bution, which guarantees a unimodal posterior and allows more reliable EP convergence. In any
case, all such methods make a global assumption about the noise distribution, and it is where this is
inappropriate that our model is most beneﬁcial.

The graphical model for the TGP is shown in ﬁgure 2b. We augment the standard process over f
with another GP over a set of variables u; this acts as a gating function, probabilistically dividing
the domain between the real and outlier components of the noise model

p(yn|fn) = σ(un)N(cid:0)yn ; fn , σ2

N (z ; 0 , 1) dz.

.

R(cid:1) + σ(−un)N(cid:0)yn ; fn , σ2
O(cid:1) ,

where σ(un)

=Z un

−∞

(2)

In the TGP likelihood, we therefore mix two forms of Gaussian corruption, one strongly peaked at
the observation, the other a broader distribution which provides the heavy tails, in proportion deter-
mined by u(x). This makes intuitive sense; crucially to us, it retains the advantage of tractability
with respect to EP updates. The two priors may have quite different covariance structure, reﬂect-
ing our different beliefs about correlations in the signal and in the noise domain. In addition, we
accommodate prior beliefs about the prevalence of outliers with a non-zero mean process on u,

p(u|X) = N (u ; mu , Ku)

p(f |X) = N (f ; 0 , Kf ) .

Our model can be understood as lying between two extremes: observe that we recover the heavy-
tailed (mixture of Gaussians) GP by forcing absolute correlation in u and adjusting the mean of
the u-process to mu = σ−1(1 − e); conversely, if we remove all correlations in u, we return to a
standard mixture model where independently we must decide to which component an input belongs.

3 Inference

We begin with a very brief account of EP; for more details, see [5, 8]. Suppose we have an intractable
distribution over f whose unnormalized form factorizes into a product of terms, such as a dense
Gaussian prior t0(f , u) and a series of independent likelihoods {tn(yn|fn, un)}N
n=1. EP constructs
the approximate posterior as a product of scaled site functions ˜tn. For computational tractability,
these sites are usually chosen from an exponential family with natural parameters θ, since in this
case their product retains the same functional form as its components. The Gaussian (µ, Σ) has a
natural parameterization (b, Π) = (Σ−1µ, − 1
2 Σ−1). If the prior is of this form, its site function is
exact:

p(f , u|y) =

1
Z

t0(f , u)

N

Yn=1

tn(yn|fn, un) ≈ q(f ; θ) = t0(f , u)

zn˜tn(fn, un; θn),

(3)

N

Yn=1

3

x1

x2

x3

xN

x1

x2

x3

xN

f1

f2

f3

y1

y2

y3

(a)

fN

yN

u1

u2

u3

uN

f1

f2

f3

fN

y1

y2

yN

y3

(b)

Figure 2: In panel (a) we show a graphical model for the Gaussian process. The data ordinates are x,
observations y, and the GP is over the latent f. The bold black lines indicate a fully-connected set.
Panel (b) shows a graphical model for the twinned Gaussian process (TGP), in which an auxiliary
set of hidden variables u describes the noisiness of the data.

where Z is the marginal likelihood and zn are the scale parameters. Ideally, we would choose θ
at the global minimum of some divergence measure d(pkq), but the necessary optimization is usu-

ally intractable. EP is an iterative procedure that ﬁnds a minimizer of KL(cid:0)p(f , u|y)kq(f , u; θ)(cid:1)

on a pointwise basis: at each iteration, we select a new site n, and from the product of the cav-
ity distribution formed by the current marginal with the omission of that site, and the true likeli-
hood term tn, we obtain the so-called tilted distribution qn(fn, un; θ\n). A simpler optimization
minθn
ment matching between the two distributions, with scale zn chosen to match the zeroth-order mo-
ments. After each site update, the moments at the remaining sites are liable to change, and several
iterations may be required before convergence.

KL(cid:0)qn(fn, un; θ\n)kq(fn, un; θ)(cid:1) then ﬁts only the parameters θn: this is equivalent to mo-

The priors over u and f are independent, but we expect correlations in the posterior after condi-
tioning on observations. To understand this, consider a single observation (xn, yn); in principle, it
admits two explanations corresponding to its classiﬁcation as either “outlier” or as “real” data: in
general terms, either un > 0 and fn ≈ yn, or un < 0 and fn respects the global structure of the
signal. A diagram to assist the visualization of the behaviour of the posterior is provided in ﬁgure 3.

Now, recall that the prior over u and f is

X! = N(cid:18)(cid:20) u

f (cid:21) ; (cid:20) mu

0 (cid:21) , (cid:20)Ku

0 Kf(cid:21)(cid:19)

0

f (cid:21)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
p (cid:20) u

and the likelihood factorizes into a product of terms (2); our site approximations ˜tn are therefore
Gaussian in (fn, un). Of importance for EP are the moments of the tilted distribution which we
seek to match. These are most easily obtained by differentiation of the zeroth moments ZR and ZO
of each component. We ﬁnd

σ(u)N(cid:0)y ; f , σ2

R(cid:1) N(cid:18)(cid:20) u
ZR =ZZf,u
writing the inner Gaussian as N(cid:18)(cid:20) zn

N(cid:18)(cid:20) z

y (cid:21) ; µ , (cid:20)1

0
0 σ2

R(cid:21) + Σ(cid:19) dz;

C BR(cid:21)(cid:19), Z R = N (y ; µf , BR) σ(q),

0

f (cid:21) ; µ , Σ(cid:19) dudf =Z ∞
µf (cid:21) , (cid:20)A C
yn (cid:21) ; (cid:20) µu
qA − C 2

µu + C
BR

q =

BR

(y − µf )

.

where

The integral for the outlier component is similar; ZO = N (y ; µf , BO) σ(−q). With partial deriva-
tives ∂ log Z
T we are equipped for EP; algorithmic details appear in Seeger’s note [8]. For
efﬁciency, we make rank-two updates of the full approximate covariance on (f , u) during the EP
loop, and refresh the posterior at the end of each cycle to avoid loss of precision.

and ∂ 2 log Z

∂µµ

∂µ

4

prior
likelihood
posterior
EP

-5

0

5

10

f

-5

0

5

10

f

prior
likelihood
posterior
EP

-5

0

5

10

f

-5

0

5

10

f

prior
likelihood
posterior
EP

-5

0

5

10

f

-5

0

5

10

f

replacements

p
g
o
l

p
g
o
l

p
g
o
l

p
g
o
l

prior
likelihood
posterior
EP

10

5

0

-5

-10

10

5

0

-5

-10

10

5

0

-5

-10

10

5

0

-5

-10

u

u

u

u

-5

0

5

10

f

-5

0

5

10

f

-5

0

5

10

f

10

5

0

-5

-10

10

5

0

-5

-10

10

5

0

-5

-10

10

5

0

-5

-10

u

u

u

u

-5

0

5

10

f

-5

0

5

10

-5

0

5

10

f

f

Figure 3: Using the twinned Gaussian process provides a natural resilience against clustered noisy
data. The left-hand column illustrates the behaviour of a ﬁxed heavy-tailed likelihood for one,
two, four and ﬁve repeated observations at f = 5. (Outliers in real data are not necessarily so
tightly packed, but the symmetry of this approximation allows us to treat them as a single unit: by
“posterior”, for example, we mean the a posteriori belief in all the observations’ (identical) latent
f .) The context is provided by the prior, which gives 95% conﬁdence to data around f = 0 ± 2. The
top-left box illustrates how the inﬂuence of isolated outliers is mitigated by the standard mixture.
However, a repeated observation (box two on the left) causes the EP solution to collapse onto the
spike at the data (the log scale is deceptive:
the second peak contributes only about 8% of the
posterior mass). The twinned GP better preserves the marginal distribution of f by maintaining a
joint distribution over both f and u: in the second and third columns respectively are contours of
the true log joint (we use a broad zero-mean prior on u) and that inferred by EP, together with the
marginal posterior over f . Only with a ﬁfth observation—ﬁnal box—is the context of f essentially
overruled by the TGP approximation. The thick bar in the central column marks the cross-section
corresponding to the unnormalized posterior from column one.

5

3.1 Predictions

If the outlier component describes nuisance noise that should be eliminated, we require at test in-
puts x⋆ only the marginal distribution p(f⋆|x⋆, X, y), obtained by marginalizing over u in the full
(approximate) posterior

N(cid:18)(cid:20) u

f (cid:21) ; (cid:20) ˆµu
p(f⋆|x⋆, X, y) =Z p(f⋆|x⋆, f )p(f |X, y)df

ˆµf (cid:21) , (cid:20) ˆΣuu

ˆΣf u

ˆΣuf

ˆΣff(cid:21)(cid:19) :

≈ N(cid:16)f⋆ ; kT

f ⋆K−1

f ˆµf , kf

⋆⋆ − kT

f ⋆K−1

f kf ⋆ + kT

f ⋆K−1

ˆΣff K−1

f

f kf ⋆(cid:17) .

The noise process may itself be of interest, in which case we need to marginalize over both u⋆ and
f⋆ in

f (cid:21)!N(cid:18)(cid:20) u
(cid:20) u

f (cid:21) ; ˆµ , ˆΣ(cid:19) du⋆df⋆dudf .

p(y⋆|x⋆, X, y) =ZZ p y⋆(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
x⋆,(cid:20) u
≈ZZZZ p y⋆(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
x⋆,(cid:20) u⋆

f (cid:21)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
X, y! dudf
f (cid:21)!p (cid:20) u
f⋆ (cid:21)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
f⋆ (cid:21)!p (cid:20) u⋆

This distribution is no longer Gaussian, but its moments may be recovered easily by the same method
used to obtain moments of the tilted distribution.

EP provides in addition to the approximate moments of the posterior distribution an estimate of the
marginal likelihood and its derivatives with respect to kernel hyperparameters. Again, we refer the
interested reader to the algorithm presented in [8], adding here only that our implementation uses
log noise values on (σ2

O) to allow for their unconstrained optimization.

R, σ2

3.2 Complexity

The EP loop is dominated by the rank-two updates of the covariance. Each such update is

O(cid:0)(2N )2(cid:1), making every N iterations O(4N 3). The posterior refresh is O(8N 3) since it re-

quires the inverse of a 2N × 2N positive semi-deﬁnite matrix, most efﬁciently achieved through
Cholesky factorization (this Cholesky factor can be retained for use in calculating the approximate
log marginal likelihood). The total number of loops required for convergence of EP is typically in-
dependent of N , and can be upper bounded by a small constant, say 10, making the entire inference
process O(8N 3) = O(N 3). Thus, our algorithm has the same limiting time complexity as i.i.d. ro-
bust regression by EP, which admittedly masks the larger coefﬁcient that appears in approximating
both u and f simultaneously. Additionally, the body of the EP loop is slightly slower, since the pre-
cision matrix in a standard GP can be obtained with a single division, whereas our model requires
the inversion of a 2 × 2 matrix.

4 Experiments

We identify two general noise characteristics for which our model may be suitable. The ﬁrst is
when the outlying observations can appear in clusters: we saw in ﬁgure 1d how these occurrences
affect the standard mixture model. In fact the problem is quite severe, since the multimodality of the
posterior impedes the convergence of EP, while the possibility of conﬂicting gradient information at
the optima hampers procedures for evidence maximization. In ﬁgure 4 we illustrate how the TGP
succeeds where the mixture and Laplace models fail; note how the mean process on u falls sharply
in the contaminated regions. This is a stable solution, and hyperparameters can be ﬁt reliably.

A data set which exhibits the superior predictive modelling of the TGP in a domain where robust
methods can also expect to perform well is provided by Kuss [7] in a variation on a set of Friedman
[9]. The samples are drawn from a function of ten-dimensional vectors x which depend only on the
ﬁrst ﬁve components:

f (x) = 10 sin(πx1x2) + 20(x3 − 0.5)2 + 10x4 + 5x5.

6

-10

0

10

-10

0

10

-10

(a) Mixture noise

(b) Laplace noise

0

(c) TGP

10

Figure 4: The corruptions are i.i.d. around x = −10, and highly correlated near x = 0.

We generated ten sets of 90 training examples and 10000 test examples by sampling x uniformly
in [0, 1]10, and adding to the training data noise N (0, 1). In our ﬁrst experiment, we replicated the
procedure of [7]: ten training points were added at random with outputs sampled from N (15, 9) (a
value likely to lie in the same range as f ). The results appear as Friedman (1) in ﬁgure 5. Observe
that the r.m.s. error for the robust methods is similar, but the TGP is able to ﬁt the variance far more
accurately. In a second experiment, the training set was augmented with two Gaussian clusters each
of ﬁve noisy observations. The cluster centres were drawn uniformly in [0, 1]10, with variance ﬁxed
at 10−3. Output values were then drawn from N (0, 1) for all ten points, to give highly correlated
values distant from the underlying function (Friedman (2)). Now the TGP excels where the other
methods offer no improvement on the standard GP; it also yields very conﬁdent predictions (cf.
Friedman (1)), because once the outliers have been accounted for there are fewer corrupted regions;
furthermore, estimates of where the data are corrupted can be recovered by considering the process
on u. In both experiments, the training data were renormalized to zero mean and unit variance, and
throughout, we used the anisotropic squared exponential for the f process (implementing so-called
relevance determination), and an isotropic version for u. The approximate marginal likelihood was
maximized on three to ﬁve randomly initialized models; we chose for testing the most favoured.

The second domain of application is when the noise on the data is believed a priori to be a function of
the input (i.e. heteroscedastic). The twinned GP can simulate this changing variance by modulating
the u process, allocating varying weight to the two components. By way of example, the behaviour
for the one-dimensional motorcycle set [10] is shown in ﬁg. 5c. However, since the input-dependent
noise is not modelled directly, there are two notable dangers associated with this approach: ﬁrst,
the predictive variance saturates when all weight has been apportioned to one or other component;
second, the “outlier” component can dominate the variance estimates of the mixture. This is partic-
ularly problematic when variance on the data ranges over several orders of magnitude, such that the
“outlier” width must be comparably broader than that of the “real” component. In such cases, only
with extreme values of u can the smallest errors be predicted, but in consequence the process tends
to sweep precipitately through the region of sensitivity where variance predictions can be made ac-
curately. To circumvent these problems we might employ the warped GP [11] to rescale the process
on u in a supervised manner, but we do not explore these ideas further here.

0.5

0

0.4

0.2

GP Lap MixTGP

GP Lap MixTGP
neg. log probability

test error
(a) Friedman (1)

0.6

0.4

0.2

0

-1

GP Lap MixTGP

GP Lap MixTGP
neg. log probability

test error
(b) Friedman (2)

(c) Motorcycle

Figure 5: Results for the Friedman data, and the predictions of the TGP on the motorcycle set.

7

5 Extensions

With prior knowledge of the nature of corruptions affecting the signal, we can seek to model the
noise distribution more accurately, for example by introducing a compound likelihood for the outlier

component pO(yn|fn) = Pj αjN(cid:0)yn ; µj(fn) , σ2

weight of outlier corruptions to be constant across the entire domain. A richer alternative is provided
by extending the single u-process on noise to a series u(1), u(2), . . . , u(ν) of noise processes, and
broadening the likelihood function appropriately. For example, with ν = 2, we may write

j(cid:1) ,Pj αj = 1. This constrains the relative

p(yn|fn, u(1)

n , u(2)

n ) = σ(u(1)

n )N(cid:0)yn ; fn , σ2

n )σ(u(2)

σ(−u(1)

R(cid:1) +
n )N(cid:0)yn ; fn , σ2

σ(−u(1)

O1(cid:1) +

n )σ(−u(2)

In the former case, the preceding analysis applies with small changes: each component of the outlier
distribution contributes moments independently. The second model introduces signiﬁcant compu-
tational difﬁculty: ﬁrstly, we must maintain a posterior distribution over f and all ν us, yielding
space requirements O(N (ν + 1)) and time complexity O(N 3(ν + 1)3). More importantly, the req-
uisite moments needed in the EP loop are now intractable, although an inner EP loop can be used
to approximate them, since the product of σs behaves in essence like the standard model for GP
classiﬁcation. We omit details, and defer experiments with such a model to future work.

n )N(cid:0)yn ; f0 , σ2
O2(cid:1) .

(4)

6 Conclusions

We have presented a method for robust GP regression that improves upon classical approaches by
allowing the noise variance to vary in the input space. We found improved convergence on problems
which upset the standard mixture model, and have shown how predictive certainty can be improved
by adopting the TGP even for problems which do not. The model also allows an arbitrary process
on u, such that specialized prior knowledge could be used to drive the inference over f to respecting
regions which may otherwise be considered erroneous. A generalization of our ideas appears as
the mixture of GPs [1], and the inﬁnite mixture [2], but both involve a slow inference procedure.
When faster solutions are required for robust inference, and a two-component mixture is an adequate
model for the task, we believe the TGP is a very attractive option.

References

[1] Volker Tresp. Mixtures of Gaussian processes. In Advances in Neural Information Processing Systems,

pages 654–660, 2000.

[2] Carl Edward Rasmussen and Zoubin Ghahramani.

Inﬁnite mixtures of gaussian process experts.

In

Advances in Neural Information Processing Systems, 2002.

[3] Paul Goldberg, Christopher Williams, and Christopher Bishop. Regression with input-dependent noise:
a Gaussian process treatment. In Advances in Neural Information Processing Systems. MIT Press, 1998.
[4] Edward Snelson and Zoubin Ghahramani. Sparse Gaussian processes using pseudo-inputs. In Advances

in Neural Information Processing Systems 18. MIT Press, 2005.

[5] Thomas Minka. A family of algorithms for approximate Bayesian inference. PhD thesis, Massachusetts

Institute of Technology, 2001.

[6] Carl Rasmussen and Christopher Williams. Gaussian processes for machine learning. MIT Press, 2006.
[7] Malte Kuss. Gaussian process models for robust regression, classiﬁcation and reinforcement learning.

PhD thesis, Technische Universit¨at Darmstadt, 2006.

[8] Matthias Seeger.

Expectation propagation for exponential

families, 2005.

http://www.cs.berkeley.edu/˜mseeger/papers/epexpfam.ps.gz.

Available from

[9] J. H. Friedman. Multivariate adaptive regression splines. Annals of Statistics, 19(1):1–67, 1991.
[10] B.W. Silverman. Some aspects of the spline smoothing approach to non-parametric regression curve

ﬁtting. Journal of the Royal Statistical Society B, 47:1–52, 1985.

[11] Edward Snelson, Carl Edward Rasmussen, and Zoubin Ghahramani. Warped Gaussian processes.

Advances in Neural Information Processing Systems 16, 2003.

In

8

"
535,2007,On Ranking in Survival Analysis: Bounds on the Concordance Index,"In this paper, we show that classical survival analysis involving censored data can naturally be cast as a ranking problem. The concordance index (CI), which quantifies the quality of rankings, is the standard performance measure for model \emph{assessment} in survival analysis. In contrast, the standard approach to \emph{learning} the popular proportional hazard (PH) model is based on Cox's partial likelihood. In this paper we devise two bounds on CI--one of which emerges directly from the properties of PH models--and optimize them \emph{directly}. Our experimental results suggest that both methods perform about equally well, with our new approach giving slightly better results than the Cox's method. We also explain why a method designed to maximize the Cox's partial likelihood also ends up (approximately) maximizing the CI.","On Ranking in Survival Analysis: Bounds on the

Concordance Index

Vikas C. Raykar, Harald Steck, Balaji Krishnapuram

CAD and Knowledge Solutions (IKM CKS), Siemens Medical Solutions Inc., Malvern, USA

{vikas.raykar,harald.steck,balaji.krishnapuram}@siemens.com

Maastro Clinic, University Hospital Maastricht, University Maastricht, GROW, The Netherlands

{cary.dehing,philippe.lambin}@maastro.nl

Cary Dehing-Oberije, Philippe Lambin

Abstract

In this paper, we show that classical survival analysis involving censored data
can naturally be cast as a ranking problem. The concordance index (CI), which
quantiﬁes the quality of rankings, is the standard performance measure for model
assessment in survival analysis. In contrast, the standard approach to learning the
popular proportional hazard (PH) model is based on Cox’s partial likelihood. We
devise two bounds on CI–one of which emerges directly from the properties of
PH models–and optimize them directly. Our experimental results suggest that all
three methods perform about equally well, with our new approach giving slightly
better results. We also explain why a method designed to maximize the Cox’s
partial likelihood also ends up (approximately) maximizing the CI.

1 Introduction

Survival analysis is a well-established ﬁeld in medical statistics concerned with analyzing/predicting
the time until the occurrence of an event of interest–e.g., death, onset of a disease, or failure of a
machine. It is applied not only in clinical research, but also in epidemiology, reliability engineering,
marketing, insurance, etc. The time between a well-deﬁned starting point and the occurrence of the
event is called the survival time or failure time, measured in clock time or in another appropriate
scale, e.g., mileage of a car. Survival time data are not amenable to standard statistical methods
because of its two special features–(1) the continuous survival time often follows a skewed distribu-
tion, far from normal, and (2) a large portion of the data is censored (see Sec. 2). In this paper we
take a machine learning perspective and cast survival analysis as a ranking problem–where the task
is to rank the data points based on their survival times rather than to predict the actual survival times.
One of the most popular performance measures for assessing learned models in survival analysis is
the Concordance Index (CI), which is similar to the Wilcoxon-Mann-Whitney statistic [13, 10] used
in bi-partite ranking problems.
Given the CI as a performance measure, we develop approaches that learn models by directly opti-
mizing the CI. As optimization of the CI is computationally expensive, we focus on maximizing two
lower bounds on the CI, namely the log-sigmoid and the exponential bounds, which are described in
Sec. 4, 5, and 6. Interestingly, the log-sigmoid bound arises in a natural way from the Proportional
Hazard (PH) model, which is the standard model used in classical survival analysis, see Sec. 5.2.
Moreover, as the PH models are learned by optimizing Cox’s partial likelihood in classical survival
analysis, we show in Sec. 8 that maximizing this likelihood also ends up (approximately) maxi-
mizing the CI. Our experiments in Sec. 9 show that optimizing our two lower bounds and Cox’s
likelihood yields very similar results with respect to the CI, with the proposed lower bounds being
slightly better.

1

2 Survival analysis

Survival analysis has been extensively studied in the statistics community for decades, e.g., [4, 8].
A primary focus is to build statistical models for survival time T ∗

i of individual i of a population.

i , i.e., our actual observation is Ti = min(T ∗

2.1 Censored data
A major problem is the fact that the period of observation C∗
i can be censored for many individuals
i. For instance, a patient may move to a different town and thus be no longer available for a clinical
trial. Also at the end of the trial a lot of patients may actually survive. For such cases the exact
survival time may be longer than the observation period. Such data are referred to as right-censored,
and C∗
i is also called the censoring time. For such individuals, we only know that they survived for
at least C∗
Let xi ∈ Rd be the associated d-dimensional vector of covariates (explanatory variables) for the
ith individual. In clinical studies, the covariates typically include demographic variables, such as
age, gender, or race; diagnosis information like lab tests; or treatment information, e.g., dosage. An
important assumption generally made is that T ∗
i are independent conditional on xi, i.e., the
cause for censoring is independent of the survival time. With the indicator function δi, which equals
1 if failure is observed (T ∗
i ), the available training data
can be summarized as D = {Ti, xi, δi}N
i=1 for N patients. The objective is to learn a predictive
model for the survival time as a function of the covariates.

i and C∗

i , C∗
i ).

i ≤ C∗

i ) and 0 if data is censored (T ∗

i > C∗

2.2 Failure time distributions

The failures times are typically modeled to follow a distribution, which absorbs both truly random
effects and causes unexplained by the (available) covariates. This distribution is characterized by the
survival function S(t) = Pr[T > t] for t > 0, which is the probability that the individual is still alive
at time t. A related function commonly used is the hazard function. If T has density function p, then
the hazard function is deﬁned by λ(t) = lim∆t→0 Pr[t < T ≤ t + ∆t|T > t]/∆t = p(t)/S(t). The
hazard function measures the instantaneous rate of failure, and provides more insight into the failure
0 λ(u)du is called the cumulative hazard function, and it holds

mechanisms. The function Λ(t) =(cid:82) t

that S(t) = e−Λ(t) [4].

2.3 Proportional hazard model

Proportional hazard (PH) models have become the standard for studying the effect of the covariates
on the survival time distributions, e.g., [8]. Speciﬁcally, the PH model assumes a multiplicative
effect of the covariates on the hazard function, i.e.,

λ(t|x) = λ0(t)ew(cid:62)x,

(1)
where λ(t|x) is the hazard function of a person with covariates x; λ0(t) is the so-called baseline
hazard function (i.e., when x = 0), which is typically based on the exponential or the Weibull
distributions; w is a set of unknown regression parameters, and ew(cid:62)x is the relative hazard function.
Equivalent formulations for the cumulative hazard function and the survival function include

−(cid:104)
(cid:105)
ew(cid:62) x(cid:82) λ0(t)dt

.

(2)

Λ(t|x) = Λ0(t)ew(cid:62)x,

and S(t|x) = e−Λ0(t)ew(cid:62) x = e

2.4 Cox’s partial likelihood

Cox noticed that a semi-parametric approach is sufﬁcient for estimating the weights w in PH models
[2, 3], i.e., the baseline hazard function can remain completely unspeciﬁed. Only a parametric
assumption concerning the effect of the covariates on the hazard function is required. Parameter
estimates in the PH model are obtained by maximizing Cox’s partial likelihood (of the weights)
[2, 3]:

(cid:80)

ew(cid:62)xi
Tj≥Ti

ew(cid:62)xj

.

(3)

L(w) = (cid:89)

Ti uncensored

2

(a)

(b)

(c)

Figure 1: Order graphs representing the ranking constraints. (a) No censored data and (b) with censored data.
The empty circle represents a censored point. The points are arranged in the increasing value of their survival
times with the lowest being at the bottom. (c) Two concave lower bounds on the 0-1 indicator function.

Each term in the product is the probability that the ith individual failed at time Ti given that exactly
one failure has occurred at time Ti and all individuals for which Tj ≥ Ti are at risk of failing. Cox
and others have shown that this partial log-likelihood can be treated as an ordinary log-likelihood to
derive valid (partial) maximum likelihood estimates of w [2, 3].
The interesting properties of the Cox’s partial likelihood include: (1) due to its parametric form, it
can be optimized in a computationally efﬁcient way; (2) it depends only on the ranks of the observed
survival times, cf. the inequality Tj ≥ Ti in Eq. 3, rather than on their actual numerical values. We
outline this connection to the ranking of the times Ti–and hence the concordance index–in Sec. 8.

3 Ordering of Survival times

Casting survival analysis as ranking problem is an elegant way of dealing not only with the typically
skewed distributions of survival times, but also with the censoring of the data: Two subjects’ survival
times can be ordered not only if (1) both of them are uncensored but also if (2) the uncensored time
of one is smaller than the censored survival time of the other. This can be visualized by means of an
order graph G = (V,E), cf. also Fig. 1. The set of vertices V represents all the individuals, where
each ﬁlled vertex indicates an observed/uncensored survival time, while an empty circle denotes a
censored observation. Existence of an edge Eij implies that Ti < Tj. An edge cannot originate
from a censored point.

3.1 Concordance index

For these reasons, the concordance index (CI) or c-index is one of the most commonly used per-
formance measures of survival models, e.g., [6]. It can be interpreted as the fraction of all pairs of
subjects whose predicted survival times are correctly ordered among all subjects that can actually be
ordered. In other words, it is the probability of concordance between the predicted and the observed
survival. It can be written as

c(D,G, f) =

1
|E|

1f (xi)<f (xj )

(4)

with the indicator function 1a<b = 1 if a < b, and 0 otherwise; |E| denotes the number of edges in
the order graph. f(xi) is the predicted survival time for subject i by the model f. Equivalently, the
concordance index can also be written explicitly as

(cid:88)

Eij

(cid:88)

(cid:88)

c =

1
|E|

Ti uncensored

Tj >Ti

1f (xi)<f (xj ).

(5)

This index is a generalization of the Wilcoxon-Mann-Whitney statistics [13, 10] and thus of the
area under the ROC curve (AUC) to regression problems in that it can (1) be applied to continuous

3

−10−8−6−4−20246810−10−8−6−4−202z  Indicator functionLog−sigmoid lower boundExponential lower boundoutput variables and (2) account for censoring of the data. Like for the AUC, c = 1 indicates perfect
prediction accuracy and c = 0.5 is as good as a random predictor.

3.2 Maximizing the CI—The Ranking Problem

Since we evaluate the predictive accuracy of a survival model in terms of the concordance index,
it is natural to formulate the learning problem to directly maximize the concordance index. Note
that, while the concordance index has been used widely to evaluate a learnt model, it is not generally
used as an objective function during training. As the concordance index is invariant to any monotone
transformation of the survival times, the model learnt by maximizing the c-index is actually a rank-
ing/scoring function. Our goal is to predict whether the survival time of one individual is larger than
the one of another individual. Very often the doctor would like to know whether a particular kind
of treatment results in an increase in the survival time and the exact absolute value of the survival
time is not important. In terms of ranking problems studied in machine learning this is an N-partite
ranking problem, where every data point is a class in itself. Formulating it as a ranking problem al-
lows us to naturally incorporate the censored data. Once we have formulated it as a ranking problem
we can use various ranking algorithms proposed in the machine learning literature [5, 7, 1, 12]. In
this paper we use the algorithm proposed by [12].
More formally, we would like to learn a ranking function f from a suitable function class F, such
that f(xi) > f(xj) implies that the survival time of patient i is larger than the one of patient j. Given

the data D and the order graph G, the optimal ranking function is (cid:98)f = arg maxf∈F c(D,G, f). As

to prevent overﬁtting on the training data, regularization can be added to this equation, see Secs. 5
and 6. In many cases, sufﬁcient regularization is also achieved by restricting the function class F,
e.g., it may contain only linear functions. For ease of exposition we will consider the family of linear
ranking functions 1 in this paper: F = {fw}, where for any x, w ∈ Rd, fw(x) = w(cid:62)x.

4 Lower bounds on the CI

Maximizing the CI is a discrete optimization problem, which is computationally expensive. For
this reason, we resort to maximizing a differentiable and concave lower bound on the 0-1 indicator
function in the concordance index, cf. Eqs. 4 and 5. In this paper we focus on the log-sigmoid lower
bound [12], cf. Sec. 5, and exponential lower bound, cf. Sec. 6, which are suitably scaled as to be
tight at the origin and also in the asymptotic limit of large positive values, see also Fig. 1(c). We will
also show how these bounds relate to the classical approaches in survival analysis: as it turns out,
for the family of linear ranking functions, these two approaches are closely related to the PH model
commonly used in survival analysis, cf. Sec. 5.2.

5 Log-sigmoid lower bound

The ﬁrst subsection discusses the lower bound on the concordance index based on the log-sigmoid
function. The second subsection shows that this bound arises naturally when using proportional
hazard models.

5.1 Lower bound
The sigmoid function is deﬁned as σ(z) = 1/(1+e−z), While it is an approximation to the indicator
function, it is not a lower bound. In contrast, the scaled version of the log of the sigmoid function,
log [2σ(z)]/ log 2, is a lower bound on the indicator function (Fig. 1(c)), i.e.,

1z>0 ≥ 1 + (log σ(z)/log 2).

(6)

The log-sigmoid function is concave and asymptotically linear for large negative values, and may
hence be considered a differentiable approximation to the hinge loss, which is commonly used for

f (x) =(cid:80)N

1Generalization to non-linear functions can be achieved easily by using kernels: the linear ranking function
class F is replaced by H, a reproducing kernel Hilbert space (RKHS). The ranking function then is of the form

i=1 αik(x, xi) where k is the kernel of the RHKS H.

4

training support vector machines. The lower bound on the concordance index (cf. Eq. 4) follows
immediately:

1 + (log σ[f(xj) − f(xi)]/log 2) ≡(cid:98)cLS,

which can efﬁciently be maximized by gradient-based methods (cf. Sec 7). Given the linear ranking

c =

(cid:88)

(cid:88)
function fw(x) = w(cid:62)x, the bound(cid:98)cLS becomes

1f (xj )−f (xi)>0 ≥ 1
|E|

1
|E|

Eij

Eij

(cid:98)cLS(w) =

1
|E|

(cid:88)

Eij

(7)

(9)

1 + (log σ[w(cid:62)(xj − xi)]/log 2).

(8)

As to avoid overﬁtting, we penalize functions with a large norm w in the standard way, and obtain
the regularized version

(cid:98)cLSreg(w) = − λ

(cid:107)w(cid:107)2 +(cid:98)cLS(w).

2

5.2 Connection to the PH model

The concordance index can be interpreted as the probability of correct ranking (as deﬁned by the
given order graph) given a function f. Its probabilistic version can thus be cast as a likelihood.
Under the assumption that each pair (j, i) is independent of any other pair, the log-likelihood reads

Pr [fw(xi) < fw(xj)|w] .

(10)

L(fw,D,G) = log(cid:89)

Eij

As this independence assumption obviously does not hold among all pairs due to transitivity (even
though the individual samples i are assumed i.i.d.), it provides a lower bound on the concordance
index.
While the probability of correct pairwise ordering, Pr [fw(xi) < fw(xj)|w], is often chosen to be
sigmoid in the ranking literature [1], we show in the following that the sigmoid function arises
naturally in the context of PH models. Let T (w(cid:62)x) denote the survival time for the patient with
covariates x or relative log-hazard w(cid:62)x. A larger hazard corresponds to a smaller survival time, cf.
Sec. 2. Hence
Pr [fw(xi) < fw(xj)|w] = Pr[T (w(cid:62)xj) > T (w(cid:62)xi)|w] =

Pr[T (w(cid:62)xj) > t]p(t|xi)dt

(cid:90) ∞

(cid:90) ∞

0

(cid:90) ∞

0

=

S(t|xj)p(t|xi)dt =

0

−S(t|xj)S

(cid:48)

(t|xi)dt,

where p(t|xi) is the density function of T for patient i with covariate xi, and S(t|xi) is the corre-
(cid:48)(t) = dS(t)/dt = −p(t). Using Eq. 2 of the PH model, we continue
sponding survival function; S
the manipulations:

Pr [fw(xi) < fw(xj)|w] = −ew(cid:62)xi

−Λ0(t)

e

(cid:90) ∞

(cid:26)
ew(cid:62) xj +ew(cid:62) xi

(cid:27)

(cid:48)
Λ
0(t)dt

0

ew(cid:62)xi

ew(cid:62)xj + ew(cid:62)xi

=

= σ[w(cid:62)(xi − xj)].

(11)

This derivation shows that the probability of correct pairwise ordering indeed follows the sigmoid
function. Assuming a prior Pr[w] = N (w|0, λ−1) for regularization, the optimal maximum a-

posteriori (MAP) estimator is of the form (cid:98)wMAP = arg max L(w), where the posterior L(w) takes

the form of a penalized log-likelihood:
L(w) = − λ
2

(cid:107)w(cid:107)2 +(cid:88)

log σ(cid:2)wT (xj − xi)(cid:3) .

Eij

(12)

This expression is equivalent to (8) except for a few constants that are irrelevant for optimization
problem, which justiﬁes our choice of regularization in Eq. 8.

5

6 Exponential lower bound
The exponential 1 − e−z can serve as an alternative lower bound on the step indicator function (see
Fig. 1(c)). The concordance index can then be lower-bounded by

Analogous to the log-sigmoid bound, for the linear ranking function fw(x) = w(cid:62)x, the lower bound

(cid:98)cE simpliﬁes to

(13)

(14)

(15)

(cid:88)

Eij

c ≥ 1
|E|

(cid:98)cE(w) =
(cid:98)cEreg(w) = − λ

2

1 − e−[f (xj )−f (xi)] ≡(cid:98)cE.
(cid:88)

1 − e−w(cid:62)(xj−xi),

1
|E|

Eij

(cid:88)

Eij

1
|E|

and, penalizing functions with large norm w, the regularized version reads
1 − e−w(cid:62)(xj−xi).

(cid:107)w(cid:107)2 +

7 Gradient based learning

In order to maximize the regularized concave surrogate we can use any gradient-based learning
technique. We use the Polak-Ribi`ere variant of nonlinear conjugate gradients (CG) algorithm [11].
The CG method only needs the gradient g(w) and does not require evaluation of the function. It also
avoids the need for computing the second derivatives. The convergence of CG is much faster than
that of the steepest descent. Using the fact that dσ(z)/dz = σ(z)[1 − σ(z)] and 1 − σ(z) = σ(−z),
(xi −

the gradient of Eq. 9 (log-sigmoid bound) is given by ∇w(cid:98)cLSreg(w) = −λw − 1
xj)σ(cid:2)wT (xi − xj)(cid:3), and the gradient of Eq. 15 (exponential bound) by ∇w(cid:98)cEreg(w) = −λw −
(cid:80)Eij

(xi − xj)e−w(cid:62)(xj−xi).

(cid:80)Eij

|E| log 2

1|E|

8 Is Cox’s partial likelihood a lower bound on the CI ?

= 1 − No|E|
terms of zi as L(w) =(cid:81)

Our experimental results (Sec. 9) indicate that the Coxs method and our proposed methods showed
similar performance when assessed using the CI. While our proposed method was formulated to
explicitly maximize a lower bound on the concordance index, the Coxs method maximized the
partial likelihood. One suspects whether Coxs partial likelihood itself is a lower bound on the
concordance index. The argument presented below could give an indication as to why a method
which maximizes the partial likelihood also ends up (approximately) maximizing the concordance
index. We re-write the exponential bound on the CI for proportional hazard models from Sec. 6
ew(cid:62)xj ]

e−w(cid:62)xi[ (cid:88)

(cid:88)

(cid:98)cE(w) =

1
|E|

(cid:88)
1 − e−w(cid:62)(xi−xj ) = 1 − 1
|E|
(cid:88)

(cid:33)

Tj≥Ti

, where

zi =

1/zi

(cid:80)

Ti uncensored
ew(cid:62)xi
Tj≥Ti

ew(cid:62)xj

Tj≥Ti
∈ [0, 1].

(cid:88)
(cid:32)

Ti uncensored
1
No

(16)

Ti uncensored

Note that we have replaced Tj > Ti by Tj ≥ Ti, assuming that there are no ties in the data, i.e., no
two survival times are identical, analogous to Cox’s partial likelihood approach (cf. Sec. 2.4). The
number of uncensored observations is denoted by No. The Cox’s partial likelihood can be written in
geom, where (cid:104)zi(cid:105)geom denotes the geometric mean of
the zi with uncensored Ti. Using the inequality zi ≥ min zi the concordance index can be bounded
as

Ti uncensored zi = (cid:104)zi(cid:105)No

c ≥ 1 − No|E|

1

min zi

.

(17)

This says maximizing min zi maximizes a lower bound on the concordance index. While this does
not say anything about the Cox’s partial likelihood it still gives a useful insight. Since max zi = 1
(because zi = 1 for the largest uncensored Ti), maximizing min zi can be expected to approximately
maximize the geometric mean of zi, and hence the Cox’s partial likelihood.

6

Table 1: Summary of the ﬁve data sets used. N is the number of patients. d is the number of covariates used.

Dataset
N
285
MAASTRO
477
SUPPORT-1
314
SUPPORT-2
SUPPORT-4
149
MELANOMA 191

d Missing Censored
30.5%
19
26
36.4%
43.0%
26
10.7%
26
4
70.2%

3.6%
14.9%
16.6%
22.0%
0.0%

9 Experiments
In this section we compare the performance of the two different lower bounds on the CI—the log-
sigmoid, exponential, and Cox’s partial likelihood—on ﬁve medical data sets.

9.1 Medical datasets
Table 1 summarizes the ﬁve data sets we used in our experiments. A substantial amount of data
is censored and also missing. The MAASTRO dataset concerns the survival time of non-small
cell lung cancer patients, which we analyzed as part of our collaboration. The other medical data
sets are publicly available: The SUPPORT dataset 2 is a random sample from Phases I and II of
the SUPPORT [9](Study to Understand Prognoses Preferences Outcomes and Risks of Treatment)
study. As suggested in [6] we split the dataset into three different datasets, each corresponding to a
different cause of death. The MELANOMA data 3 is from a clinical study of skin cancer.

9.2 Evaluation procedure
For each data set, 70% of the examples were used for training and the remaining 30% as the hold-out
set for testing. We chose the optimal value of regularization parameter λ (cf. Eqs. 9 and 15) based
on ﬁve-fold cross validation on the training set. The tolerance for the conjugate gradient procedure
was set to 10−3. The conjugate-gradient optimization procedure was initialized to the zero vector.
All the covariates were normalized to have zero mean and unit variance. As missing values were
not the focus of this paper, we used a simple imputation technique. For each missing value, we
imputed a sample drawn from a Gaussian distribution with its mean and variance estimated from the
available values of the other patients.

9.3 Results
The performance was evaluated in terms of the concordance index and the results are tabulated in
Table 2. We compare the following methods–(1) Cox’s partial likelihood method, and (2) the pro-
posed ranking methods with log-sigmoid and exponential lower bounds. The following observations
can be made–(1) The proposed linear ranking method performs slightly better than the Cox’s par-
tial likelihood method, but the difference does not appear signiﬁcant. This agrees with our insights
that Cox’s partial likelihood may also end up maximizing the CI. (2) The exponential bound shows
slightly better performance than the log-sigmoid bound, which may indicate that the tightness of the
bound for positive z in Fig. 1(c) is more important than for negative z in our data sets. However the
difference is not signiﬁcant.
10 Conclusions

In this paper, we outlined several approaches for maximizing the concordance index, the standard
performance measure in survival analysis when cast as a ranking problem. We showed that, for the
widely-used proportional hazard models, the log-sigmoid function arises as a natural lower bound
on the concordance index. We presented an approach for directly optimizing this lower bound in
a computationally efﬁcient way. This optimization procedure can also be applied to other lower
bounds, like the exponential one. Apart from that, we showed that maximizing Cox’s partial like-
lihood can be understood as (approximately) maximizing a lower bound on the concordance index,
which explains the high CI-scores of proportional hazard models observed in practice. Optimization
of each of these three lower bounds results in about the same CI-score in our experiments, with our
new approach giving tentatively better results.

2http://biostat.mc.vanderbilt.edu/twiki/bin/view/Main/DataSets.
3www.stat.uni-muenchen.de/service/datenarchiv/melanoma/melanoma_e.html

7

Table 2: Concordance indices for the different methods and datasets. The mean and the standard deviation
are computed over a ﬁve fold cross-validation. The results are also shown for a ﬁxed holdout set.

CI for

CI for
test set

MAASTRO

log-sigmoid
exponential
SUPPORT-1

log-sigmoid
exponential
SUPPORT-2

training set
mean [± std] mean [± std]
0.57 [±0.09]
0.60 [±0.06]
0.64 [±0.08]
0.74 [±0.05]
0.77 [±0.04]
0.79 [±0.02]
0.63 [±0.06]
0.68 [±0.06]
0.68 [±0.09]
0.68 [±0.09]
0.74 [±0.12]
0.73 [±0.03]
0.62 [±0.09]
0.70 [±0.10]
0.65 [±0.11]

Cox PH 0.65 [±0.02]
0.69 [±0.02]
0.69 [±0.02]
Cox PH 0.76 [±0.01]
0.83 [±0.01]
0.83 [±0.01]
Cox PH 0.70 [±0.02]
0.79 [±0.01]
0.78 [±0.02]
Cox PH 0.78 [±0.01]
0.80 [±0.01]
0.79 [±0.01]
Cox PH 0.63 [±0.03]
0.76 [±0.02]
0.76 [±0.01]

log-sigmoid
exponential
SUPPORT-4

log-sigmoid
exponential
MELANOMA

log-sigmoid
exponential

CI for
holdout

set

0.64
0.64
0.65

0.79
0.79
0.82

0.69
0.65
0.70

0.64
0.71
0.71

0.54
0.55
0.55

Acknowledgements

We are grateful to R. Bharat Rao for encouragement and support of this work, and to the anonymous
reviewers for their valuable comments.
References
[1] C.J.C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender. Learning to
rank using gradient descent. In Proceeding of the 22th International Conference on Machine Learning,
2005.

[2] D. R. Cox. Regression models and life-tables (with discussion). Journal of the Royal Statistical Society,

Series B, 34(2):187–220, 1972.

[3] D. R. Cox. Partial likelihood. Biometrika, 62(2):269–276, 1975.
[4] D. R. Cox and D. Oakes. Analysis of survival data. Chapman and Hall, 1984.
[5] Y. Freund, R. Iyer, and R. Schapire. An efﬁcient boosting algorithm for combining preferences. Journal

of Machine Learning Research, 4:933–969, 2003.

[6] F. E. Harrell Jr. Regression Modeling Strategies, With Applications to Linear Models, Logistic Regression,

and Survival Analysis. Springer, 2001.

[7] R. Herbrich, T. Graepel, P. Bollmann-Sdorra, and K. Obermayer. Learning preference relations for infor-

mation retrieval. ICML-98 Workshop: Text Categorization and Machine Learning, pages 80–84, 1998.

[8] J. D. Kalbﬂeisch and R. L. Prentice. The statistical analysis of failure time data. Wiley-Interscience,

2002.

[9] W.A. Knaus, F. E. Harrell, J. Lynn, et al. The support prognostic model: Objective estimates of survival

for seriously ill hospitalized adults. Annals of Internal Medicine, 122:191–203, 1995.

[10] H. B. Mann and D. R. Whitney. On a Test of Whether one of Two Random Variables is Stochastically

Larger than the Other. The Annals of Mathematical Statistics, 18(1):50–60, 1947.

[11] J. Nocedal and S. J. Wright. Numerical Optimization. Springer, 1999.
[12] V. C. Raykar, R. Duraiswami, and B. Krishnapuram. A fast algorithm for learning large scale preference
relations. In M. Meila and X. Shen, editors, Proceedings of the Eleventh International Conference on
Artiﬁcial Intelligence and Statistics, pages 385–392, 2007.

[13] F. Wilcoxon. Individual comparisons by ranking methods. Biometrics Bulletin, 1(6):80–83, December

1945.

8

"
599,2007,Competition Adds Complexity,"It is known that determinining whether a DEC-POMDP, namely, a cooperative partially observable stochastic game (POSG), has a cooperative strategy with positive expected reward is complete for NEXP. It was not known until now how cooperation affected that complexity. We show that, for competitive POSGs, the complexity of determining whether one team has a positive-expected-reward strategy is complete for the class NEXP with an oracle for NP.","Competition adds complexity

Judy Goldsmith

Martin Mundhenk

Department of Computer Science

Friedrich-Schiller-Universit¨at Jena

University of Kentucky

Lexington, KY

Jena, Germany

mundhenk@cs.uni-jena.de

goldsmit@cs.uky.edu

Abstract

It is known that determinining whether a DEC-POMDP, namely, a cooperative
partially observable stochastic game (POSG), has a cooperative strategy with pos-
itive expected reward is complete for NEXP. It was not known until now how
cooperation affected that complexity. We show that, for competitive POSGs, the
complexity of determining whether one team has a positive-expected-reward strat-
egy is complete for NEXPNP.

1 Introduction

From online auctions to Texas Hold’em, AI is captivated by multi-agent interactions based on com-
petition. The problem of ﬁnding a winning strategy harks back to the ﬁrst days of chess programs.
Now, we are starting to have the capacity to handle issues like stochastic games, partial informa-
tion, and real-time video inputs for human player modeling. This paper looks at the complexity of
computations involving the ﬁrst two factors: partially observable stochastic games (POSGs).

There are many factors that could affect the complexity of different POSG models: Do the players,
collectively, have sufﬁcient information to reconstruct a state? Do they communicate or cooperate?
Is the game zero sum, or do the players’ individual utilities depend on other players’ utilities? Do
the players even have models for other players’ utilities?

The ultimate question is, what is the complexity of ﬁnding a winning strategy for a particular player,
with no assumptions about joint observations or knowledge of other players’ utilities. Since a special
case of this is the DEC-POMDP, where ﬁnding an optimal (joint, cooperative) policy is known to be
NEXP-hard [1], this problem cannot be any easier than in NEXP.
We show that one variant of this problem is hard for the class NEXPNP.

2 Deﬁnitions and Preliminaries

2.1 Partially observable stochastic games

A partially observable stochastic game (POSG) describes multi-player stochastic game with imper-
fect information by its states and the consequences of the players actions on the system. We follow
the deﬁnition from [2] and denote it as a tuple M = (I,S,s0,A,O,t,o,r), where

• I is the ﬁnite set {1,2, . . . ,k} of agents (or players), S is a ﬁnite set of states, with distin-
guished initial state s0 ∈ S, A is a ﬁnite set of actions, and O is a ﬁnite set of observations
• t : S × Ak × S → [0,1] is the transition probability function, where t(s,a1, . . . ,ak,s′) is the

probability that state s′ is reached from state s when each agent i chooses action ai

• o : S × I → O is the observation function , where o(s,i) is the observation made in state s

by agent i, and

1

• r : S × Ak × I → Z is the reward function, where r(s,a1, . . . ,ak,i) is the reward gained by

agent i in state s, when the agents take actions a1, . . . ,ak. (Z is the set of integers.)

A POSG where all agents have the same reward function is called a decentralized partially-
observable Markov decision process (see [1]).
Let M = (I,S,s0,A,O,t,o,r) be a POSG. A step of M is a transition from one state to another
according to the transition probability function t. A run of M is a sequence of steps that starts in
the initial state s0. The outcome of each step is probabilistic and depends on the actions chosen. For
each agent, a policy describes how to choose actions depending on observations made during the
run of the process. A (history-dependent) policy p chooses an action dependent on all observations
made by the agent during the run of the process. This is described as a function p
: O∗ → A, mapping
each ﬁnite sequence of observations to an action.
A trajectory q of length |q | = m for M is a sequence of states q = s 1,s 2, . . . ,s m (m ≥ 1, s i ∈ S)
which starts with the initial state of M , i.e. s 1 = s0. Given policies p 1, . . . ,p k, each trajectory q
has a probability prob(q ,p 1, . . . ,p k). We will use some abbreviations in the sequel. For p 1, . . . ,p k
j,k)) we will write p k
1(q
we will write p k
j
1 )
accordingly. Then prob(q ,p 1, . . . ,p k) is deﬁned by
|q |−1

1, and for p 1(o(s 1,1) · · · o(s

j,1)), . . . ,p k(o(s 1,k) · · · o(s

prob(q ,p k

1) =

t(s i,p k

1(q i

1),s i+1) .

i=1

We use Tl(s) to denote all length l trajectories which start in the initial state s0 and end in state s. The
expected reward Ri(s,l,p k
1 is
the reward obtained in s by the actions according to p k
1 weighted by the probability that s is reached
after l steps,

1 ) obtained by agent i in state s after exactly l steps under policies p k

Ri(s,l,p k

1 ) =

r(s,p k

1 (q l

1),i) · prob(q ,p k

1) .

q ∈Tl (s),q =(s 1,...,s l )

A POSG may behave differently under different policies. The quality of a policy is determined by
its performance, i.e. by the sum of expected rewards received on it. We use |M | to denote the size
of the representation of M .1 The short-term performance for policies p k
1 for agent i with POSG M
is the expected sum of rewards received by agent i during the next |M | steps by following the policy
p k
1, i.e.

perfi(M ,p k

1) = (cid:229)

Ri(s, |M |,p k

1 ) .

s∈S
The performance is also called the expected reward.

Agents may cooperate or compete in a stochastic game. We want to know whether a stochastic game
can be won by some agents. This is formally expressed in the following decision problems.
The cooperative agents problem for k agents:

instance:
query:

a POSG M for k agents
are there policies p 1, . . . ,p k under which every agent has positive performance ?

The competing agents problem for 2k agents:

(I.e. ∃p 1, . . . ,p k :Vk

i=1 perfi(M ,p k

1) > 0 ?)

instance:
query:

a POSG M for 2k agents
are there policies p 1, . . . ,p k under which all agents 1,2, . . . ,k have positive per-
formance independent of which policies agents k + 1,k + 2, . . . ,2k choose? (I.e.

∃p 1, . . . ,p k∀p k+1, . . . ,p 2k :Vk

i=1 perfi(M ,p 2k

1 ) > 0 ?)

It was shown by Bernstein et al. [1] that the cooperative agents problem for two or more agents is
complete for NEXP.

1The size of the representation of M is the number of bits to encode the entire model, where the function
t, o, and r are encoded by tables. We do not consider smaller representations. In fact, smaller representations
may increase the complexity.

2

(cid:213)
(cid:229)
2.2 NEXPNP

A Turing machine M has exponential running time, if there is a polynomial p such that for every
input x, the machine M on input x halts after at most 2p(|x|) steps. NEXP is the class of sets that
can be decided by a nondeterministic Turing machine within exponential time. NEXPNP is the class
of sets that can be decided by a nondeterministic oracle Turing machine within exponential time,
when a set in NP is used as an oracle. Similar as for the class NPNP, it turns out that a NEXPNP
computation can be performed by an NEXP oracle machine that asks exactly one query to a co NP
oracle and accepts if and only if the oracle accepts.

2.3 Domino tilings

Domino tiling problems are useful for reductions between different kinds of computations. They
have been proposed by Wang [3], and we will use it according to the following deﬁnition.

Deﬁnition 2.1 We use [m] to denote the set {0,1,2, . . . ,m − 1}. A tile type T = (V,H) consists of
two ﬁnite sets V,H ⊆ N × N. A T -tiling of an m-square (m ∈ N) is a mapping t : [m] × [m] → N that
satisﬁes both the following conditions.

1. Every pair of two neighboured tiles in the same row is in H.
I.e. for all r ∈ [m] and c ∈ [m − 1], (t (r,c),t (r,c + 1)) ∈ H.

2. Every pair of two neighboured tiles in the same column is in V .

I.e. for all r ∈ [m − 1] and c ∈ [m], (t (r,c),t (r + 1,c)) ∈ V .

The exponential square tiling problem is the set of all pairs (T,1k), where T is a tile type and 1k is
a string consisting of k 1s (k ∈ N), such that there exists a T -tiling of the 2k-square.

It was shown by Savelsbergh and van Emde Boas [4] that the exponential square tiling problem
is complete for NEXP. We will consider the following variant, which we call the exponential S 2
square tiling problem: given a pair (T,1k), does there exist a row w of tiles and a T -tiling of the
2k-square with ﬁnal row w, such that there exists no T -tiling of the 2k-square with initial row w?
The proof technique of Theorem 2.29 in [4], which translates Turing machine computations into
tilings, is very robust in the sense that simple variants of the square tiling problem can analogously
be shown to be complete for different complexity classes. Together with the above characterization
of NEXPNP it can be used to prove the following.
Theorem 2.2 The exponential S 2 square tiling problem is complete for NEXPNP.

3 Results

POSGs can be seen as a generalization of partially-observable Markov decision processes (PO-
MDPs) in that POMDPs have only one agent and POSGs allow for many agents. Papadimitriou
and Tsitsiklis [5] proved that it is PSPACE-complete to decide the cooperative agents problem for
POMDPs. The result of Bernstein et al. [1] shows that in case of history-dependent policies, the
complexity of POSGs is greater than the complexity of POMDPs. We show that this difference
does not appear when stationary policies are considered instead of history-dependent policies. For
POMDPs, the problem appears to be NP-complete [6]. A stationary policy is a mapping O → A
from observations to actions. Whenever the same observation is made, the same action is chosen by
a stationary policy.

Theorem 3.1 For any k ≥ 2, the cooperative agents problem for k agents for stationary policies is
NP-complete.

Proof We start with proving NP-hardness. A POSG with only one agent is a POMDP. The problem
of deciding, for a given POMDP M , whether there exists a stationary policy such that the short-term
performance of M is greater than 0, is NP-complete [6]. Hence, the cooperative agents problem for
stationary policies is NP-hard.

3

It remains to show containment in NP. Let M = (I,S,s0,A,O,t,o,r) be a POSG. We assume that t
is represented in a straightforward way as a table. Let p 1, . . . ,p k be a sequence of stationary policies
for the k agents. This sequence can be straightforwardly represented using not more space than
the representation of t takes. Under a ﬁxed sequence of policies, the performance of the POSG for
all of the agents can be calculated in polynomial time. Using a guess and check approach (guess
the stationary policies and evaluate the POSG), this shows that the cooperative agents problem for
stationary policies is in NP.
2

In the same way we can characterize the complexity of a problem that we will need in the proof of
Lemma 3.3.

Corollary 3.2 The following problem is coNP-complete.

instance:
query:

a POSG M for k agents
do all agents under every stationary policy have positive performance? (I.e.

∀stationary p 1 . . .p k :Vk

i=1 perfi(M ,p k

1) > 0 ?)

The cooperative agents problem was shown to be NEXP-complete by Bernstein et al. [1]. Not
surprisingly, if the agents compete, the problem becomes harder.

Lemma 3.3 For every k ≥ 1, the competing agents problem for 2k agents is in NEXPNP.
Proof The basic idea is as follows. We guess policies p 1,p 2, . . . ,p k for agents 1,2, . . . ,k, and
construct a POSG that “implements” these policies and leaves open the actions chosen by agents
k + 1, . . . ,2k.
This new POSG has states for all short-term trajectories through the origin POSG. Therefore, its
size is exponential in the size of the origin POSG. Because the history is stored in every state, and
the POSG is loop-free, it turns out that the new POSG can be taken as a POMDP for which a (joint)
policy with positive reward is searched. This problem is known to be NP-complete.
Let M = (I,S,s0,A,O,t,o,r) be a POSG with 2k agents, and let p 1, . . . ,p k be short-term policies for
,r′) as follows2. In M ′, we have as agents
M . We deﬁne a k-agent POSG M ′ = (I′
those of M , whose policies are not ﬁxed, i.e. I′ = {k + 1, . . . ,2k}. The set of states of M ′ is the
cross product of states from M and all trajectories up to length |M | over S, i.e. S′ = S × S≤|M |+1.
The meaning of state (s,u) ∈ S′ is, that state s can be reached on a trajectory u (that ends with s)
0 = (s0,s0). The state (s0,e ) is taken as
through M with the ﬁxed policies. The initial state s′
a special sink state. After |M | + 2 steps, the sink state is entered in M ′ and it is not left thereafter.
All rewards gained in the sink state are 0. Now for the transition probabilities. If s is reached on
trajectory u in M and the actions a1, . . . ,ak are according to the ﬁxed policies p 1, . . . ,p k, then the
probabiliy of reaching state s′ on trajectory us′ according to t in M is the same as to reach (s′
,us′)
in M ′ from (s,u). In the formal description, the sink state has to be considered, too.
t′((s,u),ak, . . . ,a2k, ( ˆs, ˆu)) =

0,A,O′

0 is s′

,S′

,s′

,t′

,o′

0,

t(s,p 1(o(us,1)), · · · ,p k(o(us,k)),ak+1, . . . ,a2k, ˆs),
1,


if u 6= e and u ˆs 6= ˆu
if ˆu = u ˆs, | ˆu| ≤ |M |, u 6= e
if |u| = |M | + 1 or u = e , and ˆu = e

The observation in M ′ is the sequence of observations made in the trajectory that is contained in
each state, i.e. o′((s,w)) = o(w), where o(e ) is any element of O. Finally, the rewards. Essentially,
we are interested in the rewards obtained by the agents 1,2, . . . ,k. The rewards obtained by the other
agents have no impact on this, only the actions the other agents choose. Therefore, agent i obtains the
rewards in M ′ that are obtained by agent i − k in M . In this way, the agents k + 1, . . . ,2k obtain in
M ′ the same rewards that are obtained by agents 1,2, . . . ,k in M , and this is what we are interested
in. This results in r′((s,u),ak, . . . ,a2k,i) = r(s,p 1(o(u,1)), · · · ,p k(o(u,k)),ak+1, . . . ,a2k,i − k) for
i = k + 1, . . . ,2k.

2S≤|M | denotes the set of sequences up to |M | elements from S. The empty sequence is denoted by
e . For w ∈ S≤|M | we use o(w, i) to describe the sequence of observations made by agent i on trajectory w.
The concatenation of sequences u and w is denoted uw. We do not distinguish between elements of sets and
sequences of one element.

4

Notice that the size of M ′ is exponential in the size of M . The sink state in M ′ is the only state that
lies on a loop. This means, that on all trajectories through M ′, the sink state is the only state that
may appear more than once. All states other than the sink state contain the full history of how they
are reached. Therefore, there is a one-to-one correspondence between history-dependent policies
for M and stationary policies for M ′ (with regard to horizon |M |). Moreover, the corresponding
policies have the same performances.
Claim 1 Let p 1, . . . ,p 2k be short-term policies for M , and let ˆp k+1, . . . , ˆp 2k be their corresponding
stationary policies for M ′.
For |M | steps and i = 1,2, . . . ,k, perfi(M ,p 2k

1 ) = perfi+k(M ′

, ˆp 2k

k+1).

Thus, this yields an NEXPNP algorithm to decide the competitive agents problem. The input is a
POSG M for 2k agents. In the ﬁrst step, the policies for the agents 1,2, . . . ,k are guessed. This takes
nondeterministic exponential time. In the second step, the POSG M ′ is constructed from the input
M and the guessed policies. This takes exponential time (in the length of the input M ). Finally, the
oracle is queried whether M ′ has positive performance for all agents under all stationary policies.
This problem belongs to coNP (Corollary 3.2). Henceforth, the algorithm shows the competing
agents problem to be in NEXPNP.
2

Lemma 3.4 For every k ≥ 2, the competing agents problem for 2k agents is hard for NEXPNP.
Proof We give a reduction from the exponential S 2 square tiling problem to the competing agents
problem.
Let T = (T,1k) be an instance of the exponential S 2 square tiling problem, where T = (V,H) is a
tile type. We will show how to construct a POSG M with 4 agents from it, such that T is a positive
instance of the exponential S 2 square tiling problem if and only if (1) agents 1 and 2 have a tiling
for the 2k square with ﬁnal row w such that (2) agents 3 and 4 have no tiling for the 2k square with
initial row w.

The basic idea for checking of tilings with POSGs for two agents stems from Bernstein et al. [1],
but we give a slight simpliﬁcation of their proof technique, and in fact have to extend it for four
agents later on. The POSG is constructed so that on every trajectory each agent sees a position in
the square. This position is chosen by the process. The only action of the agent that has impact
on the process is putting a tile on the given position. In fact, the same position is observed by the
agents in different states of the POSG. From a global point of view, the process splits into two parts.
The ﬁrst part checks whether both agents know the same tiling, without checking that it is a correct
tiling. In the state where the agents are asked to put their tiles on the given position, a high negative
reward is obtained if the agents put different tiles on that position. ”High negative” means that,
if there is at least one trajectory on which such a reward is obtained, then the performance of the
whole process will be negative. The second part checks whether the tiling is correct. The idea is to
give both the agents neighboured positions in the square and to ask each which tile she puts on that
position. Notice that the agents do not know in which part of the process they are. This means, that
they do not know whether the other agent is asked for the same position, or for its upper or right
neighbour. This is why the agents cannot cheat the process. A high negative reward will be obtained
if the agents’ tiles do not ﬁt together.
For the ﬁrst part, we need to construct is a POSG Pk for two agents, that allows both agents to
make the same sequence of observations consisting of 2k bits. This sequence is randomly chosen,
and encodes a position in a 2k × 2k grid. At the end, state same is reached, at which no observation is
made. At this state, it will be checked whether both agents put the same tile at this position (see later
on). The task of Pk is to provide both agents with the same position. Figure 1 shows an example
for a 24 × 24-square. The initial state is s4. Dashed arrows indicate transitions with probability 1
2
independent of the actions. The observation of agent 1 is written on the left hand side of the states,
and the observations of agent 2 at the right hand side. In s4, the agents make no observation. In Pk
both agents always make the same observations.

The second part is more involved. The goal is to provide both agents with neighboured positions
in the square. Eventually, it is checked whether the tiles they put on the neighboured positions
are according to the tile type T . Because the positions are encoded in binary, we can make use

5

s4

s

s

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

0

0

0

0

0

0

0

0

0

0

0

0

0

1

1

1

1

1

1

1

1

0

1

1

1

1

1

1

1

1

1

1

1

1

1

1

0

1

0

0

0

0

0

0

1

0

row

0

0

column

same

hori

same

check tiles

Figure 1: P4

Figure 2: C3,4

Figure 3: L3,4

of the following fact of subsequent binary numbers. Let u = u1 . . . uk and w = w1 . . . wk be bitwise
representation of strings.
if nw = nu + 1, then for some index l it holds that (1) ui = wi for i =
1,2, . . . ,l − 1, (2) wl = 1 and ul = 0, and (3) w j = 0 and u j = 1 for j = l + 1, . . . ,k.
The POSG Cl,k is intended to provide the agents with two neighboured positions in the same row,
where the index of the leftmost bit of the column encoding where both positions distinguish is l.
(The C stands for column.) Figure 2 shows an example for the 24-square. The “ﬁnal state” of Cl,k
is the state hori, from which it is checked whether the agents put horizontically ﬁtting tiles together.
In the same way, a POSG Rl,k can be constructed (R stands for row), whose task is, to check
whether two tiles in neighboured rows correspond to a correct tiling. This POSG has the ﬁnal state
vert, from which on it is checked whether two tiles ﬁt vertically.

Finally, we have to construct the last part of the POSG. It consists of the states same, hori, vert (as
mentioned above), good, bad, and sink. All transitions between these states are deterministic (i.e.
with probability 1). From state same the state good is reached, if both agents take the same action
– otherwise bad is reached. From state hori the state good is reached, if action a1 by agent 1 and a2
by agent 2 make a pair (a1,a2) in H, i.e. in the set of horizontically correct pairs of tiles – otherwise
bad is reached. Similarly, from state vert the state good is reached, if action a1 by agent 1 and a2 by
agent 2 make a pair (a1,a2) in V . All these transitions are with reward 0. From state good the state
sink is reached on every action with reward 1, and from state bad the state sink is reached on every
action with reward −(22k+2). When the state sink is reached, the process stays there on any action,
and all agents obtain reward 0. All rewards are the same for both agents. (This part can be seen in
the overall picture in Figure 4).
From these POSGs we construct a POSG T2,k that checks whether two agents know the same correct
tiling for a 2k × 2k square, as described above. There are 2k + 1 parts of T2,k. The initial state of each
part can be reached with one step from the initial state s0 of T2,k. The parts of T2,k are as follows.

• P2k with initial state s (checks whether two agents have the same tiling)
• For each l = 1,2, . . . ,k, we take Cl,k. Let cl be the initial state of Cl,k.

6

s0

ck

r1

Ck,k

R1,k

rk

Rk,k

sk

Pk

c1

C1,k

same

hori

vert

good

bad

sink

Figure 4: T2,k

• For each l = 1,2, . . . ,k, we take Rl,k. Let rl be the initial state of Rl,k.

There are 22k + 2 · (cid:229)
k
l=1 2k · 2l−1 =: tr(k) trajectories with probability > 0 through T2,k. Notice
that tr(k) < 22k+2. From the initial state s0 of T2,k, each of the initial states of the parts is reachable
independent on the action chosen by the agents. We will give transition probabilities to the transition
from s0 to each of the initial states of the parts in a way, that eventually each trajectory has the same
probability.

t(s0,a1,a2,s′) =( 22k

tr(k) ,
2k+l−1
tr(k)

if s′ = s, i.e. the initial state of Pk
if s ∈ {rl ,cl | l = 1,2, . . . ,k}

In the initial state s0 and in the initial states of all parts, the observation e
is made. When a state
same, hori, vert is reached, each agent has made 2k + 3 observations, where the ﬁrst and last are e
and the remaining 2k are each in {0,1}. Such a state is the only one where the actions of the agents
have impact on the process. Because of the partial observability, they cannot know in which part
of T2,k they are. The agents can win, if they both know the same correct tiling and interpret the
sequence of observations as the position in the grid they are asked to put a tile on. On the other
hand, if both agents know different tilings or the tiling they share is not correct, then at least one
trajectory will end in a bad state and has reward −(22k+2). The structure of the POSG is given in
Figure 4.

Claim 2 Let (T,1k) be an instance of the exponential square tiling problem.

(1) There exists a polynomial time algorithm, that on input (T,1k) outputs T2,k.

(2) There exists a T -tiling of the 2k square if and only if there exist policies for the agents under

which T2,k has performance > 0.

Part (1) is straightforward. Part (2) is not much harder. If there exists a T -tiling of the 2k square,
both agents use the same policy according to this tiling. Under these policies, state bad will not be
reached. This guarantess performance > 0 for both agents. For the other direction: if there exist
policies for the agents under which T2,k has performance > 0, then state bad is not reached. Hence,
both agents use the same policy. It can be shown inductively that this policy “is” a T -tiling of the 2k
square.

7

The POSG for the competing agents problem with 4 agents consists of three parts. The ﬁrst part is
a copy of T2,k. It is used to check whether the ﬁrst square can be tiled correctly (by agents 1 and
2). In this part, the negative rewards are increased in a way that guarantees the performance of the
POSG to be negative whenever agents 1 and 2 do not correctly tile their square. The second part
is a modiﬁed copy of T2,k. It is used to check whether the second square can be tiled correctly (by
agents 3 and 4). Whenever state bad is left in this copy, reward 0 is obtained, and whenever state
good is left, reward −1 is obtained. The third part checks whether agent 1 puts the same tiles into
the last row of its square as agent 3 puts into the ﬁrst row of its square. (See L3,4 in Figure 3 as an
example.) If this succeeds, the performance of the third part equals 0, otherwise it has performance
1. These three parts run in parallel.

If agents 1 and 2 have a tiling for the ﬁrst square, the performance of the ﬁrst part equals 1.

• If agents 3 and 4 are able to continue this tiling through their square, the performance
of the second part equals −1 and the performance of the third part equals 0. At all, the
performance of the POSG under these policies equals 0.

• If agents 3 and 4 are not able to continue this tiling through their square, then the perfor-
mance of part 2 and part 3 is strictly greater −1. At all, the performance of the POSG under
these policies is > 0.

Lemmas 3.3 and 3.4 together yield completeness of the competing agents problem.

Theorem 3.5 For every k ≥ 2, the competing agents problem for 2k agents is complete for NEXPNP.

2

4 Conclusion

We have shown that competition makes life—and computation—more complex. However, in order
to do so, we needed teamwork. It is not yet clear what the complexity is of determining the existence
of a good strategy for Player I in a 2-person POSG, or a 1-against-many POSG.
There are other variations that can be shown to be complete for NEXPNP, a complexity class that,
shockingly, has not been well explored. We look forward to further results about the complexity of
POSGs, and to additional NEXPNP-completeness results for familiar AI and ML problems.

References

[1] Daniel S. Bernstein, Robert Givan, Neil Immerman, and Shlomo Zilberstein. The complexity
of decentralized control of Markov decision processes. Math. Oper. Res., 27(4):819–840, 2002.
[2] E. Hansen, D. Bernstein, and S. Zilberstein. Dynamic programming for partially observable
stochastic games. In Proceedings of the Nineteenth National Conference on Artiﬁcial Intelli-
gence (AAAI-04), pages 709–715, 2004.

[3] Hao Wang. Proving theorems by pattern recognition II. Bell Systems Technical Journal, 40:1–

42, 1961.

[4] M. Savelsbergh and P. van Emde Boas. Bounded tiling, an alternative to satisﬁability. In Gerd
Wechsung, editor, 2nd Frege Conference, volume 20 of Mathematische Forschung, pages 354–
363. Akademie Verlag, Berlin, 1984.

[5] C.H. Papadimitriou and J.N. Tsitsiklis. The complexity of Markov decision processes. Mathe-

matics of Operations Research, 12(3):441–450, 1987.

[6] Martin Mundhenk, Judy Goldsmith, Christopher Lusena, and Eric Allender. Complexity results
for ﬁnite-horizon Markov decision process problems. Journal of the ACM, 47(4):681–720, 2000.

8

"
919,2007,Regulator Discovery from Gene Expression Time Series of Malaria Parasites: a Hierachical Approach,"We introduce a hierarchical Bayesian model for the discovery of putative regulators from gene expression data only. The hierarchy incorporates the knowledge that there are just a few regulators that by themselves only regulate a handful of genes. This is implemented through a so-called spike-and-slab prior, a mixture of Gaussians with different widths, with mixing weights from a hierarchical Bernoulli model. For efficient inference we implemented expectation propagation. Running the model on a malaria parasite data set, we found four genes with significant homology to transcription factors in an amoebe, one RNA regulator and three genes of unknown function (out of the top ten genes considered).","Regulator Discovery from Gene Expression Time

Series of Malaria Parasites: a Hierarchical Approach

Jos´e Miguel Hern´andez-Lobato

Escuela Polit´ecnica Superior

Universidad Aut´onoma de Madrid, Madrid, Spain

Tjeerd Dijkstra

Leiden Malaria Research Group
LUMC, Leiden, The Netherlands

Josemiguel.hernandez@uam.es

t.dijkstra@lumc.nl

Tom Heskes

Institute for Computing and Information Sciences

Radboud University Nijmegen, Nijmegen, The Netherlands

t.heskes@science.ru.nl

Abstract

We introduce a hierarchical Bayesian model for the discovery of putative regula-
tors from gene expression data only. The hierarchy incorporates the knowledge
that there are just a few regulators that by themselves only regulate a handful
of genes. This is implemented through a so-called spike-and-slab prior, a mix-
ture of Gaussians with different widths, with mixing weights from a hierarchical
Bernoulli model. For efﬁcient inference we implemented expectation propaga-
tion. Running the model on a malaria parasite data set, we found four genes with
signiﬁcant homology to transcription factors in an amoebe, one RNA regulator
and three genes of unknown function (out of the top ten genes considered).

1 Introduction

Bioinformatics provides a rich source for the application of techniques from machine learning. Es-
pecially the elucidation of regulatory networks underlying gene expression has lead to a cornucopia
of approaches: see [1] for review. Here we focus on one aspect of network elucidation, the identi-
ﬁcation of the regulators of the causative agent of severe malaria, Plasmodiumfalciparum. Several
properties of the parasite necessitate a tailored algorithm for regulator identiﬁcation:

• In most species gene regulation takes place at the ﬁrst stage of gene expression when a
DNA template is transcribed into mRNA. This transcriptional control is mediated by spe-
ciﬁc transcription factors. Few speciﬁc transcription factors have been identiﬁed in Plas-
modium based on sequence homology with other species [2, 3]. This could be due to
Plasmodium possessing a unique set of transcription factors or due to other mechanisms of
gene regulation, e.g. at the level of mRNA stability or post-transcritional regulation.
• Compared with yeast, gene expression in Plasmodium is hardly changed by perturbations
e.g. by adding chemicals or changing temperature [4]. The biological interpretation of this
ﬁnding is that the parasite is so narrowly adapted to its environment inside a red blood cell
that it follows a stereotyped gene expression program. From a machine learning point of
view, this ﬁnding means that network elucidation techniques relying on perturbations of
gene expression cannot be used.
• Similar to yeast [5], data for three different strains of the parasite with time series of gene
expression are publicly available [6]. These assay all of Plasmodium’s 5,600 genes for
about 50 time points. In contrast to yeast, there are no ChIP-chip data available and fewer
then ten transcription factor binding motifs are known.

1

Together, these properties point to a vector autoregressive model making use of the gene expression
time series. The model should not rely on sequence homology information but it should be ﬂexible
enough to integrate sequence information in the future. This points to a Bayesian model as favored
approach.

2 The model

We start with a semi-realistic model of transcription based on Michaelis-Menten kinetics [1] and
subsequently simplify to obtain a linear model. Denoting the concentration of a certain mRNA
transcript at time t by z(t) we write:

dz(t)
dt =

V1a1(t)M1

K1 + a1(t)M1

··· VN aN (t)MN
KN + aN (t)MN

p(t) − 1
τz

z(t),

(1)

with aj(t) the concentration of the j-th activator (positive regulator), p(t) the concentration of RNA
polymerase and Vj, Kj, Mj and τz reaction constants. N denotes the number of potential activators.
The activator is thought to bind to DNA motifs upstream of the transcription start site and binds RNA
polymerase which reads the DNA template to produce an mRNA transcript. Mj can be thought of
as the multiplicity of the motif, τz captures the characteristic life time of the transcript. While
reasonably realistic, this equation harbors too many unknowns for reliable inference: 3N + 1 with
N ≈ 1000. We proceed with several simpliﬁcations:
• aj(t) (cid:3) Kj: activator concentration is low;
• p(t) = p0 is constant;
dt ≈ z(t+Δ)−z(t)
• dz(t)
• Δ ≈ τz: sampling period roughly equal to transcript life time.

with Δ the sampling period;

Δ

Counting time in units of Δ and taking logarithms on both sides, Equation (1) then simpliﬁes to

log z(t + 1) = C + M1 log a1(t) + ··· + MN log aN (t),

with C = log(T V1 ··· VN p0/(K1 ··· KN )). This is a linear model for gene expression level given
the expression levels of a set of activators. With a similar derivation one can include repressors [1].

2.1 A Bayesian model for sparse linear regression
Let y be a vector with the log expression of the target gene and X = (x1, . . . , xN ) a matrix whose
columns contain the log expression of the candidate regulators. Assuming that the measurements
are corrupted with additive Gaussian noise, we get y ∼ N (Xβ, σ2I) where β = (β1, . . . , βN )T
is a vector of regression coefﬁcients and σ2 is the variance of the noise. Such a linear model is
commonly used [7, 8, 9]. Both y and x1, . . . , xN are mean-centered vectors with T measurements.
We specify an inverse gamma (IG) prior for σ2 so that P(σ2) = IG(σ2, ν/2, νλ/2), where λ is a
prior estimate of σ2 and ν is the sample size associated with that estimate. We assume that a priori
all components βi are independent and take a so-called “spike and slab prior” [10] for each of them.
That is, we introduce binary latent variables γi, with γi = 1 if xi takes part in the regression of y
and γi = 0 otherwise. Given γ, the prior on β then reads

P(β|γ) =

N(cid:2)

i=1

N(cid:2)

i=1

P(βi|γi) =

N (βi, 0, v1)γi N (βi, 0, v0)1−γi ,

where N (x, μ, σ2) denotes a Gaussian density with mean μ and variance σ2 evaluated at x. In order
to enforce sparsity, the variance v1 of the slab should be larger than the variance v0 of the spike.
Instead of picking the hyperparameters v1 and v0 directly, it is convenient to pick a threshold of
practical signiﬁcance δ so that P(γi = 1) gets more weight when |βi| > δ and P(γi = 0) gets more
weight when |βi| < δ [10]. In this way, given δ and one of v1 or v0, we pick the other one such that

δ2 =

log(v1/v0)
0 − v−1
v−1

1

.

2

(2)

Finally, we assign independent Bernoulli priors to the components of the latent vector γ:

N(cid:2)

P(γ) =

Bern(γi, w) =

i=1

i=1

wγi(1 − w)1−γi ,

so that each of the x1, . . . , xN can independently take part in the regression with probability w. We
can identify the candidate genes whose expression is more likely to be correlated with the target
gene by means of the posterior distribution of γ:

P(γ, β, σ2|y, X) dβ dσ2 ∝

P(γ, β, σ2, y|X) dβ dσ2 ,

N(cid:2)

(cid:3)

β,σ2

(cid:3)

β,σ2

P(γ|y, X) =

where

P(γ, β, σ2, y|X) = N (y, Xβ, σ2I)P(β|γ)P(γ)P(σ2)
N(cid:2)

N(cid:5)

(cid:6)(cid:4)

=

N (yt,

xi,tβi, σ2)

(cid:6)

i=1

i=1

(cid:4)
(cid:4)

T(cid:2)
N(cid:2)

t=1

Bern(γi, w)

IG(σ2, ν/2, νλ/2) .

(cid:7)N (βi, 0, v1)γi N (βi, 0, v0)1−γi

(cid:8)(cid:6)

(3)

i=1

Unfortunately, this posterior distribution cannot be computed exactly if the number N of candidate
genes is larger than 25. An approximation based on Markov Chain Monte Carlo (MCMC) methods
has been proposed in [11].

2.2 A hierarchical model for gene regulation

In the section above we made use of the prior information that a target gene is typically regulated
by a small number of regulators. We have not yet made use of the prior information that a regulator
typically regulates more than one gene. We incorporate this information by a hierarchical extension
of our previous model. We introduce a vector τ of binary latent variables where τi = 1 if gene i is
a regulator and τi = 0 otherwise. The following joint distribution captures this idea:

P(τ , γ, β, σ2|X) =

N (xj,t+1,

xi,tβj,i, σ2
j )

⎡
⎣ N(cid:2)

T−1(cid:2)

j=1

t=1

N(cid:5)

i=1, i(cid:2)=j

N (βj,i, 0, v1)γj,i N (βj,i, 0, v0)1−γj,i
⎤
⎦

Bern(γj,i, w1)τi Bern(γj,i, w0)1−τi

IG(σ2

j , νj/2, νjλj/2)

(cid:6)

Bern(τi, w)

.

(4)

⎤
⎦

⎡
⎣ N(cid:2)
⎡
⎣ N(cid:2)

j=1

N(cid:2)
N(cid:2)

i=1,i(cid:2)=j

j=1

i=1,i(cid:2)=j

⎤
⎦
⎡
⎣ N(cid:2)
N(cid:2)

j=1

⎤
⎦
(cid:4)

i=1

In this hierarchical model, γ is a matrix of binary latent variables where γj,i = 1 if gene i takes
part in the regression of gene j and γj,i = 0 otherwise. The relationship between regulators and
regulatees suggests that P(γj,i = 1|τi = 1) should be bigger than P(γj,i = 1|τi = 0) and thus
w1 > w0. Matrix β contains regression coefﬁcients where βj,i is the regression coefﬁcient between
the expression of gene i and the delayed expression of gene j. Hyperparameter w represents the prior
j of the vector σ2 contain the variance
probability of any gene being a regulator and the elements σ2
of the noise in each of the N regressions. Hyperparameters λj and νj have the same meaning as in
the model for sparse linear regression. The corresponding plate model is illustrated in Figure 1.
We can identify the genes more likely to be regulators by means of the posterior distributionP(τ|X).
Compared with the sparse linear regression model we expanded the number of latent variables from
O(N) to O(N 2). In order to keep inference feasible we turn to an approximate inference technique.

3

M

L

L

τE

NEJ

γE

βE



M

M

σ

NJ

6

λ

ν



Figure 1: The hierarchical
model for gene regulation.

3 Expectation propagation

The Expectation Propagation (EP) algorithm [12] allows to perform approximate Bayesian infer-
In all Bayesian problems, the joint distribution of the model parameters θ and a data set
ence.
D = {(xi, yi) : i = 1, . . . , n} with i.i.d. elements can be expressed as a product of terms

P(θ,D) =

P(yi|xi, θ)P(θ) =

(5)
where tn+1(θ) = P(θ) is the prior distribution for θ and ti(θ) = P(yi|xi, θ) for i = 1, . . . , n.
Expectation propagation proceeds to approximate (5) with a product of simpler terms

i=1

i=1

ti(θ) ,

n(cid:2)

n+1(cid:2)

n+1(cid:2)

ti(θ) ≈ n+1(cid:2)

˜ti(θ) = Q(θ) ,

(6)
where all the term approximations ˜ti are restricted to belong to the same family F of exponential
distributions, but they do not have to integrate 1. Note that Q will also be in F because F is closed
under multiplication. Each term approximation ˜ti is chosen so that

i=1

i=1

is as close as possible to

(cid:2)

j(cid:2)=i

Q(θ) = ˜ti(θ)
(cid:2)

ti(θ)

j(cid:2)=i

˜tj(θ) = ˜ti(θ)Q\i(θ)

˜tj(θ) = ti(θ)Q\i(θ) ,

in terms of the direct Kullback-Leibler (K-L) divergence. The pseudocode of the EP algorithm is:

1. Initialize the term approximations ˜ti and Q to be uniform.
2. Repeat until all ˜ti converge:

(a) Choose a ˜ti to reﬁne and remove it from Q to get Q\i (e.g. dividing Q by ˜ti).
(b) Update the term ˜ti so that it minimizes the K-L divergence between tiQ\i and ˜tiQ\i.
(c) Re-compute Q so that Q = ˜tiQ\i.

The optimization problem in step (b) is solved by matching sufﬁcient statistics between a distribu-
tion Q(cid:3)
within the F family and tiQ\i, the new ˜ti is then equal to Q(cid:3)/Q\i. Because Q belongs to the
exponential family it is generally trivial to calculate its normalization constant. Once Q is normal-
ized it can approximate P(θ|D). Finally, EP is not guaranteed to converge, although convergence
can be improved by means of damped updates or double-loop algorithms [13].

3.1 EP for sparse linear regression

The application of EP to the models of Section 2 introduces some nontrivial technicalities.
Furthermore, we describe several techniques to speed up the EP algorithm. We approximate
P(γ, β, σ2, y|X) for sparse linear regression by means of a factorized exponential distribution:

P(γ, β, σ2, y|X) ≈

IG(σ2, a, b) ≡ Q(γ, β, σ2) ,

(7)

(cid:4)

N(cid:2)

(cid:6)
Bern(γi, qi)N (βi, μi, si)

i=1

4

where {qi, μi, si : i = 1, . . . , N}, a and b are free parameters. Note that in the approximation
Q(γ, β, σ2) all the components of the vectors γ and β and the variable σ2 are considered to be
independent; this allows the approximation of P(γ|y, X) by
i=1 Bern(γi, qi). We tune the pa-
rameters of Q(γ, β, σ2) by means of EP over the unnormalized density P(γ, β, σ2, y|X). Such
density appears in (3) as a product of T + N terms (not counting the priors) which correspond to the
ti terms in (5). This way, we have T + N term approximations with the same form as (7) and which
correspond to the term approximations ˜ti in (6). The complexity is O(T N) per iteration, because
updating any of the ﬁrst T term approximations requires N operations. However, some of the EP
update operations require to compute integrals which do not have a closed form expression. To avoid
that, we employ the following simpliﬁcations when we update the ﬁrst T term approximations:

(cid:13)n

1. When updating the parameters {μi, si : i = 1, . . . , N} of the Gaussians in the term ap-
proximations, we approximate a Student’s t-distribution by means of a Gaussian distribu-
tion with the same mean and variance. This approximation becomes more accurate as the
degrees of freedom of the t-distribution increase.

2. When updating the parameters {a, b} of the IG in the term approximations, instead of
propagating the sufﬁcient statistics of an IG distribution we propagate the expectations of
1/σ2 and 1/σ4. To achieve this, we have to perform two approximations like the one stated
above. Note that in this case we are not minimizing the direct K-L divergence. However,
at convergence, we expect the resulting IG in (7) to be sufﬁciently accurate.

In order to improve convergence, we re-update all the N last term approximations each time one
of the ﬁrst T term approximations is updated. Computational complexity does not get worse than
O(T N) and the resulting algorithm turns out to be faster. By comparison, the MCMC method
in [11] takes O(N 2) steps to generate a single sample from P(γ|y, X). On problems of much
smaller size than we will consider in our experiments, one typically requires on the order of 10000
samples to obtain reasonably accurate estimates [10].

3.2 EP for gene regulation
We approximate P(τ , γ, β, σ2|X) by the factorized exponential distribution

Q(τ , γ, β, σ2) =

⎡
⎣ N(cid:2)
⎡
⎣ N(cid:2)

j=1

N(cid:2)
N(cid:2)

i=1,i(cid:2)=j

j=1

i=1,i(cid:2)=j

(cid:6)

Bern(τi, ti)

⎤
⎦ (cid:4)
⎤
⎦
N (βj,i, μj,i, sj,i)

Bern(γj,i, wj,i)

N(cid:2)
⎡
⎣ N(cid:2)

i=1

j=1

⎤
⎦ ,

IG(σ2

j , aj, bj)

where {aj, bj, ti, wj,i, μj,i, sj,i : i = 1, . . . , N ; j = 1, . . . , N ; i (cid:7)= j} are free parameters. The
(cid:13)N
posterior probability P(τ|X) that indicates which genes are more likely to be regulators can then
i=1 Bern(τi, ti). Again, we ﬁx the parameters in Q(τ , γ, β, σ2) by means of
be approximated by
EP over the joint density P(τ , γ, β, σ2|X). It is trivial to adapt the EP algorithm used in the sparse
linear regression model to this new case: the terms to be approximated are the same as before except
for the new N(N − 1) terms for the prior on γ. As in the previous section and in order to improve
convergence, we re-update all the N(N − 1) term approximations corresponding to the prior on β
each time N of the N(T − 1) term approximations corresponding to regressions are updated. In
order to reduce memory requirements, we associate all the N(N − 1) terms for the prior on β into
a single term, which we can do because they are independent so that we only store in memory one
term approximation instead of N(N − 1). We also group the N(N − 1) terms for the prior on γ
into N independent terms and the N(T − 1) terms for the regressions into T − 1 independent terms.
Assuming a constant number of iterations (in our experiments, we need at most 20 iterations for EP
to converge), the computational complexity and the memory requirements of the resulting algorithm
are O(T N 2). This indicates that it is feasible to analyze data sets which contain the expression
pattern of thousands of genes. An MCMC algorithm would require O(N 3) to generate just a single
sample.

5

4 Experiments with artiﬁcial data

We carried out experiments with artiﬁcially generated data in order to validate the EP algorithms.
In the experiments for sparse linear regression we ﬁxed the hyperparameters in (3) so that ν = 3,
λ is the sample variance of the target vector y, v1 = 1, δ = N−1, v0 is chosen according to
(2) and w = N−1. In the experiment for gene regulation we ﬁxed the hyperparameters in (4) so
that w = (N − 1)−1, νi = 3 and λi is the sample variance of the vector xi, w1 = 10−1(N −
1)−1, w0 = 10−2(N − 1)−1, v1 = 1, δ = 0.2 and v0 is chosen according to (2). Although the
posterior probabilities are sensitive to some of the choices, the orderings of these probabilities, e.g.,
to determine the most likely regulators, are robust to even large changes.

4.1 Sparse linear regression
In the ﬁrst experiment we set T = 50 and generated x1, . . . , x6000 ∼ N (0, 32I) candidate vectors
and a target vector y = x1 − x2 + 0.5 x3 − 0.5 x4 + ε, where ε ∼ N (0, I). The EP algorithm
assigned values close to 1 to w1 and w2, the parameters w3 and w4 obtained values 5.2 · 10−3 and
0.5 respectively and w5, . . . , w6000 were smaller than 3 · 10−4. We repeated the experiment several
times (each time using new data) and obtained similar results on each run.
In the second experiment we set T = 50 and generated a target vector y ∼ N (0, 32I) and
x1, . . . , x500 candidate vectors so that xi = y + εi for i = 2, . . . , 500, where εi ∼ N (0, I).
The candidate vector x1 is generated as x1 = y + 0.5 ε1 where ε1 ∼ N (0, I). This way, the noise
in x1 is twice as small as the noise in the other candidate vectors. Note that all the candidate vec-
tors are highly correlated with each other and with the target vector. This is what happens in gene
expression data sets where many genes show similar expression patterns. We ran the EP algorithm
100 times (each time using new data) and it always assigned to all the w1, . . . , w500 more or less the
same value of 6 · 10−4. However, w1 obtained the highest value on 54 of the runs and it was among
the three ws with highest value on 87 of the runs.
Finally, we repeated these experiments setting N = 100, using the MCMC method of [11] and the
EP algorithm for sparse linear regression. Both techniques produced results that are statistically
indistinguishable (the approximations obtained through EP fall within the variation of the MCMC
method), for EP within a fraction of the time of MCMC.

4.2 Gene regulation
In this experiment we set T = 50 and generated a vector z with T + 1 values from a sinusoid. We
then generated 49 more vectors x2, ..., x50 where xi,t = zt+εi,t for i = 2, . . . , 50 and t = 1, . . . , T ,
where εi,t ∼ N (0, σ2) and σ is one fourth of the sample standard deviation of z. We also generated
a vector x1 so that x1,t = zt+1 + εt where t = 1, . . . , T and εt ∼ N (0, σ2). In this way, x1 acts as
a regulator for x2, ..., x50. A single realization of the vectors x1, . . . , x50 is displayed on the left of
Figure 2. We ran the EP algorithm for gene regulation over 100 different realizations of x1, . . . , x50.
The algorithm assigned t1 the highest value on 33 of the runs and x1 was ranked among the top ﬁve
on 74 of the runs. This indicates that the EP algorithm can successfully detect small differences in
correlations and should be able to ﬁnd new regulators in real microarray data.

5 Experiments with real microarray data

We applied our algorithm to four data sets. The ﬁrst is a yeast cell-cycle data set from [5] which is
commonly used as a benchmark for regulator discovery. Data sets two through four are from three
different Plasmodium strains [6]. Missing values were imputed by nearest neighbors [14] and the
hyperparameters were ﬁxed at the same values as in Section 4. The yeast cdc15 data set contains
23 measurements of 6178 genes. We singled out 751 genes which met a minimum criterion for cell
cycle regulation [5]. The top ten genes with the highest values for τ along with their annotation from
the Saccharomyces Genome database are listed in table 5: the top two genes are speciﬁc transcription
factors and IOC2 is associated with transcription regulation. As 4% of the yeast genome is associated
with transcription the probability of this occurring by chance is 0.0062. However, although the result
is statistically signiﬁcant, we were disappointed to ﬁnd none of the known cell-cycle regulators (like
ACE2, FKH* or SWI*) among the top ten.

6

i

n
o
s
s
e
r
p
x
E

5
.
1

0
.
1

5
.
0

0
.
0

5
.
0
−

0
.
1
−

5
.
1
−

Regulator
Regulatees

Gene PF11_321
Genes positively regulated
Genes negatively regulated

3

2

1

0

1
−

2
−

i

n
o
s
s
e
r
p
x
E

0

10

20

30

40

50

0

10

20

30

40

50

Measurement

Measurement

Figure 2: Left: Plot of the vectors x2, ..., x50 in grey and the vector x1 in black. The vector x1
contains the expression of a regulator which would determine the expressions in x2, ..., x50. Right:
Expressions of gene PF11 321 (black) and the 100 genes which are more likely to be regulated by it
(light and dark grey). Two clusters of positively and negatively regulated genes can be appreciated.

rank

standard

name

common

name

annotation

CHA4
1 YLR098c
2 YOR315w SFG1
JEM1
3 YJL073w
4 YOR023c
AHC1
5 YOR105w -
6 YLR095w IOC2
7 YOR321w PMT3
8 YLR231c
BNA5
9 YOR248w -
10 YOR247w SRL1

DNA binding transcriptional activator
putative transcription factor for growth of superﬁcial pseudohyphae
DNAJ-like chaperone
subunit of the ADA histone acetyl transferase complex
dubious open reading frame
transcription elongation
protein O-mannosyl transferase
kynureninase
dubious open reading frame
mannoprotein

The three data sets for the malaria parasite [6] contain 53 measurements (3D7), 50 measurements
(Dd2) and 48 measurements (HB3). We focus on 3D7 as this is the sequenced reference strain. We
singled out 751 genes who showed the highest variation as quantiﬁed by the interquartile range of the
expression measurements. The top ten genes with the highest values for τ along with their annotation
from PlasmoDB are listed in table 5. Recalling the motivation for our approach, the paucity of known
transcription factors, we cannot expect to ﬁnd many annotated regulators in PlasmoDB version 5.4.
Thus, we list the BLASTP hits provided by PlasmoDB instead of the absent annotation. These
hits were the highest scoring ones outside of the genus Plasmodium. We ﬁnd four genes with a
large identity to transcription factors in Dictyostelium (a recently sequenced social amoebe) and one
annotated helicase which typically functions in post-transcriptional regulation. Interestingly three
genes have no known function and could be regulators.

rank

standard name

annotation or selected BLASTP hits

PFC0950c
PF11 0321
PFI1210w

1
2
3
4 MAL6P1.233
5
6 MAL7P1.34
7 MAL6P1.182
8
9
10 MAL13P1.14

PFD0175c

PF13 0140
PF13 0138

25% identity to GATA binding TF in Dictyostelium
25% identity to putative WRKY TF in Dictyostelium
no BLASTP matches outside Plasmodium genus
no BLASTP matches outside Plasmodium genus
32% identity to GATA binding TF in Dictyostelium
35% identity to GATA binding TF in Dictyostelium
N-acetylglucosaminyl-phosphatidylinositol de-n-acetylase
dihydrofolate synthase/folylpolyglutamate synthase
no BLASTP matches outside Plasmodium genus
DEAD box helicase

Results for the HB3 strain were similar in that ﬁve putative regulators were found. Somewhat
disappointing, we found only one putative regulator (a helicase) among the top ten genes for Dd2.

7

6 Conclusion and discussion

Our approach enters a ﬁeld full of methods enforcing sparsity ([15, 8, 7, 16, 9]). Our main contri-
butions are: a hierarchical model to discover regulators, a tractable algorithm for fast approximate
inference in models with many interacting variables, and the application to malaria.

Arguably most related is the hierarchical model in [15]. The covariates in this model are a dozen
external variables, coding experimental conditions, instead of the hundreds of expression levels of
other genes as in our model. Furthermore, the prior in [15] enforces sparsity on the “columns” of
β to implement the idea that some genes are not inﬂuenced by any of the experimental conditions.
Our prior, on the other hand, enforces sparsity on the “rows” in order to ﬁnd regulators.

Future work could include more involved priors, e.g., enforcing sparsity on both “rows” and
“columns” or incorporating information from DNA sequence data. The approximate inference tech-
niques described in this paper make it feasible to evaluate such extensions in a fraction of the time
required by MCMC methods.

References
[1] T.S. Gardner and J.J. Faith. Reverse-engineering transcription control networks. Physics of

Life Reviews, 2:65–88, 2005.

[2] R. Coulson, N. Hall, and C. Ouzounis. Comparative genomics of transcriptional control in the

human malaria parasite Plasmodium falciparum. Genome Res., 14:1548–1554, 2004.

[3] S. Balaji, M.M. Babu, L.M. Iyer, and L. Aravind. Discovery of the principal speciﬁc transcrip-
tion factors of apicomplexa and their implication for the evolution of the ap2-integrase dna
binding domains. Nucleic Acids Research, 33(13):3994–4006, 2005.

[4] T. Sakata and E.A. Winzeler. Genomics, systems biology and drug development for infectuous

diseases. Molecular BioSystems, 3:841–848, 2007.

[5] P.T. Spellman, G. Sherlock, V.R. Iyer, K. Anders, M.B. Eisen, P.O. Brown, and D. Botstein.
Comprehensive identiﬁcation of cell cycle-regulated genes of the yeast Saccharomyces cere-
visiae by microarray hybridization. Molecular Biology of the Cell, 9(12):3273–3297, 1998.

[6] M. LLinas, Z. Bozdech, E. D. Wong, A.T. Adai, and J. L. DeRisi. Comparative whole
genome transcriptome analysis of three Plasmodium falciparum strains. Nucleic Acids Re-
search, 34(4):1166–1173, 2006.

[7] M. Beal. Variational Algorithms for Approximate Bayesian Inference. PhD thesis, UCL, 2003.
[8] C. Sabatti and G.M. James. Bayesian sparse hidden components analysis for transcription

regulation networks. Bioinformatics, 22(6):739–746, 2006.

[9] S.T. Jensen, G. Chen, and C.J. Stoeckert. Bayesian variable selection and data integration for

biological regulatory networks. The Annals of Applied Statistics, 1:612–633, 2007.

[10] E.I. George and R.E. McCulloch. Approaches for Bayesian variable selection. Statistica

Sinica, 7:339–374, 1997.

[11] E.I. George and R.E. McCulloch. Variable selection via Gibbs sampling. Journal of the Amer-

ican Statistical Association, 88(423):881–889, 1993.

[12] T. Minka. A family of algorithms for approximate Bayesian inference. PhD thesis, MIT, 2001.
[13] T. Heskes and O. Zoeter. Expectation propagation for approximate inference in dynamic

Bayesian networks. In UAI-2002, pages 216–223, 2002.

[14] O. Troyanskaya, M. Cantor, P. Brown, T. Hastie, R. Tibshirani, and D. Botstein. Missing value

estimation methods for dna microarrays. Bioinformatics, 17(6):520–525, 2001.

[15] J. Lucas, C. Carvalho, Q. Wang, A. Bild, J. Nevins, and M. West. Sparse statistical modelling
In K.A. Do, P. M¨uller, and M. Vannucci, editors, Bayesian

in gene expression genomics.
inference for gene expression and proteomics. Springer, 2006.

[16] M.Y. Park, T. Hastie, and R. Tibshirani. Averaged gene expressions for regression. Biostatis-

tics, 8:212–227, 2007.

8

"
628,2007,Learning Bounds for Domain Adaptation,"Empirical risk minimization offers well-known learning guarantees when training and test data come from the same domain. In the real world, though, we often wish to adapt a classifier from a source domain with a large amount of training data to different target domain with very little training data. In this work we give uniform convergence bounds for algorithms that minimize a convex combination of source and target empirical risk. The bounds explicitly model the inherent trade-off between training on a large but inaccurate source data set and a small but accurate target training set. Our theory also gives results when we have multiple source domains, each of which may have a different number of instances, and we exhibit cases in which minimizing a non-uniform combination of source risks can achieve much lower target error than standard empirical risk minimization.","LearningBoundsforDomainAdaptationJohnBlitzer,KobyCrammer,AlexKulesza,FernandoPereira,andJenniferWortmanDepartmentofComputerandInformationScienceUniversityofPennsylvania,Philadelphia,PA19146{blitzer,crammer,kulesza,pereira,wortmanj}@cis.upenn.eduAbstractEmpiricalriskminimizationofferswell-knownlearningguaranteeswhentrainingandtestdatacomefromthesamedomain.Intherealworld,though,weoftenwishtoadaptaclassiﬁerfromasourcedomainwithalargeamountoftrainingdatatodifferenttargetdomainwithverylittletrainingdata.Inthisworkwegiveuniformconvergenceboundsforalgorithmsthatminimizeaconvexcombinationofsourceandtargetempiricalrisk.Theboundsexplicitlymodeltheinherenttrade-offbetweentrainingonalargebutinaccuratesourcedatasetandasmallbutaccuratetargettrainingset.Ourtheoryalsogivesresultswhenwehavemultiplesourcedomains,eachofwhichmayhaveadifferentnumberofinstances,andweexhibitcasesinwhichminimizinganon-uniformcombinationofsourceriskscanachievemuchlowertargeterrorthanstandardempiricalriskminimization.1IntroductionDomainadaptationaddressesacommonsituationthatariseswhenapplyingmachinelearningtodi-versedata.Wehaveampledatadrawnfromasourcedomaintotrainamodel,butlittleornotrainingdatafromthetargetdomainwherewewishtousethemodel[17,3,10,5,9].Domainadaptationquestionsariseinnearlyeveryapplicationofmachinelearning.Infacerecognitionsystems,trainingimagesareobtainedunderonesetoflightingorocclusionconditionswhiletherecognizerwillbeusedunderdifferentconditions[14].Inspeechrecognition,acousticmodelstrainedbyonespeakerneedtobeusedbyanother[12].Innaturallanguageprocessing,part-of-speechtaggers,parsers,anddocumentclassiﬁersaretrainedoncarefullyannotatedtrainingsets,butappliedtotextsfromdifferentgenresorstyles[7,6].Whilemanydomain-adaptationalgorithmshavebeenproposed,thereareonlyafewtheoreticalstudiesoftheproblem[3,10].Thosestudiesfocusonthecasewheretrainingdataisdrawnfromasourcedomainandtestdataisdrawnfromadifferenttargetdomain.Wegeneralizethisapproachtothecasewherewehavesomelabeleddatafromthetargetdomaininadditiontoalargeamountoflabeledsourcedata.Ourmainresultisauniformconvergenceboundonthetruetargetriskofamodeltrainedtominimizeaconvexcombinationofempiricalsourceandtargetrisks.Thebounddescribesanintuitivetradeoffbetweenthequantityofthesourcedataandtheaccuracyofthetargetdata,andunderrelativelyweakassumptionswecancomputeitfromﬁnitelabeledandunlabeledsamplesofthesourceandtargetdistributions.Weusethetaskofsentimentclassiﬁcationtodemonstratethatourboundmakescorrectpredictionsaboutmodelerrorwithrespecttoadistancemeasurebetweensourceandtargetdomainsandthenumberoftraininginstances.Finally,weextendourtheorytothecaseinwhichwehavemultiplesourcesoftrainingdata,eachofwhichmaybedrawnaccordingtoadifferentdistributionandmaycontainadifferentnumberofinstances.Severalauthorshaveempiricallystudiedaspecialcaseofthisinwhicheachinstanceisweightedseparatelyinthelossfunction,andinstanceweightsaresettoapproximatethetargetdomaindistribution[10,5,9,11].Wegiveauniformconvergenceboundforalgorithmsthatmin-1imizeaconvexcombinationofmultipleempiricalsourcerisksandweshowthatthesealgorithmscanoutperformstandardempiricalriskminimization.2ARigorousModelofDomainAdaptationWeformalizedomainadaptationforbinaryclassiﬁcationasfollows.AdomainisapairconsistingofadistributionDonXandalabelingfunctionf:X→[0,1].1Initiallyweconsidertwodomains,asourcedomainhDS,fSiandatargetdomainhDT,fTi.Ahypothesisisafunctionh:X→{0,1}.TheprobabilityaccordingthedistributionDSthatahypothesishdisagreeswithalabelingfunctionf(whichcanalsobeahypothesis)isdeﬁnedasǫS(h,f)=Ex∼DS[|h(x)−f(x)|].Whenwewanttorefertotheriskofahypothesis,weusetheshorthandǫS(h)=ǫS(h,fS).WewritetheempiricalriskofahypothesisonthesourcedomainasˆǫS(h).WeusetheparallelnotationǫT(h,f),ǫT(h),andˆǫT(h)forthetargetdomain.WemeasurethedistancebetweentwodistributionsDandD′usingahypothesisclass-speciﬁcdis-tancemeasure.LetHbeahypothesisclassforinstancespaceX,andAHbethesetofsubsetsofXthatarethesupportofsomehypothesisinH.Inotherwords,foreveryhypothesish∈H,{x:x∈X,h(x)=1}∈AH.Wedeﬁnethedistancebetweentwodistributionsas:dH(D,D′)=2supA∈AH|PrD[A]−PrD′[A]|.Forourpurposes,thedistancedHhasanimportantadvantageovermorecommonmeansforcom-paringdistributionssuchasL1distanceortheKLdivergence:wecancomputedHfromﬁniteunlabeledsamplesofthedistributionsDandD′whenHhasﬁniteVCdimension[4].Furthermore,wecancomputeaﬁnite-sampleapproximationtodHbyﬁndingaclassiﬁerh∈Hthatmaximallydiscriminatesbetween(unlabeled)instancesfromDandD′[3].ForahypothesisspaceH,wedeﬁnethesymmetricdifferencehypothesisspaceH∆HasH∆H={h(x)⊕h′(x):h,h′∈H},where⊕istheXORoperator.Eachhypothesisg∈H∆HlabelsaspositiveallpointsxonwhichagivenpairofhypothesesinHdisagree.WecanthendeﬁneAH∆HinthenaturalwayasthesetofallsetsAsuchthatA={x:x∈X,h(x)6=h′(x)}forsomeh,h′∈H.ThisallowsustodeﬁneasaboveadistancedH∆Hthatsatisﬁesthefollowingusefulinequalityforanyhypothesesh,h′∈H,whichisstraight-forwardtoprove:|ǫS(h,h′)−ǫT(h,h′)|≤12dH∆H(DS,DT).Weformalizethedifferencebetweenlabelingfunctionsbymeasuringerrorrelativetootherhypothe-sesinourclass.Theidealhypothesisminimizescombinedsourceandtargetrisk:h∗=argminh∈HǫS(h)+ǫT(h).Wedenotethecombinedriskoftheidealhypothesisbyλ=ǫS(h∗)+ǫT(h∗).Theidealhypothesisexplicitlyembodiesournotionofadaptability.Whentheidealhypothesisperformspoorly,wecannotexpecttolearnagoodtargetclassiﬁerbyminimizingsourceerror.2Ontheotherhand,forthekindsoftasksmentionedinSection1,weexpectλtobesmall.Ifthisisthecase,wecanreasonablyapproximatetargetriskusingsourceriskandthedistancebetweenDSandDT.Weillustratethekindofresultavailableinthissettingwiththefollowingboundonthetargetriskintermsofthesourcerisk,thedifferencebetweenlabelingfunctionsfSandfT,andthedistancebetweenthedistributionsDSandDT.ThisboundisessentiallyarestatementofthemaintheoremofBen-Davidetal.[3],withasmallcorrectiontothestatementoftheirtheorem.1Thisnotionofdomainisnotthedomainofafunction.Toavoidconfusion,wewillalwaysmeanaspeciﬁcdistributionandfunctionpairwhenwesaydomain.2Ofcourseitisstillpossiblethatthesourcedatacontainsrelevantinformationaboutthetargetfunctionevenwhentheidealhypothesisperformspoorly—suppose,forexample,thatfS(x)=1ifandonlyiffT(x)=0—butaclassiﬁertrainedusingsourcedatawillperformpoorlyondatafromthetargetdomaininthiscase.2Theorem1LetHbeahypothesisspaceofVC-dimensiondandUS,UTbeunlabeledsamplesofsizem′each,drawnfromDSandDT,respectively.LetˆdH∆HbetheempiricaldistanceonUS,UT,inducedbythesymmetricdifferencehypothesisspace.Withprobabilityatleast1−δ(overthechoiceofthesamples),foreveryh∈H,ǫT(h)≤ǫS(h)+12ˆdH∆H(US,UT)+4s2dlog(2m′)+log(4δ)m′+λ.ThecorrectedproofofthisresultcanbefoundAppendixA.3Themainstepintheproofisavariantofthetriangleinequalityinwhichthesidesofthetrianglerepresenterrorsbetweendifferentdecisionrules[3,8].Theboundisrelativetoλ.Whenthecombinederroroftheidealhypothesisislarge,thereisnoclassiﬁerthatperformswellonboththesourceandtargetdomains,sowecannothopetoﬁndagoodtargethypothesisbytrainingonlyonthesourcedomain.Ontheotherhand,forsmallλ(themostrelevantcasefordomainadaptation),Theorem1showsthatsourceerrorandunlabeledH∆H-distanceareimportantquantitiesforcomputingtargeterror.3ALearningBoundCombiningSourceandTargetDataTheorem1showshowtorelatesourceandtargetrisk.Wenowproceedtogivealearningboundforempiricalriskminimizationusingcombinedsourceandtargettrainingdata.Inordertosimplifythepresentationofthetrade-offsthatariseinthisscenario,westatetheboundintermsofVCdimension.Similar,tighterboundscouldbederivedusingmoresophisticatedmeasuresofcomplexitysuchasPAC-Bayes[15]orRademachercomplexity[2]inananalogousway.AttraintimealearnerreceivesasampleS=(ST,SS)ofminstances,whereSTconsistsofβminstancesdrawnindependentlyfromDTandSSconsistsof(1−β)minstancesdrawnindependentlyfromDS.ThegoalofalearneristoﬁndahypothesisthatminimizestargetriskǫT(h).Whenβissmall,asindomainadaptation,minimizingempiricaltargetriskmaynotbethebestchoice.Weanalyzelearnersthatinsteadminimizeaconvexcombinationofempiricalsourceandtargetrisk:ˆǫα(h)=αˆǫT(h)+(1−α)ˆǫS(h)Wedenoteasǫα(h)thecorrespondingweightedcombinationoftruesourceandtargetrisks,mea-suredwithrespecttoDSandDT.Weboundthetargetriskofadomainadaptationalgorithmthatminimizesˆǫα(h).Theproofoftheboundhastwomaincomponents,whichwestateaslemmasbelow.FirstweboundthedifferencebetweenthetargetriskǫT(h)andweightedriskǫα(h).Thenweboundthedifferencebetweenthetrueandempiricalweightedrisksǫα(h)andˆǫα(h).Theproofsoftheselemmas,aswellastheproofofTheorem2,areinAppendixB.Lemma1LethbeahypothesisinclassH.Then|ǫα(h)−ǫT(h)|≤(1−α)(cid:18)12dH∆H(DS,DT)+λ(cid:19).Thelemmashowsthatasαapproaches1,werelyincreasinglyonthetargetdata,andthedistancebetweendomainsmatterslessandless.TheproofusesasimilartechniquetothatofTheorem1.Lemma2LetHbeahypothesisspaceofVC-dimensiond.IfarandomlabeledsampleofsizemisgeneratedbydrawingβmpointsfromDTand(1−β)mpointsfromDS,andlabelingthemaccordingtofSandfTrespectively,thenwithprobabilityatleast1−δ(overthechoiceofthesamples),foreveryh∈H|ˆǫα(h)−ǫα(h)|<sα2β+(1−α)21−βrdlog(2m)−logδ2m.3Alongerversionofthispaperthatincludestheomittedappendixcanbefoundontheauthors’websites.3Theproofissimilartostandarduniformconvergenceproofs[16,1],butitusesHoeffding’sin-equalityinadifferentwaybecausetheboundontherangeoftherandomvariablesunderlyingtheinequalityvarieswithαandβ.Thelemmashowsthatasαmovesawayfromβ(whereeachinstanceisweightedequally),ourﬁnitesampleapproximationtoǫα(h)becomeslessreliable.Theorem2LetHbeahypothesisspaceofVC-dimensiond.LetUSandUTbeunlabeledsamplesofsizem′each,drawnfromDSandDTrespectively.LetSbealabeledsampleofsizemgeneratedbydrawingβmpointsfromDTand(1−β)mpointsfromDS,labelingthemaccordingtofSandfT,respectively.Ifˆh∈Histheempiricalminimizerofˆǫα(h)onSandh∗T=minh∈HǫT(h)isthetargetriskminimizer,thenwithprobabilityatleast1−δ(overthechoiceofthesamples),ǫT(ˆh)≤ǫT(h∗T)+2sα2β+(1−α)21−βrdlog(2m)−logδ2m+2(1−α)12ˆdH∆H(US,UT)+4s2dlog(2m′)+log(4δ)m′+λ.Whenα=0(thatis,weignoretargetdata),theboundisidenticaltothatofTheorem1,butwithanempiricalestimateforthesourceerror.Similarlywhenα=1(thatis,weuseonlytargetdata),theboundisthestandardlearningboundusingonlytargetdata.Attheoptimalα(whichminimizestherighthandside),theboundisalwaysatleastastightaseitherofthesetwosettings.Finallynotethatbychoosingdifferentvaluesofα,theboundallowsustoeffectivelytradeoffthesmallamountoftargetdataagainstthelargeamountoflessrelevantsourcedata.Weremarkthatwhenitisknownthatλ=0,thedependenceonminTheorem2canbeimproved;thiscorrespondstotherestrictedorrealizablesetting.4ExperimentalResultsWeevaluateourtheorybycomparingitspredictionstoempiricalresults.WhileideallyTheorem2couldbedirectlycomparedwithtesterror,thisisnotpracticalbecauseλisunknown,dH∆Hiscomputationallyintractable[3],andtheVCdimensiondistoolargetobeausefulmeasureofcomplexity.Instead,wedevelopasimpleapproximationofTheorem2thatwecancomputefromunlabeleddata.Formanyadaptationtasks,λissmall(thereexistsaclassiﬁerwhichissimultane-ouslygoodforbothdomains),soweignoreithere.WeapproximatedH∆Hbytrainingalinearclassiﬁertodiscriminatebetweenthetwodomains.Weuseastandardhingeloss(normalizedbydividingbythenumberofinstances)andapplythequantity1−(cid:0)hingeloss(cid:1)inplaceoftheactualdH∆H.Letζ(US,UT)beourapproximationtodH∆H,computedfromsourceandtargetunlabeleddata.Fordomainsthatcanbeperfectlyseparatedwithmargin,ζ(US,UT)=1.Fordomainsthatareindistinguishable,ζ(US,UT)=0.FinallywereplacetheVCdimensionsamplecomplexitytermwithatighterconstantC.TheresultingapproximationtotheboundofTheorem2isf(α)=sCm(cid:18)α2β+(1−α)21−β(cid:19)+(1−α)ζ(US,UT).(1)Ourexperimentalresultsareforthetaskofsentimentclassiﬁcation.Sentimentclassiﬁcationsystemshaverecentlygainedpopularitybecauseoftheirpotentialapplicabilitytoawiderangeofdocumentsinmanygenres,fromcongressionalrecordstoﬁnancialnews.Becauseofthelargenumberofpotentialgenres,sentimentclassiﬁcationisanidealareafordomainadaptation.WeusethedataprovidedbyBlitzeretal.[6],whichconsistsofreviewsofeighttypesofproductsfromAmazon.com:apparel,books,DVDs,electronics,kitchenappliances,music,video,andacatchallcategory“other”.Thetaskisbinaryclassiﬁcation:givenareview,predictwhetheritispositive(4or5outof5stars)ornegative(1or2stars).Wechosethe“apparel”domainasourtargetdomain,andalloftheplotsontheright-handsideofFigure1areforthisdomain.Weobtainempiricalcurvesfortheerrorasafunctionofαbytrainingaclassiﬁerusingaweightedhingeloss.Supposethetargetdomainhasweightαandthereareβmtargettraininginstances.Thenwescalethelossoftargettraininginstancebyα/βandthelossofasourcetraininginstanceby(1−α)/(1−β).4(a)varydistance,mS=2500,(c)ζ(US,UT)=0.715,(e)ζ(US,UT)=0.715,mT=1000mS=2500,varymTvarymS,mT=250000.20.40.60.81Dist: 0.780Dist: 0.715Dist: 0.447Dist: 0.33600.20.40.60.81mT: 250mT: 500mT: 1000mT: 200000.20.40.60.81mS: 250mS: 500mS: 1000mS: 2500(b)varysources,mS=2500,(d)source=dvd,mS=2500,(f)source=dvd,mT=1000varymTvarymS,mT=250000.10.40.60.81books: 0.78dvd: 0.715electronics: 0.447kitchen: 0.33600.20.40.60.81mT: 250mT: 500mT: 1000mT: 20000  0.20.40.60.81  mS: 250mS: 500mS: 1000mS: 2500Figure1:Comparingtheboundwithtesterrorforsentimentclassiﬁcation.Thex-axisofeachﬁgureshowsα.They-axisshowsthevalueoftheboundortestseterror.(a),(c),and(e)depictthebound,(b),(d),and(f)thetesterror.Eachcurvein(a)and(b)representsadifferentdistance.Curvesin(c)and(d)representdifferentnumbersoftargetinstances.Curvesin(e)and(f)representdifferentnumbersofsourceinstances.Figure1showsaseriesofplotsofequation1(onthetop)coupledwithcorrespondingplotsoftesterror(onthebottom)asafunctionofαfordifferentamountsofsourceandtargetdataanddifferentdistancesbetweendomains.Ineachpairofplots,asingleparameter(distance,numberoftargetinstancesmT,ornumberofsourceinstancesmS)isvariedwhiletheothertwoareheldconstant.Notethatβ=mT/(mT+mS).TheplotsonthetoppartofFigure1arenotmeanttobenumericalproxiesforthetrueerror(Forthesourcedomains“books”and“dvd”,thedistancealoneiswellabove12).Instead,theyarescaledtoillustratethattheboundissimilarinshapetothetrueerrorcurveandthatrelativerelationshipsarepreserved.BychoosingadifferentCinequation1foreachcurve,onecanachievecompletecontrolovertheirminima.Inordertoavoidthis,weonlyuseasinglevalueofC=1600forall12curvesonthetoppartofFigure1.Firstnotethatineverypairofplots,theempiricalerrorcurveshavearoughlyconvexshapethatmimicstheshapeofthebounds.Furthermorethevalueofαwhichminimizestheboundalsohasalowempiricalerrorforeachcorrespondingcurve.ThissuggeststhatchoosingαtominimizetheboundofTheorem2andsubsequentlytrainingaclassiﬁertominimizetheempiricalerrorˆǫα(h)canworkwellinpractice,providedwehaveareasonablemeasureofcomplexity.4Figures1aand1bshowthatmoredistantsourcedomainsresultinhighertargeterror.Figures1cand1dillustratethatformoretargetdata,wehavenotonlylowererroringeneral,butalsoahigherminimizingα.Finally,ﬁgures1eand1fdepictthelimitationofdistantsourcedata.Withenoughtargetdata,nomatterhowmuchsourcedataweinclude,wealwaysprefertouseonlythetargetdata.Thisisreﬂectedinourboundasaphasetransitioninthevalueoftheoptimalα(governingthetradeoffbetweensourceandtargetdata).ThephasetransitionoccurswhenmT=C/ζ(US,UT)2(SeeFigure2).4AlthoughTheorem2doesnotholduniformlyforallαasstated,thisiseasilyremediedviaanapplicationoftheunionbound.Theresultingboundwillcontainanadditionallogarithmicfactorinthecomplexityterm.5SourceTarget ×102  5,00050,000722,00011 million167 million323028262400.51Figure2:Anexampleofthephasetransitionintheoptimalα.Thevalueofαwhichminimizestheboundisindicatedbytheintensity,whereblackmeansα=1(correspondingtoignoringsourceandlearningonlyfromtargetdata).WeﬁxC=1600andζ(US,UT)=0.715,asinoursentimentresults.Thex-axisshowsthenumberofsourceinstances(log-scale).They-axisshowsthenumberoftargetinstances.Aphasetransitionoccursat3,130targetinstances.Withmoretargetinstancesthanthis,itismoreeffectivetoignoreevenaninﬁniteamountofsourcedata.5LearningfromMultipleSourcesWenowexploreanextensionofourtheorytothecaseofmultiplesourcedomains.Wearepre-sentedwithdatafromNdistinctsources.EachsourceSjisassociatedwithanunknownunderlyingdistributionDjoverinputpointsandanunknownlabelingfunctionfj.FromeachsourceSj,wearegivenmjlabeledtraininginstances,andourgoalistousetheseinstancestotrainamodeltoperformwellonatargetdomainhDT,fTi,whichmayormaynotbeoneofthesources.Thissettingismotivatedbyseveralnewdomainadaptationalgorithms[10,5,11,9]thatweighthelossfromtraininginstancesdependingonhow“far”theyarefromthetargetdomain.Thatis,eachtraininginstanceisitsownsourcedomain.Asintheprevioussections,wewillexaminealgorithmsthatminimizeconvexcombinationsoftrainingerrorsoverthelabeledexamplesfromeachsourcedomain.Asbefore,weletmj=βjmwithPNj=1βj=1.Givenavectorα=(α1,···,αN)ofdomainweightswithPjαj=1,wedeﬁnetheempiricalα-weightederroroffunctionhasˆǫα(h)=NXj=1αjˆǫj(h)=NXj=1αjmjXx∈Sj|h(x)−fj(x)|.Thetrueα-weightederrorǫα(h)isdeﬁnedanalogously.LetDαbeamixtureoftheNsourcedistributionswithmixingweightsequaltothecomponentsofα.Finally,analogoustoλinthesingle-sourcesetting,wedeﬁnetheerrorofthemulti-sourceidealhypothesisforaweightingαasγα=minh{ǫT(h)+ǫα(h)}=minh{ǫT(h)+NXj=1αjǫj(h)}.Thefollowingtheoremgivesalearningboundforempiricalriskminimizationusingtheempiricalα-weightederror.Theorem3SupposewearegivenmjlabeledinstancesfromsourceSjforj=1...N.Foraﬁxedvectorofweightsα,letˆh=argminh∈Hˆǫα(h),andleth∗T=argminh∈HǫT(h).Thenforanyδ∈(0,1),withprobabilityatleast1−δ(overthechoiceofsamplesfromeachsource),ǫT(ˆh)≤ǫT(h∗T)+2vuutNXj=1α2jβjrdlog2m−logδ2m+2(cid:18)γα+12dH∆H(Dα,DT)(cid:19).6(a)Source.Moregirlsthanboys(b)Target.Separatorfrom(c)WeightingsourcestomatchuniformmixtureissuboptimaltargetisoptimalFemalesMaleslearnedseparatoroptimalseparatorlearnedseparatorerrorsFemalesMalesTargetoptimal &learnedseparatorFigure3:A1-dimensionalexampleillustratinghownon-uniformmixtureweightingcanresultinoptimalerror.Weobserveonefeature,whichweusetopredictgender.(a)Attraintimeweobservemorefemalesthanmales.(b)Learningbyuniformlyweightingthetrainingdatacausesustolearnasuboptimaldecisionboundary,(c)butbyweightingthemalesmorehighly,wecanmatchthetargetdataandlearnanoptimalclassiﬁer.ThefullproofisinappendixC.LiketheproofofTheorem2,itissplitintotwoparts.Theﬁrstpartboundsthedifferencebetweentheα-weightederrorandthetargeterrorsimilartolemma1.Thesecondisauniformconvergenceboundforˆǫα(h)similartolemma2.Theorem3reducestoTheorem2whenwehaveonlytwosources,oneofwhichisthetargetdomain(thatis,wehavesomesmallnumberoftargetinstances).Itismoregeneral,though,becausebymanipulatingαwecaneffectivelychangethesourcedomain.Thishastwoconsequences.First,wedemandthatthereexistsahypothesish∗whichhaslowerroronboththeα-weightedconvexcombinationofsourcesandthetargetdomain.Second,wemeasuredistancebetweenthetargetandamixtureofsources,ratherthanbetweenthetargetandasinglesource.Onequestionwemightaskiswhetherthereexistsettingswhereanon-uniformweightingcanleadtoasigniﬁcantlylowervalueoftheboundthanauniformweighting.Thiscanhappenifsomenon-uniformweightingofsourcesaccuratelyapproximatesthetargetdomain.Asahypotheticalexample,supposewearetryingtopredictgenderfromheight(Figure3).Eachinstanceisdrawnfromagender-speciﬁcGaussian.Inthisexample,wecanﬁndtheoptimalclassiﬁerbyweightingthe“males”and“females”componentsofthesourcetomatchthetarget.6RelatedWorkDomainadaptationisawidely-studiedarea,andwecannothopetocovereveryaspectandap-plicationofithere5.Instead,inthissectionwefocusonothertheoreticalapproachestodomainadaptation.Whilewedonotexplicitlyaddresstherelationshipinthispaper,wenotethatdomainadaptationiscloselyrelatedtothesettingofcovariateshift,whichhasbeenstudiedinstatistics.InadditiontotheworkofHuangetal.[10],severalotherauthorshaveconsideredlearningbyassigningseparateweightstothecomponentsofthelossfunctioncorrespondingtoseparateinstances.Bickelatal.[5]andJiangandZhai[11]suggestpromisingempiricalalgorithmsthatinpartinspireourTheorem3.Wehopethatourworkcanhelptoexplainwhenthesealgorithmsareeffective.Daietal.[9]consideredweightinginstancesusingatransfer-awarevariantofboosting,butthelearningboundstheygivearenostrongerthanboundswhichcompletelyignorethesourcedata.Crammeretal.[8]considerlearningwhenthemarginaldistributiononinstancesisthesameacrosssourcesbutthelabelingfunctionmaychange.ThiscorrespondsinourtheorytocaseswheredH∆H=0butλislarge.Likeustheyconsidermultiplesources,buttheirnotionofweightingislessgeneral.Theyconsideronlyincludingordiscardingasourceentirely.LiandBilmes[13]givePAC-Bayesianlearningboundsforadaptationusing“divergencepriors”.Theyplacesource-centeredpriorontheparametersofamodellearnedinthetargetdomain.Like5TheNIPS2006WorkshoponLearningWhenTestandTrainingInputshaveDifferentDistributions(http://ida.first.fraunhofer.de/projects/different06/)containsagoodsetofrefer-encesondomainadaptationandrelatedtopics.7ourmodel,thedivergenceprioralsoemphasizesthetradeoffbetweensourceandtarget.Inourmodel,though,wemeasurethedivergence(andconsequentlythebias)ofthesourcedomainfromunlabeleddata.Thisallowsustochoosethebesttradeoffbetweensourceandtargetlabeleddata.7ConclusionInthisworkweinvestigatethetaskofdomainadaptationwhenwehavealargeamountoftrain-ingdatafromasourcedomainbutwishtoapplyamodelinatargetdomainwithamuchsmalleramountoftrainingdata.Ourmainresultisauniformconvergencelearningboundforalgorithmswhichminimizeconvexcombinationsofsourceandtargetempiricalrisk.Ourboundreﬂectsthetrade-offbetweenthesizeofthesourcedataandtheaccuracyofthetargetdata,andwegiveasimpleapproximationtoitthatiscomputablefromﬁnitelabeledandunlabeledsamples.Thisap-proximationmakescorrectpredictionsaboutmodeltesterrorforasentimentclassiﬁcationtask.Ourtheoryalsoextendsinastraightforwardmannertoamulti-sourcesetting,whichwebelievehelpstoexplainthesuccessofrecentempiricalworkindomainadaptation.Ourfutureworkhastworelateddirections.First,wewishtotightenourbounds,bothbyconsideringmoresophisticatedmeasuresofcomplexity[15,2]andbyfocusingourdistancemeasureonthemostrelevantfeatures,ratherthanallthefeatures.WealsoplantoinvestigatealgorithmsthatchooseaconvexcombinationofmultiplesourcestominimizetheboundinTheorem3.8AcknowledgementsThismaterialisbaseduponworkpartiallysupportedbytheDefenseAdvancedResearchProjectsAgency(DARPA)underContractNo.NBCHD030010.Anyopinions,ﬁndings,andconclusionsorrecommendationsexpressedinthismaterialarethoseoftheauthorsanddonotnecessarilyreﬂecttheviewsoftheDARPAorDepartmentofInterior-NationalBusinessCenter(DOI-NBC).References[1]M.AnthonyandP.Bartlett.NeuralNetworkLearning:TheoreticalFoundations.CambridgeUniversityPress,Cambridge,1999.[2]P.BarlettandS.Mendelson.Rademacherandgaussiancomplexities:Riskboundsandstructuralresults.JMLR,3:463–482,2002.[3]S.Ben-David,J.Blitzer,K.Crammer,andF.Pereira.Analysisofrepresentationsfordomainadaptation.InNIPS,2007.[4]S.Ben-David,J.Gehrke,andD.Kifer.Detectingchangeindatastreams.InVLDB,2004.[5]S.Bickel,M.Br¨uckner,andT.Scheffer.Discriminativelearningfordifferingtrainingandtestdistribu-tions.InICML,2007.[6]J.Blitzer,M.Dredze,andF.Pereira.Biographies,bollywood,boomboxesandblenders:Domainadapta-tionforsentimentclassiﬁcation.InACL,2007.[7]C.ChelbaandA.Acero.Empiricalmethodsinnaturallanguageprocessing.InEMNLP,2004.[8]K.Crammer,M.Kearns,andJ.Wortman.Learningfrommultiplesources.InNIPS,2007.[9]W.Dai,Q.Yang,G.Xue,andY.Yu.Boostingfortransferlearning.InICML,2007.[10]J.Huang,A.Smola,A.Gretton,K.Borgwardt,andB.Schoelkopf.Correctingsampleselectionbiasbyunlabeleddata.InNIPS,2007.[11]J.JiangandC.Zhai.Instanceweightingfordomainadaptation.InACL,2007.[12]C.LegetterandP.Woodland.Maximumlikelihoodlinearregressionforspeakeradaptationofcontinuousdensityhiddenmarkovmodels.ComputerSpeechandLanguage,9:171–185,1995.[13]X.LiandJ.Bilmes.Abayesiandivergencepriorforclassiﬁcationadaptation.InAISTATS,2007.[14]A.Martinez.Recognitionofpartiallyoccludedand/orimpreciselylocalizedfacesusingaprobabilisticapproach.InCVPR,2007.[15]D.McAllester.SimpliﬁedPAC-Bayesianmarginbounds.InCOLT,2003.[16]V.Vapnik.StatisticalLearningTheory.JohnWiley,NewYork,1998.[17]P.WuandT.Dietterich.Improvingsvmaccuracybytrainingonauxiliarydatasources.InICML,2004.8"
894,2007,Unconstrained On-line Handwriting Recognition with Recurrent Neural Networks,"On-line handwriting recognition is unusual among sequence labelling tasks in that the underlying generator of the observed data, i.e. the movement of the pen, is recorded directly. However, the raw data can be difficult to interpret because each letter is spread over many pen locations. As a consequence, sophisticated pre-processing is required to obtain inputs suitable for conventional sequence labelling algorithms, such as HMMs. In this paper we describe a system capable of directly transcribing raw on-line handwriting data. The system consists of a recurrent neural network trained for sequence labelling, combined with a probabilistic language model. In experiments on an unconstrained on-line database, we record excellent results using either raw or pre-processed data, well outperforming a benchmark HMM in both cases.","Unconstrained Online Handwriting Recognition with

Recurrent Neural Networks

Alex Graves
TUM, Germany

alex@idsia.ch

Santiago Fern´andez
IDSIA, Switzerland

santiago@idsia.ch

Marcus Liwicki

University of Bern, Switzerland
liwicki@iam.unibe.ch

Horst Bunke

University of Bern, Switzerland

bunke@iam.unibe.ch

J¨urgen Schmidhuber

IDSIA, Switzerland and TUM, Germany

juergen@idsia.ch

Abstract

In online handwriting recognition the trajectory of the pen is recorded during writ-
ing. Although the trajectory provides a compact and complete representation of
the written output, it is hard to transcribe directly, because each letter is spread
over many pen locations. Most recognition systems therefore employ sophisti-
cated preprocessing techniques to put the inputs into a more localised form. How-
ever these techniques require considerable human effort, and are speciﬁc to par-
ticular languages and alphabets. This paper describes a system capable of directly
transcribing raw online handwriting data. The system consists of an advanced re-
current neural network with an output layer designed for sequence labelling, com-
bined with a probabilistic language model. In experiments on an unconstrained
online database, we record excellent results using either raw or preprocessed data,
well outperforming a state-of-the-art HMM based system in both cases.

1 Introduction

Handwriting recognition is traditionally divided into ofﬂine and online recognition. Ofﬂine recogni-
tion is performed on images of handwritten text. In online handwriting the location of the pen-tip on
a surface is recorded at regular intervals, and the task is to map from the sequence of pen positions
to the sequence of words.
At ﬁrst sight, it would seem straightforward to label raw online inputs directly. However, the fact that
each letter or word is distributed over many pen positions poses a problem for conventional sequence
labelling algorithms, which have difﬁculty processing data with long-range interdependencies. The
problem is especially acute for unconstrained handwriting, where the writing style may be cursive,
printed or a mix of the two, and the degree of interdependency is therefore difﬁcult to determine
in advance. The standard solution is to preprocess the data into a set of localised features. These
features typically include geometric properties of the trajectory in the vicinity of every data point,
pseudo-ofﬂine information from a generated image, and character level shape characteristics [6, 7].
Delayed strokes (such as the crossing of a ‘t’ or the dot of an ‘i’) require special treatment because
they split up the characters and therefore interfere with localisation. HMMs [6] and hybrid systems
incorporating time-delay neural networks and HMMs [7] are commonly trained with such features.
The issue of classifying preprocessed versus raw data has broad relevance to machine learning, and
merits further discussion. Using hand crafted features often yields superior results, and in some
cases can render classiﬁcation essentially trivial. However, there are three points to consider in
favour of raw data. Firstly, designing an effective preprocessor requires considerable time and ex-
pertise. Secondly, hand coded features tend to be more task speciﬁc. For example, features designed

1

for English handwriting could not be applied to languages with substantially different alphabets,
such as Arabic or Chinese. In contrast, a system trained directly on pen movements could be ap-
plied to any alphabet. Thirdly, using raw data allows feature extraction to be built into the classiﬁer,
and the whole system to be trained together. For example, convolutional neural networks [10], in
which a globally trained hierarchy of network layers is used to extract progressively higher level
features, have proved effective at classifying raw images, such as objects in cluttered scenes or iso-
lated handwritten characters [15, 11]. (Note than convolution nets are less suitable for unconstrained
handwriting, because they require the text images to be presegmented into characters [10]).
In this paper, we apply a recurrent neural network (RNN) to online handwriting recognition. The
RNN architecture is bidirectional Long Short-Term Memory [3], chosen for its ability to process data
with long time dependencies. The RNN uses the recently introduced connectionist temporal classi-
ﬁcation output layer [2], which was speciﬁcally designed for labelling unsegmented sequence data.
An algorithm is introduced for applying grammatical constraints to the network outputs, thereby
providing word level transcriptions. Experiments are carried out on the IAM online database [12]
which contains forms of unconstrained English text acquired from a whiteboard. The performance
of the RNN system using both raw and preprocessed input data is compared to that of an HMM
based system using preprocessed data only [13]. To the best of our knowledge, this is the ﬁrst time
whole sentences of unconstrained handwriting have been directly transcribed from raw online data.
Section 2 describes the network architecture, the output layer and the algorithm for applying gram-
matical constraints. Section 3 provides experimental results, and conclusions are given in Section 4.

2 Method

2.1 Bidirectional Long Short-Term Memory

One of the key beneﬁts of RNNs is their ability to make use of previous context. However, for
standard RNN architectures, the range of context that can in practice be accessed is limited. The
problem is that the inﬂuence of a given input on the hidden layer, and therefore on the network
output, either decays or blows up exponentially as it cycles around the recurrent connections. This
is often referred to as the vanishing gradient problem [4].
Long Short-Term Memory (LSTM; [5]) is an RNN architecture designed to address the vanishing
gradient problem. An LSTM layer consists of multiple recurrently connected subnets, known as
memory blocks. Each block contains a set of internal units, known as cells, whose activation is
controlled by three multiplicative ‘gate’ units. The effect of the gates is to allow the cells to store
and access information over long periods of time.
For many tasks it is useful to have access to future as well past context. Bidirectional RNNs [14]
achieve this by presenting the input data forwards and backwards to two separate hidden layers, both
of which are connected to the same output layer. Bidirectional LSTM (BLSTM) [3] combines the
above architectures to provide access to long-range, bidirectional context.

2.2 Connectionist Temporal Classiﬁcation

Connectionist temporal classiﬁcation (CTC) [2] is an objective function designed for sequence la-
belling with RNNs. Unlike previous objective functions it does not require pre-segmented training
data, or postprocessing to transform the network outputs into labellings. Instead, it trains the network
to map directly from input sequences to the conditional probabilities of the possible labellings.
A CTC output layer contains one more unit than there are elements in the alphabet L of labels for
the task. The output activations are normalised with the softmax activation function [1]. At each
time step, the ﬁrst |L| outputs are used to estimate the probabilities of observing the corresponding
labels. The extra output estimates the probability of observing a ‘blank’, or no label. The combined
output sequence estimates the joint probability of all possible alignments of the input sequence with
all possible labellings. The probability of a particular labelling can then be estimated by summing
over the probabilities of all the alignments that correspond to it.
More precisely, for an input sequence x of length T , choosing a label (or blank) at every time
step according to the probabilities implied by the network outputs deﬁnes a probability distribution

2

over the set of length T sequences of labels and blanks. We denote this set L(cid:48)T , where L(cid:48) = L ∪
{blank}. To distinguish them from labellings, we refer to the elements of L(cid:48)T as paths. Assuming
that the label probabilities at each time step are conditionally independent given x, the conditional
probability of a path π ∈ L(cid:48)T is given by

p(π|x) =

yt
πt

,

(1)

T(cid:89)

t=1

k is the activation of output unit k at time t. Denote the set of sequences of length less than
where yt
or equal to T on the alphabet L as L≤T . Then Paths are mapped onto labellings l ∈ L≤T by an
operator B that removes ﬁrst the repeated labels, then the blanks. For example, both B(a,−, a, b,−)
and B(−, a, a,−,−, a, b, b) yield the labelling (a,a,b). Since the paths are mutually exclusive, the
conditional probability of a given labelling l ∈ L≤T is the sum of the probabilities of all paths
corresponding to it:

p(l|x) = (cid:88)

π∈B−1(l)

p(π|x).

(2)

Although a naive calculation of the above sum would be unfeasible, it can be efﬁciently evaluated
with a graph-based algorithm [2], similar to the forward-backward algorithm for HMMs.
To allow for blanks in the output paths, for each label sequence l ∈ L≤T consider a modiﬁed label
sequence l(cid:48) ∈ L(cid:48)≤T , with blanks added to the beginning and the end and inserted between every
pair of labels. The length of l(cid:48) is therefore |l(cid:48)| = 2|l| + 1.
For a labelling l, deﬁne the forward variable αt(s) as the summed probability of all paths whose
length t preﬁxes are mapped by B onto the length s/2 preﬁx of l, i.e.

αt(s) = P (π1:t : B(π1:t) = l1:s/2, πt = l(cid:48)

yt(cid:48)
πt(cid:48) ,

(3)

s|x) = (cid:88)

t(cid:89)

t(cid:48)=1
B(π1:t)=l1:s/2

π:

s|x) = (cid:88)

T(cid:89)

π:

B(πt:T )=ls/2:|l|

t(cid:48)=t+1

where, for some sequence s, sa:b is the subsequence (sa, sa+1, ..., sb−1, sb), and s/2 is rounded
down to an integer value.
The backward variables βt(s) are deﬁned as the summed probability of all paths whose sufﬁxes
starting at t map onto the sufﬁx of l starting at label s/2

βt(s) = P (πt+1:T : B(πt:T ) = ls/2:|l|, πt = l(cid:48)

yt(cid:48)
πt(cid:48)

(4)

Both the forward and backward variables are calculated recursively [2]. The label sequence proba-
bility is given by the sum of the products of the forward and backward variables at any time step:

p(l|x) =

αt(s)βt(s).

(5)

|l(cid:48)|(cid:88)

s=1

The objective function for CTC is the negative log probability of the network correctly labelling the
entire training set. Let S be a training set, consisting of pairs of input and target sequences (x, z),
where target sequence z is at most as long as input sequence x. Then the objective function is:

ln (p(z|x)).

(6)

OCT C = − (cid:88)

(x,z)∈S

The network can be trained with gradient descent by differentiating OCT C with respect to the out-
puts, then using backpropagation through time to differentiate with respect to the network weights.
Noting that the same label (or blank) may be repeated several times for a single labelling l, we deﬁne
s = k}, which may be empty. We
the set of positions where label k occurs as lab(l, k) = {s : l(cid:48)
(cid:88)
then set l = z and differentiate (5) with respect to the unnormalised network outputs at
k to obtain:

∂OCT C

= − ∂ln (p(z|x))

∂at
k

= yt

k − 1

p(z|x)

∂at
k

s∈lab(z,k)

αt(s)βt(s).

(7)

3

Once the network is trained, we would ideally label some unknown input sequence x by choosing
the most probable labelling l∗:

Using the terminology of HMMs, we refer to the task of ﬁnding this labelling as decoding. Unfortu-
nately, we do not know of a tractable decoding algorithm that is guaranteed to give optimal results.
However a simple and effective approximation is given by assuming that the most probable path
corresponds to the most probable labelling, i.e.

arg max

π

p(π|x)

.

(9)

l∗ = arg max

p(l|x).

l

l∗ ≈ B(cid:16)

(cid:17)

(8)

(10)

2.3

Integration with an External Grammar

For some tasks we want to constrain the output labellings according to a predeﬁned grammar. For
example, in speech and handwriting recognition, the ﬁnal transcriptions are usually required to form
sequences of dictionary words. In addition it is common practice to use a language model to weight
the probabilities of particular sequences of words.
We can express these constraints by altering the probabilities in (8) to be conditioned on some
probabilistic grammar G, as well as the input sequence x:

l∗ = arg max

p(l|x, G).

l

Absolute requirements, for example that l contains only dictionary words, can be incorporated by
setting the probability of all sequences that fail to meet them to 0.
At ﬁrst sight, conditioning on G seems to contradict a basic assumption of CTC: that the labels
are conditionally independent given the input sequences (see Eqn. (1)). Since the network attempts
to model the probability of the whole labelling at once, there is nothing to stop it from learning
inter-label transitions direct from the data, which would then be skewed by the external grammar.
However, CTC networks are typically only able to learn local relationships such as commonly occur-
ring pairs or triples of labels. Therefore as long as G focuses on long range label interactions (such
as the probability of one word following another when the outputs are letters) it doesn’t interfere
with the dependencies modelled by CTC.
The basic rules of probability tell us that p(l|x, G) = p(l|x)p(l|G)p(x)
, where we have used the fact
that x is conditionally independent of G given l. If we assume x is independent of G, this reduces
to p(l|x, G) = p(l|x)p(l|G)
. That assumption is in general false, since both the input sequences and
the grammar depend on the underlying generator of the data, for example the language being spo-
ken. However it is a reasonable ﬁrst approximation, and is particularly justiﬁable in cases where
the grammar is created using data other than that from which x was drawn (as is common prac-
tice in speech and handwriting recognition, where independent textual corpora are used to generate
language models).
Finally, if we assume that all label sequences are equally probable prior to any knowledge about the
input or the grammar, we can drop the p(l) term in the denominator to get

p(x|G)p(l)

p(l)

l∗ = arg max

p(l|x)p(l|G).

l

(11)
Note that, since the number of possible label sequences is ﬁnite (because both L and |l| are ﬁnite),
assigning equal prior probabilities does not lead to an improper prior.
We now describe an algorithm, based on the token passing algorithm for HMMs [16], that allows us
to ﬁnd an approximate solution to (11) for a simple grammar.
Let G consist of a dictionary D containing W words, and a set of W 2 bigrams p(w| ˆw) that deﬁne
the probability of making a transition from word ˆw to word w. The probability of any labelling that
does not form a sequence of dictionary words is 0.
For each word w, deﬁne the modiﬁed word w(cid:48) as w with blanks added at the beginning and end and
between each pair of labels. Therefore |w(cid:48)| = 2|w| + 1. Deﬁne a token tok = (score, history)
to be a pair consisting of a real valued score and a history of previously visited words. In fact,

4

each token corresponds to a particular path through the network outputs, and its score is the log
probability of that path. The basic idea of the token passing algorithm is to pass along the highest
scoring tokens at every word state, then maximise over these to ﬁnd the highest scoring tokens at
the next state. The transition probabilities are used when a token is passed from the last state in one
word to the ﬁrst state in another. The output word sequence is given by the history of the highest
scoring end-of-word token at the ﬁnal time step.
At every time step t of the length T output sequence, each segment s of each modiﬁed word w(cid:48) holds
a single token tok(w, s, t). This is the highest scoring token reaching that segment at that time. In
addition we deﬁne the input token tok(w, 0, t) to be the highest scoring token arriving at word w at
time t, and the output token tok(w,−1, t) to be the highest scoring token leaving word w at time t.

else
tok(w, s, 1) = (−∞, ()) for all s (cid:54)= −1

sort output tokens tok(w,−1, t − 1) by ascending score
for all words w ∈ D do

b ), (w))
w1), (w))
tok(w,−1, 1) = tok(w, 2, 1)
tok(w,−1, 1) = (−∞, ())

1: Initialisation:
2: for all words w ∈ D do
tok(w, 1, 1) = (ln(y1
3:
tok(w, 2, 1) = (ln(y1
4:
if |w| = 1 then
5:
6:
7:
8:
9:
10: Algorithm:
11: for t = 2 to T do
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24: Termination:
25: ﬁnd output token tok∗(w,−1, T ) with highest score at time T
26: output tok∗(w,−1, T ).history

P = {tok(w, s, t − 1), tok(w, s − 1, t − 1)}
if w(cid:48)
s then

s (cid:54)= blank and s > 2 and w(cid:48)
add tok(w, s − 2, t − 1) to P

s−2 (cid:54)= w(cid:48)

w∗ = arg max ˆw∈D tok( ˆw,−1, t − 1).score + ln (p(w| ˆw))
tok(w, 0, t).score = tok(w∗,−1, t − 1).score + ln (p(w|w∗))
tok(w, 0, t).history = tok(w∗,−1, t − 1).history + w
for segment s = 1 to |w(cid:48)| do

tok(w, s, t) = token in P with highest score
tok(w, s, t).score += ln(yt
w(cid:48)

)

tok(w,−1, t) = highest scoring of {tok(w,|w(cid:48)|, t), tok(w,|w(cid:48)| − 1, t)}

s

Algorithm 1: CTC Token Passing Algorithm

The algorithm’s worst case complexity is O(T W 2), since line 14 requires a potential search through
all W words. However, because the output tokens tok(w,−1, T ) are sorted in order of score, the
search can be terminated when a token is reached whose score is less than the current best score
with the transition included. The typical complexity is therefore considerably lower, with a lower
bound of O(T W logW ) to account for the sort. If no bigrams are used, lines 14-16 can be replaced
by a simple search for the highest scoring output token, and the complexity reduces to O(T W ).
Note that this is the same as the complexity of HMM decoding, if the search through bigrams is
exhaustive. Much work has gone into developing more efﬁcient decoding techniques (see e.g. [9]),
typically by pruning improbable branches from the tree of labellings. Such methods are essential
for applications where a rapid response is required, such as real time transcription. In addition,
many decoders use more sophisticated language models than simple bigrams. Any HMM decoding
algorithm could be applied to CTC outputs in the same way as token passing. However, we have
stuck with a relatively basic algorithm since our focus here is on recognition rather than decoding.

5

3 Experiments

The experimental task was online handwriting recognition, using the IAM-OnDB handwriting
database [12], which is available for public download from http://www.iam.unibe.ch/ fki/iamondb/
For CTC, we record both the character error rate, and the word error rate using Algorithm 1 with
a language model and a dictionary. For the HMM system, the word error rate is quoted from the
literature [13]. Both the character and word error rate are deﬁned as the total number of insertions,
deletions and substitutions in the algorithm’s transcription of test set, divided by the combined length
of the target transcriptions in the test set.
We compare results using both raw inputs direct from the pen sensor, and a preprocessed input
representation designed for HMMs.

3.1 Data and Preprocessing

IAM-OnDB consists of pen trajectories collected from 221 different writers using a ‘smart white-
board’ [12]. The writers were asked to write forms from the LOB text corpus [8], and the position of
their pen was tracked using an infra-red device in the corner of the board. The input data consisted
of the x and y pen coordinates, the points in the sequence when individual strokes (i.e. periods when
the pen is pressed against the board) end, and the times when successive position measurements
were made. Recording errors in the x, y data were corrected by interpolating to ﬁll in for missing
readings, and removing steps whose length exceeded a certain threshold.
IAM-OnDB is divided into a training set, two validation sets, and a test set, containing respectively
5364, 1438, 1518 and 3859 written lines taken from 775, 192, 216 and 544 forms. The data sets
contained a total of 3,298,424, 885,964, 1,036,803 and 2,425,5242 pen coordinates respectively. For
our experiments, each line was used as a separate sequence (meaning that possible dependencies
between successive lines were ignored).
The character level transcriptions contain 80 distinct target labels (capital letters, lower case letters,
numbers, and punctuation). A dictionary consisting of the 20, 000 most frequently occurring words
in the LOB corpus was used for decoding, along with a bigram language model optimised on the
training and validation sets [13]. 5.6% of the words in the test set were not in the dictionary.
Two input representations were used. The ﬁrst contained only the offset of the x, y coordinates
from the top left of the line, the time from the beginning of the line, and the marker for the ends of
strokes. We refer to this as the raw input representation. The second representation used state-of-the-
art preprocessing and feature extraction techniques [13]. We refer to this as the preprocessed input
representation. Brieﬂy, in order to account for the variance in writing styles, the pen trajectories
were normalised with respect to such properties as the slant, skew and width of the letters, and the
slope of the line as a whole. Two sets of input features were then extracted, the ﬁrst consisting of
‘online’ features, such as pen position, pen speed, line curvature etc., and the second consisting of
‘ofﬂine’ features created from a two dimensional window of the image created by the pen.

3.2 Experimental Setup

The CTC network used the BLSTM architecture, as described in Section 2.1. The forward and
backward hidden layers each contained 100 single cell memory blocks. The input layer was fully
connected to the hidden layers, which were fully connected to themselves and the output layer. The
output layer contained 81 units (80 characters plus the blank label). For the raw input representation,
there were 4 input units and a total of 100,881 weights. For the preprocessed representation, there
were 25 inputs and 117,681 weights. tanh was used for the cell activation functions and logistic
sigmoid in the range [0, 1] was used for the gates. For both input representations, the data was
normalised so that each input had mean 0 and standard deviation 1 on the training set. The network
was trained with online gradient descent, using a learning rate of 10−4 and a momentum of 0.9.
Training was stopped after no improvement was recorded on the validation set for 50 training epochs.
The HMM setup [13] contained a separate, left-to-right HMM with 8 states for each character (8 ∗
81 = 648 states in total). Diagonal mixtures of 32 Gaussians were used to estimate the observation

6

Table 1: Word Error Rate (WER) on IAM-OnDB. LM = language model. CTC results are a mean
over 4 runs, ± standard error. All differences were signiﬁcant (p < 0.01)

LM WER

System Input
HMM preprocessed (cid:88)
CTC
raw

preprocessed
CTC

(cid:88)
CTC
raw
preprocessed (cid:88)
CTC

35.5% [13]
30.1 ± 0.5%
26.0 ± 0.3%
22.8 ± 0.2%
20.4 ± 0.3%

probabilities. All parameters, including the word insertion penalty and the grammar scale factor,
were optimised on the validation set.

3.3 Results
The character error rate for the CTC network with the preprocessed inputs was 11.5 ± 0.05%.
From Table 1 we can see that with a dictionary and a language model this translates into a mean
word error rate of 20.4%, which is a relative error reduction of 42.5% compared to the HMM.
Without the language model, the error reduction was 26.8%. With the raw input data CTC achieved
a character error rate of 13.9 ± 0.1%, and word error rates that were close to those recorded with
the preprocessed data, particularly when the language model was present.
The key difference between the input representations is that the raw data is less localised, and there-
fore requires more use of context. A useful indication of the network’s sensitivity to context is
provided by the derivatives of the output yt
k at a particular point t in the data sequence with respect
k at all points 1 ≤ t(cid:48) ≤ T . We refer to these derivatives as the sequential Jacobian.
to the inputs xt(cid:48)
Looking at the relative magnitude of the sequential Jacobian over time gives an idea of the range of
context used, as illustrated in Figure 1.

4 Conclusion

We have combined a BLSTM CTC network with a probabilistic language model. We have applied
this system to an online handwriting database and obtained results that substantially improve on a
state-of-the-art HMM based system. We have also shown that the network’s performance with raw
sensor inputs is comparable to that with sophisticated preprocessing. As far as we are aware, our
system is the ﬁrst to successfully recognise unconstrained online handwriting using raw inputs only.

Acknowledgments

This research was funded by EC Sixth Framework project “NanoBioTact”, SNF grant 200021-
111968/1, and the SNF program “Interactive Multimodal Information Management (IM)2”.

References
[1] J. S. Bridle. Probabilistic interpretation of feedforward classiﬁcation network outputs, with relationships
to statistical pattern recognition. In F. Fogleman-Soulie and J.Herault, editors, Neurocomputing: Algo-
rithms, Architectures and Applications, pages 227–236. Springer-Verlag, 1990.

[2] A. Graves, S. Fern´andez, F. Gomez, and J. Schmidhuber. Connectionist temporal classiﬁcation: Labelling
unsegmented sequence data with recurrent neural networks. In Proc. 23rd Int. Conf. on Machine Learning,
Pittsburgh, USA, 2006.

[3] A. Graves and J. Schmidhuber. Framewise phoneme classiﬁcation with bidirectional LSTM and other

neural network architectures. Neural Networks, 18(5-6):602–610, June/July 2005.

[4] S. Hochreiter, Y. Bengio, P. Frasconi, and J. Schmidhuber. Gradient ﬂow in recurrent nets: the difﬁculty
of learning long-term dependencies. In S. C. Kremer and J. F. Kolen, editors, A Field Guide to Dynamical
Recurrent Neural Networks. IEEE Press, 2001.

[5] S. Hochreiter and J. Schmidhuber. Long Short-Term Memory. Neural Comp., 9(8):1735–1780, 1997.
[6] J. Hu, S. G. Lim, and M. K. Brown. Writer independent on-line handwriting recognition using an HMM

approach. Pattern Recognition, 33:133–147, 2000.

7

Figure 1: Sequential Jacobian for an excerpt from the IAM-OnDB, with raw inputs (left) and pre-
processed inputs (right). For ease of visualisation, only the derivative with highest absolute value
is plotted at each time step. The reconstructed image was created by plotting the pen coordinates
recorded by the sensor. The individual strokes are alternately coloured red and black. For both rep-
resentations, the Jacobian is plotted for the output corresponding to the label ‘i’ at the point when
‘i’ is emitted (indicated by the vertical dashed lines). Because bidirectional networks were used, the
range of sensitivity extends in both directions from the dashed line. For the preprocessed data, the
Jacobian is sharply peaked around the time when the output is emitted. For the raw data it is more
spread out, suggesting that the network makes more use of long-range context. Note the spike in
sensitivity to the very end of the raw input sequence: this corresponds to the delayed dot of the ‘i’.

[7] S. Jaeger, S. Manke, J. Reichert, and A. Waibel. On-line handwriting recognition: the NPen++ recognizer.

Int. Journal on Document Analysis and Recognition, 3:169–180, 2001.

[8] S. Johansson, R. Atwell, R. Garside, and G. Leech. The tagged LOB corpus user’s manual; Norwegian

Computing Centre for the Humanities, 1986.

[9] P. Lamere, P. Kwok, W. Walker, E. Gouvea, R. Singh, B. Raj, and P. Wolf. Design of the CMU Sphinx-4

decoder. In Proc. 8th European Conf. on Speech Communication and Technology, Aug. 2003.

[10] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition.

Proc. IEEE, 86(11):2278–2324, Nov. 1998.

[11] Y. LeCun, F. Huang, and L. Bottou. Learning methods for generic object recognition with invariance to

pose and lighting. In Proc. of CVPR’04. IEEE Press, 2004.

[12] M. Liwicki and H. Bunke. IAM-OnDB - an on-line English sentence database acquired from handwritten
text on a whiteboard. In Proc. 8th Int. Conf. on Document Analysis and Recognition, volume 2, pages
956–961, 2005.

[13] M. Liwicki, A. Graves, S. Fern´andez, H. Bunke, and J. Schmidhuber. A novel approach to on-line
handwriting recognition based on bidirectional long short-term memory networks. In Proc. 9th Int. Conf.
on Document Analysis and Recognition, Curitiba, Brazil, Sep. 2007.

[14] M. Schuster and K. K. Paliwal. Bidirectional recurrent neural networks. IEEE Transactions on Signal

Processing, 45:2673–2681, Nov. 1997.

[15] P. Y. Simard, D. Steinkraus, and J. C. Platt. Best practices for convolutional neural networks applied
to visual document analysis. In Proc. 7th Int. Conf. on Document Analysis and Recognition, page 958,
Washington, DC, USA, 2003. IEEE Computer Society.

[16] S. Young, N. Russell, and J. Thornton. Token passing: A simple conceptual model for connected speech
recognition systems. Technical Report CUED/F-INFENG/TR38, Cambridge University Eng. Dept., 1989.

8

"
785,2007,The Epoch-Greedy Algorithm for Multi-armed Bandits with Side Information,"We present Epoch-Greedy, an algorithm for multi-armed bandits with observable side information. Epoch-Greedy has the following properties: No knowledge of a time horizon $T$ is necessary. The regret incurred by Epoch-Greedy is controlled by a sample complexity bound for a hypothesis class. The regret scales as $O(T^{2/3} S^{1/3})$ or better (sometimes, much better). Here $S$ is the complexity term in a sample complexity bound for standard supervised learning.","The Epoch-Greedy Algorithm for Contextual

Multi-armed Bandits

John Langford
Yahoo! Research

jl@yahoo-inc.com

Tong Zhang

tongz@rci.rutgers.edu

Department of Statistics

Rutgers University

Abstract

We present Epoch-Greedy, an algorithm for contextual multi-armed bandits (also
known as bandits with side information). Epoch-Greedy has the following prop-
erties:

1. No knowledge of a time horizon T is necessary.
2. The regret incurred by Epoch-Greedy is controlled by a sample complexity

bound for a hypothesis class.

3. The regret scales as O(T 2/3S1/3) or better (sometimes, much better). Here S
is the complexity term in a sample complexity bound for standard supervised
learning.

1 Introduction

The standard k-armed bandits problem has been well-studied in the literature (Lai & Robbins, 1985;
Auer et al., 2002; Even-dar et al., 2006, for example).
It can be regarded as a repeated game
between two players, with every stage consisting of the following: The world chooses k rewards
r1, ..., rk ∈ [0, 1]; the player chooses an arm i ∈ {1, k} without knowledge of the world’s chosen
rewards, and then observes the reward ri. The contextual bandits setting considered in this paper
is the same except for a modiﬁcation of the ﬁrst step, in which the player also observes context
information x which can be used to determine which arm to pull.
The contextual bandits problem has many applications and is often more suitable than the standard
bandits problem, because settings with no context information are rare in practice. The setting
considered in this paper is directly motivated by the problem of matching ads to web-page contents
on the internet. In this problem, a number of ads (arms) are available to be placed on a number of
web-pages (context information). Each page visit can be regarded as a random draw of the context
information (one may also include the visitor’s online proﬁle as context information if available)
from an underlying distribution that is not controlled by the player. A certain amount of revenue is
generated when the visitor clicks on an ad. The goal is to put the most relevant ad on each page to
maximize the expected revenue. Although one may potentially put multiple ads on each web-page,
we focus on the problem that only one ad is placed on each page (which is like pulling an arm given
context information). The more precise deﬁnition is given in Section 2.
Prior Work. The problem of bandits with context has been analyzed previously (Pandey et al., 2007;
Wang et al., 2005), typically under additional assumptions such as a correct prior or knowledge of
the relationship between the arms. This problem is also known as associative reinforcement learning
(Strehl et al., 2006, for example) or bandits with side information. A few results under as weak or
weaker assumptions are directly comparable.

1. The Exp4 algorithm (Auer et al., 1995) notably makes no assumptions about the world.
Epoch-Greedy has a worse regret bound in T (O(T 2/3) rather than O(T 1/2)) and is only

1

analyzed under an IID assumption. An important advantage of Epoch-Greedy is a much
better dependence on the size of the set of predictors. In the situation where the number
of predictors is inﬁnite but with ﬁnite VC-Dimension d, Exp4 has a vacuous regret bound
while Epoch-Greedy has a regret bound no worse than O(T 2/3(ln m)1/3). Sometimes we
can achieve much better dependence on T , depending on the structure of the hypothe-
sis space. For example, we will show that it is possible to achieve O(ln T ) regret bound
using Epoch-Greedy, while this is not possible with Exp4 or any simple modiﬁcation of
it. Another substantial advantage is reduced computational complexity. The ERM step in
Epoch-Greedy can be replaced with any standard learning algorithm that achieves approxi-
mate loss minimization, making guarantees that degrade gracefully with the approximation
factor. Exp4 on the other hand requires computation proportional to the explicit count of
hypotheses in a hypothesis space.

2. The random trajectories method (Kearns et al., 2000) for learning policies in reinforcement
learning with hard horizon T = 1 is essentially the same setting. In this paper, bounds are
stated for a batch oriented setting where examples are formed and then used for choosing
a hypothesis. Epoch-Greedy takes advantage of this idea, but it also has analysis which
states that it trades off the number of exploration and exploitation steps so as to maximize
the sum of rewards incurred during both exploration and exploitation.

What we do. We present and analyze the Epoch-Greedy algorithm for multiarmed bandits with
context. This has all the nice properties stated in the abstract, resulting in a practical algorithm for
solving this problem.
The paper is broken up into the following sections.

1. In Section 2 we present basic deﬁnitions and background.
2. Section 3 presents the Epoch-Greedy algorithm along with a regret bound analysis which

holds without knowledge of T .

3. Section 4 analyzes the instantiation of the Epoch-Greedy algorithm in several settings.

2 Contextual bandits

We ﬁrst formally deﬁne contextual bandit problems and algorithms to solve them.

Deﬁnition 2.1 (Contextual bandit problem) In a contextual bandits problem, there is a distribu-
tion P over (x, r1, ..., rk), where x is context, a ∈ {1, . . . , k} is one of the k arms to be pulled,
and ra ∈ [0, 1] is the reward for arm a. The problem is a repeated game: on each round, a sample
(x, r1, ..., rk) is drawn from P , the context x is announced, and then for precisely one arm a chosen
by the player, its reward ra is revealed.
Deﬁnition 2.2 (Contextual bandit algorithm) A contextual bandits algorithm B determines an
arm a ∈ {1, . . . , k} to pull at each time step t, based on the previous observation sequence
(x1, a1, ra,1), . . . , (xt−1, at−1, ra,t−1), and the current context xt.

Our goal is to maximize the expected total rewardPT

t=1 E(xt,~rt)∼P [ra,t]. Note that we use the
notation ra,t = rat to improve readability. Similar to supervised learning, we assume that we are
given a set H consisting of hypotheses h : X → {1, . . . , k}. Each hypothesis maps side information
x to an arm a. A natural goal is to choose arms to compete with the best hypothesis in H. We
introduce the following deﬁnition.

Deﬁnition 2.3 (Regret) The expected reward of a hypothesis h is

R(h) = E(x,~r)∼D

Consider any contextual bandits algorithm B. Let Z T = {(x1, ~r1), . . . , (xT , ~rT )}, and the expected
regret of B with respect to a hypothesis h be:

∆R(B, h, T ) = T R(h) − EZT ∼P T

rB(x),t.

(cid:2)rh(x)

(cid:3) .
TX

t=1

2

The expected regret of B up to time T with respect to hypothesis space H is deﬁned as

∆R(B,H, T ) = sup
h∈H

∆R(B, h, T ).

The main challenge of the contextual bandits problem is that when we pull an arm, rewards of
other arms are not observed. Therefore it is necessary to try all arms (explore) in order to form an
accurate estimation. In this context, methods we investigate in the paper make explicit distinctions
between exploration and exploitation steps. In an exploration step, the goal is to form unbiased
samples by randomly pulling all arms to improve the accuracy of learning. Because it does not
focus on the best arm, this step leads to large immediate regret but can potentially reduce regret
for the future exploitation steps. In an exploitation step, the learning algorithm suggests the best
hypothesis learned from the samples formed in the exploration steps, and the arm given by the
hypothesis is pulled: the goal is to maximize immediate reward (or minimize immediate regret).
Since the samples in the exploitation steps are biased (toward the arm suggested by the learning
algorithm using previous exploration samples), we do not use them to learn the hypothesis for the
future steps. That is, in methods we consider, exploitation does not help us to improve learning
accuracy for the future.
More speciﬁcally, in an exploration step, in order to form unbiased samples, we pull an arm a ∈
{1, . . . , k} uniformly at random. Therefore the expected regret comparing to the best hypothesis
in H can be as large as O(1). In an exploitation step, the expected regret can be much smaller.
Therefore a central theme we examine in this paper is to balance the trade-off between exploration
and exploitation, so as to achieve a small overall expected regret up to some time horizon T .
Note that if we decide to pull a speciﬁc arm a with side information x, we do not observe rewards
ra0 for a0 6= a. In order to apply standard sample complexity analysis, we ﬁrst show that exploration
samples, where a is picked uniformly at random, can create a standard learning problem without
missing observations. This is simply achieved by setting fully observed rewards r0 such that

r0
a0(ra) = kI(a0 = a)ra,

(1)
where I(·) is the indicator function. The basic idea behind this transformation from partially ob-
served to fully observed data dates back to the analysis of “Sample Selection Bias” (Heckman,
1979). The above rule is easily generalized to other distribution over actions p(a) by replacing k
with 1/p(a).
The following lemma shows that this method of ﬁlling missing reward components is unbiased.
Lemma 2.1 For all arms a0: E~r∼P|x [ra0] = E~r∼P|x,a∼U (1,...,k) [r0
pothesis h(x), we have R(h) = E(x,~r)∼P,a∼U (1,...,k)

a0(ra)]. Therefore for any hy-

h

i

r0
h(x)(ra)

.

Proof We have:

E~r∼P|x,a∼U (1,...,k) [r0

a0(ra)] =E~r∼P|x

=E~r∼P|x

kX
kX

a=1

a=1

k−1 [r0

a0(ra)]

k−1 [kraI(a0 = a)] = E~r∼P|x [ra0] .

samples as P

Lemma 2.1 implies that we can estimate reward R(h) of any hypothesis h(x) using expectation
with respect to exploration samples (x, a, ra). The right hand side can then be replaced by empirical
t I(h(xt) = at)ra,t for hypotheses in a hypothesis space H. The quality of this

estimation can be obtained with uniform convergence learning bounds.

3 Exploration with the Epoch-Greedy algorithm

The problem of treating contextual bandits as standard bandits is that the information in x is lost.
That is, the optimal arm to pull should be a function of the context x, but this is not captured by the

3

standard bandits setting. An alternative approach is to regard each hypothesis h as a separate artiﬁ-
cial “arm”, and then apply a standard bandits algorithm to these artiﬁcial arms. Using this approach,
let m be the number of hypotheses, we can get a bound of O(m). However, this solution ignores
the fact that many hypotheses can share the same arm so that choosing an arm yields information
for many hypotheses. For this reason, with a simple algorithm, we can get a bound that depends on
m logarithmically, instead of O(m) as would be the case for the standard bandits solution discussed
above.
As discussed earlier, the key issue in the algorithm is to determine when to explore and when to
exploit, so as to achieve appropriate balance. If we are given the time horizon T in advance, and
would like to optimize performance with the given T , then it is always advantageous to perform a
ﬁrst phase of exploration steps, followed by a second phase of exploitation steps (until time step T ).
The reason that there is no advantage to take any exploitation step before the last exploration step is:
by switching the two steps, we can more accurately pick the optimal hypothesis in the exploitation
step due to more samples from exploration. With ﬁxed T , assume that we have taken n steps of
exploration, and obtain an average regret bound of n for each exploitation step at the point, then
we can bound the regret of the exploration phase as n, and the exploitation phase as n(T − n). The
total regret is n + (T − n)n. Using this bound, we shall switch from exploration to exploitation at
the point n that minimizes the sum.
Without knowing T in advance, but with the same generalization bound, we can run explo-
ration/exploitation in epochs, where at the beginning of each epoch ‘, we perform one step of explo-
ration, followed by d1/ne steps of exploitation. We then start the next epoch. After epoch L, the
n=1(1 + nd1/ne) ≤ 3L. Moreover, the epoch L∗ contain-
ing T is no more than the optimal regret bound minn[n + (T − n)n] (with known T and optimal
stopping point). Therefore the performance of our method (which does not need to know T ) is no
worse than three time the optimal bound with known T and optimal stopping point. This motivates
a modiﬁed algorithm in Figure 1. The idea described above is related to forcing in (Lai & Yakowitz,
1995).
Proposition 3.1 Consider a sequence of nonnegative and monotone non-increasing numbers {n}.

total average regret is no more thanPL

Let L∗ = min{L :PL

‘=1(1 + d1/‘e) ≥ T}, then

L∗ ≤ min
n∈[0,T ]

[n + (T − n)n].

Proof Let n∗ = arg minn∈[0,T ][n + (T − n)n]. The bound is trivial if n∗ ≥ L∗. We only
‘=1 (1 + 1/‘) ≤ T − 1. Since
‘=n∗ 1/‘ ≥ (L∗ − n∗)1/n∗, we have L∗ − 1 + (L∗ − n∗)1/n∗ ≤ T − 1.

need consider the case n∗ ≤ L∗ − 1. By assumption, PL∗−1
PL∗−1
‘=1 1/‘ ≥ PL∗−1
Rearranging, we have L∗ ≤ n∗ + (T − L∗)n∗.

1 ) = d1/n(Z n

1 )e, where n(Z n

1 ) is a sample-dependent (integer valued) exploitation step count. Proposition 3.1
In Figure 1, s(Z n
suggests that choosing s(Z n
1 ) is a sample dependent average gen-
eralization bound, yields performance comparable to the optimal bound with known time horizon
T .
Deﬁnition 3.1 (Epoch-Greedy Exploitation Cost) Consider a hypothesis space H consisting of
hypotheses that take values in {1, 2, . . . , k}. Let Zt = (xt, at, ra,t) for i = 1, . . . , n be indepen-
dent random samples, where ai is uniform randomly distributed in {1, . . . , k}, and ra,t ∈ [0, 1] is
1 = {Z1, . . . , Zn}, and the empirical reward maximization
the observed (random) reward. Let Z n
estimator

1 ) = arg max
h∈H
Given any ﬁxed n, δ ∈ [0, 1], and observation Z n
step count. Then the per-epoch exploitation cost is deﬁned as:

t=1

ˆh(Z n

ra,tI(h(xt) = at).

1 , we denote by s(Z n

1 ) a data-dependent exploitation

µn(H, s) = EZn

1

R(h) − R(ˆh(Z n
1 ))

s(Z n

1 ).

(cid:19)

nX

4

(cid:18)

sup
h∈H

Epoch-Greedy (s(W‘)) /*parameter s(W‘): exploitation steps*/
initialize: exploration samples W0 = {} and t1 = 1
iterate ‘ = 1, 2, . . .
t = t‘, and observe xt /*do one-step exploration*/
select an arm at ∈ {1, . . . , k} uniformly at random
receive reward ra,t ∈ [0, 1]
W‘ = W‘−1 ∪ {(xt, at, ra,t)}
ﬁnd best hypothesis ˆh‘ ∈ H by solving
raI(h(x) = a)

maxh∈HP

(x,a,ra)∈W‘
t‘+1 = t‘ + s(W‘) + 1
for t = t‘ + 1,··· , t‘+1 − 1 /*do s(W‘)-steps exploitation*/
select arm at = ˆh‘(xt)
receive reward ra,t ∈ [0, 1]
end for

end iterate

Figure 1: Exploration by -greedy in epochs

Theorem 3.1 For all T, n‘, L such that: T ≤ L +PL
LX

in Figure 1 is bounded by

∆R(Epoch-Greedy,H, T ) ≤ L +

‘=1 n‘, the expected regret of Epoch-Greedy

LX

µ‘(H, s) + T

P [s(Z ‘

1) < n‘].

‘=1

‘=1

This theorem statement is very general, because we want to allow sample dependent bounds to be
used. When sample-independent bounds are used the following simple corollary holds:

Corollary 3.1 Assume we choose s(Z ‘

1) = s‘ ≤ b1/µ‘(H, 1)c (‘ = 1, . . .), and let LT =
‘=1 s‘ ≥ T}. Then the expected regret of Epoch-Greedy in Figure 1 is

arg minL{L : L +PL

bounded by

∆R(Epoch-Greedy,H, T ) ≤ 2LT .

(of the main theorem) Let B be the Epoch-Greedy algorithm. One of the following events

Proof
will occur:

• A: s(Z ‘
• B: s(Z ‘

1) < n‘ for some ‘ = 1, . . . , L.
1) ≥ n‘ for all ‘ = 1, . . . , L.

If event A occurs, then since each reward is in [0,1], up to time T , regret cannot be larger than T .
Thus the total expected contribution of A to the regret ∆R(B,H, T ) is at most

LX

T P (A) ≤ T

P [s(Z ‘

1) < n‘].

(2)

‘=1

If event B occurs, then t‘+1 − t‘ ≥ n‘ + 1 for ‘ = 1, . . . , L, and thus tL+1 > T . Therefore the
expected contribution of B to the regret ∆R(B,H, T ) is at most the sum of expected regret in the
ﬁrst L epochs.
By deﬁnition and construction, after the ﬁrst step of epoch ‘, W‘ consists of ‘ random observations
Zj = (xj, aj, ra,j) where aj is drawn uniformly at random from {1, . . . , k}, and j = 1, . . . , ‘.
This is independent of the number of exploitation steps before epoch ‘. Therefore we can treat
W‘ as ‘ independent samples. This means that the expected regret associated with exploitation
steps in epoch ‘ is µ‘(H, s). Since the exploration step in each epoch contributes at most 1 to the

5

It follows that there exists a universal constant c > 0 such that

Therefore in Figure 1, if we choose

i + c0k ln(1/η) ≤ c0

i=1

Ex2

nX
µn(H, 1) ≤ c−1pk ln m/n.
1) = bcp‘/(k ln m)c,
n‘ = bcp‘/(k ln m)c.

s(Z ‘

total expected regret for epochs ‘ = 1, . . . , L is at most L +PL

expected regret, the total expected regret for each epoch ‘ is at most 1 + µ‘(H, s). Therefore the
‘=1 µ‘(H, s). Combined with (2),

we obtain the desired bound.

In the theorem, we bound the expected regret of each exploration step by one. Clearly this assumes
the worst case scenario and can often be improved. Some consequences of the theorem with speciﬁc
function classes are given in Section 4.

4 Examples

Theorem 3.1 is quite general. In this section, we present a few simple examples to illustrate the
potential applications.

4.1 Finite hypothesis space worst case bound
Consider the ﬁnite hypothesis space situation, with m = |H| < ∞. We apply Theorem 3.1 with a
worst-case deviation bound.
Let x1, . . . , xn ∈ [0, k] be iid random variables, such that Exi ≤ 1, then Bernstein inequality
implies that there exists a constant c0 > 0 such that ∀η ∈ (0, 1), with probability 1 − η:

pnk ln(1/η) + c0k ln(1/η).

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) nX

i=1

xi − nX

i=1

vuutln(1/η)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤ c0

Exi

then µ‘(H, s) ≤ 1: this is consistent with the choice recommended in Proposition 3.1.
In order to obtain a performance bound of this scheme using Theorem 3.1, we can simply take

satisﬁes the condition T ≤ PL

This implies that P (s(Z ‘

that for any given T , we can take

1) < n‘) = 0. Moreover, with this choice, for any T , we can pick an L that
‘=1 n‘. It implies that there exists a universal constant c0 > 0 such

in Theorem 3.1.
In summary, if we choose s(Z ‘

L = bc0T 2/3(k ln m)1/3c

1) = bcp‘/(k ln m)c in Figure 1, then

∆(Epoch-Greedy,H, T ) ≤ 2L ≤ 2c0T 2/3(k ln m)1/3.

Reducing the problem to standard bandits, as discussed at the beginning of Section 3, leads to
a bound of O(m ln T ) (Lai & Robbins, 1985; Auer et al., 2002). Therefore when m is large,
the Epoch-Greedy algorithm in Figure 1 can perform signiﬁcantly better. In this particular situa-
√
tion, Epoch-Greedy does not do as well as Exp4 in (Auer et al., 1995), which implies a regret of
kT ln m). However, the advantage of Epoch-Greedy is that any learning bound can be applied.
O(
For many hypothesis classes, the ln m factor can be improved for Epoch-Greedy. In fact, a similar
result can be obtained for classes with inﬁnitely many hypotheses but ﬁnite VC dimensions. More-
over, as we will see next, under additional assumptions, it is possible to obtain much better bounds
in terms of T for Epoch-Greedy, such as O(k ln m + k ln T ). This extends the classical O(ln T )
bound for standard bandits, and is not possible to achieve using Exp4 or simple variations of it.

6

4.2 Finite hypothesis space with unknown expected reward gap

This example illustrates the importance of allowing sample-dependent s(Z ‘
1). We still assume a
ﬁnite hypothesis space, with m = |H| < ∞. However, we would like to improve the performance
bound by imposing additional assumptions. In particular we note that the standard bandits problem
has regret of the form O(ln T ) while in the worst case, our method for the contextual bandits problem
has regret O(T 2/3). A natural question is then: what are the assumptions we can impose so that the
Epoch-Greedy algorithm can have a regret of the form O(ln T ).
The main technical reason that the standard bandits problem has regret O(ln T ) is that the expected
reward of the best bandit and that of the second best bandit has a gap:
the constant hidden in
the O(ln T ) bound depends on this gap, and the bound becomes trivial (inﬁnity) when the gap
approaches zero. In this example we show that a similar assumption for contextual bandits problems
leads to a similar regret bound of O(ln T ) for the Epoch-Greedy algorithm.
Let H = {h1, . . . , hm}, and assume without loss of generality that R(h1) ≥ R(h2) ≥ ··· ≥
R(hm). Suppose that we know that R(h1) ≥ R(h2) + ∆ for some ∆ > 0, but the value of ∆ is not
known in advance.
Although ∆ is not known, it can be estimated from the data Z n
be

1 . Let the empirical reward of h ∈ H

nX

t=1

ˆR(h|Z n

1 ) = k
n

ra,tI(h(xt) = at).

Let ˆh1 be the hypothesis with highest empirical reward on Z n
highest empirical reward. We deﬁne the empirical gap as

1 , and ˆh2 be the hypothesis with second

ˆ∆(Z n

1 ) = ˆR(ˆh1|Z n

1 ) − ˆR(ˆh2|Z n
1 ).

Let h1 be the hypothesis with the highest true expected reward, then we suffer a regret when ˆh1 6=
h1. Again, the standard large deviation bound implies that there exists a universal constant c > 0
such that for all j ≥ 1:

P ( ˆ∆(Z n

1 ) ≥ (j − 1)∆, ˆh1 6= h1) ≤me−ck−1n(1+j2)∆2

1 ) ≤ 0.5∆) ≤me−ck−1n∆2

.

1 )2c. With this choice, there exists a constant c0 > 0

1 ) ≤ j∆}P ( ˆ∆(Z n

1 ) ∈ [(j − 1)∆, j∆], ˆh1 6= h1)

P ( ˆ∆(Z n

1 ) ∈ [(j − 1)∆, j∆], ˆh1 6= h1)

Now we can set s(Z n
such that

µn(H, s) ≤

j=1

j=1

≤

sup{s(Z n

1 ) : ˆ∆(Z n

P ( ˆ∆(Z n
1 ) = bm−1e(2k)−1cn ˆ∆(Zn
d∆−1eX
d∆−1eX
d∆−1eX
d∆−1eX
≤c0pk/n∆−1e−ck−1n∆2

e−ck−1n(0.5j2+1)∆2

m−1e(2k)−1cnj2∆2

≤

≤

j=1

j=1

.

e(2k)−1cnj2∆2−ck−1n(1+j2)∆2

There exists a constant c00 > 0 such that for any L:

∞X

pk/‘∆−1e−ck−1‘∆2

LX

µ‘(H, s) ≤L + c0

‘=1

‘=1

≤L + c00k∆−2.

7

Now, consider any time horizon T . If we set n‘ = 0 when ‘ < L, nL = T , and

(cid:24)8k(ln m + ln(T + 1))

(cid:25)

c∆2

,

L =

then

P (s(Z L

1 ) ≤ nL) ≤ P ( ˆ∆(Z L
1 ) = bm−1e(2k)−1cn ˆ∆(Zn

1 ) ≤ 0.5∆) ≤ me−ck−1L∆2 ≤ 1/T.
(cid:25)

(cid:24)8k(ln m + ln(T + 1))

1 )2c in Figure 1, then

That is, if we choose s(Z n
∆R(Epoch-Greedy,H, T ) ≤ 2L + 1 + c00k∆−2 ≤ 2

c∆2

+ 1 + c00k∆−2.

1 ) choice of Section 4.1 when ˆ∆(Z n

The regret for this choice is O(ln T ), which is better than O(T 2/3) of Section 4.1. However, the
constant depends on the gap ∆ which can be small. It is possible to combine the two strategies (that
is, use the s(Z n
1 ) is small) and obtain bounds that not only work
well when the gap ∆ is large, but also not much worse than the bound of Section 4.1 when ∆ is small.
As a special case, we can apply the method in this section to solve the standard bandits problem.
The O(k ln T ) bound of the Epoch-Greedy method matches those more specialized algorithms for
the standard bandits problem, although our algorithm has a larger constant.

5 Conclusion

We consider a generalization of the multi-armed bandits problem, where observable context can
be used to determine which arm to pull and investigate the sample complexity of the explo-
ration/exploitation trade-off for the Epoch-Greedy algorithm.
The Epoch-Greedy algorithm analysis leaves one important open problem behind. Epoch-Greedy is
much better at dealing with large hypothesis spaces or hypothesis spaces with special structures due
to its ability to employ any data-dependent sample complexity bound. However, for ﬁnite hypothesis
space, in the worst case scenario, Exp4 has better dependency on T . In such situations, it’s possible
that a better designed algorithm can achieve both strengths.

References
Auer, P., Cesa-Bianchi, N., & Fischer, P. (2002). Finite time analysis of the multi-armed bandit

problem. Machine Learning, 47, 235–256.

Auer, P., Cesa-Bianchi, N., Freund, Y., & Schapire, R. E. (1995). Gambling in a rigged casino: The

adversarial multi-armed bandit problem. FOCS.

Even-dar, E., Mannor, S., & Mansour, Y. (2006). Action elimination and stopping conditions for the

multi-armed bandit and reinforcement learning problems. JMLR, 7, 1079–1105.

Heckman, J. (1979). Sample selection bias as a speciﬁcation error. Econometrica, 47, 153–161.
Kearns, M., Mansour, Y., & Ng, A. Y. (2000). Approximate planning in large pomdps via reusable

trajectories. NIPS.

Lai, T., & Robbins, H. (1985). Asymptotically efﬁcient adaptive allocation rules. Advances in

Applied Mathematics, 6, 4–22.

Lai, T., & Yakowitz, S. (1995). Machine learning and nonparametric bandit theory. IEEE TAC, 40,

1199–1209.

Pandey, S., Agarwal, D., Chakrabarti, D., & Josifovski, V. (2007). Bandits for taxonomies: a model-

based approach. SIAM Data Mining Conference.

Strehl, A. L., Mesterharm, C., Littman, M. L., & Hirsh, H. (2006). Experience-efﬁcient learning in

associative bandit problems. ICML.

Wang, C.-C., Kulkarni, S. R., & Poor, H. V. (2005). Bandit problems with side observations. IEEE

Transactions on Automatic Control, 50, 338–355.

8

"
945,2007,Using Deep Belief Nets to Learn Covariance Kernels for Gaussian Processes,"We show how to use unlabeled data and a deep belief net (DBN) to learn a good covariance kernel for a Gaussian process. We first learn a deep generative model of the unlabeled data using the fast, greedy algorithm introduced by Hinton et.al. If the data is high-dimensional and highly-structured, a Gaussian kernel applied to the top layer of features in the DBN works much better than a similar kernel applied to the raw input. Performance at both regression and classification can then be further improved by using backpropagation through the DBN to discriminatively fine-tune the covariance kernel.","Using Deep Belief Nets to Learn Covariance Kernels

for Gaussian Processes

Ruslan Salakhutdinov and Geoffrey Hinton

Department of Computer Science, University of Toronto

6 King’s College Rd, M5S 3G4, Canada

rsalakhu,hinton@cs.toronto.edu

Abstract

We show how to use unlabeled data and a deep belief net (DBN) to learn a good
covariance kernel for a Gaussian process. We ﬁrst learn a deep generative model
of the unlabeled data using the fast, greedy algorithm introduced by [7]. If the
data is high-dimensional and highly-structured, a Gaussian kernel applied to the
top layer of features in the DBN works much better than a similar kernel applied
to the raw input. Performance at both regression and classiﬁcation can then be
further improved by using backpropagation through the DBN to discriminatively
ﬁne-tune the covariance kernel.

1 Introduction
Gaussian processes (GP’s) are a widely used method for Bayesian non-linear non-parametric re-
gression and classiﬁcation [13, 16]. GP’s are based on deﬁning a similarity or kernel function that
encodes prior knowledge of the smoothness of the underlying process that is being modeled. Be-
cause of their ﬂexibility and computational simplicity, GP’s have been successfully used in many
areas of machine learning.

Many real-world applications are characterized by high-dimensional, highly-structured data with a
large supply of unlabeled data but a very limited supply of labeled data. Applications such as infor-
mation retrieval and machine vision are examples where unlabeled data is readily available. GP’s
are discriminative models by nature and within the standard regression or classiﬁcation scenario,
unlabeled data is of no use. Given a set of i.i.d.
n=1 and their
associated target labels {yn}N
n=1 ∈ {−1, 1} for regression/classiﬁcation, GP’s
n=1 ∈ R or {yn}N
model p(yn|xn) directly. Unless some assumptions are made about the underlying distribution of
the input data X = [Xl, Xu], unlabeled data, Xu, cannot be used. Many researchers have tried to
use unlabeled data by incorporating a model of p(X). For classiﬁcation tasks, [11] model p(X) as
p(xn|yn)p(yn) and then infer p(yn|xn), [15] attempts to learn covariance kernels
based on p(X), and [10] assumes that the decision boundaries should occur in regions where the
data density, p(X), is low. When faced with high-dimensional, highly-structured data, however,
none of the existing approaches have proved to be particularly successful.

labeled input vectors Xl = {xn}N

a mixture Pyn

In this paper we exploit two properties of DBN’s. First, they can be learned efﬁciently from unla-
beled data and the top-level features generally capture signiﬁcant, high-order correlations in the data.
Second, they can be discriminatively ﬁne-tuned using backpropagation. We ﬁrst learn a DBN model
of p(X) in an entirely unsupervised way using the fast, greedy learning algorithm introduced by [7]
and further investigated in [2, 14, 6]. We then use this generative model to initialize a multi-layer,
non-linear mapping F (x|W ), parameterized by W , with F : X → Z mapping the input vectors in
X into a feature space Z. Typically the mapping F (x|W ) will contain millions of parameters. The
top-level features produced by this mapping allow fairly accurate reconstruction of the input, so they
must contain most of the information in the input vector, but they express this information in a way
that makes explicit a lot of the higher-order structure in the input data.
After learning F (x|W ), a natural way to deﬁne a kernel function is to set K(xi, xj) =
exp (−||F (xi|W ) − F (xj|W )||2). Note that the kernel is initialized in an entirely unsupervised
way. The parameters W of the covariance kernel can then be ﬁne-tuned using the labeled data by

1

maximizing the log probability of the labels with respect to W . In the ﬁnal model most of the in-
formation for learning a covariance kernel will have come from modeling the input data. The very
limited information in the labels will be used only to slightly adjust the layers of features already
discovered by the DBN.

2 Gaussian Processes for Regression and Binary Classiﬁcation
For a regression task, we are given a data set D of i.i.d . labeled input vectors Xl = {xn}N
their corresponding target labels {yn}N
regression model:

n=1 and
n=1 ∈ R. We are interested in the following probabilistic

ǫ ∼ N (ǫ|0, σ2)

yn = f (xn) + ǫ,

(1)
A Gaussian process regression places a zero-mean GP prior over the underlying latent function f
we are modeling, so that a-priori p(f |Xl) =N (f |0, K), where f = [f (x1), ..., f (xn)]T and K is the
covariance matrix, whose entries are speciﬁed by the covariance function Kij = K(xi, xj). The
covariance function encodes our prior notion of the smoothness of f, or the prior assumption that
if two input vectors are similar according to some distance measure, their labels should be highly
correlated. In this paper we will use the spherical Gaussian kernel, parameterized by θ = {α, β}:

Integrating out the function values f, the marginal log-likelihood takes form:

Kij = α exp(cid:0) −

(xi − xj)T (xi − xj)(cid:1)

L = log p(y|Xl) = −

log 2π −

log |K + σ2I| −

yT (K + σ2I)−1y

1
2

1
2β

1
2

N
2

(2)

(3)

(5)

(6)

which can then be maximized with respect to the parameters θ and σ. Given a new test point x
, a
∗
prediction is obtained by conditioning on the observed data and θ. The distribution of the predicted
value y

takes the form:
, D, θ, σ2) = N (y

∗
p(y

at x
∗
|x
∗

∗
= K(x
∗

|kT
∗
∗
= K(x
∗

(K + σ2I)−1y, k

− kT
∗

(K + σ2I)−1k
∗

∗∗

+ σ2)

(4)

, Xl), and k

where k
∗
For a binary classiﬁcation task, we similarly place a zero mean GP prior over the underlying latent
function f, which is then passed through the logistic function g(x) = 1/(1 + exp(−x)) to deﬁne a
prior p(yn = 1|xn) = g(f (xn)). Given a new test point x
, inference is done by ﬁrst obtaining the
∗
distribution over the latent function f

, x
∗

∗∗

):

).

= f (x
∗

∗

p(f

|x
∗

∗

, D) = Z p(f

|x
∗

∗

, Xl, f )p(f |Xl, y)df

which is then used to produce a probabilistic prediction:

p(y

= 1|x
∗

∗

, D) = Z g(f

∗

)p(f

|x
∗

∗

, D)df

∗

The non-Gaussian likelihood makes the integral in Eq. 5 analytically intractable. In our experiments,
we approximate the non-Gaussian posterior p(f |Xl, y) with a Gaussian one using expectation prop-
agation [12]. For more thorough reviews and implementation details refer to [13, 16].

3 Learning Deep Belief Networks (DBN’s)
In this section we describe an unsupervised way of learning a DBN model of the input data X =
[Xl, Xu], that contains both labeled and unlabeled data sets. A DBN can be trained efﬁciently by
using a Restricted Boltzmann Machine (RBM) to learn one layer of hidden features at a time [7].
Welling et. al. [18] introduced a class of two-layer undirected graphical models that generalize
RBM’s to exponential family distributions. This framework will allow us to model real-valued
images of face patches and word-count vectors of documents.

3.1 Modeling Real-valued Data
We use a conditional Gaussian distribution for modeling observed “visible” pixel values x (e.g.
images of faces) and a conditional Bernoulli distribution for modeling “hidden” features h (Fig. 1):

p(xi = x|h) = 1

√2πσi

exp(−

(x−bi−σiPj

2σ2
i

hj wij )2

)

p(hj = 1|x) = g(cid:0)bj +Pi wij

xi

σi(cid:1)

2

(7)

(8)

h

W

x

Binary
Hidden Features

Gaussian
Visible
Units

1000
W
3
1000

1000
W
2
1000

1000
W
1

RBM

RBM

RBM

target y

GP

1000
T
W
3
1000
T
W
2
1000
T
W
1

Feature
Representation
F(X|W)

Input X

Figure 1: Left panel: Markov random ﬁeld of the generalized RBM. The top layer represents stochastic binary
hidden features h and and the bottom layer is composed of linear visible units x with Gaussian noise. When
using a Constrained Poisson Model, the top layer represents stochastic binary latent topic features h and the
bottom layer represents the Poisson visible word-count vector x. Middle panel: Pretraining consists of learning
a stack of RBM’s. Right panel: After pretraining, the RBM’s are used to initialize a covariance function of the
Gaussian process, which is then ﬁne-tuned by backpropagation.

where g(x) = 1/(1 + exp(−x)) is the logistic function, wij is a symmetric interaction term between
input i and feature j, σ2
i is the variance of input i, and bi, bj are biases. The marginal distribution
over visible vector x is:

p(x) = Xh

exp (−E(x, h))

RuPg exp (−E(u, g))du

(9)

. The param-

where E(x, h) is an energy term: E(x, h) = Pi

eter updates required to perform gradient ascent in the log-likelihood is obtained from Eq. 9:

(xi−bi)2

2σ2
i

−Pj bjhj −Pi,j hjwij

xi
σi

∆wij = ǫ

∂ log p(x)

∂wij

= ǫ(<zihj>data − <zihj>model)

(10)

where ǫ is the learning rate, zi = xi/σi, < ·>data denotes an expectation with respect to the data
distribution and < ·>model is an expectation with respect to the distribution deﬁned by the model.
To circumvent the difﬁculty of computing <·>model, we use 1-step Contrastive Divergence [5]:

∆wij = ǫ(<zihj>data − <zihj>recon)

(11)
The expectation < zihj >data deﬁnes the expected sufﬁcient statistics of the data distribution and
is computed as zip(hj = 1|x) when the features are being driven by the observed data from the
training set using Eq. 8. After stochastically activating the features, Eq. 7 is used to “reconstruct”
real-valued data. Then Eq. 8 is used again to activate the features and compute <zihj>recon when
the features are being driven by the reconstructed data. Throughout our experiments we set variances
i = 1 for all visible units i, which facilitates learning. The learning rule for the biases is just a
σ2
simpliﬁed version of Eq. 11.

3.2 Modeling Count Data with the Constrained Poisson Model
We use a conditional “constrained” Poisson distribution for modeling observed “visible” word count
data x and a conditional Bernoulli distribution for modeling “hidden” topic features h:

exp (λi +Pj hjwij )
Pk exp(cid:0)λk +Pj hjWkj(cid:1)

× N(cid:19), p(hj = 1|x) = g(bj +Xi

p(xi = n|h) = Pois(cid:18)n,
where Pois(cid:0)n, λ(cid:1) = e−λλn/n!, wij is a symmetric interaction term between word i and feature
j, N = Pi xi is the total length of the document, λi is the bias of the conditional Poisson model

for word i, and bj is the bias of feature j. The Poisson rate, whose log is shifted by the weighted
combination of the feature activations, is normalized and scaled up by N . We call this the “Con-
strained Poisson Model” since it ensures that the mean Poisson rates across all words sum up to the
length of the document. This normalization is signiﬁcant because it makes learning stable and it
deals appropriately with documents of different lengths.

wij xi) (12)

3

The marginal distribution over visible count vectors x is given in Eq. 9 with an “energy” given by

E(x, h) = −Xi

λixi +Xi

log (xi!) −Xj

bjhj −Xi,j

xihjwij

The gradient of the log-likelihood function is:

∆wij = ǫ

∂ log p(v)

∂wij

= ǫ(<xihj>data − <xihj>model)

(13)

(14)

3.3 Greedy Recursive Learning of Deep Belief Nets

A single layer of binary features is not the best way to capture the structure in the input data. We
now describe an efﬁcient way to learn additional layers of binary features.

After learning the ﬁrst layer of hidden features we have an undirected model that deﬁnes p(v, h)
by deﬁning a consistent pair of conditional probabilities, p(h|v) and p(v|h) which can be used to
sample from the model distribution. A different way to express what has been learned is p(v|h) and
p(h). Unlike a standard, directed model, this p(h) does not have its own separate parameters. It is a
complicated, non-factorial prior on h that is deﬁned implicitly by p(h|v) and p(v|h). This peculiar
decomposition into p(h) and p(v|h) suggests a recursive algorithm: keep the learned p(v|h) but
replace p(h) by a better prior over h, i.e. a prior that is closer to the average, over all the data
vectors, of the conditional posterior over h. So after learning an undirected model, the part we keep
is part of a multilayer directed model.

We can sample from this average conditional posterior by simply using p(h|v) on the training data
and these samples are then the “data” that is used for training the next layer of features. The only
difference from learning the ﬁrst layer of features is that the “visible” units of the second-level RBM
are also binary [6, 3]. The learning rule provided in the previous section remains the same [5].
We could initialize the new RBM model by simply using the existing learned model but with the
roles of the hidden and visible units reversed. This ensures that p(v) in our new model starts out
being exactly the same as p(h) in our old one. Provided the number of features per layer does not
decrease, [7] show that each extra layer increases a variational lower bound on the log probability
of data. To suppress noise in the learning signal, we use the real-valued activation probabilities for
the visible units of every RBM, but to prevent hidden units from transmitting more than one bit of
information from the data to its reconstruction, the pretraining always uses stochastic binary values
for the hidden units.

The greedy, layer-by-layer training can be repeated several times to learn a deep, hierarchical model
in which each layer of features captures strong high-order correlations between the activities of
features in the layer below.

4 Learning the Covariance Kernel for a Gaussian Process
After pretraining, the stochastic activities of the binary features in each layer are replaced by deter-
ministic, real-valued probabilities and the DBN is used to initialize a multi-layer, non-linear map-
ping f (x|W ) as shown in ﬁgure 1. We deﬁne a Gaussian covariance function, parameterized by
θ = {α, β} and W as:

Note that this covariance function is initialized in an entirely unsupervised way. We can now maxi-
mize the log-likelihood of Eq. 3 with respect to the parameters of the covariance function using the
labeled training data[9]. The derivative of the log-likelihood with respect to the kernel function is:

∂L
∂Ky

=

1

2(cid:0)K−1

y yyT K−1

y − K−1
y (cid:1)

(16)

where Ky = K + σ2I is the covariance matrix. Using the chain rule we readily obtain the necessary
gradients:

∂L
∂θ

=

∂L
∂Ky

∂Ky
∂θ

and

∂L
W

=

∂L
∂Ky

∂Ky

∂F (x|W )

∂F (x|W )

∂W

(17)

4

Kij = α exp(cid:0) −

||F (xi|W ) − F (xj |W )||2(cid:1)

1
2β

(15)

−22.07

32.99

−41.15

66.38

27.49

Unlabeled

Training Data

Test Data

A

B

Figure 2: Top panel A: Randomly sampled examples of the training and test data. Bottom panel B: The same
sample of the training and test images but with rectangular occlusions.

Training GPstandard GP-DBNgreedy GP-DBNﬁne
ARD
labels
15.01
6.84
6.31
18.59
10.12
9.23

ARD Sph.
28.57
17.94
12.71
18.16
11.22
16.36
23.15
28.32
15.16
21.06
17.98
14.15

Sph.
15.28
7.25
6.42
19.75
10.56
9.13

ARD
18.37
8.96
8.77
19.42
11.01
10.43

A 100
500
1000
B 100
500
1000

Sph.
22.24
17.25
16.33
26.94
20.20
19.20

GPpca

Sph.
18.13 (10)
14.75 (20)
14.86 (20)
25.91 (10)
17.67 (10)
16.26 (10)

ARD
16.47 (10)
10.53 (80)
10.00 (160)
19.27 (20)
14.11 (20)
11.55 (80)

Table 1: Performance results on the face-orientation regression task. The root mean squared error (RMSE) on
the test set is shown for each method using spherical Gaussian kernel and Gaussian kernel with ARD hyper-
parameters. By row: A) Non-occluded face data, B) Occluded face data. For the GPpca model, the number of
principal components that performs best on the test data is shown in parenthesis.

where ∂F (x|W )/∂W is computed using standard backpropagation. We also optimize the observa-
tion noise σ2. It is necessary to compute the inverse of Ky, so each gradient evaluation has O(N 3)
complexity where N is the number of the labeled training cases. When learning the restricted Boltz-
mann machines that are composed to form the initial DBN, however, each gradient evaluation scales
linearly in time and space with the number of unlabeled training cases. So the pretraining stage
can make efﬁcient use of very large sets of unlabeled data to create sensible, high-level features and
when the amount of labeled data is small. Then the very limited amount of information in the labels
can be used to slightly reﬁne those features rather than to create them.

5 Experimental Results
In this section we present experimental results for several regression and classiﬁcation tasks that
involve high-dimensional, highly-structured data. The ﬁrst regression task is to extract the orienta-
tion of a face from a gray-level image of a large patch of the face. The second regression task is
to map images of handwritten digits to a single real-value that is as close as possible to the integer
represented by the digit in the image. The ﬁrst classiﬁcation task is to discriminate between images
of odd digits and images of even digits. The second classiﬁcation task is to discriminate between
two different classes of news story based on the vector of word counts in each story.

5.1 Extracting the Orientation of a Face Patch
The Olivetti face data set contains ten 64×64 images of each of forty different people. We con-
structed a data set of 13,000 28×28 images by randomly rotating (−90◦ to +90◦), cropping, and
subsampling the original 400 images. The data set was then subdivided into 12,000 training images,
which contained the ﬁrst 30 people, and 1,000 test images, which contained the remaining 10 peo-
ple. 1,000 randomly sampled face patches from the training set were assigned an orientation label.
The remaining 11,000 training images were used as unlabeled data. We also made a more difﬁcult
version of the task by occluding part of each face patch with randomly chosen rectangles. Panel A
of ﬁgure 2 shows randomly sampled examples from the training and test data.

For training on the Olivetti face patches we used the 784-1000-1000-1000 architecture shown in
ﬁgure 1. The entire training set of 12,000 unlabeled images was used for greedy, layer-by-layer
training of a DBN model. The 2.8 million parameters of the DBN model may seem excessive for
12,000 training cases, but each training case involves modeling 625 real-values rather than just a
single real-valued label. Also, we only train each layer of features for a few passes through the
training data and we penalize the squared weights.

5

1.0 

0.8 

2
1
3
 
e
r
u
t
a
e
F

0.6 

0.4 

0.2 

Input Pixel Space

2

3

log β

4

5

6

Feature Space

More Relevant

45

40

35

30

25

20

15

10

5

0
1

90

80

70

60

50

40

30

20

10

 0  

0.2 

0.4 
0.6 
Feature 992

0.8 

1.0 

0
−1

0

1

2

log β

3

4

5

6

Figure 3: Left panel shows a scatter plot of the two most relevant features, with each point replaced by the
corresponding input test image. For better visualization, overlapped images are not shown. Right panel displays
the histogram plots of the learned ARD hyper-parameters log β.

After the DBN has been pretrained on the unlabeled data, a GP model was ﬁtted to the labeled
data using the top-level features of the DBN model as inputs. We call this model GP-DBNgreedy.
GP-DBNgreedy can be signiﬁcantly improved by slightly altering the weights in the DBN. The
GP model gives error derivatives for its input vectors which are the top-level features of the DBN.
These derivatives can be backpropagated through the DBN to allow discriminative ﬁne-tuning of
the weights. Each time the weights in the DBN are updated, the GP model is also reﬁtted. We call
this model GP-DBNﬁne. For comparison, we ﬁtted a GP model that used the pixel intensities of
the labeled images as its inputs. We call this model GPstandard. We also used PCA to reduce the
dimensionality of the labeled images and ﬁtted several different GP models using the projections
onto the ﬁrst m principal components as the input. Since we only want a lower bound on the error
of this model, we simply use the value of m that performs best on the test data. We call this model
GPpca. Table 1 shows the root mean squared error (RMSE) of the predicted face orientations using
all four types of GP model on varying amounts of labeled data. The results show that both GP-
DBNgreedy and GP-DBNﬁne signiﬁcantly outperform a regular GP model. Indeed, GP-DBNﬁne
with only 100 labeled training cases outperforms GPstandard with 1000.

To test the robustness of our approach to noise in the input we took the same data set and created
artiﬁcial rectangular occlusions (see Fig. 2, panel B). The number of rectangles per image was
drawn from a Poisson with λ = 2. The top-left location, length and width of each rectangle was
sampled from a uniform [0,25]. The pixel intensity of each occluding rectangle was set to the mean
pixel intensity of the entire image. Table 1 shows that the performance of all models degrades, but
their relative performances remain the same and GP-DBNﬁne on occluded data is still much better
than GPstandard on non-occluded data.

We have also experimented with using a Gaussian kernel with ARD hyper-parameters, which is a
common practice when the input vectors are high-dimensional:

Kij = α exp(cid:0) −

(xi − xj)T D(xi − xj)(cid:1)

1
2

(18)

where D is the diagonal matrix with Dii = 1/βi, so that the covariance function has a separate
length-scale parameter for each dimension. ARD hyper-parameters were optimized by maximizing
the marginal log-likelihood of Eq. 3. Table 1 shows that ARD hyper-parameters do not improve
GPstandard, but they do slightly improve GP-DBNﬁne and they strongly improve GP-DBNgreedy
and GPpca when there are 500 or 1000 labeled training cases.

The histogram plot of log β in ﬁgure 3 reveals that there are a few extracted features that are very
relevant (small β) to our prediction task. The same ﬁgure (left panel) shows a scatter plot of the two
most relevant features of GP-DBNgreedy model, with each point replaced by the corresponding in-
put test image. Clearly, these two features carry a lot of information about the orientation of the face.

6

Train
labels

A 100
500
1000
100
500
1000

B

GPstandard
ARD
2.27
1.62
1.36

Sph.
1.86
1.42
1.25

GP-DBNgreedy
ARD
Sph.
1.61
1.68
1.27
1.19
1.07
1.14

GP-DBNﬁne
ARD
1.58
1.22
1.10

Sph.
1.63
1.16
1.03

0.0884
0.0222
0.0129

0.1087
0.0541
0.0385

0.0528
0.0100
0.0058

0.0597
0.0161
0.0059

0.0501
0.0055
0.0050

0.0599
0.0104
0.0100

GPpca

Sph.
1.73 (20)
1.32 (40)
1.19 (40)
0.0785 (10)
0.0160 (40)
0.0091 (40)

ARD
2.00 (20)
1.36 (20)
1.22 (80)
0.0920 (10)
0.0235 (20)
0.0127 (40)

Table 2: Performance results on the digit magnitude regression task (A) and and discriminating odd vs. even
digits classiﬁcation task (B). The root mean squared error for regression task on the test set is shown for each
method. For classiﬁcation task the area under the ROC (AUROC) metric is used. For each method we show
1-AUROC on the test set. All methods were tried using both spherical Gaussian kernel, and a Gaussian kernel
with ARD hyper-parameters. For the GPpca model, the number of principal components that performs best on
the test data is shown in parenthesis.

Number of labeled
cases (50% in each class)
100
500
1000

GPstandard GP-DBNgreedy GP-DBNﬁne

0.1295
0.0875
0.0645

0.1180
0.0793
0.0580

0.0995
0.0609
0.0458

Table 3: Performance results using the area under the ROC (AUROC) metric on the text classiﬁcation task.
For each method we show 1-AUROC on the test set.
We suspect that the GP-DBNﬁne model does not beneﬁt as much from the ARD hyper-parameters
because the ﬁne-tuning stage is already capable of turning down the activities of irrelevant top-level
features.

5.2 Extracting the Magnitude Represented by a Handwritten Digit and Discriminating

between Images of Odd and Even Digits

The MNIST digit data set contains 60,000 training and 10,000 test 28×28 images of ten handwritten
digits (0 to 9). 100 randomly sampled training images of each class were assigned a magnitude label.
The remaining 59,000 training images were used as unlabeled data. As in the previous experiment,
we used the 784-1000-1000-1000 architecture with the entire training set of 60,000 unlabeled digits
being used for greedily pretraining the DBN model. Table 2, panel A, shows that GP-DBNﬁne and
GP-DBNgreedy perform considerably better than GPstandard both with and without ARD hyper-
parameters. The same table, panel B, shows results for the classiﬁcation task of discriminating be-
tween images of odd and images of even digits. We used the same labeled training set, but with each
digit categorized into an even or an odd class. The same DBN model was used, so the Gaussian co-
variance function was initialized in exactly the same way for both regression and classiﬁcation tasks.
The performance of GP-DBNgreedy demonstrates that the greedily learned feature representation
captures a lot of structure in the unlabeled input data which is useful for subsequent discrimination
tasks, even though these tasks are unknown when the DBN is being trained.

5.3 Classifying News Stories
The Reuters Corpus Volume II is an archive of 804,414 newswire stories The corpus covers four
major groups: Corporate/Industrial, Economics, Government/Social, and Markets. The data was
randomly split into 802,414 training and 2000 test articles. The test set contains 500 articles of each
major group. The available data was already in a convenient, preprocessed format, where common
stopwords were removed and all the remaining words were stemmed. We only made use of the 2000
most frequently used word stems in the training data. As a result, each document was represented
as a vector containing 2000 word counts. No other preprocessing was done.

For the text classiﬁcation task we used a 2000-1000-1000-1000 architecture. The entire unlabeled
training set of 802,414 articles was used for learning a multilayer generative model of the text docu-
ments. The bottom layer of the DBN was trained using a Constrained Poisson Model. Table 3 shows
the area under the ROC curve for classifying documents belonging to the Corporate/Industrial vs.
Economics groups. As expected, GP-DBNﬁne and GP-DBNgreedy work better than GPstandard.
The results of binary discrimination between other pairs of document classes are very similar to the
results presented in table 3. Our experiments using a Gaussian kernel with ARD hyper-parameters
did not show any signiﬁcant improvements. Examining the histograms of the length-scale parame-

7

ters β, we found that most of the input word-counts as well as most of the extracted features were
relevant to the classiﬁcation task.

6 Conclusions and Future Research
In this paper we have shown how to use Deep Belief Networks to greedily pretrain and discrimina-
tively ﬁne-tune a covariance kernel for a Gaussian Process. The discriminative ﬁne-tuning produces
an additional improvement in performance that is comparable in magnitude to the improvement pro-
duced by using the greedily pretrained DBN. For high-dimensional, highly-structured data, this is
an effective way to make use of large unlabeled data sets, especially when labeled training data is
scarce. Greedily pretrained DBN’s can also be used to provide input vectors for other kernel-based
methods, including SVMs [17, 8] and kernel regression [1], and our future research will concentrate
on comparing our method to other kernel-based semi-supervised learning algorithms [4, 19].

Acknowledgments
We thank Radford Neal for many helpful suggestions. This research was supported by NSERC, CFI
and OTI. GEH is a fellow of CIAR and holds a CRC chair.

References

[1] J. K. Benedetti. On the nonparametric estimation of regression functions. Journal of the Royal Statistical

Society series B, 39:248–253, 1977.

[2] Y. Bengio and Y. Le Cun. Scaling learning algorithms towards AI. In L. Bottou, O. Chapelle, D. DeCoste,

and J. Weston, editors, Large-Scale Kernel Machines. MIT Press, 2007.

[3] Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle. Greedy layer-wise training of deep networks. In

Advances in Neural Information Processing Systems, 2006.

[4] O. Chapelle, B. Sch¨olkopf, and A. Zien. Semi-Supervised Learning. MIT Press, 2006.
[5] G. E. Hinton. Training products of experts by minimizing contrastive divergence. Neural Computation,

14(8):1711–1800, 2002.

[6] G. E. Hinton and R. Salakhutdinov. Reducing the dimensionality of data with neural networks. Science,

313, 2006.

[7] Geoffrey E. Hinton, Simon Osindero, and Yee Whye Teh. A fast learning algorithm for deep belief nets.

Neural Computation, 18(7):1527–1554, 2006.

[8] F. Lauer, C. Y. Suen, and G. Bloch. A trainable feature extractor for handwritten digit recognition. Pattern

Recognition, 40(6):1816–1824, 2007.

[9] N. D. Lawrence and J. Qui˜nonero Candela. Local distance preservation in the GP-LVM through back
In William W. Cohen and Andrew Moore, editors, ICML, volume 148, pages 513–520.

constraints.
ACM, 2006.

[10] N. D. Lawrence and M. I. Jordan. Semi-supervised learning via gaussian processes. In NIPS, 2004.
[11] N. D. Lawrence and B. Sch¨olkopf. Estimating a kernel Fisher discriminant in the presence of label
noise. In Proc. 18th International Conf. on Machine Learning, pages 306–313. Morgan Kaufmann, San
Francisco, CA, 2001.

[12] T. P. Minka. Expectation propagation for approximate bayesian inference. In Jack Breese and Daphne

Koller, editors, UAI, pages 362–369, San Francisco, CA, 2001. Morgan Kaufmann Publishers.

[13] C. E. Rasmussen and C. Williams. Gaussian Processes for Machine Learning. The MIT Press, 2006.
[14] R. Salakhutdinov and G. E. Hinton. Learning a nonlinear embedding by preserving class neighbourhood

structure. In AI and Statistics, 2007.

[15] M. Seeger. Covariance kernels from bayesian generative models.

In Thomas G. Dietterich, Suzanna

Becker, and Zoubin Ghahramani, editors, NIPS, pages 905–912. MIT Press, 2001.

[16] M. Seeger. Gaussian processes for machine learning. Int. J. Neural Syst, 14(2):69–106, 2004.
[17] V. Vapnik. Statistical Learning Theory. Wiley, 1998.
[18] M. Welling, M. Rosen-Zvi, and G. Hinton. Exponential family harmoniums with an application to infor-

mation retrieval. In NIPS 17, pages 1481–1488, Cambridge, MA, 2005. MIT Press.

[19] Xiaojin Zhu, Jaz S. Kandola, Zoubin Ghahramani, and John D. Lafferty. Nonparametric transforms of

graph kernels for semi-supervised learning. In NIPS, 2004.

8

"
121,2007,Kernels on Attributed Pointsets with Applications,"This paper introduces kernels on attributed pointsets, which are sets of vectors embedded in an euclidean space. The embedding gives the notion of neighborhood, which is used to define positive semidefinite kernels on pointsets. Two novel kernels on neighborhoods are proposed, one evaluating the attribute similarity and the other evaluating shape similarity. Shape similarity function is motivated from spectral graph matching techniques. The kernels are tested on three real life applications: face recognition, photo album tagging, and shot annotation in video sequences, with encouraging results.","Kernels on Attributed Pointsets with Applications

Mehul Parsana1

mehul.parsana@gmail.com

Sourangshu Bhattacharya1
sourangshu@gmail.com

Chiranjib Bhattacharyya1

chiru@csa.iisc.ernet.in

K. R. Ramakrishnan2

krr@ee.iisc.ernet.in

Abstract

This paper introduces kernels on attributed pointsets, which are sets of vectors em-
bedded in an euclidean space. The embedding gives the notion of neighborhood,
which is used to deﬁne positive semideﬁnite kernels on pointsets. Two novel ker-
nels on neighborhoods are proposed, one evaluating the attribute similarity and
the other evaluating shape similarity. Shape similarity function is motivated from
spectral graph matching techniques. The kernels are tested on three real life ap-
plications: face recognition, photo album tagging, and shot annotation in video
sequences, with encouraging results.

1 Introduction

In recent times, one of the major challenges in kernel methods has been design of kernels on struc-
tured data e.g. sets [9, 17, 15], graphs [8, 3], strings, automata, etc. In this paper, we propose kernels
on a type of structured objects called attributed pointsets [18]. Attributed pointsets are points em-
bedded in a euclidean space with a vector of attributes attached to each point. The embedding of
points in the euclidean space yields a notion of neighborhood of each point which is exploited in
designing new kernels. Also, we describe the notion of similarity between pointsets which model
many real life scenarios and incorporate it in the proposed kernels.

The main contribution of this paper is deﬁnition of two different kernels on neighborhoods. These
neighborhood kernels are then used to deﬁne kernels on the entire pointsets. The ﬁrst kernel treats the
neighborhoods as sets of vectors for calculating the similarity. Second kernel calculates similarity
in shape of the two neighborhoods. It is motivated using spectral graph matching techniques [16].

We demonstrate practical applications of the kernels on the well known task of face recognition [20],
and two other novel tasks of tagging photo albums and annotation of shots in video sequences. For
the face recognition task, we test our kernels on benchmark datasets and compare their performance
with state-of-the-art algorithms. Our kernels outperform the existing methods in many cases. The
kernels also perform according to expectation on the two novel applications. Section 2 deﬁnes
attributed pointsets and contrasts it with related notions. Section 3 proposes two kernels and section
4 describes experimental results.

2 Deﬁnition and related work

An attributed pointset [18, 1] (a.k.a. point pattern) X is sets of points in Rk with attributes or labels
(real vectors in this case) attached to each point. Thus, X = {(xi, di)|i = 1 . . . n}, where xi ∈ Ru
and di ∈ Rv, l being the dimension of the attribute vector. The number of points in a pointset,

1Dept. of Computer Science & Automation, 2Dept. of Electrical Engineering, Indian Institute of Science,

Bangalore - 560012, India.

1

n, is variable. Also, for practical purposes pointsets with u = 2, 3 are of interest. The construct
of pointsets are richer than sets of vectors [17] because of the structure formed by embedding of
the points in a euclidean space. However, they are less general than attributed graphs because all
attributed graphs cannot be embedded onto a euclidean space. Pointsets are useful in several domains
including computer vision [18], computational biology [5], etc.

The notion of similarity between pointsets is also different from those between sets of vectors,
or graphs. The main aspect of similarity is that there should be correspondences (1-1 mappings)
between the points of a pointset such that the relative positions of corresponding point are same.
Also the attribute vectors of the matching points should be similar. In case of sets of vectors, the
kernel function captures the similarity between aggregate properties of the two sets, such as the
principle angles between spanned subspaces [17], or distance between the distributions generating
the vectors [9]. Kernels on graphs try to capture similarity in the graph topology by comparing the
number of similar paths [3], or comparing steady state distributions on of linear systems on graphs
[8].

For example, consider recognizing faces using local descriptors calculated at some descriptor points
(corner points in this case) on the face. It is necessary that subsets of descriptor points found in two
images of the same face should be approximately superimposable (slight changes may be due to
change of expression) and that the descriptor values for the corresponding points should be roughly
same to ensure similar local features. Thus, a face can be modeled as an attributed pointset X =
{(xi, di)|i = 1 . . . n}, where xi ∈ R2 is the coordinate of ith descriptor point and di ∈ Rv is the
local descriptor vector at the ith descriptor point. Similar arguments can be provided for any object
recognition task.

i )|i = 1 . . . nA} and X B = {(xB

A local descriptor based kernel was proposed for object recognition in similar setting in [12]. Sup-
pose X A = {(xA
i )|i = 1 . . . nB} are two pointsets. The
normalized sum kernel [12] was deﬁned as KN S(X A, X B) = 1
j ))p,
where K(dA
j ) is some kernel function on the descriptors. It was argued in [12] that raising
the kernel to a high power p approximately calculates similarity between matched pairs of vectors.
Using the RBF kernel KRBF (x, y) = e− kx−yk2
, and adjusting the parameter p in σ, we get the
normalized sum kernels as:

nAnB PnA

i=1 PnB

j=1(K(dA

i , dB

i , dB

i , dA

i , dB

σ2

KN S(X A, X B) =

1

nAnB

nA

nB

X

i=1

X

j=1

KRBF (dA

i , dB
j )

(1)

Observe that this kernel doesn’t use the in formation in xi anywhere, and thus is actually a kernel
on a set of vectors. In fact, this kernel can be derived as a special case of the set kernel proposed
in [15]. The kernel K(A, B) = trace(cid:16)Pr(AT ˆGrB) ˆFr(cid:17) becomes K(A, B) = Pij k(ai, bj)fij
for ˆGr = I and F = Pr Fr (whose entries are fij) should be positive semideﬁnite [15]. Thus,
and using KRBF as the
choosing F = 11T (all entries 1) and multiplying the kernel by
kernel on vectors, we get back the kernel deﬁned in (1). The normalized sum kernel is used as the
basic kernel for development and validation of the new kernels proposed here. In the next section,
we incorporate position xi of the points using the concept of neighborhood.

1
An2

n2

B

3 Kernels

3.1 Neighborhood kernels

The key idea in this section is to use spatially co-occurring points of a point to improve the similarity
values given by the kernel function. In other words, we hypothesize that similar points from two
pointsets should also have neighboring points which are similar. Thus, for each point we deﬁne a
neighborhood of the point and weight the similarity between each pair of points with the similarity
between their neighborhoods.
The k-neighborhood Ni of a point (xi, di) in a pointset X is deﬁned as the set of points (including
itself) that are closest to it in the embedding euclidean space. So, Ni = {(xj, dj) ∈ X|kxi − xjk ≤
kxi − xlk∀(xl, dl) 6∈ Ni and |Ni| = k}. The neighborhood kernel between two points (xA
i , dA
i )

2

Figure 1: Correspondences implicitly found by sum and neighborhood kernels

and (xB

j , dB

j ) is deﬁned as:

KN ((xA

i , dA

i ), (xB

j , dB

j )) = KRBF (dA

i , dB

j )×

1

|N A

i ||N B

j | X

(xA

s ,dA

s )∈N A
i

X
t )∈N B
t ,dB
j

(xB

KRBF (dA

s , dB
t )

(2)

(3)

The neighborhood kernel (NK) between two pointsets X A and X B is thus deﬁned as:

KN K(X A, X B) =

1

nAnB

×

nA

nB

X

i=1

X

j=1

KN ((xA

i , dA

i ), (xB

j , dB

j ))

It is easy to see that KN K is a positive semideﬁnite kernel function. Even though KN K is a straight-
forward extension, it considerably improves accuracy of KN S. Figure 1 shows values of KN S and
KN K for 4 pairs of point from two pointsets modeling faces. Dark blue lines indicate best matches
given by KN S while bright blue lines indicate best matches by the KN K. In both cases, KN K gives
the correct match while the KN S fails. Computational complexity of KN K is O(k2n2), k being
neighborhood size and n number of points. The next section proposes a kernel which uses positions
of points (xi) in a neighborhood more strongly to calculate similarity in shape.

3.2 Spectral Neighborhood Kernel

The kernel deﬁned in the previous section still uses a set of vectors kernel for ﬁnding similarity
between the neighborhoods. Here, we are interested in a kernel function which evaluates the simi-
larity in relative position of the corresponding points. Since the neighborhoods being compared are
of ﬁxed size, we assume that all points in a neighborhood have a corresponding point in the other.
Thus, the correspondences are given by a permutation of points in one of the neighborhoods. This
problem can be formulated as the weighted graph matching problem [16], for which spectral method
is one of the popular heuristics. We use the features given by spectral decomposition of adjacency
matrix of the neighborhood to deﬁne a kernel function.

α

Given
a
e− kxs−xtk
N A
the neighborhoods (say N B
norm of a matrix.

i and N B

neighborhood Ni we

=
, ∀s, t|(xs, ds), (xt, dt) ∈ Ni, where α is a parameter. Given two neighborhoods
j , we are thus interested in a permutation π of the basis of adjacency matrix of one of
j )kF is minimized, k.kF being the frobenius

j ), such that kAA

adjacency matrix Ai

as Ai(s, t)

i − π(AB

deﬁne

its

It is well known that a matrix can be fully reconstructed from its spectral decomposition. Also, in the
case that fewer eigenvectors are used, the equation kA − Pk
j , suggests
that eigenvectors corresponding to the higher eigenvalues will give better reconstruction. We use one
eigenvector corresponding to largest eigenvalue. Thus, the approximate adjacency matrix becomes
ˆA = λ1ζ1ζ T
1 .
Let π∗ be the optimal permutation that minimizes k ˆAA
j )kF . Note that here π applied on a
matrix implies permutation of the basis. It is easy to see that same permutation is induced on basis

F = Pn

i − π( ˆAB

i=1 λiζiζ T

j=k+1 λ2

i k2

3

i = |ζ A
i and N B

j (1)|, the spectral projection vectors
of the eigenvectors ζ B
j (1). Call f A
j (1) are eigenvectors corresponding
corresponding to neighborhoods N A
to largest eigenvalue of ˆAA
j , and |ζ(1)| is the vector of absolute values of components of ζ(1).
f (s) can be thought of as projection of the sth point in the corresponding neighborhood on R1. It is
equivalent to seek a permutation π∗ which minimizes kf A
j )k, for comparing neighborhoods
N A

i (1)| and f B
j . Here ζ A

j . The resulting similarity score is:

j = |ζ B
i (1), ζ B

i and N B

i − π(f B

i , ˆAB

S(N A

i , N B

j ) = max
π∈Π

T − kf A

i − π(f B

j )k2
2

(4)

where, T is a threshold for converting the distance measure to similarity, and Π is the set of all
permutations. However, this similarity function is not necessarily positive semideﬁnite.

and f B
j ,
To construct a positive semideﬁnite kernel giving similarity between the vectors f A
i
Let x ∈ X be a
we use the convolution kernel
composite object formed using parts from X1, . . . , Xm.
Let R be a relation over X1 ×
· · · × Xm × X such that R(x1, . . . , xm, x) is true if x is composed of x1, . . . , xm. Let
R−1(x) = (x1, . . . , xm) ∈ X1 × · · · × Xm|R(x1, . . . , xm, x) = true and K1, . . . , Km be kernels
on X1, . . . , Xm, respectively. The convolution kernel K over X is deﬁned as:

technique [7] on discrete structures.

K(x, y) =

X

(x1,...,xm)∈R−1(x),(y1,...,ym)∈R−1(y)

m

Y

i=1

Ki(xi, yi)

(5)

Haussler [7] showed that if K1, . . . , Km are symmetric and positive semideﬁnite, so is K.
For us, let X be the set of all neighborhoods and X1, . . . , Xm be the sets of spectral projections
of all points from all the neighborhoods. Here, note that even if the same point appears in dif-
ferent neighborhoods, the appearances will be considered to be different because the projections
are relative to the neighborhoods. Since, each neighborhood has size k, in our case m = k. The
i ) is true iff the vector (f (1), . . . , f (k)) = π(f A
relation R is deﬁned as R(f (1), . . . , f (k), N A
i )
for some permutation π.
i ) is true iff f (1), . . . , f (k) are
spectral projections the points of neighborhood N A
i ). Also, let Ki, i = 1 . . . k all be RBF ker-
nels with the same parameter β. Thus, from the above equation, the convolution kernel becomes

In other words, R(f (1), . . . , f (k), N A

i , N B

l=1(f A
K(N A
constant (k!)2, we get kernel KSN as:

j ) = k!Pπ∈Π e

β Pl

−1

i (l)−f B

j (π(l)))2

= k!Pπ∈Π e

−kf A

i −π(f B

j )k2

β

. Dividing by the

KSN (N A

i , N B

j ) =

1
k! X

π∈Π

e

−kf A

i −π(f B

j )k2

β

The spectral kernel (SK) KSK between two pointsets X A and X B is thus deﬁned as:

KSK(X A, X B) =

1

nAnB

nA

nB

X

i=1

X

j=1

KRBF (dA

i , dB

j )KSN (N A

i , N B
j )

Following theorem relates KSN (N A

i , N B

j ) to S(N A

i , N B

j ) (eqn 4).

(6)

(7)

Theorem 3.1 Let Ni and Nj be two sub-structures with spectral projection vectors f i and f j. For
large enough value of T such that all points are matched.
e−T
k!

KSN (Ni, Nj))β =

eS(Ni,Nj )

lim
β→0

Proof: Let π∗ be the permutation that gives the optimal score S(Ni, Nj). By deﬁnition, eS(Ni,Nj ) =
eT e−kf i−π∗(f j )k2.

limβ→0(KSN (Ni, Nj))β = limβ→0( 1
= 1
= −1

k! e−kf i−π∗(f j )k2
k! e−kf i−π∗(f j )k2

limβ→0(1 + Pπ∈Π\{π∗} e

k! Pπ∈Π(l) e

−1

−kf i−π(f j )k2

β

)β

β (kf i−π(f j )k2−kf i−π∗(f j )k2))β

4

Angry
88.9%
79.3%
92.9%
96.0%
96.3%

Table 1: Recognition accuracy on AR face dataset (section 4.1)
Scarf
Smile
3.0%
96.3%
2.2%
94.1%
47.4%
78.6%
82.0%
96.0%
97.8%
85.2%
96.19% 95.23% 83.80% 89.52% 60.00%
98.09% 98.09% 85.71% 94.28% 65.71%
99.04% 99.04% 86.66% 93.33% 65.71%

Scream Glasses
48.1%
57.0%
44.4%
32.9%
74.8%
31.3%
80.0%
56.0%
66.7%
80.7%

98.5%
86.66%
92.38%
90.47%

1-NN
PCA
LEM
AMM
Face-ARG
Sum(eq (1))
NK (eq (3))
SK (eq (7))

Left-Light Right-Light

22.2%
7.4%
92.9%

NA

17.8%
7.4%
91.1%

NA

96.3%
80.95%
86.66%
84.76%

Computational complexity of this kernel is O(k!n2), where k is neighborhood size and n is no.
of descriptor points. However, since in practice only small neighborhood sizes are considered, the
computation time doesn’t become prohibitive.

(cid:3)

4 Experimental Results

In order to study the effectiveness of proposed kernels for practical visual tasks, we applied them
on three problems. Firstly, the kernels were applied to the well known problem of face recognition
[20], and results on two benchmark datasets (AR and ORL) were compared to existing state-of-the-
art methods. Next we used the spectral kernel to tag images in personal photo albums using faces
of people present in them. Finally, the spectral kernel was used for annotation of video sequences
using faces of people present.
Attribute For face recognition, faces were modeled as attributed pointsets using local gabor descrip-
tors [10] calculated at the corner points using Harris corner point detector [6]. At each point, gabor
despite for three different scales and four different orientations were calculated. Descriptors for 5
points (4 pixel neighbors and itself) were used for each of the 12 combinations, making a total of
60 descriptors per point. For image tagging and video annotation, faces were modeled as attributed
pointsets using SIFT local descriptors [11], having 128 descriptors per point.

The kernels were implemented in GNU C/C++. LAPACK [2] was used for calculation of eigen-
vectors and GNU GSL for calculation of permutations. LIBSVM [4] was used as the SVM based
classiﬁer for classifying pointsets. The face detector provided in OpenCV was used for detecting
faces in album images and video frames.
Dataset The AR dataset [13] is composed of color images of 135 people (75 men and 60 women).
The DB includes frontal view images with different facial expressions, illumination conditions, and
occlusion by sunglasses and scarf. After removing persons with corrupted images or missing any of
the 8 types of required images, a total of 105 persons (56 men and 49 women) were selected. All the
images were converted to greyscale and rescaled to 154 × 115 pixels. The ORL dataset is composed
of 10 images for each of the 40 persons. The images have minor variations in pose, illumination and
scale. All the 400, 112 × 92 pixel images were used for experiments.

4.1 Face Recognition in AR face DB

The kernels proposed in this paper, were tested pointsets derived from images in AR face DB.
Face recognition was posed as a multiclass classiﬁcation problem, and SVMs were along with the
proposed kernels. The AR face DB is a standard benchmark dataset, on which a recent comparison
of state of the art methods for face recognition has been given in [14]. In table 1, we have restated
the results provided in [14] along with the results of our kernels. All the results reported in table
1 have been obtained using one normal (no occlusion or change of expression) face image as the
training set.

It can be seen that for all the images showing change of expression (Smile, Angry and Scream),
the pointset kernels outperform existing methods. Also, in case of occlusion of face by glasses, the

5

Table 2: Recognition accuracy on ORL dataset (section 4.2)

# of training images →
Sum (eq (1))
NK (eq (3))
SK (eq (7))

1

3

5

70.83% 92.50% 98.00%
71.38% 93.57% 98.00%
71.94% 93.92% 98.00%

Figure 2: Representative cluster from tagging of album

pointset kernels give better results than existing methods. However, in case of occlusion by scarf,
the kernel based method do not perform as well as the Face-ARG or AMM. This failure is due to
introduction of a large number of points in the scarf themselves. It was observed that about 50% of
the descriptor points in the faces having scarfs were in the scarf region of the image. Summing the
similarities over such a large number of extra points makes the overall kernel value noisy.

The proposed approach doesn’t perform better than existing methods on images taken under extreme
variation in lighting conditions. This is due to the fact that values of the local descriptors change
drastically with illumination. Also, some of the corner points disappear under different lighting
condition. However, performance of the kernels is comparable to the existing methods, thus demon-
strating the effectiveness of modeling faces as attributed pointsets.

4.2 Recognition performance on ORL Dataset

Real life problems in face recognition also show minor variations in pose, which are addressed by
testing the kernels on images in the ORL dataset. The problem was posed as a multiclass classi-
ﬁcation problem and SVM was used along with the kernels for classiﬁcation. Table 2 reports the
recognition accuracies of all the three kernels for two different values of parameters, and for 1, 3
and 5 training images.

It can be seen that even with images showing minor variations in pose, the proposed kernels perform
reasonably well. Also, due to change in pose the relative position of points in the pointsets change.
This is reﬂected in the fact that improvement due to addition of position information in kernels
is minor as compared to those shown in AR dataset. For higher number of training images, the
performance of all the kernels saturate at 98%.

4.3 Tagging images in personal albums based on faces

The problem of tagging images in personal albums with names of people present in them, is a prob-
lem of high practical relevance [19]. The spectral kernels were used solve this problem. Images
from publicly available sources like http://www.flickr.com 1 were used for experimenta-
tion. Five personal albums having 20 - 55 images each were downloaded and many images had upto
6 people. Face detector from openCV library was used to automatically detect faces in images. De-
tected faces are cropped and resized to 100 × 100 px resolution. 47 - 265 such faces detected from
each album. To the best of our knowledge, there are no openly available techniques to benchmark
our method against.

Due to non-availability of training data, the problem of image tagging was posed as a clustering
problem. Faces detected from the images were represented as attributed pointsets using SIFT local
descriptors, and spectral kernel was evaluated on them. A threshold based clustering scheme was
used on the distance metric induced by the kernel (d(x, y) = pK(x, x) + K(y, y) − 2 ∗ k(x, y)).
Ideally, each cluster thus obtained should represent a person and images containing faces from a
given cluster should be tagged with the name of that person.

1We intend to make the dataset publicly available if no copyrights are violated

6

Table 3: Face based album tagging

Album no.

1
2
3
4
5

No. of people

(Actual)

(Identiﬁed)

-
14
8
4
3

2
6
4
2
2

% Identiﬁed % False +ve

90%
84%

66.66%
83.33%
80.00%

0%

10.52%
8.33%
19.44%
14.70%

Figure 3: Keyframes of a few shots detected with annotation

Table 3 reports results from tagging experiments for ﬁve albums. No. of people identiﬁed reports
the number clusters having more than one faces, as singleton cluster will always be correct for that
person. Thus, people appearing only once in the entire album are not reported, which reduce the
no. of identiﬁed people. % identiﬁed and % false +ve are averaged over all clusters detected in the
album, and are calculated for each cluster as: % identif ied = N o. of correct f aces in the cluster
T otal no. of f aces of the person and
T otal no. of f aces in the cluster . It can be seen that the kernel performs reasonably
% f alse + ve =
well on the dataset. Figure 2 shows a representative cluster with the ﬁrst 8 images as true +ves and
rest as false +ves.

f alse +ves in the cluster

4.4 Video annotation based on faces

The kernels were also used to perform video shot annotation based faces detected in video se-
quences. Experimentation was performed on videos from “News and Public affair” section of
www.archive.org and music videos from www.youtube.com. Video was sampled at 1 frame
per second and experimental methodology was similar section 4.3 was used on the frames.

Figure 3 shows two representative shots from corresponding to two candidates from “Election 2004,
presidential debate part 2”, and one from “Westlife- Seasons in the Sun” video. The faces annotating
the shots are shown in the left as thumbnails. It may be noted that for videos, high pose variation did
not reduce accuracy of recognition due to gradual changing of pose. The results on detecting shots
were highly encouraging, thus demonstrating the varied applicability of proposed attributed pointset
kernels.

5 Conclusion

In this article, we propose kernels on attributed pointsets. We deﬁne the notion of neighborhood
in an attributed pointset and propose two new kernels. The ﬁrst kernel evaluates attribute similari-
ties between the neighborhoods and uses the co-occurrence information to improve the performance
of kernels on sets of vectors. The second kernel uses the position information more strongly and

7

matches the shapes of neighborhoods. This kernel function is motivated from spectral graph match-
ing techniques.

The proposed kernels were validated on the well known task on face recognition on two popular
benchmark datasets. Results show that the current kernels perform competitively with the state-of-
the-art techniques for face recognition. The spectral kernel was also used to perform two real life
tasks of tagging images in personal photo albums and annotating shots in videos. The results were
encouraging in both cases.

References

[1] Helmut Alt and Leonidas J. Guibas. Discrete geometric shapes: Matching, interpolation, and

approximation A survey. Technical Report B 96-11, 1996.

[2] E. Anderson, Z. Bai, C. Bischof, S. Blackford, J. Demmel, J. Dongarra, J. Du Croz, A. Green-
baum, S. Hammarling, A. McKenney, and D. Sorensen. LAPACK Users’ Guide. Society for
Industrial and Applied Mathematics, Philadelphia, PA, third edition, 1999.

[3] Karsten M. Borgwardt and Hans-Peter Kriegel. Shortest-path kernels on graphs.

In ICDM
’05: Proceedings of the Fifth IEEE International Conference on Data Mining, pages 74–81,
Washington, DC, USA, 2005. IEEE Computer Society.

[4] Chih-Chung Chang and Chih-Jen Lin. LIBSVM: a library for support vector machines, 2001.

Software available at http://www.csie.ntu.edu.tw/∼cjlin/libsvm.

[5] Ingvar Eidhammer, Inge Jonassen, and William R. Taylor. Structure comparison and structure

patterns. Journal of Computational Biology, 7(5):685–716, 2000.

[6] C. Harris and M.J. Stephens. A combined corner and edge detector. In Proc. of Alvey Vision

Conf., 1988.

[7] David Haussler. Convolution kernels on discrete structures. Technical report, University of

California, Santa Cruz, 1999.

[8] Koji Tsuda Hisashi Kashima and Akihiro Inokuchi. Marginalized kernels between labeled

graphs. In Twentieth International Conference on Machine Learning (ICML), 2003.

[9] Risi Kondor and Tony Jebara. A kernel between sets of vectors. In Twentieth International

Conference on Machine Learning (ICML), 2003.

[10] Tai Sing Lee. Image representation using 2d gabor wavelets. IEEE TPAMI, 18(10):959–971,

1996.

[11] D. Lowe. Distinctive image features from scale-invariant keypoints. Int. Journal of Computer

Vision, 20:91–110, 2003.

[12] Siwei Lyu. Mercer kernels for object recognition with local features. In IEEE CVPR, 2005.
[13] A.M. Martinez and R. Benavente. The ar face database. CVC Technical Report, 24, 1998.
[14] Bo Gun Park, Kyoung Mu Lee, and Sang Uk Lee. Face recognition using face-arg matching.

IEEE TPAMI, 27(12):1982–1988, 2005.

[15] Amnon Shashua and Tamir Hazan. Algebraic set kernels with application to inference over

local image representations. In Neural Information Processing Systems (NIPS), 2004.

[16] Shinji Umeyama. An eigendecomposition approach to weighted graph matching problems.

IEEE transactions on pattern analysis and machine intelligence, 10(5):695–703, 1988.

[17] Lior Wolf and Amnon Shashua. Learning over sets using kernel principal angles. Journal of

Machine Learning Research, (4):913–931, 2003.

[18] Haim J. Wolfson and Isidore Rigoutsos. Geometric hashing: An overview. IEEE Comput. Sci.

Eng., 4(4):10–21, 1997.

[19] L. Zhang, L. Chen, M. Li, and H. Zhang. Automated annotation of human faces in family

albums, 2003.

[20] W. Zhao, R. Chellappa, P. J. Phillips, and A. Rosenfeld. Face recognition: A literature survey.

ACM Comput. Surv., 35(4):399–458, 2003.

8

"
1056,2007,Testing for Homogeneity with Kernel Fisher Discriminant Analysis,"We propose to test for the homogeneity of two samples by using Kernel Fisher discriminant Analysis. This provides us with a consistent nonparametric test statistic, for which we derive the asymptotic distribution under the null hypothesis. We give experimental evidence of the relevance of our method on both artificial and real datasets.","Testing for Homogeneity

with Kernel Fisher Discriminant Analysis

Za¨ıd Harchaoui

LTCI, TELECOM ParisTech and CNRS

46, rue Barrault, 75634 Paris cedex 13, France

zaid.harchaoui@enst.fr

Francis Bach

Willow Project, INRIA-ENS

45, rue d’Ulm, 75230 Paris, France
francis.bach@mines.org

´Eric Moulines

LTCI, TELECOM ParisTech and CNRS

46, rue Barrault, 75634 Paris cedex 13, France

eric.moulines@enst.fr

Abstract

We propose to investigate test statistics for testing homogeneity based on kernel
Fisher discriminant analysis. Asymptotic null distributions under null hypothesis
are derived, and consistency against ﬁxed alternatives is assessed. Finally, exper-
imental evidence of the performance of the proposed approach on both artiﬁcial
and real datasets is provided.

1 Introduction

1 , . . . , X (1)

An important problem in statistics and machine learning consists in testing whether the distributions
of two random variables are identical under the alternative that they may differ in some ways. More
precisely, let {X (1)
n2 } be independent random variables taking val-
ues in the input space (X, d), with common distributions P1 and P2, respectively. The problem con-
sists in testing the null hypothesis H0 : P1 = P2 against the alternative HA : P1 6= P2. This problem
arises in many applications, ranging from computational anatomy [10] to process monitoring [7]. We
shall allow the input space X to be quite general, including for example ﬁnite-dimensional Euclidean
spaces or more sophisticated structures such as strings or graphs (see [17]) arising in applications
such as bioinformatics [4].

n1 } and {X (2)

1 , . . . , X (2)

Traditional approaches to this problem are based on distribution functions and use a certain distance
between the empirical distributions obtained from the two samples. The most popular procedures
are the two-sample Kolmogorov-Smirnov tests or the Cramer-Von Mises tests, that have been the
standard for addressing these issues (at least when the dimension of the input space is small, and
most often when X = R). Although these tests are popular due to their simplicity, they are known
to be insensitive to certain characteristics of the distribution, such as densities containing high-
frequency components or local features such as bumps. The low-power of the traditional density
based statistics can be improved on using test statistics based on kernel density estimators [2] and
[1] and wavelet estimators [6]. Recent work [11] has shown that one could difference in means in
RKHSs in order to consistently test for homogeneity. In this paper, we show that taking into account
the covariance structure in the RKHS allows to obtain simple limiting distributions.

The paper is organized as follows: in Section 2 and Section 3, we state the main deﬁnitions and we
construct the test statistics. In Section 4, we give the asymptotic distribution of our test statistic under
the null hypothesis, and investigate, the consistency and the power of the test for ﬁxed alternatives. In

1

Section 5 we provide experimental evidence of the performance of our test statistic on both artiﬁcial
and real datasets. Detailed proofs are presented in the last sections.

2 Mean and covariance in reproducing kernel Hilbert spaces

We ﬁrst highlight the main assumptions we make in the paper on the reproducing kernel, then intro-
duce operator-theoretic tools for working with distributions in inﬁnite-dimensional spaces.

2.1 Reproducing kernel Hilbert spaces

Let (X, d) be a separable metric space, and denote by X the associated σ-algebra. Let X be X-
valued random variable, with probability measure P; the corresponding expectation is denoted E.
Consider a Hilbert space (H,h·,·iH) of functions from X to R. The Hilbert space H is an RKHS if
at each x ∈ X, the point evaluation operator δx : H → R, which maps f ∈ H to f (x) ∈ R, is a
bounded linear functional. To each point x ∈ X, there corresponds an element Φ(x) ∈ H (we call Φ
the feature map) such that hΦ(x), fiH = f (x) for all f ∈ H, and hΦ(x), Φ(y)iH = k(x, y), where
k : X × X → R is a positive deﬁnite kernel. We denote by kfkH = hf, fi1/2
H the associated norm.
It is assumed in the remainder that H is a separable Hilbert space. Note that this is always the case
if X is a separable metric space and if the kernel is continuous (see [18]). Throughout this paper, we
make the following two assumptions on the kernel:

(A1) The kernel k is bounded, that is |k|∞ = sup(x,y)∈X×X k(x, y) < ∞.
(A2) For all probability measures P on (X,X ), the RKHS associated with k(·,·) is dense in

L2(P).

The asymptotic normality of our test statistics is valid without assumption (A2), while consistency
results against ﬁxed alternatives does need (A2). Assumption (A2) is true for translation-invariant
kernels [8], and in particular for the Gaussian kernel on Rd [18]. Note that we do not require the
compactness of X as in [18],

2.2 Mean element and covariance operator

We shall need some operator-theoretic tools to deﬁne mean elements and covariance operators in
RKHS. A linear operator T is said to be bounded if there is a number C such that kT fkH ≤ C kfkH
for all f ∈ H. The operator-norm of T is then deﬁned as the inﬁmum of such numbers C, that is
kTk = supkf kH≤1 kT fkH (see [9]).
We recall below some basic facts about ﬁrst and second-order moments of RKHS-valued random
variables. IfR k1/2(x, x)P(dx) < ∞, the mean element µP is deﬁned for all functions f ∈ H as the
unique element in H satisfying,

(1)

(2)

If furthermoreR k(x, x)P(dx) < ∞, then the covariance operator ΣP is deﬁned as the unique linear
operator onto H satisfying for all f, g ∈ H,

hµP, fiH = Pf def= Z f dP .

hf, ΣPgiH

def= Z (f − Pf )(g − Pg)dP .

Note that when assumption (A2) is satisﬁed, then the map from P 7→ µP is injective. The operator
ΣP is a self-adjoint nonnegative trace-class operator. In the sequel, the dependence of µP and ΣP in
P is omitted whenever there is no risk of confusion.
Given a sample {X1, . . . , Xn}, the empirical estimates respectively of the mean element and the
covariance operator are then deﬁned using empirical moments and lead to:

ˆµ = n−1

nXi=1

k(Xi,·) ,

ˆΣ = n−1

nXi=1

2

k(Xi,·) ⊗ k(Xi,·) − ˆµ ⊗ ˆµ .

(3)

The operator Σ is a self-adjoint nonnegative trace-class operators. Hence, it can de diagonalized in
an orthonormal basis, with a spectrum composed of a strictly decreasing sequence λp > 0 tending

to zero and potentially a null space N (Σ) composed of functions f in H such thatR {f − Pf}2dP =

0 [5], i.e., functions which are constant in the support of P.
The null space may be reduced to the null element (in particular for the Gaussian kernel), or may
be inﬁnite-dimensional. Similarly, there may be inﬁnitely many strictly positive eigenvalues (true
nonparametric case) or ﬁnitely many (underlying ﬁnite dimensional problems).

3 KFDA-based test statistic

1 , . . . , X (1)

1 , . . . , X (2)

n1 } and {X (2)

In the feature space, the two-sample homogeneity test procedure can be formulated as follows. Given
{X (1)
n2 } from distributions P1 and P2, two independent identically
distributed samples respectively from P1 and P2, having mean and covariance operators respectively
given by (µ1, Σ1) and (µ2, Σ2), we wish to test the null hypothesis H0, µ1 = µ2 and Σ1 = Σ2,
against the alternative hypothesis HA, µ1 6= µ2.
In this paper, we tackle the problem by using a (regularized) kernelized version of the Fisher dis-
def= (n1/n)Σ1 +(n2/n)Σ2 the pooled covariance operator, where
criminant analysis. Denote by ΣW
n def= n1 + n2, corresponding to the within-class covariance matrix in the ﬁnite-dimensional setting
def= (n1n2/n2)(µ2−µ1)⊗(µ2−µ1) the between-class covariance oper-
(see [14]. Let us denote ΣB
ator. For a = 1, 2, denote by (ˆµa, ˆΣa) respectively the empirical estimates of the mean element and
def= (n1/n) ˆΣ1 + (n2/n) ˆΣ2
the covariance operator, deﬁned as previously stated in (3). Denote ˆΣW
def= (n1n2/n2)(ˆµ2 − ˆµ1) ⊗ (ˆµ2 − ˆµ1) the em-
the empirical pooled covariance estimator, and ˆΣB
pirical between-class covariance operator. Let {γn}n≥0 be a sequence of strictly positive numbers.
The maximum Fisher discriminant ratio serves as a basis of our test statistics:

n max
f ∈H

Df, ˆΣBfEH

Df, ( ˆΣW + γnI)fEH

=

n1n2

n

(cid:13)(cid:13)(cid:13)( ˆΣW + γnI)− 1
2 ˆδ(cid:13)(cid:13)(cid:13)

2

H

,

(4)

where I denotes the identity operator. Note that if the input space is Euclidean, e.g. X = Rd, the
kernel is linear k(x, y) = x⊤y and γn = 0, this quantity matches the so-called Hotelling’s T 2-
statistic in the two-sample case [15]. Moreover, in practice it may be computed thanks to the kernel
trick, adapted to the kernel Fisher discriminant analysis and outlined in [17, Chapter 6]. We shall
make the following assumptions respectively on Σ1 and Σ2

p

p=1 λ1/2

(B1) For u = 1, 2, the eigenvalues {λp(Σu)}p≥1 satisfyP∞
(B2) For u = 1, 2, there are inﬁnitely many strictly positive eigenvalues {λp(Σu)}p≥1 of Σu.
The statistical analysis conducted in Section 4 shall demonstrate, as γn → 0 at an appropriate
rate, the need to respectively recenter and rescale (a standard statistical transformation known as
studentization) the maximum Fisher discriminant ratio, in order to get a theoretically well-calibrated
test statistic. These roles, recentering and rescaling, will be played respectively by d1(ΣW , γ) and
d2(ΣW , γ), where for a given compact operator Σ with decreasing eigenvalues λp(S), the quantity
dr(Σ, γ) is deﬁned for all q ≥ 1 as

(Σu) < ∞.

dr(Σ, γ) def= ( ∞Xp=1

p)1/r

(λp + γ)−rλr

.

(5)

4 Theoretical results

We consider in the sequel the following studentized test statistic:

n1n2

n

bTn(γn) =

2

(cid:13)(cid:13)(cid:13)( ˆΣW + γnI)−1/2ˆδ(cid:13)(cid:13)(cid:13)

√2d2( ˆΣW , γn)

3

H − d1( ˆΣW , γn)

.

(6)

In this paper, we ﬁrst consider the asymptotic behavior of bTn under the null hypothesis, and then

against a ﬁxed alternative. This will establish that our nonparametric test procedure is consistent in
power.

4.1 Asymptotic normality under null hypothesis

In this section, we derive the distribution of the test statistics under the null hypothesis H0 : P1 = P2
of homogeneity, i.e. µ1 = µ2 and Σ1 = Σ2 = Σ. As γn → 0 tends to zero,
Theorem 1. Assume (A1) and (B1). If P1 = P2 = P and if γn + γ−1

n n−1/2 → 0, then

(7)

bTn(γn) D−→ N (0, 1)

The proof is postponed to Section 7. Under the assumptions of Theorem 1, the sequence of tests that
rejects the null hypothesis when ˆTn(γn) ≥ z1−α, where z1−α is the (1− α)-quantile of the standard
normal distribution, is asymptotically level α. Note that the limiting distribution does not depend on
the kernel nor on the regularization parameter.

4.2 Power consistency

We study the power of the test based on bTn(γn) under alternative hypotheses. The minimal re-

quirement is to to prove that this sequence of tests is consistent in power. A sequence of tests of
constant level α is said to be consistent in power if the probability of accepting the null hypothesis
of homogeneity goes to zero as the sample size goes to inﬁnity under a ﬁxed alternative.

The following proposition shows that the limit is ﬁnite, strictly positive and independent of the kernel
otherwise (see [8] for similar results for canonical correlation analysis). The following result gives
on

, i.e.the population counterpart of(cid:13)(cid:13)(cid:13)( ˆΣ−1/2

n n−1/2 → 0, then for any probability distributions

which our test statistics is based upon.
Proposition 2. Assume (A1) and (A2). If γn +γ−1
P1 and P2,

some useful insights on(cid:13)(cid:13)(cid:13)Σ−1/2
W δ(cid:13)(cid:13)(cid:13)H
ρ1ρ2(cid:18)1 −Z

=

1

H

2

W + γnI)−1/2ˆδ(cid:13)(cid:13)(cid:13)H
dρ(cid:19)−1

,

p1p2

ρ1p1 + ρ2p2

dν(cid:19)(cid:18)Z

p1p2

ρ1p1 + ρ2p2

W δ(cid:13)(cid:13)(cid:13)
(cid:13)(cid:13)(cid:13)Σ−1/2
The norm(cid:13)(cid:13)(cid:13)Σ−1/2
W δ(cid:13)(cid:13)(cid:13)

2

H

where ν is any probability measure such that P1 and P2 are absolutely continuous w.r.t. ν and p1
and p2 are the densities of P1 and P2 with respect to ν.

zero if the χ2-divergence is null, that is, if and only if P1 = P2.

is ﬁnite when the χ2-divergenceR p−1

1 (p2 − p1)2dρ is ﬁnite. It is equal to

By combining the two previous propositions, we therefore obtain the following consistency Theo-
rem.
Theorem 3. Assume (A1) and (A2). Let P1 and P2 be two distributions over (X,X ), such that
P2 6= P1. If γn + γ−1

n n−1/2 → 0, then

5 Experiments

PHA(bTn(γ) > z1−α) → ∞ .

(8)

In this section, we investigate the experimental performances of our test statistic KFDA, and com-
pare it in terms of power against other nonparametric test statistics.

5.1 Artiﬁcial data

We shall focus here on a particularly simple setting, in order analyze the major issues arising in
applying our approach in practice. Indeed, we consider the periodic smoothing spline kernel (see

4

10−1

γ =
KFDA 0.01±0.0032
MMD 0.01±0.0023

10−4
0.11±0.0062
id.

10−7
0.98±0.0031
id.

10−10
0.99±0.0001
id.

Table 1: Evolution of power of KFDA and MMD respectively, as γ goes to 0.

[19] for a detailed derivation), for which explicit formulae are available for the eigenvalues of the
corresponding covariance operator when the underlying distribution is uniform. This allows us to
alleviate the issue of estimating the spectrum of the covariance operator, and weigh up the practical
impact of the regularization on the power of our test statistic.

Periodic smoothing spline kernel Consider X as the two-dimensional circle identiﬁed with the
interval [0, 1] (with periodicity conditions). We consider the strictly positive sequence Kν =
(2πν)−2m and the following norm:

H = hf, c0i2
K0

kfk2

hf, cνi2 + hf, sνi2

Kν

+Xν>0

where cν(t) = √2 cos 2πνt and sν(t) = √2 sin 2πνt for ν ≥ 1 and c0(t) = 1X. This is always an

RKHS norm associated with the following kernel
(−1)m−1
(2m)!

K(s, t) =

B2m((s − t) − ⌊s − t⌋)

where B2m is the 2m-th Bernoulli polynomial. We have B2(x) = x2 − x + 1/6.
We consider the following testing problem

H0 :
HA :

p1 = p2
p2 6= p2

with p1 the uniform density (i.e., the density with respect to the Lebesgue measure is equal to c0),
and densities p2 = p1(c0 + .25∗ c4). The covariance operator Σ(p1) has eigenvectors c0, cν, sν with
eigenvalues 0 for c0 and Kν for others.

Comparison with MMD We conducted experimental comparison in terms of power, for m = 2
and n = 104 and ε = 0.5. All quantities involving the eigenvalues of the covariance operator were
computed from their counterparts instead of being estimated. The sampling from pn
2 was performed
by inverting the cumulative distribution function. The table below displays the results, averaged
over 10 Monte-Carlo runs.

5.2 Speaker veriﬁcation

We conducted experiments in a speaker veriﬁcation task [3], on a subset of 8 female speakers using
data from the NIST 2004 Speaker Recognition Evaluation. We refer the reader to [16] for instance
for details on the pre-processing of data. The ﬁgure shows averaged results over all couples of speak-
ers. For each couple of speaker, at each run we took 3000 samples of each speaker and launched our
KFDA-test to decide whether samples come from the same speaker or not, and computed the type
II error by comparing the prediction to ground truth. We averaged the results for 100 runs for each
couple, and all couples of speaker. The level was set to α = 0.05, since the empirical level seemed
to match the prescribed for this value of the level as we noticed in previous subsection. We per-
formed the same experiments for the Maximum Mean Discrepancy and the Tajvidi-Hall test statistic
(TH, [13]). We summed up the results by plotting the ROC-curve for all competing methods. Our
method reaches good empirical power for a small value of the prescribed level (1 − β = 90% for
α = 0.05%). Maximum Mean Discrepancy also yields good empirical performance on this task.

6 Conclusion

We proposed a well-calibrated test statistic, built on kernel Fisher discriminant analysis, for which
we proved that the asymptotic limit distribution under null hypothesis is standard normal distribu-
tion. Our test statistic can be readily computed from Gram matrices once a kernel is deﬁned, and

5

1

0.8

0.6

0.4

0.2

r
e
w
o
P

0
 
0

0.1

ROC Curve

 

KFDA
MMD
TH

0.4

0.5

0.2

0.3

Level

Figure 1: Comparison of ROC curves in a speaker veriﬁcation task

allows us to perform nonparametric hypothesis testing for homogeneity for high-dimensional data.
The KFDA-test statistic yields competitive performance for speaker identiﬁcation.

7 Sketch of proof of asymptotic normality under null hypothesis

Outline. The proof of the asymptotic normality of the test statistics under null hypothesis follows
four steps. As a ﬁrst step, we derive an asymptotic approximation of the test statistics as γn +
n n−1/2 → 0 , where the only remaining stochastic term is ˆδ. The test statistics is then spanned
γ−1
onto the eigenbasis of Σ, and decomposed into two terms Bn and Cn. The second step allows to
prove the asymptotic negligibility of Bn, while the third step establishes the asymptotic normality
of Cn by a martingale central limit theorem (MCLT).

Step 1: bTn(γn) = ˜Tn(γn) + oP (1). First, we may prove, using perturbation results of covariance

n n−1/2 → 0 , we have

operators, that, as γn + γ−1

(n1n2/n) (cid:13)(cid:13)(cid:13)(Σ + γI)−1/2 ˆδ(cid:13)(cid:13)(cid:13)

√2d2(Σ, γ)

2

H − d1(Σ, γ)

+ oP (1) .

(9)

For ease of notation, in the following, we shall often omit Σ in quantities involving it. Hence, from
now on, λp, λq, d2,n stand for λp(Σ), λq(Σ), d2(Σ, γn). Deﬁne

bTn(γn) =
def= 
(cid:16) n2
n1n(cid:17)1/2(cid:16)ep(X (1)
−(cid:16) n1
n2n(cid:17)1/2(cid:16)ep(X (2)
nXi=1

i

Yn,p,i

1 )](cid:17)
) − E[ep(X (1)
1 )](cid:17) n1 + 1 ≤ i ≤ n .
i−n1 ) − E[ep(X (2)

1 ≤ i ≤ n1 ,

(10)

We now give formulas for the moments of {Yn,p,i}1≤i≤n,p≥1, often used in the proof. Straightfor-
ward calculations give

E[Yn,p,iYn,q,i] = λ1/2

p λ1/2

q

δp,q ,

(11)

while the Cauchy-Schwarz inequality and the reproducing property give

Denote Sn,p
with

An

def=

n1n2

n

.

p λ1/2

q

Cov(Y 2

n,p,i, Y 2

n,q,i) ≤ Cn−2|k|∞λ1/2

(12)
i=1 Yn,p,i. Using Eq. (11), our test statistics now writes as ˜Tn = (√2d2,n)−1An
n,p(cid:9) = Bn + 2Cn .

(λp + γn)−1(cid:8)S2

def= Pn
(cid:13)(cid:13)(cid:13)(Σ + γnI)−1/2ˆδ(cid:13)(cid:13)(cid:13)

n,p − ES2

− d1,n =

∞Xp=1

(13)

2

6

Step 2: Bn = oP (1). The proof consists in computing the variance of this term. Since the variables

(14)

(15)

.

where Bn and Cn are deﬁned as follows

Bn

def=

∞Xp=1
∞Xp=1

nXi=1(cid:8)Y 2

n,p,i − EY 2
nXi=1

n,p,i(cid:9) ,
Yn,p,i
i−1Xj=1
Yn,p,i and Yn,q,j are independent if i 6= j, then Var(Bn) =Pn
n,p,i − E[Y 2

(λp + γn)−1{Y 2

(λp + γn)−1

def=

Cn

Yn,p,j
n,p,i]}!

i=1 vn,i, where

vn,i

def= Var ∞Xp=1
∞Xp,q=1
Using Eq. (12), we get Pn

=

negligible, since by assumption we have γ−1

i=1 vn,i ≤ Cn−1γ−2

p=1 λ1/2

p (cid:17)2
n (cid:16)P∞
n n−1/2 → 0 andP∞

p=1 λ1/2

p < ∞.

(λp + γn)−1(λq + γn)−1Cov(Y 2

n,p,i, Y 2

n,q,i) .

where the RHS above is indeed

Step 3: d−1
martingale differences (see e.g. [12, Theorem 3.2]). For = 1, . . . , n, denote

D−→ N(0, 1/2). We use the central limit theorem (MCLT) for triangular arrays of

2,nCn

ξn,i

def= d−1
2,n

(λp + γn)−1Yn,p,iMn,p,i−1 , where Mn,p,i

def=

Yn,p,j ,

(16)

and let Fn,i = σ (Yn,p,j, p ∈ {1, . . . , n}, j ∈ {0, . . . , i}). Note that, by construction, ξn,i is a mar-
tingale increment, i.e. E [ ξn,i |Fn,i−1] = 0. The ﬁrst step in the proof of the CLT is to establish
that

iXj=1

∞Xp=1

s2
n =

nXi=1

n,i(cid:12)(cid:12)Fn,i−1(cid:3) P−→ 1/2 .
E(cid:2) ξ2

The second step of the proof is to establish the negligibility condition. We use [12, Theorem
3.2], which requires to establish that max1≤i≤n |ξn,i| P−→ 0 (smallness) and E(max1≤i≤n ξ2
n,i)
is bounded in n (tightness), where ξn,i is deﬁned in (16). We will establish the two conditions
simultaneously by checking that

ξ2

n,i(cid:19) = o(1) .

1≤i≤n

E(cid:18) max
nXi=1

M 2

nXi=1

∞Xp=1
2,nXp6=q

Splitting the sum s2

n, between diagonal terms Dn, and off-diagonal terms En, we have

Dn = d−2
2,n

(λp + γn)−2

n,p,i−1

E[Y 2

n,p,i] ,

En = d−2

(λp + γn)−1(λq + γn)−1

Mn,p,i−1Mn,q,i−1E[Yn,p,iYn,q,i] .

Consider ﬁrst the diagonal terms En. We ﬁrst compute its mean. Note that E[M 2

n,p,i] =

j=1

Pi
∞Xp=1

E[Y 2

n,p,j]. Using Eq. (11) we get

(λp + γn)−2

n,p,j]E[Y 2

n,p,i]

E[Y 2

nXi=1
i−1Xj=1
(λp + γn)−2
"" nXi=1

=

1
2

∞Xp=1

E[Y 2

n,p,i]#2

−

nXi=1

E2[Y 2

n,p,i]

7

=

1
2

d2

2,n(cid:8)1 + O(n−1)(cid:9) .

(17)

(18)

(19)

(20)

Therefore, E[Dn] = 1/2 + o(1). Next, we may prove that Dn − E[Dn] = oP (1) is negligible, by
checking that Var[Dn] = o(1). We ﬁnally consider En deﬁned in (20), and prove that En = oP (1)
using Eq. (11). This concludes the proof of Eq. (17).
We ﬁnally show Eq. (18). Since |Yn,p,i| ≤ n−1/2|k|1/2

∞ P-a.s we may bound

1≤i≤n|ξn,i| ≤ Cd−1
max

2,nn−1/2

∞Xp=1

(λp + γn)−1 max

1≤i≤n|Mn,p,i−1| .

(21)

n,p,n−1] ≤ Cλ1/2

p

.

Then, the Doob inequality implies that E1/2[max1≤i≤n |Mn,p,i−1|2] ≤ E1/2[M 2
Plugging this bound in (21), the Minkowski inequality
p ) ,

n,i(cid:19) ≤ C(d−1

E1/2(cid:18) max

2,nγ−1

n n−1/2

λ1/2

∞Xp=1

ξ2

1≤i≤n

and the proof is concluded using the fact that γn + γ−1

n n−1/2 → 0 and Assumption (B1).

References
[1] D. L. Allen. Hypothesis testing using an L1-distance bootstrap. The American Statistician, 51(2):145–

150, 1997.

[2] N. H. Anderson, P. Hall, and D. M. Titterington. Two-sample test statistics for measuring discrepancies
between two multivariate probability density functions using kernel-based density estimates. Journal of
Multivariate Analysis, 50(1):41–54, 1994.

[3] F. Bimbot, J.-F. Bonastre, C. Fredouille, G. Gravier, I. Magrin-Chagnolleau, S. Meignier, T. Merlin,
J. Ortega-Garcia, D. Petrovska-Delacretaz, and D. A. Reynolds. A tutorial on text-independent speaker
veriﬁcation. EURASIP, 4:430–51, 2004.

[4] K. Borgwardt, A. Gretton, M. Rasch, H.-P. Kriegel, Sch¨olkopf, and A. J. Smola. Integrating structured

biological data by kernel maximum mean discrepancy. Bioinformatics, 22(14):49–57, 2006.

[5] H. Brezis. Analyse Fonctionnelle. Masson, 1980.
[6] C. Butucea and K. Tribouley. Nonparametric homogeneity tests. Journal of Statistical Planning and

Inference, 136(3):597–639, 2006.

[7] E. Carlstein, H. M¨uller, and D. Siegmund, editors. Change-point Problems, number 23 in IMS Mono-

graph. Institute of Mathematical Statistics, Hayward, CA, 1994.

[8] K. Fukumizu, A. Gretton, X. Sunn, and B. Sch¨olkopf. Kernel measures of conditional dependence. In

Adv. NIPS, 2008.

[9] I. Gohberg, S. Goldberg, and M. A. Kaashoek. Classes of Linear Operators Vol. I. Birkh¨auser, 1990.
[10] U. Grenander and M. Miller. Pattern Theory: from representation to inference. Oxford Univ. Press, 2007.
[11] A. Gretton, K. Borgwardt, M. Rasch, B. Schoelkopf, and A. Smola. A kernel method for the two-sample

problem. In Adv. NIPS, 2006.

[12] P. Hall and C. Heyde. Martingale Limit Theory and Its Application. Academic Press, 1980.
[13] P. Hall and N. Tajvidi. Permutation tests for equality of distributions in high-dimensional settings.

Biometrika, 89(2):359–374, 2002.

[14] T. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical Learning. Springer Series in

Statistics. Springer, 2001.

[15] E. Lehmann and J. Romano. Testing Statistical Hypotheses (3rd ed.). Springer, 2005.
[16] J. Louradour, K. Daoudi, and F. Bach. Feature space mahalanobis sequence kernels: Application to svm

speaker veriﬁcation. IEEE Transactions on Audio, Speech and Language Processing, 2007. To appear.

[17] J. Shawe-Taylor and N. Cristianini. Kernel Methods for Pattern Analysis. Cambridge Univ. Press, 2004.
[18] I. Steinwart, D. Hush, and C. Scovel. An explicit description of the reproducing kernel hilbert spaces of

gaussian RBF kernels. IEEE Transactions on Information Theory, 52:4635–4643, 2006.

[19] G. Wahba. Spline Models for Observational Data. SIAM, 1990.

8

"
934,2007,Sparse deep belief net model for visual area V2,"Motivated in part by the hierarchical organization of cortex, a number of algorithms have recently been proposed that try to learn hierarchical, or ``deep,'' structure from unlabeled data. While several authors have formally or informally compared their algorithms to computations performed in visual area V1 (and the cochlea), little attempt has been made thus far to evaluate these algorithms in terms of their fidelity for mimicking computations at deeper levels in the cortical hierarchy. This paper presents an unsupervised learning model that faithfully mimics certain properties of visual area V2. Specifically, we develop a sparse variant of the deep belief networks of Hinton et al. (2006). We learn two layers of nodes in the network, and demonstrate that the first layer, similar to prior work on sparse coding and ICA, results in localized, oriented, edge filters, similar to the Gabor functions known to model V1 cell receptive fields. Further, the second layer in our model encodes correlations of the first layer responses in the data. Specifically, it picks up both collinear (``contour'') features as well as corners and junctions. More interestingly, in a quantitative comparison, the encoding of these more complex ``corner'' features matches well with the results from the Ito & Komatsu's study of biological V2 responses. This suggests that our sparse variant of deep belief networks holds promise for modeling more higher-order features.","Sparse deep belief net model for visual area V2

Chaitanya Ekanadham

Computer Science Department

Andrew Y. Ng

Honglak Lee

Stanford University
Stanford, CA 94305

{hllee,chaitu,ang}@cs.stanford.edu

Abstract

Motivated in part by the hierarchical organization of the cortex, a number of al-
gorithms have recently been proposed that try to learn hierarchical, or “deep,”
structure from unlabeled data. While several authors have formally or informally
compared their algorithms to computations performed in visual area V1 (and the
cochlea), little attempt has been made thus far to evaluate these algorithms in terms
of their ﬁdelity for mimicking computations at deeper levels in the cortical hier-
archy. This paper presents an unsupervised learning model that faithfully mimics
certain properties of visual area V2. Speciﬁcally, we develop a sparse variant of
the deep belief networks of Hinton et al. (2006). We learn two layers of nodes in
the network, and demonstrate that the ﬁrst layer, similar to prior work on sparse
coding and ICA, results in localized, oriented, edge ﬁlters, similar to the Gabor
functions known to model V1 cell receptive ﬁelds. Further, the second layer in our
model encodes correlations of the ﬁrst layer responses in the data. Speciﬁcally, it
picks up both colinear (“contour”) features as well as corners and junctions. More
interestingly, in a quantitative comparison, the encoding of these more complex
“corner” features matches well with the results from the Ito & Komatsu’s study
of biological V2 responses. This suggests that our sparse variant of deep belief
networks holds promise for modeling more higher-order features.

1 Introduction
The last few years have seen signiﬁcant interest in “deep” learning algorithms that learn layered,
hierarchical representations of high-dimensional data. [1, 2, 3, 4]. Much of this work appears to
have been motivated by the hierarchical organization of the cortex, and indeed authors frequently
compare their algorithms’ output to the oriented simple cell receptive ﬁelds found in visual area V1.
(E.g., [5, 6, 2]) Indeed, some of these models are often viewed as ﬁrst attempts to elucidate what
learning algorithm (if any) the cortex may be using to model natural image statistics.
However, to our knowledge no serious attempt has been made to directly relate, such as through
quantitative comparisons, the computations of these deep learning algorithms to areas deeper in the
cortical hierarchy, such as to visual areas V2, V4, etc. In this paper, we develop a sparse variant
of Hinton’s deep belief network algorithm, and measure the degree to which it faithfully mimics
biological measurements of V2. Speciﬁcally, we take Ito & Komatsu [7]’s characterization of V2 in
terms of its responses to a large class of angled bar stimuli, and quantitatively measure the degree to
which the deep belief network algorithm generates similar responses.
Deep architectures attempt to learn hierarchical structure, and hold the promise of being able to
ﬁrst learn simple concepts, and then successfully build up more complex concepts by composing
together the simpler ones. For example, Hinton et al. [1] proposed an algorithm based on learning
individual layers of a hierarchical probabilistic graphical model from the bottom up. Bengio et al. [3]
proposed a similarly greedy algorithm, one based on autoencoders. Ranzato et al. [2] developed an
energy-based hierarchical algorithm, based on a sequence of sparsiﬁed autoencoders/decoders.

1

In related work, several studies have compared models such as these, as well as non-
hierarchical/non-deep learning algorithms, to the response properties of neurons in area V1. A study
by van Hateren and van der Schaaf [8] showed that the ﬁlters learned by independent components
analysis (ICA) [9] on natural image data match very well with the classical receptive ﬁelds of V1
simple cells. (Filters learned by sparse coding [10, 11] also similarly give responses similar to V1
simple cells.) Our work takes inspiration from the work of van Hateren and van der Schaaf, and
represents a study that is done in a similar spirit, only extending the comparisons to a deeper area in
the cortical hierarchy, namely visual area V2.
2 Biological comparison
2.1 Features in early visual cortex: area V1
The selectivity of neurons for oriented bar stimuli in cortical area V1 has been well documented [12,
13]. The receptive ﬁeld of simple cells in V1 are localized, oriented, bandpass ﬁlters that resemble
gabor ﬁlters. Several authors have proposed models that have been either formally or informally
shown to replicate the gabor-like properties of V1 simple cells. Many of these algorithms, such
as [10, 9, 8, 6], compute a (approximately or exactly) sparse representation of the natural stimuli
data. These results are consistent with the “efﬁcient coding hypothesis” which posits that the goal
of early visual processing is to encode visual information as efﬁciently as possible [14]. Some
hierarchical extensions of these models [15, 6, 16] are able to learn features that are more complex
than simple oriented bars. For example, hierarchical sparse models of natural images have accounted
for complex cell receptive ﬁelds [17], topography [18, 6], colinearity and contour coding [19]. Other
models, such as [20], have also been shown to give V1 complex cell-like properties.
2.2 Features in visual cortex area V2
It remains unknown to what extent the previously described algorithms can learn higher order fea-
tures that are known to be encoded further down the ventral visual pathway. In addition, the response
properties of neurons in cortical areas receiving projections from area V1 (e.g., area V2) are not
nearly as well documented. It is uncertain what type of stimuli cause V2 neurons to respond opti-
mally [21]. One V2 study by [22] reported that the receptive ﬁelds in this area were similar to those
in the neighboring areas V1 and V4. The authors interpreted their ﬁndings as suggestive that area
V2 may serve as a place where different channels of visual information are integrated. However,
quantitative accounts of responses in area V2 are few in number. In the literature, we identiﬁed two
sets of quantitative data that give us a good starting point for making measurements to determine
whether our algorithms may be computing similar functions as area V2.
In one of these studies, Ito and Komatsu [7] investigated how V2 neurons responded to angular stim-
uli. They summarized each neuron’s response with a two-dimensional visualization of the stimuli
set called an angle proﬁle. By making several axial measurements within the proﬁle, the authors
were able to compute various statistics about each neuron’s selectivity for angle width, angle ori-
entation, and for each separate line component of the angle (see Figure 1). Approximately 80% of
the neurons responded to speciﬁc angle stimuli. They found neurons that were selective for only
one line component of its peak angle as well as neurons selective for both line components. These
neurons yielded angle proﬁles resembling those of Cell 2 and Cell 5 in Figure 1, respectively. In
addition, several neurons exhibited a high amount of selectivity for its peak angle producing angle
proﬁles like that of Cell 1 in Figure 1. No neurons were found that had more elongation in a di-
agonal axis than in the horizontal or vertical axes, indicating that neurons in V2 were not selective
for angle width or orientation. Therefore, an important conclusion made from [7] was that a V2
neuron’s response to an angle stimulus is highly dependent on its responses to each individual line
component of the angle. While the dependence was often observed to be simply additive, as was
the case with neurons yielding proﬁles like those of Cells 1 and 2 in Figure 1(right), this was not
always the case. 29 neurons had very small peak response areas and yielded proﬁles like that of Cell
1 in Figure 1(right), thus indicating a highly speciﬁc tuning to an angle stimulus. While the former
responses suggest a simple linear computation of V1 neural responses, the latter responses suggest
a nonlinear computation [21]. The analysis methods adopted in [7] are very useful in characterizing
the response properties, and we use these methods to evaluate our own model.
Another study by Hegde and Van Essen [23] studied the responses of a population of V2 neurons
to complex contour and grating stimuli. They found several V2 neurons responding maximally for
angles, and the distribution of peak angles for these neurons is consistent with that found by [7]. In
addition, several V2 neurons responded maximally for shapes such as intersections, tri-stars, ﬁve-
point stars, circles, and arcs of varying length.

2

Figure 1: (Images from [7]; courtesy of Ito and Komatsu) Left: Visualization of angle proﬁles. The upper-right
and lower-left triangles contain the same stimuli. (A,B) Darkened squares correspond to stimuli that elicited a
large response. The peak responses are circled. (C) The arrangement of the ﬁgure is so that one line component
remains constant as one moves along any vertical or horizontal axis. (D) The angles width remains constant
as one moves along a the diagonal indicated (E) The angle orientation remains constant as one moves along
the diagonal indicated. After identifying the optimal stimuli for a neuron in the proﬁle, the number of stimuli
along these various axes (as in C,D,E) eliciting responses larger than 80% of the peak response measure the
neuron’s tolerance to perturbations to the line components, peak angle width, and orientation, respectively.
Right: Examples of 4 typical angle proﬁles. As before, stimuli eliciting large responses are highlighted. Cell 1
has a selective response to a stimulus, so there is no elongation along any axis. Cell 2 has one axis of elongation,
indicating selectivity for one orientation. Cell 5 has two axes of elongation, and responds strongly so long as
either of two edge orientations is present. Cell 4 has no clear axis of elongation.
3 Algorithm
Hinton et al. [1] proposed an algorithm for learning deep belief networks, by treating each layer as a
restricted Boltzmann machine (RBM) and greedily training the network one layer at a time from the
bottom up [24, 1]. In general, however, RBMs tend to learn distributed, non-sparse representations.
Based on results from other methods (e.g., sparse coding [10, 11], ICA [9], heavy-tailed models [6],
and energy based models [2]), sparseness seems to play a key role in learning gabor-like ﬁlters.
Therefore, we modify Hinton et al.’s learning algorithm to enable deep belief nets to learn sparse
representations.
3.1 Sparse restricted Boltzmann machines
We begin by describing the restricted Boltzmann machine (RBM), and present a modiﬁed version of
it. An RBM has a set of hidden units h, a set of visible units v, and symmetric connections weights
between these two layers represented by a weight matrix W . Suppose that we want to model k
dimensional real-valued data using an undirected graphical model with n binary hidden units. The
negative log probability of any state in the RBM is given by the following energy function:1

− log P (v, h) = E(v, h) =

1
2σ2

i − 1
v2
σ2

X

i

X

civi +X

bjhj +X

i

j

i,j

 .

viwijhj

(1)

Here, σ is a parameter, hj are hidden unit variables, vi are visible unit variables. Informally, the
maximum likelihood parameter estimation problem corresponds to learning wij, ci and bj so as to
minimize the energy of states drawn from the data distribution, and raise the energy of states that
are improbable given the data.
Under this model, we can easily compute the conditional probability distributions. Holding either h
or v ﬁxed, we can sample from the other as follows:

P (vi|h) = N(cid:16)
j wijhj, σ2(cid:17)
ci +P
P (hj|v) = logistic(cid:0) 1
σ2 (bj +P
j bjhj +P
i civi +P

,

i wijvi)(cid:1) .

3

1Due to space constraints, we present an energy function only for the case of real-valued visible units. It is
also straightforward to formulate a sparse RBM with binary-valued visible units; for example, we can write the

energy function as E(v, h) = −1/σ2(P

i,j viwijhj) (see also [24]).

(2)

(3)

l=1 logP

h P (v(l), h(l)) + λPn

minimize{wij ,ci,bj} −Pm

Here, N (·) is the gaussian density, and logistic(·) is the logistic function.
For training the parameters of the model, the objective is to maximize the log-likelihood of the data.
We also want hidden unit activations to be sparse; thus, we add a regularization term that penalizes
a deviation of the expected activation of the hidden units from a (low) ﬁxed level p.2 Thus, given a
training set {v(1), . . . , v(m)} comprising m examples, we pose the following optimization problem:
(4)
where E[·] is the conditional expectation given the data, λ is a regularization constant, and p is
a constant controlling the sparseness of the hidden units hj. Thus, our objective is the sum of a
log-likelihood term and a regularization term. In principle, we can apply gradient descent to this
problem; however, computing the gradient of the log-likelihood term is expensive. Fortunately, the
contrastive divergence learning algorithm gives an efﬁcient approximation to the gradient of the log-
likelihood [25]. Building upon this, on each iteration we can apply the contrastive divergence update
rule, followed by one step of gradient descent using the gradient of the regularization term.3 The
details of our procedure are summarized in Algorithm 1.
Algorithm 1 Sparse RBM learning algorithm

E[h(l)

j |v(l)]|2,

j=1 | p − 1

m

Pm

l=1

1. Update the parameters using contrastive divergence learning rule. More speciﬁcally,

wij := wij + α(hvihjidata − hvihjirecon)
ci := ci + α(hviidata − hviirecon)
bj := bj + α(hbjidata − hbjirecon),

where α is a learning rate, and h·irecon is an expectation over the reconstruction data, estimated
using one iteration of Gibbs sampling (as in Equations 2,3).
2. Update the parameters using the gradient of the regularization term.
3. Repeat Steps 1 and 2 until convergence.

3.2 Learning deep networks using sparse RBM
Once a layer of the network is trained, the parameters wij, bj, ci’s are frozen and the hidden unit
values given the data are inferred. These inferred values serve as the “data” used to train the next
higher layer in the network. Hinton et al. [1] showed that by repeatedly applying such a procedure,
one can learn a multilayered deep belief network. In some cases, this iterative “greedy” algorithm
can further be shown to be optimizing a variational bound on the data likelihood, if each layer has
at least as many units as the layer below (although in practice this is not necessary to arrive at a
desirable solution; see [1] for a detailed discussion). In our experiments using natural images, we
learn a network with two hidden layers, with each layer learned using the sparse RBM algorithm
described in Section 3.1.
4 Visualization
4.1 Learning “strokes” from handwritten digits
We applied the sparse RBM algorithm to the MNIST
handwritten digit dataset.4 We learned a sparse RBM
with 69 visible units and 200 hidden units. The learned
bases are shown in Figure 2. (Each basis corresponds to
one column of the weight matrix W left-multiplied by
the unwhitening matrix.) Many bases found by the al-
gorithm roughly represent different “strokes” of which
handwritten digits are comprised. This is consistent

Figure 2: Bases learned from MNIST data

2Less formally, this regularization ensures that the “ﬁring rate” of the model neurons (corresponding to the
latent random variables hj) are kept at a certain (fairly low) level, so that the activations of the model neurons
are sparse. Similar intuition was also used in other models (e.g., see Olshausen and Field [10]).

3To increase computational efﬁciency, we made one additional change. Note that the regularization term is
deﬁned using a sum over the entire training set; if we use stochastic gradient descent or mini-batches (small
subsets of the training data) to estimate this term, it results in biased estimates of the gradient. To ameliorate
this, we used mini-batches, but in the gradient step that tries to minimize the regularization term, we update
only the bias terms bj’s (which directly control the degree to which the hidden units are activated, and thus their
sparsity), instead of updating all the parameters bj and wij’s.

4Downloaded from http://yann.lecun.com/exdb/mnist/. Each pixel was normalized to the
unit interval, and we used PCA whitening to reduce the dimension to 69 principal components for computational
efﬁciency. (Similar results were obtained without whitening.)

4

Figure 3: 400 ﬁrst layer bases learned from the van Hateren natural image dataset, using our algorithm.

Figure 4: Visualization of 200 second layer bases (model V2 receptive ﬁelds), learned from natural images.
Each small group of 3-5 (arranged in a row) images shows one model V2 unit; the leftmost patch in the group
is a visualization of the model V2 basis, and is obtained by taking a weighted linear combination of the ﬁrst
layer “V1” bases to which it is connected. The next few patches in the group show the ﬁrst layer bases that
have the strongest weight connection to the model V2 basis.

with results obtained by applying different algorithms to learn sparse representations of this data
set (e.g., [2, 5]).
4.2 Learning from natural images
We also applied the algorithm to a training set a set of 14-by-14 natural image patches, taken from
a dataset compiled by van Hateren.5 We learned a sparse RBM model with 196 visible units and
400 hidden units. The learned bases are shown in Figure 3; they are oriented, gabor-like bases and
resemble the receptive ﬁelds of V1 simple cells.6
4.3 Learning a two-layer model of natural images using sparse RBMs
We further learned a two-layer network by stacking one sparse RBM on top of another (see Sec-
tion 3.2 for details.)7 After learning, the second layer weights were quite sparse—most of the
weights were very small, and only a few were either highly positive or highly negative. Positive

5The images were obtained from http://hlab.phys.rug.nl/imlib/index.html. We used
100,000 14-by-14 image patches randomly sampled from an ensemble of 2000 images; each subset of 200
patches was used as a mini-batch.

6Most other authors’ experiments to date using regular (non-sparse) RBMs, when trained on such data,
seem to have learned relatively diffuse, unlocalized bases (ones that do not represent oriented edge ﬁlters).
While sensitive to the parameter settings and requiring a long training time, we found that it is possible in
some cases to get a regular RBM to learn oriented edge ﬁlter bases as well. But in our experiments, even in
these cases we found that repeating this process to build a two layer deep belief net (see Section 4.3) did not
encode a signiﬁcant number of corners/angles, unlike one trained using the sparse RBM; therefore, it showed
signiﬁcantly worse match to the Ito & Komatsu statistics. For example, the fraction of model V2 neurons that
respond strongly to a pair of edges near right angles (formally, have peak angle in the range 60-120 degrees)
was 2% for the regular RBM, whereas it was 17% for the sparse RBM (and Ito & Komatsu reported 22%). See
Section 5.1 for more details.

7For the results reported in this paper, we trained the second layer sparse RBM with real-valued visible
units; however, the results were very similar when we trained the second layer sparse RBM with binary-valued
visible units (except that the second layer weights became less sparse).

5

Figure 5: Top: Visualization of four learned model V2 neurons. (Visualization in each row of four or ﬁve
patches follows format in Figure 4.) Bottom: Angle stimulus response proﬁle for model V2 neurons in the top
row. The 36*36 grid of stimuli follows [7], in which the orientation of two lines are varied to form different
angles. As in Figure 1, darkened patches represent stimuli to which the model V2 neuron responds strongly;
also, a small black square indicates the overall peak response.

weights represent excitatory connections between model V1 and model V2 units, whereas negative
elements represent inhibitory connections. By visualizing the second layer bases as shown in Fig-
ure 4, we observed bases that encoded co-linear ﬁrst layer bases as well as edge junctions. This
shows that by extending the sparse RBM to two layers and using greedy learning, the model is able
to learn bases that encode contours, angles, and junctions of edges.
5 Evaluation experiments
We now more quantitatively compare the algorithm’s learned responses to biological measure-
ments.8
5.1 Method: Ito-Komatsu paper protocol
We now describe the procedure we used to compare our model with the experimental data in [7]. We
generated a stimulus set consisting of the same set of angles (pairs of edges) as [7]. To identify the
“center” of each model neuron’s receptive ﬁeld, we translate all stimuli densely over the 14x14 input
image patch, and identify the position at which the maximum response is elicited. All measures are
then taken with all angle stimuli centered at this position.9
Using these stimuli, we compute the hidden unit probabilities from our model V1 and V2 neurons.
In other words, for each stimulus we compute the ﬁrst hidden layer activation probabilities, then
feed this probability as data to the second hidden layer and compute the activation probabilities
again in the same manner. Following a protocol similar to [7], we also eliminate from consideration
the model neurons that do not respond strongly to corners and edges.10 Some representative results
are shown in Figure 5. (The four angle proﬁles shown are fairly typical of those obtained in our
experiments.) We see that all the V2 bases in Figure 5 have maximal response when its strongest
V1-basis components are aligned with the stimulus. Thus, some of these bases do indeed seem to
encode edge junctions or crossings.
We also compute similar summary statistics as [7] (described in Figure 1(C,D,E)), that more quanti-
tatively measure the distribution of V2 or model V2 responses to the different angle stimuli. Figure 6
plots the responses of our model, together with V2 data taken from [7]. Along many dimensions,
the results from our model match that from the Macaque V2 fairly well.

8The results we report below were very insensitive to the choices of σ and λ. We set σ to 0.4 and 0.05
for the ﬁrst and second layers (chosen to be on the same scale as the standard deviation of the data and the
ﬁrst-layer activations), and λ = 1/p in each layer. We used p = 0.02 and 0.05 for the ﬁrst and second layers.
9Other details: The stimulus set is created by generating a binary-mask image, that is then scaled to nor-
malize contrast. To determine this scaling constant, we used single bar images by translating and rotating to
all possible positions, and ﬁxed the constant such that the top 0.5% (over all translations and rotations) of the
stimuli activate the model V1 cells above 0.5. This normalization step corrects for the RBM having been trained
on a data distribution (natural images) that had very different contrast ranges than our test stimulus set.

10In detail, we generated a set of random low-frequency stimulus, by generating small random KxK
(K=2,3,4) images with each pixel drawn from a standard normal distribution, and rescaled the image using
bicubic interpolation to 14x14 patches. These stimulus are scaled such that about 5% of the V2 bases ﬁres
maximally to these random stimuli. We then exclude the V2 bases that are maximally activated to these ran-
dom stimuli from the subsequent analysis.

6

Figure 6: Images show distributions over stimulus response statistics (averaged over 10 trials) from our algo-
rithm (blue) and in data taken from [7] (green). The ﬁve ﬁgures show respectively (i) the distribution over peak
angle response (ranging from 0 to 180 degrees; each bin represents a range of 30 degrees), (ii) distribution over
tolerance to primary line component (Figure 1C, in dominant vertical or horizontal direction), (iii) distribution
over tolerance to secondary line component (Figure 1C, in non-dominant direction), (iv) tolerance to angle
width (Figure 1D), (v) tolerance to angle orientation (Figure 1E). See Figure 1 caption, and [7], for details.

Figure 7: Visualization of a number of model V2 neurons that maximally respond to various complex stimuli.
Each row of seven images represents one V2 basis. In each row, the leftmost image shows a linear combination
of the top three weighted V1 components that comprise the V2 basis; the next three images show the top three
optimal stiimuli; and the last three images show the top three weighted V1 bases. The V2 bases shown in the
ﬁgures maximally respond to acute angles (left), obtuse angles (middle), and tri-stars and junctions (right).
5.2 Complex shaped model V2 neurons
Our second experiment represents a comparison to a subset of the results described in Hegde and van
Essen [23]. We generated a stimulus set comprising some [23]’s complex shaped stimuli: angles,
single bars, tri-stars (three line segments that meet at a point), and arcs/circles, and measured the
response of the second layer of our sparse RBM model to these stimuli.11 We observe that many V2
bases are activated mainly by one of these different stimulus classes. For example, some model V2
neurons activate maximally to single bars; some maximally activate to (acute or obtuse) angles; and
others to tri-stars (see Figure 7). Further, the number of V2 bases that are maximally activated by
acute angles is signiﬁcantly larger than the number of obtuse angles, and the number of V2 bases
that respond maximally to the tri-stars was much smaller than both preceding cases. This is also
consistent with the results described in [23].
6 Conclusions
We presented a sparse variant of the deep belief network model. When trained on natural images,
this model learns local, oriented, edge ﬁlters in the ﬁrst layer. More interestingly, the second layer
captures a variety of both colinear (“contour”) features as well as corners and junctions, that in a
quantitative comparison to measurements of V2 taken by Ito & Komatsu, appeared to give responses
that were similar along several dimensions. This by no means indicates that the cortex is a sparse
RBM, but perhaps is more suggestive of contours, corners and junctions being fundamental to the
statistics of natural images.12 Nonetheless, we believe that these results also suggest that sparse

11All the stimuli were 14-by-14 pixel image patches. We applied the protocol described in Section 5.1 to the

stimulus data, to compute the model V1 and V2 responses.

12In preliminary experiments, we also found that when these ideas are applied to self-taught learning [26] (in
which one may use unlabeled data to identify features that are then useful for some supervised learning task),
using a two-layer sparse RBM usually results in signiﬁcantly better features for object recognition than using
only a one-layer network.

7

15457510513516500.10.20.30.40.5peak angles  sparse DBNIto & Komatsu123456789101100.050.10.150.2primary line axis  sparse DBNIto & Komatsu123456789101100.10.20.30.40.5secondary line axis  sparse DBNIto & Komatsu123456789101100.20.40.60.8angle width axis  sparse DBNIto & Komatsu123456789101100.20.40.60.81angle orientation axis  sparse DBNIto & Komatsudeep learning algorithms, such as our sparse variant of deep belief nets, hold promise for modeling
higher-order features such as might be computed in the ventral visual pathway in the cortex.
Acknowledgments
We give warm thanks to Minami Ito, Geoffrey Hinton, Chris Williams, Rajat Raina, Narut Sereewat-
tanawoot, and Austin Shoemaker for helpful discussions. Support from the Ofﬁce of Naval Research
under MURI N000140710747 is gratefully acknowledged.
References
[1] G. E. Hinton, S. Osindero, and Y.-W. Teh. A fast learning algorithm for deep belief nets. Neural Compu-

tation, 18(7):1527–1554, 2006.

[2] M. Ranzato, C. Poultney, S. Chopra, and Y. LeCun. Efﬁcient learning of sparse representations with an

energy-based model. In NIPS, 2006.

[3] Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle. Greedy layer-wise training of deep networks. In

NIPS, 2006.

[4] H. Larochelle, D. Erhan, A. Courville, J. Bergstra, and Y. Bengio. An empirical evaluation of deep

architectures on problems with many factors of variation. In ICML, 2007.

[5] G. E. Hinton, S. Osindero, and K. Bao. Learning causally linked MRFs. In AISTATS, 2005.
[6] S. Osindero, M. Welling, and G. E. Hinton. Topographic product models applied to natural scene statistics.

Neural Computation, 18:381–344, 2006.

[7] M. Ito and H. Komatsu. Representation of angles embedded within contour stimuli in area v2 of macaque

monkeys. The Journal of Neuroscience, 24(13):3313–3324, 2004.

[8] J. H. van Hateren and A. van der Schaaf. Independent component ﬁlters of natural images compared with

simple cells in primary visual cortex. Proc.R.Soc.Lond. B, 265:359–366, 1998.

[9] A. J. Bell and T. J. Sejnowski. The ‘independent components’ of natural scenes are edge ﬁlters. Vision

Research, 37(23):3327–3338, 1997.

[10] B. A. Olshausen and D. J. Field. Emergence of simple-cell receptive ﬁeld properties by learning a sparse

code for natural images. Nature, 381:607–609, 1996.

[11] H. Lee, , A. Battle, R. Raina, and A. Y. Ng. Efﬁcient sparse coding algorithms. In NIPS, 2007.
[12] D. Hubel and T. Wiesel. Receptive ﬁelds and functional architecture of monkey striate cortex. Journal of

Physiology, 195:215–243, 1968.

[13] R. L. DeValois, E. W. Yund, and N. Hepler. The orientation and direction selectivity of cells in macaque

visual cortex. Vision Res., 22:531–544, 1982a.

[14] H. B. Barlow. The coding of sensory messages. Current Problems in Animal Behavior, 1961.
[15] P. O. Hoyer and A. Hyvarinen. A multi-layer sparse coding network learns contour coding from natural

images. Vision Research, 42(12):1593–1605, 2002.

[16] Y. Karklin and M. S. Lewicki. A hierarchical bayesian model for learning non-linear statistical regularities

in non-stationary natural signals. Neural Computation, 17(2):397–423, 2005.

[17] A. Hyvarinen and P. O. Hoyer. Emergence of phase and shift invariant features by decomposition of

natural images into independent feature subspaces. Neural Computation, 12(7):1705–1720, 2000.

[18] A. Hyv¨arinen, P. O. Hoyer, and M. O. Inki. Topographic independent component analysis. Neural

Computation, 13(7):1527–1558, 2001.

[19] A. Hyvarinen, M. Gutmann, and P. O. Hoyer. Statistical model of natural stimuli predicts edge-like

pooling of spatial frequency channels in v2. BMC Neuroscience, 6:12, 2005.

[20] L. Wiskott and T. Sejnowski. Slow feature analysis: Unsupervised learning of invariances. Neural Com-

putation, 14(4):715–770, 2002.

[21] G. Boynton and J. Hegde. Visual cortex: The continuing puzzle of area v2. Current Biology,

14(13):R523–R524, 2004.

[22] J. B. Levitt, D. C. Kiper, and J. A. Movshon. Receptive ﬁelds and functional architecture of macaque v2.

Journal of Neurophysiology, 71(6):2517–2542, 1994.

[23] J. Hegde and D.C. Van Essen. Selectivity for complex shapes in primate visual area v2. Journal of

Neuroscience, 20:RC61–66, 2000.

[24] G. E. Hinton and R. R. Salakhutdinov. Reducing the dimensionality of data with neural networks. Science,

313(5786):504–507, 2006.

[25] G. E. Hinton. Training products of experts by minimizing contrastive divergence. Neural Computation,

14:1771–1800, 2002.

[26] R. Raina, A. Battle, H. Lee, B. Packer, and A. Y. Ng. Self-taught learning: Transfer learning from

unlabeled data. In ICML, 2007.

8

"
770,2007,Second Order Bilinear Discriminant Analysis for single trial EEG analysis,"Traditional analysis methods for single-trial classification of electro-encephalography (EEG) focus on two types of paradigms: phase locked methods, in which the amplitude of the signal is used as the feature for classification, i.e. event related potentials; and second order methods, in which the feature of interest is the power of the signal, i.e event related (de)synchronization. The process of deciding which paradigm to use is ad hoc and is driven by knowledge of neurological findings. Here we propose a unified method in which the algorithm learns the best first and second order spatial and temporal features for classification of EEG based on a bilinear model. The efficiency of the method is demonstrated in simulated and real EEG from a benchmark data set for Brain Computer Interface.","Second Order Bilinear Discriminant Analysis for

single-trial EEG analysis

The Graduate Center of the City University of New York

Christoforos Christoforou

Department of Computer Science

365 Fifth Avenue

New York, NY 10016-4309

cchristoforou@gc.cuny.edu

Paul Sajda

Department of Biomedical Engineering

Columbia University

351 Engineering Terrace Building, MC 8904

1210 Amsterdam Avenue

New York, NY 10027

ps629@columbia.edu

Lucas C. Parra

Department of Biomedical Engineering

The City College of The City University of New York

Convent Avenue 138th Street
New York,NY 10031, USA
parra@ccny.cuny.edu

Abstract

single-trial

classiﬁcation of

for

analysis methods

electro-
Traditional
encephalography (EEG) focus on two types of paradigms:
phase locked
methods, in which the amplitude of the signal is used as the feature for classiﬁca-
tion, e.g. event related potentials; and second order methods, in which the feature
of interest is the power of the signal, e.g. event related (de)synchronization. The
procedure for deciding which paradigm to use is ad hoc and is typically driven
by knowledge of the underlying neurophysiology. Here we propose a principled
method, based on a bilinear model, in which the algorithm simultaneously learns
the best ﬁrst and second order spatial and temporal features for classiﬁcation of
EEG. The method is demonstrated on simulated data as well as on EEG taken
from a benchmark data used to test classiﬁcation algorithms for brain computer
interfaces.

1 Introduction

1.1 Utility of discriminant analysis in EEG

Brain computer interface (BCI) algorithms [1][2][3][4] aim to decode brain activity, on a single-
trial basis, in order to provide a direct control pathway between a user’s intentions and a computer.
Such an interface could provide “locked in patients” a more direct and natural control over a neu-
roprosthesis or other computer applications [2]. Further, by providing an additional communication

1

channel for healthy individuals, BCI systems can be used to increase productivity and efﬁciency in
high-throughput tasks [5, 6].

Single-trial discriminant analysis has also been used as a research tool to study the neural correlates
of behavior. By extracting activity that differs maximally between two experimental conditions, the
typically low signal-noise ratio of EEG can be overcome. The resulting discriminant components
can be used to identify the spatial origin and time course of stimulus/response speciﬁc activity,
while the improved SNR can be leveraged to correlate variability of neural activity across trials to
behavioral variability and behavioral performance [7, 5]. In essence, discriminant analysis adds to
the existing set of multi-variate statistical tools commonly used in neuroscience research (ANOVA,
Hoteling T 2, Wilks’ Λ test).

1.2 Linear and quadratic approaches

In EEG the signal-to-noise ratio of individual channels is low, often at -20dB or less. To overcome
this limitation, all analysis methods perform some form of averaging, either across repeated trials,
across time, or across electrodes. Traditional EEG analysis averages signals across many repeated
trials for individual electrodes. A conventional method is to average the measured potentials follow-
ing stimulus presentation, thereby canceling uncorrelated noise that is not reproducible from one
trial to the next. This averaged activity, called an event related potential (ERP), captures activity that
is time-locked to the stimulus presentation but cancels evoked oscillatory activity that is not locked
in phase to the timing of the stimulus. Alternatively, many studies compute the oscillatory activity
in speciﬁc frequency bands by ﬁltering and squaring the signal prior to averaging. Thus, changes in
oscillatory activity are termed event related synchronization or desynchronization (ERS/ERD).

Surprisingly, discriminant analysis methods developed thus far by the machine learning community
have followed this dichotomy: First order methods in which the amplitude of the EEG signal is
considered to be the feature of interest in classiﬁcation – corresponding to ERP – and second or-
der methods in which the power of the feature is considered to be of importance for classiﬁcation
– corresponding to ERS/ERD. First order methods include temporal ﬁltering + thresholding [2],
hierarchical linear classiﬁers [5] and bilinear discriminant analysis [8, 9]. Second order methods
include the logistic regression with a quadratic term [11] and the well known common spatial pat-
terns method (CSP) [10] and its variants: common spatio-spectral patterns (CSSP)[12], and common
sparse spectral spatial patterns (CSSSP)[13] .

Choosing what kind of features to use traditionally has been an ad hoc process motivated by knowl-
edge of the underlying neurophysiology and task. From a machine-learning point of view, it seems
limiting to commit a priori to only one type of feature. Instead it would be desirable for the analysis
method to extract the relevant neurophysiological activity de novo with minimal prior expectations.
In this paper we present a new framework that combines both the ﬁrst order features and the sec-
ond order features in the analysis of EEG. We use a bilinear formulation which can simultaneously
extract spatial linear components as well as temporal (ﬁltered) features.

2 Second order bilinear discriminant analysis

2.1 Problem setting

Given a set of sample points D = {Xn, yn}N
n=1, X ∈ RD × T , y ∈ {−1, 1} , where Xn corresponds
to the EEG signal of D channels and T sample points and yn indicate the class that corresponds
to one of two conditions (e.g.
right or left hand imaginary movement, stimulus versus control
conditions, etc.), the task is then to predict the class label y for an unobserved trial X.

2.2 Second order bilinear model

Deﬁne a function,

f (X; θ) = C Trace(UTXV) + (1 − C) Trace(ΛAT(XB)(XB)TA)

(1)

where θ = {U ∈ RD × R, V ∈ RT × R, A ∈ RD × K B ∈ RT × T 0
} are the parameters of the model,
Λ ∈ diag({−1, 1}) a given diagonal matrix with elements {−1, 1} and C ∈ [0, 1]. We consider the

2

following discriminative model; we model the log-odds ratio of the posterior class probability to be
the sum of a bilinear function with respect to the EEG signal amplitude and linear with respect to
the second order statistics of the EEG signal:

log

P (y = +1|X)
P (y = −1|X)

= f (X|θ)

(2)

2.2.1

Interpretation of the model

The ﬁrst term of the equation (1) can be interpreted as a spatio-temporal projection of the signal,
under the bilinear model, and captures the ﬁrst order statistics of the signal. Speciﬁcally, the columns
ur of U represent R linear projections in space (rows of X). Similarly, each of the R columns of
vk in matrix V represent linear projections in time (columns of X). By re-writing the term as:

Trace(UTXV) = Trace(VUTX) = Trace(WTX)

(3)
where we deﬁned W = UVT, it is easy to see that the bilinear projection is a linear combination
of elements of X with the rank − R constrained on W. This expression is linear in X and thus
captures directly the amplitude of the signal directly. In particular, the polarity of the signal (positive
evoked response versus negative evoked response) will contribute signiﬁcantly to discrimination if
it is consistent across trials. This term, therefore, captures phase locked event related potentials in
the EEG signal.

The second term of equation (1), is a projection of the power of the ﬁltered signal, which captures
the second order statistics of the signal. As before, each column of matrix A and B, represent
components that project the data in space and time respectively. Depending on the structure one
enforces in matrix B different interpretations of the model can be archived. In the general case
where no structure on B is assumed, the model captures a linear combination of the elements of a
rank − T 0 second order matrix approximation of the signal Σ = XB(XB)T. In the case where
Toeplitz structure is enforced on B, then B deﬁnes a temporal ﬁlter on the signal and the model
captures the linear combination of the power of the second order matrix of the ﬁltered signal. For
example if B is ﬁxed to a Toeplitz matrix with coefﬁcients corresponding to a 8Hz-12Hz band pass
ﬁlter, then the second term is able to extract differences in the alpha-band which is known to be
modulated during motor related tasks. Further, by learning B from the data, we may be able to
identify new frequency bands that have so far not been identiﬁed in novel experimental paradigms.
The spatial weights A together with the Trace operation ensure that the power is measured, not
in individual electrodes, but in some component space that may reﬂect activity distributed across
several electrodes.

Finally, the scaling factor λ (which may seem superﬂuous given the available degrees of freedom)
is necessary once regularization terms are added to the log-likelihood function.

2.3 Logistic regression

We use a logistic Rregression (LR) formalism as it is particularly convenient when imposing ad-
ditional statistical properties on the matrices U, V, A, B such as smoothness or sparseness.
In
addition, in our experience, LR performs well in strongly overlapping high-dimensional datasets
and is insensitive to outliers, the later being of particular concern when including quadratic features.

Under the logistic regression model (2) the class posterior probability P (y|X; θ) is modeled as

and the resulting log likelihood is given by

P (y|X; θ) =

1

1 + e−y(f (X;θ)+wo)

L(θ) = −

N

X

n=1

log(1 + e−y(f (Xn;θ)+wo))

(4)

(5)

We minimize the negative log likelihood and add a log-prior on each of the columns of U, V and A
and parameters of B that act as a regularization term, which is written as:

argmin

U,V,A,B,wo


−L(θ) −

R

X

r=1

(log p(ur) + log p(vr)) −

K

X

k=1

log p(ak) −

T 0

X

t=1

log(p(bt))


(6)

3

log p(vk) = uT
k

where the log-priors are given for each of the parameters as log p(uk) = uT
K(u)uk
k
K(b)bk.
K(a)ak and log p(bk) = bT
,
k
K(u) ∈ RD×D, K(v) ∈ RT ×T , K(a) ∈ RD×D, K(b) ∈ RT ×T are kernel matrices that con-
trol the smoothness of the parameter space. Details on the regularization procedure can be found in
[8].

log p(ak) = aT
k

K(v)uk,

Analytic gradients of the log likelihood (5) with respect to the various parameters are given
by:

∂L(θ)
∂ur

∂L(θ)
∂vr

∂L(θ)
∂ar

∂L(θ)
∂bt

=

=

N

X

n=1

N

X

n=1

ynπ(Xn)Xnvr

ynπ(Xn)urXn

= 2

= 2

N

X

n=1

N

X

n=1

ynπ(Xn)Λ

r,r(XnB)(XnB)Tar

ynπ(Xn)XTAΛATXbt

where we deﬁne

π(Xn) = 1 − P (y|X) =

e−y(f (Xn;θ)+wo)

1 + e−y(f (Xn;θ)+wo)

where ui, vi, ai, and bi correspond to the ith columns of U, V, A, B respectively.

2.4 Fourier Basis for B

(7)

(8)

(9)

(10)

(11)

If matrix B is constrained to have a circular toepliz structure then it can be represented as B =
F−1DF, where F−1 denotes the inverse Fourier matrix, and D is a diagonal complex-valued matrix
of Fourier coefﬁcients. In such a case, we can re-write equations (9) and (10) as

∂L(θ)
∂ar

∂L(θ)

∂di

= 2

= 2

N

X

n=1

N

X

n=1

ynπ(Xn)Λ

r,r(XnF−1 ˆDF−TXT

n )ar

ynπ(Xn)(F−TXT
n

AΛATXnF−1)i,idi

(12)

(13)

(14)

where ˆD = DDT, and the parameters are now optimized with respect to Fourier coefﬁcients di =
Di,i. An iterative minimization procedure can be used to solve the above minimization.

3 Results

3.1 Simulated data

In order to validate our method and its ability to capture both linear and second order features, we
generated simulated data that contained both types of features; namely ERP type of features and
ERS/ERD type of features. The simulated signals were generated with a signal to noise ratio of
−20dB which is a typical noise level for EEG. A total of 28 channels, 500 ms long signals and at a
sampling frequency of 100Hz where generated, resulting in a matrix of X of 28 by 50 elements, for
each trial. Data corresponding to a total of 1000 trials were generated; 500 trials contained only zero
mean Gaussian noise (representing baseline conditions), with the other 500 trials having the signal
of interest added to the noise (representing the stimulus condition): For channels 1-9 the signal was
composed of a 10Hz sinusoid with random phase in each of the nine channels, and across trials. The

4

U component

V Component

1.5

1

0.5

0

0.4

0.3

0.2

0.1

0

e
d
u
t
i
l

p
m
a

−0.5

0

10

20

channels

30

−0.1

0

A component

50

100

150

200

250

300

350

400

450

500

time(m/s)

B component

1.5

1

0.5

0

0.15

0.1

0.05

0

−0.05

−0.1

−0.15

e
t
u
t
i
l

p
m
a

−0.5

0

10

20

channels

30

−0.2

0

50

100

150

200

250

300

350

400

450

500

time (m/s)

Figure 1: Spatial and temporal component extracted on simulated data for the linear term (top) and
quadratic term (bottom).

sinusoids were scaled to match the −20dB SNR. This simulates an ERS type feature. For channels
10-18, a peak represented by a half cycle sinusoid was added at approximately 400 ms, which T
simulates an ERP type feature.

The extracted components are shown in Figure 1. The linear component U (in this case only a col-
umn vector) has non-zero coefﬁcients for channels 10 to 18 only, showing that the method correctly
identiﬁed the ERP activity. Furthermore, the associated temporal component V has a temporal
proﬁle that matches the time course of the simulated evoked response. Similarly, the second order
components A have non-zero weights for only channels 1-9 showing that the method also identiﬁed
the spatial distribution of the non-phase locked activity. The temporal ﬁlter B was trained in the
frequency domain and the resulting ﬁlter is shown here in the time domain. It exhibits a dominant
10Hz component, which is indeed the frequency of the non-phase locked activity.

3.2 BCI competition dataset

To evaluate the performance of the proposed method on real data we applied the algorithm to an
EEG data set that was made available through The BCI Competition 2003 ([14], Data Set IV).
EEG was recorded on 28 channels for a single subject performing self-paced key typing, that is,
pressing the corresponding keys with the index and little ﬁngers in a self-chosen order and timing
(i.e. self-paced). Key-presses occurred at an average speed of 1 key per second. Trial matrices
were extracted by epoching the data starting 630ms before each key-press. A total of 416 epochs
were recorded, each of length 500ms. For the competition, the ﬁrst 316 epochs were to be used for
classiﬁer training, while the remaining 100 epochs were to be used as a test set. Data were recorded
at 1000 Hz with a pass-band between 0.05 and 200 Hz, then downsampled to 100Hz sampling rate.

For this experiment,
the matrix B was ﬁxed to a Toeplitz structure that encodes a 10Hz-
33Hz bandpass ﬁlter and only the parameters U, V, A and w0 were trained. The number of
columns of U and V were set to 1, where two columns were used for A. The temporal ﬁlter
was selected based on prior knowledge of the relevant frequency band. This demonstrates the
ﬂexibility of our approach to either incorporate prior knowledge when available or extract it from

5

U component

V component

0.1

0.05

0

−0.05

−0.1

0

100

200

300

time (m/s)

400

500

First Column of A

Second Column of A

Figure 2: Spatial and temporal component (top), and two spatial components for second order fea-
tures (bottom) learned on the benchmark dataset

data otherwise. Regularization parameters where chosen via a ﬁve fold cross validation procedure
(details can be found in [8]). The resulting components for this dataset are shown in Figure 2.

Benchmark performance was measured on the test set which had not been used during either train-
ing or cross validation. The number of misclassiﬁed trials in the test set was 13 which places
our method on a new ﬁrst place given the results of the competition which can be found on-
line http://ida.ﬁrst.fraunhofer.de/projects/bci/competition ii/results/index.html ([14]). Hence, our
method works as a classiﬁer producing a state-of-the art result on a realistic data set. The receiver-
operator characteristic (ROC) curve for cross validation and for the independent testset are shown in
Figure 3. Figure 3.2 also shows the contribution of the linear and quadratic terms for every trial for
the two types of key-presses. The ﬁgure shows that the two terms provide independent information
and that in this case the optimal relative weighting factor is C ≈ 0.5.

4 Conclusion

In this paper we have presented a framework for uncovering spatial as well as temporal features in
EEG that combine the two predominant paradigms used in EEG analysis: event related potentials
and oscillatory power. These represent phase locked activity (where polarity of the activity matters),
and non-phase locked activity (where only the power of the signal is relevant). We used the proba-
bilistic formalism of logistic regression that readily incorporates prior probabilities to regularize the
increased number of parameters. We have evaluated the proposed method on both simulated data,
and a real BCI benchmark dataset, achieving state-of-the-art classiﬁcation performance.

The proposed method provides a basis for various future directions. For example, different sets of
basis functions (other than a Fourier basis) can be enforced on the temporal decomposition of the
data through the matrix B (e.g. wavelet basis). Further, the method can be easily generalized to

6

AUC : 0.96

AUC : 0.935  #errors:13

t

e
a
r
 

e
v
i
t
i
s
o
p
e
u
r
T

 

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

0.2

0.4

0.6

False positive rate

0.8

1

t

e
a
r
 

e
v
i
t
i
s
o
p
e
u
r
T

 

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

0.2

0.4

0.6

False positive rate

0.8

1

Figure 3: ROC curve with area under the curve 0.96 for the cross validation on the benchmark dataset
(left). ROC curve with area under the curve 0.93, on the independent test set, for the benchmark
dataset. There were a total of 13 errors on unseen data, which is less than any of the results previously
reported, placing this method in ﬁrst place in the benchmark ranking.

10

5

0

−5

−10

m
r
e
t
 
r
e
d
r
o

 

d
n
o
c
e
s

−15

−20

Training Set

Testing set

m
r
e
t
 
r
e
d
r
o

 

d
n
o
c
e
s

5

0

−5

−10

−10

0

10

first order term

−15

−10

−5

0

5

10

first order term

Figure 4: Scatter plot of the ﬁrst order term vs second order term of the model, on the training and
testing set for the benchmark dataset (’+’ left key, and ’o’ right key). It is clear that the two types
of features contain independent information that can help improve the classiﬁcation performance.

7

multi-class problems by using a multinomial distribution on y. Finally, different regularizations (i.e
L1 norm, L2 norm) can be applied to the different types of parameters of the model.

References

[1] J. R. Wolpaw, N. Birbaumer, D. J. McFarland, G. Pfurtscheller, and T. M. Vaughan. Brain-computer

interfaces for communication and control. Clin Neurophysiol, 113(6):767–791, June 2002.

[2] N. Birbaumer, N. Ghanayim, T. Hinterberger, I. Iversen, B. Kotchoubey, A. Kubler, J. Perelmouter,
E. Taub, and H. Flor. A spelling device for the paralysed. Nature, 398(6725):297–8, Mar February-
May 1999.

[3] B. Blankertz, G. Curio, and K. uller. Classifying single trial eeg: Towards brain computer interfacing.
In T. G. Diettrich, S. Becker, and Z. Ghahramani, editors, Advances in Neural Information Processing
Systems 14. MIT Press, 2002., 2002.

[4] B. Blankertz, G. Dornhege, C. Schfer, R. Krepki, J. Kohlmorgen, K. Mller, V. Kunzmann, F. Losch, and
G. Curio. Boosting bit rates and error detection for the classiﬁcation of fast-paced motor commands based
on single-trial eeg analysis. IEEE Trans. Neural Sys. Rehab. Eng., 11(2):127–131, 2003.

[5] Adam D. Gerson, Lucas C. Parra, and Paul Sajda. Cortically-coupled computer vision for rapid image
search. IEEE Transactions on Neural Systems and Rehabilitation Engineering, 14:174–179, June 2006.
[6] Lucas C. Parra, Christoforos Christoforou, Adam D. Gerson, Mads Dyrholm, An Luo, Mark Wagner,
Marios G. Philiastides, and Paul Sajda. Spatiotemporal linear decoding of brain state: Application to
performance augmentation in high-throughput tasks. IEEE, Signal Processing Magazine, January 2008.
[7] Philiastides Marios G., Ratcliff Roger, and Sajda Paul. Neural representation of task difﬁculty and de-
cision making during perceptual categorization: A timing diagram. Journal of Neuroscience, 26(35):
8965–8975, August 2006.

[8] Mads Dyrholm, Christoforos Christoforou, and Lucas C. Parra. Bilinear discriminant component analysis.

J. Mach. Learn. Res., 8:1097–1111, 2007.

[9] Ryota Tomioka and Kazuyuki Aihara. Classifying matrices with a spectral regularization. In 24th Inter-

national Conference on Machine Learning, 2007.

[10] H. Ramoser, J. M¨uller-Gerking, and G. Pfurtscheller. Optimal spatial ﬁltering of single trial EEG during

imagined hand movement. IEEE Trans. Rehab. Eng., 8:441–446, December 2000.

[11] Ryota Tomioka, Kazuyuki Aihara, and Klaus-Robert Mller. Logistic regression for single trial eeg clas-
siﬁcation. In B. Sch¨olkopf, J. Platt, and T. Hoffman, editors, Advances in Neural Information Processing
Systems 19, pages 1377–1384. MIT Press, Cambridge, MA, 2007.

[12] S. Lemm, B. Blankertz, G. Curio, and K. Muller. Spatio-spectral ﬁlters for improving the classiﬁcation

of single trial eeg. IEEE Trans Biomed Eng., 52(9):1541–8, 2005., 2005.

[13] Dornhege G., Blankertz B, and K.R. Krauledat M. Losch F. Curio G.Muller. Combined optimization of
spatial and temporal ﬁlters for improving brain-computer interfacing. IEEE Trans. Biomed. Eng. 2006,
2006.

[14] B. Blankertz, K.-R. Muller, G. Curio, T.M. Vaughan, G. Schalk, J.R. Wolpaw, A. Schlogl, C. Neuper,
G. Pfurtscheller, T. Hinterberger, M. Schroder, and N. Birbaumer. The bci competition 2003: progress
and perspectives in detection and discrimination of eeg single trials. Biomedical Engineering, IEEE
Transactions on, 51(6):1044–1051, 2004.

8

"
707,2007,A configurable analog VLSI neural network with spiking neurons and self-regulating plastic synapses,"We summarize the implementation of an analog VLSI chip hosting a network of 32 integrate-and-fire (IF) neurons with spike-frequency adaptation and 2,048 Hebbian plastic bistable spike-driven stochastic synapses endowed with a self-regulating mechanism which stops unnecessary synaptic changes. The synaptic matrix can be flexibly configured and provides both recurrent and AER-based connectivity with external, AER compliant devices. We demonstrate the ability of the network to efficiently classify overlapping patterns, thanks to the self-regulating mechanism.","A conﬁgurable analog VLSI neural network with
spiking neurons and self-regulating plastic synapses

which classiﬁes overlapping patterns

M. Giulioni∗

M. Pannunzi

Italian National Inst. of Health, Rome, Italy

Italian National Inst. of Health, Rome, Italy

INFN-RM1, Rome, Italy

INFN-RM2, Rome, Italy

giulioni@roma2.infn.it

D. Badoni

INFN-RM2, Rome, Italy

V. Dante

Italian National Inst. of Health, Rome, Italy

INFN-RM1, Rome, Italy

P. Del Giudice

Italian National Inst. of Health, Rome, Italy

INFN-RM1, Rome, Italy

Abstract

We summarize the implementation of an analog VLSI chip hosting a network
of 32 integrate-and-ﬁre (IF) neurons with spike-frequency adaptation and 2,048
Hebbian plastic bistable spike-driven stochastic synapses endowed with a self-
regulating mechanism which stops unnecessary synaptic changes. The synaptic
matrix can be ﬂexibly conﬁgured and provides both recurrent and AER-based con-
nectivity with external, AER compliant devices. We demonstrate the ability of the
network to efﬁciently classify overlapping patterns, thanks to the self-regulating
mechanism.

1 Introduction

Neuromorphic analog, VLSI devices [12] try to derive organizational and computational principles
from biologically plausible models of neural systems, aiming at providing in the long run an elec-
tronic substrate for innovative, bio-inspired computational paradigms.

In line with standard assumptions in computational neuroscience, neuromorphic devices are en-
dowed with adaptive capabilities through various forms of plasticity in the synapses which connect
the neural elements. A widely adopted framework goes under the name of Hebbian learning, by
which the efﬁcacy of a synapse is potentiated (the post-synaptic effect of a spike is enhanced) if the
pre- and post-synaptic neurons are simultaneously active on a suitable time scale. Different mech-
anisms have been proposed, some relying on the average ﬁring rates of the pre- and post-synaptic
neurons, (rate-based Hebbian learning), others based on tight constraints on the time lags between
pre- and post-synaptic spikes (“Spike-Timing-Dependent-Plasticity”).

The synaptic circuits described in what follows implement a stochastic version of rate-based Heb-
bian learning. In the last decade, it has been realized that general constraints plausibly met by any
concrete implementation of a synaptic device in a neural network, bear profound consequences on

∗http://neural.iss.infn.it/

1

the capacity of the network as a memory system. Speciﬁcally, once one accepts that a synaptic
element can neither have an unlimited dynamic range (i.e. synaptic efﬁcacy is bounded), nor can it
undergo arbitrarily small changes (i.e. synaptic efﬁcacy has a ﬁnite analog depth), it has been proven
([1], [7]) that a deterministic learning prescription implies an extremely low memory capacity, and a
severe “palimpsest” property: new memories quickly erase the trace of older ones. It turns out that a
stochastic mechanism provides a general, logically appealing and very efﬁcient solution: given the
pre- and post-synaptic neural activities, the synapse is still made eligible for changing its efﬁcacy
according to a Hebbian prescription, but it actually changes its state with a given probability. The
stochastic element of the learning dynamics would imply ad hoc new elements, were it not for the
fact that for a spike-driven implementation of the synapse, the noisy activity of the neurons in the
network can provide the needed “noise generator” [7]. Therefore, for an efﬁcient learning electronic
network, the implementation of the neuron as a spiking element is not only a requirement of “biolog-
ical plausibility”, but a compelling computational requirement. Learning in networks of spiking IF
neurons with stochastic plastic synapses has been studied theoretically [7], [10], [2], and stochastic,
bi-stable synaptic models have been implemented in silicon [8], [6]. One of the limitations so far,
both at the theoretical and the implementation level, has been the artiﬁcially simple statistics of the
stimuli to be learnt (e.g., no overlap between their neural representations). Very recently in [4] a
modiﬁcation of the above stochastic, bi-stable synaptic model has been proposed, endowed with a
regulatory mechanism termed “stop learning” such that synaptic up or down-regulation depends on
the average activity of the postsynaptic neuron in the recent past; a synapse pointing to a neuron that
is found to be highly active, or poorly active, should not be further potentiated or depressed, respec-
tively. The reason behind the prescription is essentially that for correlated patterns to be learnt by
the network, a successful strategy should de-emphasize the coherent synaptic Hebbian potentiation
that would result for the overlapping part of the synaptic matrix, and that would ultimately spoil the
ability to distinguish the patterns. A detailed learning strategy along this line was proven in [13] to
be appropriate for linearly separable patterns for a Perceptron-like network; the extension to spiking
and recurrent networks is currently studied.

In section 2 we give an overview of the chip architecture and of the implemented synaptic model.
In section 3 we show an example of the measures effectuated on the chip useful to characterize the
synaptic and neuronal parameters. In section 4 we report some characterization results compared
with a theoretical prediction obtained from a chip-oriented simulation. The last paragraph describes
chip performances in a simple classiﬁcation task, and illustrate the improvement brought about by
the stop-learning mechanism.

2 Chip architecture and main features

The chip, already described in [3] implements a recurrent network of 32 integrate-and-ﬁre neurons
with spike-frequency adaptation and bi-stable, stochastic, Hebbian synapses. A completely recon-
ﬁgurable synaptic matrix supports up to all-to-all recurrent connectivity, and AER-based external
connectivity. Besides establishing an arbitrary synaptic connectivity, the excitatory/inhibitory na-
ture of each synapse can also be set.

The implemented neuron is the IF neuron with constant leakage term and a lower bound for the
membrane potential V (t) introduced in [12] and studied theoretically in [9]. The circuit is borrowed
from the low-power design described in [11], to which we refer the reader for details. Only 2
neurons can be directly probed (i.e., their “membrane potential” sampled), while for all of them the
emitted spikes can be monitored via AER [5]. The dendritic tree of each neuron is composed of
up to 31 activated recurrent synapses and up to 32 activated external, AER ones. For the recurrent
synapses, each impinging spike triggers short-time (and possibly long-term) changes in the state of
the synapse, as detailed below. Spikes from neurons outside the chip come in the form of AER
events, and are targeted to the correct AER synapse by the X-Y Decoder. Synapses which are set to
be excitatory, either AER or recurrent are plastic; inhibitory synapses are ﬁxed. Spikes generated by
the neurons in the chip are arbitrated for access to the AER bus for monitoring and/or mapping to
external targets.

The synaptic circuit described in [3] implements the model proposed in [4] and brieﬂy motivated in
the Introduction. The synapse possesses only two states of efﬁcacy (a bi-stable device): the internal
synaptic dynamics is associated with an internal variable X; when X > θX the efﬁcacy is set to be

2

potentiated, otherwise is set to be depressed. X is subjected to short-term, spike-driven dynamics:
upon the arrival of an impinging spike, X is candidate for an upward or downward jump, depending
on the instantaneous value of the post-synaptic potential Vpost being above or below a threshold θV .
The jump is actually performed or not depending on a further variable as explained below. In the
absence of intervening spikes X is forced to drift towards a “high” or “low” value depending on
whether the last jump left it above or below θX. This preserves the synaptic efﬁcacy on long time
scale.

A further variable is associated with the post-synaptic neuron dynamics, which essentially mea-
sures the average ﬁring activity. Following [4], by analogy with the role played by the intracellular
concentration of calcium ions upon spike emission, we will call it a “calcium variable” C(t). C(t)
undergoes an upward jump when the postsynaptic neuron emits a spike, and linearly decays between
two spikes. It therefore integrates the spikes sequence and, when compared to suitable thresholds as
detailed below, it determines which candidate synaptic jumps will be allowed to occur; for example,
it can constrain the synapse to stop up-regulating because the post-synaptic neuron is already very
active. C(t) acts as a regulatory element of the synaptic dynamics.
The resulting short-term dynamics for the internal synaptic variable X is described by the following
conditions: X(t) → X(t) + Jup if Vpost(t) > θV and VT H1 < C(t) < VT H3; X(t) → X(t) − Jdw
if Vpost(t) ≤ θV and VT H1 < C(t) < VT H2 where Jup and Jdw are positive constants. Detailed
description of circuits implementing these conditions can be found in [3].

In ﬁgure 1 we illustrate the effect of the calcium dynamics on X. Increasing input forces the post-
synaptic neuron to ﬁre at increasing frequencies. As long as C(t) < VT H2 = VT H3 X undergoes
both up and down jumps. When C(t) > VT H2 = VT H3 jumps are inhibited and X is forced to drift
towards its lower bound.

V (t)

post

C(t)

X(t)

V (t)

pre

VTH2

40 ms

1V

1V

1V

2V

Figure 1: Illustrative example of the stop-learning mechanism (see text). Top to bottom: post-
synaptic neuron potential Vpost, calcium variable C, internal synaptic variable X, pre-synaptic neu-
ron potential Vpre

3 LTP/LTD probabilities: measurement VS chip-oriented simulation

We report synapse potentiation (LTP) / depression (LTD)from the chip and we compare experimental
results to simulations.

For each synapse in a subset of 31, we generate a pre-synaptic poisson spike train at 70 Hz. The
post synaptic neuron is forced to ﬁre a poisson spike train by applying an external DC current and a
poisson train of inhibitory spikes through AER. Setting to zero both the potentiated and depressed
efﬁcacies, the activity of the post-synaptic neuron can be easily tuned by varying the amplitude of
the DC current and the frequency of the inhibitory AER train. We initialize the 31 (AER) synapses
to depressed (potentiated) and we monitor the post-synaptic neuron activity during a stimulation

3

trial lasting 0.5 seconds. At the end of the trial we read the synaptic state using an AER protocol
developed to this purpose. For each chosen value of the post-synaptic ﬁring rate, we evaluate the
probability to ﬁnd synapses in a potentiated (depressed) state repeating the test 50 times. The results
reported in ﬁgure 2 (solid lines) represent the average LTP and LTD probabilities per trail over the 31
synapses. Tests were performed both with active and inactive Calcium mechanism. When calcium
mechanism is inactive, the LTP is monotonically increasing with the post-synaptic ﬁring rate while
when the calcium circuit is activated the LTP probability has a max form Vpost around 80 Hz.
Identical tests were also run in simulation (dashed curves in ﬁgure 2). For the purpose of a meaning-
ful comparison with the chip behaviour relevant parameter affecting neural and synaptic dynamics
and their distributions (due to inhomogenities and mismatches) are characterized.

Simulated and measured data are in qualitative agreement. The parameters we chose for these tests
are the same used for the classiﬁcation task described in the next paragraph.

Fraction of potentiated synapses

0.6

0.5

0.4

+
w

0.3

0.2

0.1

0
0

Experiment: solid line
Simulation: dashed line

50

100

150

ν
post [Hz]

200

250

300

Figure 2: Transition probabilities. Red and blue lines are LTP probabilities with and without cal-
cium stop-learning mechanism respectively. Gray lines are LTD probabilities without calcium stop-
learning mechanism, the case LTD with Ca mechanism is not shown. Error bars are standard devia-
tions over the 50 trials

4 Learning overlapping patterns

We conﬁgured the synaptic matrix to have a perceptron like network with 1 output and 32 inputs (32
AER synapses). 31 synapses are set as plastic excitatory ones, the 32nd is set as inhibitory and used
to modulate the post-synpatic neuron activity. Our aim is to teach the perceptron to classify two
patterns through a semi-supervised learning strategy: “Up” and “Down”. We expect that after learn-
ing the perceptron will respond with high output frequency for pattern “Up” and with low output
frequency for pattern “Down”. The self regulating Ca mechanism is exploited to improve perfor-
mances when Up and Down patterns have a signiﬁcant overlap. The learning is semi-supervised:
for each pattern a “teacher” input is sent to the output neuron steering its activity to be high or low,
as desired. At the end of the learning period the “teacher” is turned off and the perceptron output is
driven only by the input stimuli: in this conditions its classiﬁcation ability is tested.

We present learning performances for input patterns with increasing overlap, and demonstrate the
effect of the stop learning mechanism (overlap ranging from 6 to 14 synapses).

Upon stimulation active pre-synaptic inputs are poisson spike trains at 70 Hz, while inactive inputs
are poisson spike trains at 10 Hz. Each trial lasts half a second. Up and Down patterns are randomly
presented with equal probability. The teaching signal, a combination of an excitatory constant cur-

4

rent and of an inhibitory AER spike train, forces the output ﬁring rate to 50 or 0.5 Hz. One run lasts
for 150 trials which is sufﬁcient for the stabilization of the output frequencies. At the end of each
trial we turn off the teaching signal, we freeze the synaptic dynamics and we read the state of each
synapse using an AER protocol developed for this purpose. In these conditions we performed a 5
seconds test (“Checking Phase”) to measure the perceptron frequencies when pattern Up or pattern
Down are presented. Each experiment includes 50 runs. For each run we change: a) the “deﬁnition”
of patterns Up and Down: inputs activated by pattern Up and Down are chosen randomly at the
beginning of each run; b) the initial synaptic state, with the constraint that only about 30 % of the
synapses are potentiated; c) the stimulation sequence.

For the ﬁrst experiment we turned off the stop learning mechanism and we chose orthogonal patterns.
In this case the perceptron was able to correctly classify the stimuli: after about 50 trials, choosing a
suitable threshold, one can discriminate the perceptron ouput to different patterns (lower left panel
on ﬁgure 4). The output frequency separation slightly increases until trial number 100 remaining
almost stable after that point.

We then studied the case of overlapped patterns both with active and inactive Calcium mechanism.
We repeated the experiment with an increasing overlap: 6, 10 and 14. (implying an increase in
the coding level from 0.5 for the orthogonal case to 0.7 for the overlap equal to 14). Only the
threshold K up
high is active (the threshold above which up jumps are inhibnited). The Calcium circuit
parameters are tuned so that the Ca variable passes K up
high for the mean ﬁring rate of the post-
synaptic neuron around 80 Hz. We show in ﬁgure 3 the distributions of the potentiated fraction of
the synapses over the 50 runs at different stages along the run for overlap 10 with inactive (upper
panels) and active (lower panels) calcium mechanism. We divided synapses in three subgroups: Up
(red) synapses with pre-synaptic input activated solely by Up pattern, Down (blue) synapses with
pre-synaptic inputs activated only by Down pattern, and Overlap (green) synapses with pre-synpatic
inputs activated by both pattern Up and Down. The state of the synapses is recorded after every
learning step. Accumulating statistics over the 50 runs we obtain the distributions reported in ﬁgure
3. The fraction of potentiated synapses is calculated over the number of synapses belonging to each
subgroup. When the stop learning mechanism is inactive, at the end of the experiment, the green

trial 2

0.5
w+

trial 2

1

0.8

0.6

0.4

0.2

0

0

1

0.8

0.6

0.4

0.2

1

0.8

0.6

0.4

0.2

1

0

0

1

0.8

0.6

0.4

0.2

Ca mechanism inactive
trial 50

trial 100

1

0.8

0.6

0.4

0.2

0

0

1

0.5
w+
Ca mechanism active
trial 50

1

0.8

0.6

0.4

0.2

1

0.8

0.6

0.4

0.2

1

0

0

0.5
w+

trial 100

1

0.8

0.6

0.4

0.2

trial 150

synapses Overlap
synapses Up
synapses Down

0.5
w+

1

trial 150

synapses Overlap
synapses Up
synapses Down

 
 
)

+
w
P

(

 
 
)

+
w
P

(

0

0

0.5
w+

1

0

0

0.5
w+

1

0

0

0.5
w+

1

0

0

0.5
w+

1

Figure 3: Distribution of the fraction of potentiated synapses. The number of inputs belonging to
both patterns is 10.

distribution of overlap synapses is broad, when the Calcium mechanism is active, synapses overlap
tend to be depotentiated. This result is the “microscopic” effect of the stop learning mechanism:
once the number of potentiated synapses is sufﬁcient to drive the perceptron output frequency above
80 Hz, the overlap synapses tend to be depotentiated. Overlap synapses would be pushed half of the

5

times to the potentiated state and half of the times to the depressed state, so that it is more likely for
the Up synapses to reach earlier the potentiated state. When the stop learning mechanism is active,
the potentiated synapses are enogh to drive the output neuron about 80 Hz, further potentiation is
inhibited for all synapses so that overlap synapses get depressed on average. This happens under the
condition that the transition probability are sufﬁciently small to avoid that at each trial the learning is
completely disrupted. The distribution of the output frequencies for increasing overlap is illustrated
in ﬁgure 4 (Ca mechanism inactive in the upper panels, active for the lower panels). The frequencies
are recorded during the “checking phase”. In blue the histograms of the output frequency for the
down pattern, in red those for up pattern.
It is clear from the ﬁgure that the output frequency
distribution remain well separated even for high overlap when the Calcium mechanism is active.

A quantitative parameter to describe the distribution separation is

δ =

νup − νdn
σ2
νup + σ2

νdn

(1)

δ values are summarized in table 1.

Overlap 0

Ca mechanism inactive
Overlap 6

Overlap 10

1

0.8

0.6

0.4

0.2

0

0

100
ν
ck

 [Hz]

200

1

0.8

0.6

0.4

0.2

0

Overlap 14

Pattern Down
Pattern Up

1

0.8

0.6

0.4

0.2

200

0

0

100
ν
ck

 [Hz]

200

)

k
c

ν
(
P

)

k
c

ν
(
P

1

0.8

0.6

0.4

0.2

0

0

1

0.8

0.6

0.4

0.2

0

0

200

0

100
ν
ck

 [Hz]

100
ν
ck

 [Hz]
Ca mechanism active

Overlap 0

Overlap 6

Overlap 10

Overlap 14

1

0.8

0.6

0.4

0.2

0

0

100
ν
ck

 [Hz]

200

1

0.8

0.6

0.4

0.2

0

0

100
ν
ck

 [Hz]

200

1

0.8

0.6

0.4

0.2

0

0

Pattern Down
Pattern Up

100
ν
ck

 [Hz]

200

100
ν
ck

 [Hz]

200

Figure 4: Distributions of perceptron frequencies after learning two overlapped patterns. Blue bars
refer to pattern Down stimulation, red bars refers to pattern Up. Each panel refers to overlap.

Table 1: Discrimination power [seconds]

overlap 0

overlap 6

overlap 10

overlap 14

Ca OFF
Ca ON

4.39
5.29

1.87
2.20

1.59
1.88

0.99
1.66

For each run the number of potentiated synapses is different due to the random choices of Up, Down
and Overlap synapses for each run and the mismatches affecting the behavior of different synapses.
The failure of the discrimination for high overlap in the absence of this stop learning mechanism
is due to the fact that the number of potentiated synapses can overcome the effect of the teaching
signal for the down pattern. The Calcium mechanism, deﬁning a maximum number of allowed
potentiated synapses, limits this problem. This offer the possibility of establishing a priori threshold
to discriminate the perceptron outputs on the basis of the frequency corresponding to the maximum
value of the LTP probability curve.

6

5 Conclusions

We brieﬂy illustrate an analog VLSI chip implementing a network of 32 IF neurons and 2,048
reconﬁgurable, Hebbian, plastic, stop-learning synapses. Circuit parameters has been measured as
well as their dispersion across the chip. Using these data a chip-oriented simulation was set up and
its results, compared to experimental ones, demonstrate that circuits behavior follow the theoretical
predictions. Once conﬁgured the network as a perceptron (31 AER synapses and one output neuron),
a classiﬁcation task has been performed. Stimuli with an increasing overlap have been used. The
results show the ability of the network to efﬁciently classify the presented patterns as well as the
improvement of the performances due to the calcium stop-learning mechanism.

References

[1] D.J. Amit and S. Fusi. Neural Computation, 6:957, 1994.
[2] D.J. Amit and G. Mongillo. Neural Computation, 15:565, 2003.
[3] D. Badoni, M. Giulioni, V. Dante, and P. Del Giudice. In Proc. IEEE International Symposium

on Circuits and Systems ISCAS06, pages 1227–1230, 2006.

[4] J.M. Brader, W. Senn, and S. Fusi. Neural Computation (in press), 2007.
[5] V. Dante, P. Del Giudice, and A. M. Whatley. The neuromorphic engineer newsletter. 2005.
[6] E. Chicca et al. IEEE Transactions on Neural Networks, 14(5):1297, 2003.
[7] S. Fusi. Biological Cybernetics, 87:459, 2002.
[8] S. Fusi, M. Annunziato, D. Badoni, A. Salamon, and D.J. Amit. Neural Computation, 12:2227,

2000.

[9] S. Fusi and M. Mattia. Neural Computation, 11:633, 1999.
[10] P. Del Giudice, S. Fusi, and M. Mattia. Journal of Physiology Paris, 97:659, 2003.
[11] G. Indiveri. In Proc. IEEE International Symposium on Circuits and Systems, 2003.
[12] C. Mead. Analog VLSI and neural systems. Addison-Wesley, 1989.
[13] W. Senn and S. Fusi. Neural Computation, 17:2106, 2005.

7

"
874,2007,The discriminant center-surround hypothesis for bottom-up saliency,"The classical hypothesis, that bottom-up saliency is a center-surround process, is combined with a more recent hypothesis that all saliency decisions are optimal in a decision-theoretic sense. The combined hypothesis is denoted as discriminant center-surround saliency, and the corresponding optimal saliency architecture is derived. This architecture equates the saliency of each image location to the discriminant power of a set of features with respect to the classification problem that opposes stimuli at center and surround, at that location. It is shown that the resulting saliency detector makes accurate quantitative predictions for various aspects of the psychophysics of human saliency, including non-linear properties beyond the reach of previous saliency models. Furthermore, it is shown that discriminant center-surround saliency can be easily generalized to various stimulus modalities (such as color, orientation and motion), and provides optimal solutions for many other saliency problems of interest for computer vision. Optimal solutions, under this hypothesis, are derived for a number of the former (including static natural images, dense motion fields, and even dynamic textures), and applied to a number of the latter (the prediction of human eye fixations, motion-based saliency in the presence of ego-motion, and motion-based saliency in the presence of highly dynamic backgrounds). In result, discriminant saliency is shown to predict eye fixations better than previous models, and produce background subtraction algorithms that outperform the state-of-the-art in computer vision.","The discriminant center-surround hypothesis for

bottom-up saliency

Dashan Gao

Vijay Mahadevan

Nuno Vasconcelos

Department of Electrical and Computer Engineering

University of California, San Diego

{dgao, vmahadev, nuno}@ucsd.edu

Abstract

The classical hypothesis, that bottom-up saliency is a center-surround process, is
combined with a more recent hypothesis that all saliency decisions are optimal in
a decision-theoretic sense. The combined hypothesis is denoted as discriminant
center-surround saliency, and the corresponding optimal saliency architecture is
derived. This architecture equates the saliency of each image location to the dis-
criminant power of a set of features with respect to the classiﬁcation problem that
opposes stimuli at center and surround, at that location. It is shown that the result-
ing saliency detector makes accurate quantitative predictions for various aspects
of the psychophysics of human saliency, including non-linear properties beyond
the reach of previous saliency models. Furthermore, it is shown that discriminant
center-surround saliency can be easily generalized to various stimulus modalities
(such as color, orientation and motion), and provides optimal solutions for many
other saliency problems of interest for computer vision. Optimal solutions, under
this hypothesis, are derived for a number of the former (including static natural
images, dense motion ﬁelds, and even dynamic textures), and applied to a num-
ber of the latter (the prediction of human eye ﬁxations, motion-based saliency in
the presence of ego-motion, and motion-based saliency in the presence of highly
dynamic backgrounds). In result, discriminant saliency is shown to predict eye
ﬁxations better than previous models, and produces background subtraction algo-
rithms that outperform the state-of-the-art in computer vision.

1 Introduction

The psychophysics of visual saliency and attention have been extensively studied during the last
decades. As a result of these studies, it is now well known that saliency mechanisms exist for a
number of classes of visual stimuli, including color, orientation, depth, and motion, among others.
More recently, there has been an increasing effort to introduce computational models for saliency.
One approach that has become quite popular, both in the biological and computer vision communi-
ties, is to equate saliency with center-surround differencing. It was initially proposed in [12], and
has since been applied to saliency detection in both static imagery and motion analysis, as well
as to computer vision problems such as robotics, or video compression. While difference-based
modeling is successful at replicating many observations from psychophysics, it has three signiﬁ-
cant limitations. First, it does not explain those observations in terms of fundamental computational
principles for neural organization. For example, it implies that visual perception relies on a linear
measure of similarity (difference between feature responses in center and surround). This is at odds
with well known properties of higher level human judgments of similarity, which tend not to be
symmetric or even compliant with Euclidean geometry [20]. Second, the psychophysics of saliency
offers strong evidence for the existence of both non-linearities and asymmetries which are not eas-
ily reconciled with this model. Third, although the center-surround hypothesis intrinsically poses

1

saliency as a classiﬁcation problem (of distinguishing center from surround), there is little basis on
which to justify difference-based measures as optimal in a classiﬁcation sense. From an evolutionary
perspective, this raises questions about the biological plausibility of the difference-based paradigm.

An alternative hypothesis is that all saliency decisions are optimal in a decision-theoretic sense.
This hypothesis has been denoted as discriminant saliency in [6], where it was somewhat narrowly
proposed as the justiﬁcation for a top-down saliency algorithm. While this algorithm is of interest
only for object recognition, the hypothesis of decision theoretic optimality is much more general,
and applicable to any form of center-surround saliency. This has motivated us to test its ability to
explain the psychophysics of human saliency, which is better documented for the bottom-up neural
pathway. We start from the combined hypothesis that 1) bottom-up saliency is based on center-
surround processing, and 2) this processing is optimal in a decision theoretic sense. In particular,
it is hypothesized that, in the absence of high-level goals, the most salient locations of the visual
ﬁeld are those that enable the discrimination between center and surround with smallest expected
probability of error. This is referred to as the discriminant center-surround hypothesis and, by
deﬁnition, produces saliency measures that are optimal in a classiﬁcation sense. It is also clearly
tied to a larger principle for neural organization: that all perceptual mechanisms are optimal in a
decision-theoretic sense.

In this work, we present the results of an experimental evaluation of the plausibility of the discrim-
inant center-surround hypothesis. Our study evaluates the ability of saliency algorithms, that are
optimal under this hypothesis, to both

• reproduce subject behavior in classical psychophysics experiments, and
• solve saliency problems of practical signiﬁcance, with respect to a number of classes of

visual stimuli.

We derive decision-theoretic optimal center-surround algorithms for a number of saliency problems,
ranging from static spatial saliency, to motion-based saliency in the presence of egomotion or even
complex dynamic backgrounds. Regarding the ability to replicate psychophysics, the results of this
study show that discriminant saliency not only replicates all anecdotal observations that can be ex-
plained by linear models, such as that of [12], but can also make (surprisingly accurate) quantitative
predictions for non-linear aspects of human saliency, which are beyond the reach of the existing
approaches. With respect to practical saliency algorithms, they show that discriminant saliency not
only is more accurate than difference-based methods in predicting human eye ﬁxations, but actu-
ally produces background subtraction algorithms that outperform the state-of-the-art in computer
vision. In particular, it is shown that, by simply modifying the probabilistic models employed in
the (decision-theoretic optimal) saliency measure - from well known models of natural image statis-
tics, to the statistics of simple optical-ﬂow motion features, to more sophisticated dynamic texture
models - it is possible to produce saliency detectors for either static or dynamic stimuli, which are
insensitive to background image variability due to texture, egomotion, or scene dynamics.

2 Discriminant center-surround saliency

A common hypothesis for bottom-up saliency is that the saliency of each location is determined by
how distinct the stimulus at the location is from the stimuli in its surround (e.g., [11]). This hypoth-
esis is inspired by the ubiquity of “center-surround” mechanisms in the early stages of biological
vision [10]. It can be combined with the hypothesis of decision-theoretic optimality, by deﬁning a
classiﬁcation problem that equates

• the class of interest, at location l, with the observed responses of a pre-deﬁned set of fea-

tures X within a neighborhood W 1

l of l (the center),

• the null hypothesis with the responses within a surrounding window W 0

l (the surround ),

The saliency of location l∗ is then equated with the power of the feature set X to discriminate
between center and surround. Mathematically, the feature responses within the two windows are
interpreted as observations drawn from a random process X(l) = (X1(l), . . . , Xd(l)), of dimension
d, conditioned on the state of a hidden random variable Y (l). The observed feature vector at any
location j is denoted by x(j) = (x1(j), . . . , xd(j)), and feature vectors x(j) such that j ∈ W c
l , c ∈

2

{0, 1} are drawn from class c (i.e., Y (l) = c), according to conditional densities PX(l)|Y (l)(x|c).
The saliency of location l, S(l), is quantiﬁed by the mutual information between features, X, and
class label, Y ,

S(l) = Il(X; Y ) = Xc

Z pX(l),Y (l)(x, c) log

pX(l),Y (l)(x, c)
pX(l)(x)pY (l)(c)

dx.

(1)

The l subscript emphasizes the fact that the mutual information is deﬁned locally, within Wl. The
function S(l) is referred to as the saliency map.

3 Discriminant saliency detection in static imagery

Since human saliency has been most thoroughly studied in the domain of static stimuli, we ﬁrst
derive the optimal solution for discriminant saliency in this domain. We then study the ability of
the discriminant center-surround saliency hypothesis to explain the fundamental properties of the
psychophysics of pre-attentive vision.

3.1 Feature decomposition

The building blocks of the static discriminant saliency detector are shown in Figure 1. The ﬁrst
stage, feature decomposition, follows the proposal of [11], which closely mimics the earliest stages
of biological visual processing. The image to process is ﬁrst subject to a feature decomposition into
an intensity map and four broadly-tuned color channels, I = (r + g + b)/3, R = b˜r − (˜g + ˜b)/2c+,
G = b˜g − (˜r + ˜b)/2c+, B = b˜b − ˜(r + ˜g)/2c+, and Y = b(˜r + ˜g)/2 − |˜r − ˜g|/2c+, where
˜r = r/I, ˜g = g/I, ˜b = b/I, and bxc+ = max(x, 0). The four color channels are, in turn, combined
into two color opponent channels, R − G for red/green and B − Y for blue/yellow opponency.
These and the intensity map are convolved with three Mexican hat wavelet ﬁlters, centered at spatial
frequencies 0.02, 0.04 and 0.08 cycle/pixel, to generate nine feature channels. The feature space X
consists of these channels, plus a Gabor decomposition of the intensity map, implemented with a
dictionary of zero-mean Gabor ﬁlters at 3 spatial scales (centered at frequencies of 0.08, 0.16, and
0.32 cycle/pixel) and 4 directions (evenly spread from 0 to π).

3.2 Leveraging natural image statistics

In general, the computation of (1) is impractical, since it requires density estimates on a potentially
high-dimensional feature space. This complexity can, however, be drastically reduced by exploiting
a well known statistical property of band-pass natural image features, e.g. Gabor or wavelet coefﬁ-
cients: that features of this type exhibit strongly consistent patterns of dependence (bow-tie shaped
conditional distributions) across a very wide range of classes of natural imagery [2, 9, 21]. The
consistency of these feature dependencies suggests that they are, in general, not greatly informative
about the image class [21, 2] and, in the particular case of saliency, about whether the observed
feature vectors originate in the center or surround. Hence, (1) can usually be well approximated by
the sum of marginal mutual informations [21]1, i.e.,

S(l) =

d

Xi=1

Il(Xi; Y ).

(2)

Since (2) only requires estimates of marginal densities, it has signiﬁcantly less complexity than (1).
This complexity can, indeed, be further reduced by resorting to the well known fact that the marginal
densities are accurately modeled by a generalized Gaussian distribution (GGD) [13]. In this case, all
computations have a simple closed form [4] and can be mapped into a neural network that replicates
the standard architecture of V1: a cascade of linear ﬁltering, divisive normalization, quadratic non-
linearity and spatial pooling [7].

1Note that this approximation does not assume that the features are independently distributed, but simply

that their dependencies are not informative about the class.

3

Feature maps

Feature saliency 

maps

(a)

Saliency map

(cid:54)

y
c
n
e

i
l

a
S

3

2.5

2

1.5

1

0.5

0

5 10

20

1.9

1.85

1.8

1.75

y
c
n
e

i
l

a
S

70

80

90

1.7
0

10

20

30

40

50

60

70

80

90

Orientation contrast (deg)

30

40

50

Orientation contrast (deg)

60

)

Y
B

/

 
,

G
R

/

(
 
r
o
l
o
C

n
o
i
t
i
s
o
p
m
o
c
e
d
 
e
r
u
t
a
e
F

y
t
i
s
n
e
t
n
I

n
o
i
t
a
t
n
e
i
r

O

Figure 1: Bottom-up discriminant saliency detector.

(b)

(c)

Figure 2: The nonlinearity of human saliency re-
sponses to orientation contrast [14] (a) is replicated
by discriminant saliency (b), but not by the model
of [11] (c).

3.3 Consistency with psychophysics
To evaluate the consistency of discriminant saliency with psychophysics, we start by applying the
discriminant saliency detector to a series of displays used in classical studies of visual attention [18,
19, 14]2. In [7], we have shown that discriminant saliency reproduces the anecdotal properties of
saliency - percept of pop-out for single feature search, disregard of feature conjunctions, and search
asymmetries for feature presence vs. absence - that have previously been shown possible to replicate
with linear saliency models [11]. Here, we focus on quantitative predictions of human performance,
and compare the output of discriminant saliency with both human data and that of the difference-
based center-surround saliency model [11]3.
The ﬁrst experiment tests the ability of the saliency models to predict a well known nonlinearity
of human saliency. Nothdurft [14] has characterized the saliency of pop-out targets due to ori-
entation contrast, by comparing the conspicuousness of orientation deﬁned targets and luminance
deﬁned ones, and using luminance as a reference for relative target salience. He showed that the
saliency of a target increases with orientation contrast, but in a non-linear manner: 1) there exists a
threshold below which the effect of pop-out vanishes, and 2) above this threshold saliency increases
with contrast, saturating after some point. The results of this experiment are illustrated in Figure 2,
which presents plots of saliency strength vs orientation contrast for human subjects [14] (in (a)),
for discriminant saliency (in (b)), and for the difference-based model of [11]. Note that discrim-
inant saliency closely predicts the strong threshold and saturation effects characteristic of subject
performance, but the difference-based model shows no such compliance.

The second experiment tests the ability of the models to make accurate quantitative predictions of
search asymmetries. It replicates the experiment designed by Treisman [19] to show that the asym-
metries of human saliency comply with Weber’s law. Figure 3 (a) shows one example of the displays
used in the experiment, where the central target (vertical bar) differs from distractors (a set of iden-
tical vertical bars) only in length. Figure 3 (b) shows a scatter plot of the values of discriminant
saliency obtained across the set of displays. Each point corresponds to the saliency at the target
location in one display, and the dashed line shows that, like human perception, discriminant saliency
follows Weber’s law: target saliency is approximately linear in the ratio between the difference of
target/distractor length (∆x) and distractor length (x). For comparison, Figure 3 (c) presents the cor-
responding scatter plot for the model of [11], which clearly does not replicate human performance.

4 Applications of discriminant saliency
We have, so far, presented quantitative evidence in support of the hypothesis that pre-attentive vi-
sion implements decision-theoretical center-surround saliency. This evidence is strengthened by the

2For the computation of the discriminant saliency maps, we followed the common practice of psychophysics
and physiology [18, 10], to set the size of the center window to a value comparable to that of the display items,
and the size of the surround window is 6 times of that of the center. Informal experimentation has shown that
the saliency results are not substantively affected by variations around the parameter values adopted.

3Results obtained with the MATLAB implementation available in [22].

4

(a)

1.95

1.9

1.85

1.8

1.75

1.7

y
c
n
e
i
l
a
S

0.2

0.4
∆ x/x

0.6

0.8

1.65
0

0.2

0.4
∆ x/x

0.6

0.8

1

0.8

y
c
n
e
i
l
a
S

0.6

0.4

0.2

0
0

(b)

(c)

Figure 3: An example display (a) and perfor-
mance of saliency detectors (discriminant saliency
(b) and [11] (c)) on Weber’s law experiment.

 

a
e
r
a
C
O
R
 
y
c
n
e

i
l

a
S

0.95

0.9

0.85

0.8

0.75

0.7

0.65

0.6

0.8

discriminant saliency
Itti et al.
Bruce et al.

0.85

0.9

Inter−subject ROC area

0.95

0.98

Figure 4: Average ROC area, as a function of
inter-subject ROC area, for the saliency algo-
rithms.

Saliency model Discriminant

Itti et al. [11] Bruce et al. [1]

ROC area

0.7694

0.7287

0.7547

Table 1: ROC areas for different saliency models with respect to all human ﬁxations.

already mentioned one-to-one mapping between the discriminant saliency detector proposed above
and the standard model for the neurophysiology of V1 [7]. Another interesting property of discrim-
inant saliency is that its optimality is independent of the stimulus dimension under consideration, or
of speciﬁc feature sets. In fact, (1) can be applied to any type of stimuli, and any type of features, as
long as it is possible to estimate the required probability distributions from the center and surround
neighborhoods. This encouraged us to derive discriminant saliency detectors for various computer
vision applications, ranging from the prediction of human eye ﬁxations, to the detection of salient
moving objects, to background subtraction in the context of highly dynamic scenes. The outputs
of these discriminant saliency detectors are next compared with either human performance, or the
state-of-the-art in computer vision for each application.

4.1 Prediction of eye ﬁxations on natural images
We start by using the static discriminant saliency detector of the previous section to predict human
eye ﬁxations. For this, the saliency maps were compared to the eye ﬁxations of human subjects in
an image viewing task. The experimental protocol was that of [1], using ﬁxation data collected from
20 subjects and 120 natural images. Under this protocol, all saliency maps are ﬁrst quantized into
a binary mask that classiﬁes each image location as either a ﬁxation or non-ﬁxation [17]. Using
the measured human ﬁxations as ground truth, a receiver operator characteristic (ROC) curve is
then generated by varying the quantization threshold. Perfect prediction corresponds to an ROC
area (area under the ROC curve) of 1, while chance performance occurs at an area of 0.5. The
predictions of discriminant saliency are compared to those of the methods of [11] and [1].

Table 1 presents average ROC areas for all detectors, across the entire image set. It is clear that
discriminant saliency achieves the best performance among the three detectors. For a more detailed
analysis, we also plot (in Figure 4) the ROC areas of the three detectors as a function of the “inter-
subject” ROC area (a measure of the consistency of eye movements among human subjects [8]), for
the ﬁrst two ﬁxations - which are more likely to be driven by bottom-up mechanisms than the later
ones [17]. Again, discriminant saliency exhibits the strongest correlation with human performance,
this happens at all levels of inter-subject consistency, and the difference is largest when the latter
is strong. In this region, the performance of discriminant saliency (.85) is close to 90% of that of
humans (.95), while the other two detectors only achieve close to 85% (.81).

4.2 Discriminant saliency on motion ﬁelds
Similarly to the static case, center-surround discriminant saliency can produce motion-based
saliency maps if combined with motion features. We have implemented a simple motion-based de-
tector by computing a dense motion vector map (optical ﬂow) between pairs of consecutive images,
and using the magnitude of the motion vector at each location as motion feature. The probability
distributions of this feature, within center and surround, were estimated with histograms, and the
motion saliency maps computed with (2).

5

Figure 5: Optical ﬂow-based saliency in the presence of egomotion.

Despite the simplicity of our motion representation, the discriminant saliency detector exhibits in-
teresting performance. Figure 5 shows several frames (top row) from a video sequence, and their
discriminant motion saliency maps (bottom row). The sequence depicts a leopard running in a grass-
land, which is tracked by a moving camera. This results in signiﬁcant variability of the background,
due to egomotion, making the detection of foreground motion (leopard), a non-trivial task. As shown
in the saliency maps, discriminant saliency successfully disregards the egomotion component of the
optical ﬂow, detecting the leopard as most salient.

4.3 Discriminant Saliency with dynamic background
While the results of Figure 5 are probably within the reach of previously proposed saliency models,
they illustrate the ﬂexibility of discriminant saliency. In this section we move to a domain where
traditional saliency algorithms almost invariably fail. This consists of videos of scenes with com-
plex and dynamic backgrounds (e.g. water waves, or tree leaves). In order to capture the motion
patterns characteristic of these backgrounds it is necessary to rely on reasonably sophisticated prob-
abilistic models, such as the dynamic texture model [5]. Such models are very difﬁcult to ﬁt in the
conventional, e.g. difference-based, saliency frameworks but naturally compatible with the discrim-
inant saliency hypothesis. We next combine discriminant center-surround saliency with the dynamic
texture model, to produce a background-subtraction algorithm for scenes with complex background
dynamics. While background subtraction is a classic problem in computer vision, there has been
relatively little progress for these type of scenes (e.g. see [15] for a review).

A dynamic texture (DT) [5, 3] is an autoregressive, generative model for video. It models the spatial
component of the video and the underlying temporal dynamics as two stochastic processes. A video
is represented as a time-evolving state process xt ∈ Rn, and the appearance of a frame yt ∈ Rm is
a linear function of the current state vector with some observation noise. The system equations are

xt = Axt−1 + vt
yt = Cxt + wt

(3)

where A ∈ Rn×n is the state transition matrix, C ∈ Rm×n is the observation matrix. The state and
observation noise are given by vt ∼iid N (0, Q,) and wt ∼iid N (0, R), respectively. Finally, the
initial condition is distributed as x1 ∼ N (µ, S). Given a sequence of images, the parameters of the
dynamic texture can be learned for the center and surround regions at each image location, enabling
a probabilistic description of the video, with which the mutual information of (2) can be evaluated.

We applied the dynamic texture-based discriminant saliency (DTDS) detector to three video se-
quences containing objects moving in water. The ﬁrst (Water-Bottle from [23]) depicts a bottle
ﬂoating in water which is hit by rain drops, as shown in Figure 7(a). The second and third, Boat and
Surfer, are composed of boats/surfers moving in water, and shown in Figure 8(a) and 9(a). These
sequences are more challenging, since the micro-texture of the water surface is superimposed on a
lower frequency sweeping wave (Surfer) and interspersed with high frequency components due to
turbulent wakes (created by the boat, surfer, and crest of the sweeping wave). Figures 7(b), 8(b)
and 9(b), show the saliency maps produced by discriminant saliency for the three sequences. The
DTDS detector performs surprisingly well, in all cases, at detecting the foreground objects while ig-
noring the movements of the background. In fact, the DTDS detector is close to an ideal background-
subtraction algorithm for these scenes.

6

1

0.8

0.6

0.4

0.2

)

R
D

(
 
e
t
a
r
 
n
o
i
t
c
e
t
e
D

0
0

1

0.8

0.6

0.4

0.2

)

R
D

(
 
e
t
a
r
 
n
o
i
t
c
e
t
e
D

1

0
0

Discriminant Salliency
GMM

0.2

0.4

0.6

0.8

False positive rate (FPR)

1

0.8

0.6

0.4

0.2

)

R
D

(
 
e
t
a
r
 
n
o
i
t
c
e
t
e
D

1

0
0

Discriminant Salliency
GMM

0.2

0.4

0.6

0.8

False positive rate (FPR)

1

Discriminant Saliency
GMM

0.2

0.4

0.6

0.8

False positive rate (FPR)

(a)

(b)

(c)

Figure 6: Performance of background subtraction algorithms on: (a) Water-Bottle, (b) Boat, and (c) Surfer.

(a)

(b)

(c)

Figure 7: Results on Bottle: (a) original; b) discriminant saliency with DT; and c) GMM model of [16, 24].

For comparison, we present the output of a state-of-the-art background subtraction algorithm, a
Gaussian mixture model (GMM) [16, 24]. As can be seen in Figures 7(c), 8(c) and 9(c), the resulting
foreground detection is very noisy, and cannot adapt to the highly dynamic nature of the water
surface. Note, in particular, that the waves produced by boat and surfer, as well as the sweeping
wave crest, create serious difﬁculties for this algorithm. Unlike the saliency maps of DTDS, the
resulting foreground maps would be difﬁcult to analyze by subsequent vision (e.g. object tracking)
modules. To produce a quantitative comparison of the saliency maps, these were thresholded at a
large range of values. The results were compared with ground-truth foreground masks, and an ROC
curve produced for each algorithm. The results are shown in Figure 6, where it is clear that while
DTDS tends to do well on these videos, the GMM based background model does fairly poorly.
References
[1] N. D. Bruce and J. K. Tsotsos. Saliency based on information maximization. In Proc. NIPS, 2005.
[2] R. Buccigrossi and E. Simoncelli. Image compression via joint statistical characterization in the wavelet

domain. IEEE Transactions on Image Processing, 8:1688–1701, 1999.

[3] A. B. Chan and N. Vasconcelos. Modeling, clustering, and segmenting video with mixtures of dynamic

textures. IEEE Trans. PAMI, In Press.

[4] M. N. Do and M. Vetterli. Wavelet-based texture retrieval using generalized gaussian density and

kullback-leibler distance. IEEE Trans. Image Processing, 11(2):146–158, 2002.

[5] G. Doretto, A. Chiuso, Y. N. Wu, and S. Soatto. Dynamic textures. Int. J. Comput. Vis., 51, 2003.
[6] D. Gao and N. Vasconcelos. Discriminant saliency for visual recognition from cluttered scenes. In Proc.

NIPS, pages 481–488, 2004.

[7] D. Gao and N. Vasconcelos. Decision-theoretic saliency: computational principle, biological plausibility,

and implications for neurophysiology and psychophysics. submitted to Neural Computation, 2007.

[8] J. Harel, C. Koch, and P. Perona. Graph-based visual saliency. In Proc. NIPS, 2006.
[9] J. Huang and D. Mumford. Statistics of Natural Images and Models. In Proc. IEEE Conf. CVPR, 1999.
[10] D. H. Hubel and T. N. Wiesel. Receptive ﬁelds and functional architecture in two nonstriate visual areas

(18 and 19) of the cat. J. Neurophysiol., 28:229–289, 1965.

7

(a)

(b)

(c)

Figure 8: Results on Boats: (a) original; b) discriminant saliency with DT; and c) GMM model of [16, 24].

(a)

(b)

(c)

Figure 9: Results on Surfer: (a) original; b) discriminant saliency with DT; and c) GMM model of [16, 24].

[11] L. Itti and C. Koch. A saliency-based search mechanism for overt and covert shifts of visual attention.

Vision Research, 40:1489–1506, 2000.

[12] L. Itti, C. Koch, and E. Niebur. A model of saliency-based visual attention for rapid scene analysis. IEEE

Trans. PAMI, 20(11), 1998.

[13] S. G. Mallat. A theory for multiresolution signal decomposition: The wavelet representation. IEEE Trans.

PAMI, 11(7):674–693, 1989.

[14] H. C. Nothdurft. The conspicuousness of orientation and motion contrast. Spat. Vis., 7, 1993.
[15] Y. Sheikh and M. Shah. Bayesian modeling of dynamic scenes for object detection.

IEEE Trans. on

PAMI, 27(11):1778–92, 2005.

[16] C. Stauffer and W. Grimson. Adaptive background mixture models for real-time tracking.

pages 246–52, 1999.

In CVPR,

[17] B. W. Tatler, R. J. Baddeley, and I. D. Gilchrist. Visual correlates of ﬁxation selection: effects of scale

and time. Vision Research, 45:643–659, 2005.

[18] A. Treisman and G. Gelade. A feature-integratrion theory of attention. Cognit. Psych., 12, 1980.
[19] A. Treisman and S. Gormican. Feature analysis in early vision: Evidence from search asymmetries.

Psychological Review, 95:14–58, 1988.

[20] A. Tversky. Features of similarity. Psychol. Rev., 84, 1977.
[21] N. Vasconcelos. Scalable discriminant feature selection for image retrieval. In CVPR, 2004.
[22] D. Walther and C. Koch. Modeling attention to salient proto-objects. Neural Networks, 19, 2006.
[23] J. Zhong and S. Sclaroff. Segmenting foreground objects from a dynamic textured background via a

robust Kalman ﬁlter. In ICCV, 2003.

[24] Z. Zivkovic. Improved adaptive Gaussian mixture model for background subtraction. In ICVR, 2004.

8

"
539,2007,Adaptive Embedded Subgraph Algorithms using Walk-Sum Analysis,"We consider the estimation problem in Gaussian graphical models with arbitrary structure. We analyze the Embedded Trees algorithm, which solves a sequence of problems on tractable subgraphs thereby leading to the solution of the estimation problem on an intractable graph. Our analysis is based on the recently developed walk-sum interpretation of Gaussian estimation. We show that non-stationary iterations of the Embedded Trees algorithm using any sequence of subgraphs converge in walk-summable models. Based on walk-sum calculations, we develop adaptive methods that optimize the choice of subgraphs used at each iteration with a view to achieving maximum reduction in error. These adaptive procedures provide a significant speedup in convergence over stationary iterative methods, and also appear to converge in a larger class of models.","Adaptive Embedded Subgraph Algorithms using

Walk-Sum Analysis

Venkat Chandrasekaran, Jason K. Johnson, and Alan S. Willsky

Department of Electrical Engineering and Computer Science

Massachusetts Institute of Technology

venkatc@mit.edu, jasonj@mit.edu, willsky@mit.edu

Abstract

We consider the estimation problem in Gaussian graphical models with arbitrary
structure. We analyze the Embedded Trees algorithm, which solves a sequence of
problems on tractable subgraphs thereby leading to the solution of the estimation
problem on an intractable graph. Our analysis is based on the recently developed
walk-sum interpretation of Gaussian estimation. We show that non-stationary it-
erations of the Embedded Trees algorithm using any sequence of subgraphs con-
verge in walk-summable models. Based on walk-sum calculations, we develop
adaptive methods that optimize the choice of subgraphs used at each iteration with
a view to achieving maximum reduction in error. These adaptive procedures pro-
vide a signiﬁcant speedup in convergence over stationary iterative methods, and
also appear to converge in a larger class of models.

1 Introduction

Stochastic processes deﬁned on graphs offer a compact representation for the Markov structure in a
large collection of random variables. We consider the class of Gaussian processes deﬁned on graphs,
or Gaussian graphical models, which are used to model natural phenomena in many large-scale ap-
plications [1, 2]. In such models, the estimation problem can be solved by directly inverting the
information matrix. However, the resulting complexity is cubic in the number of variables, thus
being prohibitively complex in applications involving hundreds of thousands of variables. Algo-
rithms such as Belief Propagation and the junction-tree method are effective for computing exact
estimates in graphical models that are tree-structured or have low treewidth [3], but for graphs with
high treewidth the junction-tree approach is intractable.

We describe a rich class of iterative algorithms for estimation in Gaussian graphical models with
arbitrary structure. Speciﬁcally, we discuss the Embedded Trees (ET) iteration [4] that solves a
sequence of estimation problems on trees, or more generally tractable subgraphs, leading to the so-
lution of the original problem on the intractable graph. We analyze non-stationary iterations of the
ET algorithm that perform inference calculations on an arbitrary sequence of subgraphs. Our anal-
ysis is based on the recently developed walk-sum interpretation of inference in Gaussian graphical
models [5]. We show that in the broad class of so-called walk-summable models, the ET algorithm
converges for any arbitrary sequence of subgraphs used. The walk-summability of a model is easily
tested [5, 6], thus providing a simple sufﬁcient condition for the convergence of such non-stationary
algorithms. Previous convergence results [6, 7] analyzed stationary or “cyclo-stationary” iterations
that use the same subgraph at each iteration or cycle through a ﬁxed sequence of subgraphs. The
focus of this paper is on analyzing, and developing algorithms based on, arbitrary non-stationary
iterations that use any (non-cyclic) sequence of subgraphs, and the recently developed concept of
walk-sums appears to be critical to this analysis.

1

Given this great ﬂexibility in choosing successive iterative steps, we develop algorithms that adap-
tively optimize the choice of subgraphs to achieve maximum reduction in error. These algorithms
take advantage of walk-sum calculations, which are useful in showing that our methods minimize
an upper-bound on the error at each iteration. We develop two procedures to adaptively choose sub-
graphs. The ﬁrst method ﬁnds the best tree at each iteration by solving an appropriately formulated
maximum-weight spanning tree problem, with the weight of each edge being a function of the par-
tial correlation coefﬁcient of the edge and the residual errors at the nodes that compose the edge.
The second method, building on this ﬁrst method, adds extra edges in a greedy manner to the tree
resulting from the ﬁrst method to form a thin hypertree. Simulation results demonstrate that these
non-stationary algorithms provide a signiﬁcant speedup in convergence over stationary and cyclic
iterative methods. Since the class of walk-summable models is broad (including attractive models,
diagonally dominant models, and so-called pairwise-normalizable models), our methods provide a
convergent, computationally attractive method for inference. We also provide empirical evidence to
show that our adaptive methods (with a minor modiﬁcation) converge in many non-walk-summable
models when stationary iterations diverge. The estimation problem in Gaussian graphical models
involves solving a linear system with a sparse, symmetric, positive-deﬁnite matrix. Such systems
are commonly encountered in other areas of machine learning and signal processing as well [8, 9].
Therefore, our methods are broadly applicable beyond estimation in Gaussian models.

Some of the results presented here appear in more detail in a longer paper [10], which provides
complete proofs as well as a detailed description of walk-sum diagrams that give a graphical inter-
pretation of our algorithms (we show an example in this paper). The report also considers problems
involving communication “failure” between nodes for distributed sensor network applications.

2 Background

Let G = (V,E) be a graph with vertices V , and edges E ⊂(cid:0)V
Here,(cid:0)V

(cid:1) that link pairs of vertices together.
(cid:1) represents the set of all unordered pairs of vertices. Consider a Gaussian distribution in

2

2

information form [5] p(x) ∝ exp{− 1
2 xT Jx + hT x}, where J−1 is the covariance matrix and J−1h
is the mean. The matrix J, also called the information matrix, is sparse according to graph G, i.e.
Js,t = Jt,s = 0 if and only if {s, t} /∈ E. Thus, G represents the graph with respect to which p(x)
is Markov, i.e. p(x) satisﬁes the conditional independencies implied by the separators of G. The
Gaussian mean estimation problem reduces to solving the following linear system of equations:

(1)
where x is the mean vector. Convergent iterations that compute the mean can also be used in turn to
compute variances using a variety of methods [4, 11]. Thus, we focus on the problem of estimating
the mean at each node. Throughout the rest of this paper, we assume that J is normalized to have
1’s along the diagonal.1 Such a re-scaling does not affect the convergence results in this paper, and
our analysis and algorithms can be easily generalized to the un-normalized case [10].

Jx = h,

2.1 Walk-sums
We give a brief overview of the walk-sum framework developed in [5]. Let J = I − R. The off-
diagonal entries of the matrix R have the same sparsity structure as that of J, and consequently that
of the graph G. For Gaussian processes deﬁned on graphs, element Rs,t corresponds to the condi-
tional correlation coefﬁcient between the variables at vertices s and t conditioned on knowledge of
all the other variables (also known as the partial correlation coefﬁcient [5]). A walk is a sequence of
vertices {wi}‘
i=0 such that each step {wi, wi+1} ∈ E, 0 ≤ i ≤ ‘ − 1, with no restriction on crossing
the same vertex or traversing the same edge multiple times. The weight of a walk is the product of the
i=0 Rwi,wi+1.
We then have that (R‘)s,t is the sum of the weights of all length-‘ walks from s to t in G. With this
point of view, we can interpret J−1 as follows:

edge-wise partial correlation coefﬁcients of the edges composing the walk: φ(w) ,Q‘−1

∞X

∞X

(J−1)s,t = ((I − R)−1)s,t =

(R‘)s,t =

φ(s ‘→ t),

(2)

1This can be achieved by performing the transformation ˜J ← D

containing the diagonal entries of J.

− 1

2 JD

− 1

2 , where D is a diagonal matrix

‘=0

‘=0

2

where φ(s ‘→ t) represents the sum of the weights of all the length-‘ walks from s to t (the set
of all such walks is ﬁnite). Thus, (J−1)s,t is the length-ordered sum over all walks in G from s
to t. This, however, is a very speciﬁc way to compute the inverse that converges if the spectral
radius (R) < 1. Other algorithms may compute walks according to different orders (rather than
length-based orders). To analyze arbitrary algorithms that submit to a walk-sum interpretation, the
following concept of walk-summability was developed in [5]. A model is said to be walk-summable
if for each pair of vertices s, t ∈ V , the absolute sum over all walks from s to t in G converges:

|φ(w)| < ∞.

(3)

¯φ(s → t) , X

w∈W(s→t)

Here, W(s → t) represents the set of all walks from s to t, and ¯φ(s → t) denotes the absolute
walk-sum2 over this set. Based on the absolute convergence condition, walk-summability implies
that walk-sums over a countable set of walks in G can be computed in any order. As a result, we
have the following interpretation in walk-summable models:

(J−1)s,t = φ(s → t),

xt = (J−1h)t =X

(4)

hsφ(s → t) , φ(h;∗ → t),

s∈V

(5)
where the wildcard character ∗ denotes a union over all vertices in V , and φ(h;W) denotes a re-
weighting of each walk in W by the corresponding h value at the starting node. Note that in (4) we
relax the constraint that the sum is ordered by length, and do not explicitly specify an ordering on
the walks (such as in (2)). In words, (J−1)s,t is the walk-sum over the set of all walks from s to t,
and xt is the walk-sum over all walks ending at t, re-weighted by h.
As shown in [5], the walk-summability of a model is equivalent to ( ¯R) < 1, where ¯R denotes the
matrix of the absolute values of the elements of R. Also, a broad class of models are walk-summable,
including diagonally-dominant models, so-called pairwise normalizable models, and models for
which the underlying graph G is non-frustrated, i.e. each cycle has an even number of negative
partial correlation coefﬁcients. Walk-summability implies that a model is valid, i.e. has positive-
deﬁnite information/covariance.

Concatenation of walks We brieﬂy describe the concatenation operation for walks and walk-sets,
which plays a key role in walk-sum analysis. Let u = u0 ··· uend and v = vstartv1 ··· v‘(v) be walks
with uend = vstart. The concatenation of these walks is deﬁned to be u· v , u0 ··· uendv1 ··· v‘(v).
Now consider a walk-set U with all walks ending at uend and another walk-set V with all walks
beginning at vstart. If uend = vstart, then the concatenation of U and V is deﬁned:

U ⊗ V , {u · v : u ∈ U, v ∈ V}.

2.2 Embedded Trees algorithm

We describe the Embedded Trees iteration that performs a sequence of updates on trees, or more
generally tractable subgraphs, leading to the solution of (1) on an intractable graph. Each iteration
involves an inference calculation on a subgraph of all the variables V . Let (V,S) be some subgraph
of G, i.e. S ⊂ E (see examples in Figure 1). Let J be split according to S as J = JS − KS, so that
the entries of J corresponding to edges in S are assigned to JS, and those corresponding to E\S are
part of KS. The diagonal entries of J are all part of JS; thus, KS has zeroes along the diagonal.3
JSbx(n) = KSbx(n−1) + h. If JS is invertible, and it is tractable to apply J−1S to a vector, then ET
Based on this splitting, we can transform (1) to JS x = KS x+h, which suggests a natural recursion:
offers an effective method to solve (1) (assuming (J−1S KS) < 1). If the subgraph used changes
with each iteration, then we obtain the following non-stationary ET iteration:

(6)
where {Sn}∞
n=1 is any arbitrary sequence of subgraphs. An important degree of freedom is the
choice of the subgraph Sn at iteration n, which forms the focus of Section 4 of this paper. In [10] we
also consider a more general class of algorithms that update subsets of variables at each iteration.

bx(n) = J−1Sn

(KSnbx(n−1) + h),

2We generally denote the walk-sum of the set W(∼) by φ(∼).
3KS can have non-zero diagonal in general, but we only consider the zero diagonal case here.

3

Figure 1: (Left) G and three embedded trees S1,S2,S3; (Right) Corresponding walk-sum diagram.
3 Walk-Sum Analysis and Convergence of the Embedded Trees algorithm

Sn−→ t)

Sn−→ t)

Sn−→ t),

In this section, we provide a walk-sum interpretation for the ET algorithm. Using this analysis, we
show that the non-stationary ET iteration (6) converges in walk-summable models for an arbitrary
choice of subgraphs {Sn}∞
n=1. Before proceeding with the analysis, we point out that one potential
complication with the ET algorithm is that the matrix JS corresponding to some subgraph S may be
indeﬁnite or singular, even if the original model J is positive-deﬁnite. Importantly, such a problem
never arises in walk-summable models with JS being positive-deﬁnite for any subgraph S if J is
walk-summable. This is easily seen because walks in the subgraph S are a subset of the walks
in G, and thus if absolute walk-sums in G are well-deﬁned, then so are absolute walk-sums in S.
Therefore, JS is walk-summable, and hence, positive-deﬁnite.
Consider the following recursively deﬁned set of walks for s, t ∈ V :
Wn(s → t) =

(cid:20)
= Wn−1(s → ∗) ⊗ W(∗ E\Sn(1)−→ •) ⊗ W(• Sn−→ t) [ W(s

∪u,v∈V Wn−1(s → u) ⊗ W(u

(cid:21) [ W(s

E\Sn(1)−→ v) ⊗ W(v

(7)
with W0(s → t) = ∅. Here, ∗ and • are used as wildcard characters (a union over all elements in V ),
and ⊗ denotes concatenation of walk-sets as described previously. The set Wn−1(s → ∗) denotes
walks that start at node s computed at the previous iteration. The middle term W(∗ E\Sn(1)−→ •)
denotes a length-1 walk (called a hop) across an edge in E\Sn. Finally, W(• Sn−→ t) denotes walks
in Sn that end at node t. Thus, the ﬁrst term in (7) refers to previously computed walks starting at s,
which hop across an edge in E\Sn, and then ﬁnally propagate only in Sn (ending at t). The second
Sn−→ t) denotes walks from s to t that only live within Sn. The following proposition
term W(s
(proved in [10]) shows that the walks contained in these walk-sets are precisely those computed by
the ET algorithm at iteration n. For simplicity, we denote φ(Wn(s → t)) by φn(s → t).
Proposition 1 Let bx(n) be the estimate at iteration n in the ET algorithm (6) with initial guess
bx(0) = 0. Then,bx(n)
can be interpreted as a walk-sum algorithm: bx(n)

We note that the classic Gauss-Jacobi algorithm [6], a stationary iteration with JS = I and KS = R,
in this method computes all walks up to length n
ending at t. Figure 1 gives an example of a walk-sum diagram, which provides a graphical repre-
sentation of the walks accumulated by the walk-sets (7). The diagram is the three-level graph on the
right, and corresponds to an ET iteration based on the subgraphs S1,S2,S3 of the 3 × 3 grid G (on
the left). Each level n in the diagram consists of the subgraph Sn used at iteration n (solid edges),
and information from the previous level (iteration) n − 1 is transmitted through the dashed edges
in E\Sn. The directed nature of these dashed edges is critical as they capture the one-directional
ﬂow of computations from iteration to iteration, while the undirected edges within each level capture
the inference computation at each iteration. Consider a node v at level n of the diagram. Walks in
the diagram that start at any node and end at v at level n, re-weighted by h, are exactly the walks

t = φn(h;∗ → t) =P

s∈V hsφn(s → t) in walk-summable models.

t

computed by the ET algorithm inbx(n)

v . For more examples of such diagrams, see [10].

Given this walk-sum interpretation of the ET algorithm, we can analyze the walk-sets (7) to prove
the convergence of ET in walk-summable models by showing that the walk-sets eventually contain
all the walks required for the computation of J−1h in (5). We have the following convergence
theorem for which we only provide a brief sketch of the complete proof [10].

4

Theroem 1 Letbx(n) be the estimate at iteration n in the ET algorithm (6) with initial guessbx(0) =
0. Then,bx(n) → J−1h element-wise as n → ∞ in walk-summable models.

Proof outline: Proving this statement is done in the following stages.
Validity: The walks in Wn are valid walks in G, i.e. Wn(s → t) ⊆ W(s → t).
Nesting: The walk-sets Wn(s → t) are nested, i.e. Wn−1(s → t) ⊆ Wn(s → t),∀n.
Completeness: Let w ∈ W(s → t). There exists an N > 0 such that w ∈ WN (s → t). Using the
nesting property, we conclude that for all n ≥ N, w ∈ Wn(s → t).
These steps combined together allow us to conclude that φn(s → t) → φ(s → t) as n → ∞. This
conclusion relies on the fact that φ(Wn) → φ(∪nWn) as n → ∞ for a sequence of nested walk-sets
Wn−1 ⊆ Wn in walk-summable models, which is a consequence of the sum-partition theorem for
absolutely summable series [5, 10, 12]. Given the walk-sum interpretation from Proposition 1, one

can check thatbx(n) → J−1h element-wise as n → ∞. (cid:3)
sequence of subgraphs withbx(0) = 0. It is then straightforward to show that convergence can be

Thus, the ET algorithm converges to the correct solution of (1) in walk-summable models for any

achieved for any initial guess [10]. Note that we have taken advantage of the absolute convergence
property in walk-summable models (3) by not focusing on the order in which walks are computed,
but only that they are eventually computed.
In [10], we prove that walk-summability is also a
necessary condition for the complete ﬂexibility in the choice of subgraphs — there exists at least
one sequence of subgraphs that results in a divergent ET iteration in non-walk-summable models.

4 Adaptive algorithms

Let e(n) = x−bx(n) be the error at iteration n and let h(n) = Je(n) = h−Jbx(n) be the corresponding

residual error (which is tractable to compute). We begin by describing an algorithm to choose the
“next-best” tree Sn in the ET iteration (6). The error at iteration n can be re-written as follows:

e(n) = (J−1 − J−1Sn

)h(n−1).

t = φ(h(n−1);∗ G\Sn−→ t), where G\Sn denotes walks
Thus, we have the walk-sum interpretation e(n)
that do not live entirely within Sn. Using this expression for the error, we have the following bound
that is tight for attractive models (Rs,t ≥ 0 for all s, t ∈ V ) and non-negative h(n−1):

ke(n)k‘1 = X

t∈V

|φ(h(n−1);∗ G\Sn−→ t)|

≤ ¯φ(|h(n−1)|;G\Sn)
= ¯φ(|h(n−1)|;G) − ¯φ(|h(n−1)|;Sn).

(8)
Hence, minimizing the error at iteration n corresponds to ﬁnding the tree Sn that maximizes the
second term ¯φ(|h(n−1)|;Sn). This leads us to the following maximum walk-sum tree problem:

arg max

Sn a tree

¯φ(|h(n−1)|;Sn)

Finding the optimal such tree is combinatorially complex. Therefore, we develop a relaxation that
minimizes a looser upper bound than (8). Speciﬁcally, consider an edge {u, v} and all the walks that
live on this single edge W({u, v}) = {uv, vu, uvu, vuv, uvuv, vuvu, . . .}. One can check that the
contribution based on these single-edge walks can be computed as:

¯φ(|h(n−1)|; w) =

| + |h(n−1)

v

|(cid:17) |Ru,v|

1 − |Ru,v| .

σu,v = X

w∈W({u,v})

This weight provides a measure of the error-reduction capacity of edge {u, v} by itself at iteration
n. These single-edge walks for edges in Sn are a subset of all the walks in Sn, and consequently
provide a lower-bound on ¯φ(|h(n−1)|;Sn). Therefore, the maximization

arg max

Sn a tree

σu,v

(11)

(9)

(10)

(cid:16)|h(n−1)

u

X

{u,v}∈Sn

5

Figure 2: Grayscale images of residual errors in an 8 × 8 grid at successive iterations, and corre-
sponding trees chosen by adaptive method.

Figure 3: Grayscale images of residual errors in an 8 × 8 grid at successive iterations, and corre-
sponding hypertrees chosen by adaptive method.

is equivalent to minimizing a looser upper-bound than (8). This relaxed problem can be solved
efﬁciently using a maximum-weight spanning tree algorithm that has complexity O(|E| log log |V |)
for sparse graphs [13].

Given the maximum-weight spanning tree of the graph, a natural extension is to build a thin hyper-
tree by adding extra “strong” edges to the tree, subject to the constraint that the resulting graph has
low treewidth. Unfortunately, to do so optimally is an NP-hard optimization problem [14]. Hence,
we settle on a simple greedy algorithm. For each edge not included in the tree, in order of decreas-
ing edge weight, we add the edge to the graph if two conditions are met: ﬁrst, we are able to easily
verify that the treewidth stays less than M, and second, the length of the unique path in Sn between
the endpoints is less than L. In order to bound the tree width, we maintain a counter at each node
of the total number of added edges that result in a path through that node. Comparing to another
method for constructing junction trees from spanning trees [15], one can check that the maximum
node count is an upper-bound on the treewidth. We note that by using an appropriate directed repre-
sentation of Sn relative to an arbitrary root, it is simple to identify the path between two nodes with
complexity linear in path length (< L).4 Hence, the additional complexity of this greedy algorithm
over that of the tree-selection procedure described previously is O(L|E|).
In Figure 2 and Figure 3 we present a simple demonstration of the tree and hypertree selection
procedures respectively, and the corresponding change in error achieved. The grayscale images
represent the residual errors at the nodes of an 8 × 8 grid similar to G in Figure 1 (with white
representing 1 and black representing 0), and the graphs beside them show the trees/hypertrees
chosen based on these residual errors using the methods described above (the grid edge partial
correlation coefﬁcients are the same for all edges). Notice that the ﬁrst tree in Figure 2 tries to
include as many edges as possible that are incident on the nodes with high residual error. Such
edges are useful for capturing walks ending at the high-error nodes, which contribute to the set of
walks in (5). The ﬁrst hypertree in Figure 3 actually includes all the edges incident on the high-
error nodes. The residual errors after inference on these subgraphs are shown next in Figure 2 and
Figure 3. As expected, the hypertree seems to achieve greater reduction in error compared to the
spanning tree. Again, at this iteration, the subgraphs chosen by our methods adapt based on the
errors at the various nodes.

5 Experimental illustration

5.1 Walk-summable models

We test the adaptive algorithms on densely connected nearest-neighbor grid-structured models (sim-
ilar to G in Figure 1). We generate random grid models — the grid edge partial correlation coef-

4One sets two pointers into the tree starting from any two nodes and then iteratively walks up the tree, always

advancing from the point that is deeper in the tree, until the nearest ancestor of the two nodes is reached.

6

Figure 4: (Left) Average number of iterations required for the normalized residual to reduce by a
factor of 10−6 over 100 randomly generated 75 × 75 grid models; (Center) Convergence plot for a
randomly generated 511×511 grid model; (Right) Convergence range in terms of partial correlation
for 16-node cyclic model with edges to neighbors two steps away.

Figure 5: (Left) 16-node graphical model; (Right) two embedded spanning trees T1, T2.

ﬁcients are chosen uniformly from [−1, 1] and R is scaled so that ( ¯R) = 0.99. The vector h is
chosen to be the all-ones vector. The table on the left in Figure 4 shows the average number of
iterations required by various algorithms to reduce the normalized residual error
by a factor
of 10−6. The average was computed based on 100 randomly generated 75 × 75 grid models. The
plot in Figure 4 shows the decrease in the normalized residual error as a function of the number of
iterations on a randomly generated 511 × 511 grid model. All these models are poorly conditioned
because they are barely walk-summable (( ¯R) = 0.99). The stationary one-tree iteration uses a tree
similar to S1 in Figure 1, and the two-tree iteration alternates between trees similar to S1 and S3 in
Figure 1 [4]. The adaptive hypertree method uses M = 6 and L = 8. We also note that in practice
the per-iteration costs of the adaptive tree and hypertree algorithms are roughly comparable.

kh(n)k2
kh(0)k2

These results show that our adaptive algorithms demonstrate signiﬁcantly superior convergence
properties compared to stationary methods, thus providing a convergent, computationally attrac-
tive method for estimation in walk-summable models. Our methods are applicable beyond Gaussian
estimation to other problems that require solution of linear systems based on sparse, symmetric,
positive-deﬁnite matrices. Several recent papers that develop machine learning algorithms are based
on solving such systems of equations [8, 9]; in fact, both of these papers involve linear systems
based on diagonally-dominant matrices, which are walk-summable.

5.2 Non-walk-summable models

Next, we give empirical evidence that our adaptive methods provide convergence over a broader
range of models than stationary iterations. One potential complication in non-walk-summable mod-
els is that the subgraph models chosen by the stationary and adaptive algorithms may be indeﬁnite
or singular even though J is positive-deﬁnite. In order to avoid this problem in the adaptive ET
algorithm, the trees Sn chosen at each iteration must be valid (i.e., have positive-deﬁnite JSn). A
simple modiﬁcation to the maximum-weight spanning tree algorithm achieves this goal — we add
an extra condition to the algorithm to test for diagonal dominance of the chosen tree model (as
all symmetric, diagonally-dominant models are positive deﬁnite [6]). That is, at each step of the
maximum-weight spanning tree algorithm, we only add an edge if it does not create a cycle and
maintains a diagonally-dominant tractable subgraph model. Consider the 16-node model on the left
in Figure 5. Let all the edge partial correlation coefﬁcients be r. The range of r for which J is
positive-deﬁnite is roughly (−0.46, 0.25), and the range for which the model is walk-summable is
(−0.25, 0.25) (in this range all the algorithms, both stationary and adaptive, converge). For the one-
tree iteration we use tree T1, and for the two-tree iteration we alternate between trees T1 and T2 (see

7

Figure 5). As the table on the right in Figure 4 demonstrates, the adaptive tree algorithm without the
diagonal-dominance (DD) check provides convergence over a much broader range of models than
the one-tree and two-tree iterations, but not for all valid models. However, the modiﬁed adaptive
tree algorithm with the DD check appears to converge almost up to the validity threshold. We have
also observed such behavior empirically in many other (though not all) non-walk-summable models
where the adaptive ET algorithm with the DD condition converges while stationary methods diverge.
Thus, our adaptive methods, compared to stationary iterations, not only provide faster convergence
rates in walk-summable models but also converge for a broader class of models.

6 Discussion

We analyze non-stationary iterations of the ET algorithm that use any sequence of subgraphs for
estimation in Gaussian graphical models. Our analysis is based on the recently developed walk-sum
interpretation of inference in Gaussian models, and we show that the ET algorithm converges for
any sequence of subgraphs used in walk-summable models. These convergence results motivate
the development of methods to choose subgraphs adaptively at each iteration to achieve maximum
reduction in error. The adaptive procedures are based on walk-sum calculations, and minimize an
upper bound on the error at each iteration. Our simulation results show that the adaptive algorithms
provide a signiﬁcant speedup in convergence over stationary methods. Moreover, these adaptive
methods also seem to provide convergence over a broader class of models than stationary algorithms.

Our adaptive algorithms are greedy in that they only choose the “next-best” subgraph. An interest-
ing question is to develop tractable methods to compute the next K best subgraphs jointly to achieve
maximum reduction in error after K iterations. The experiment with non-walk-summable models
suggests that walk-sum analysis could be useful to provide convergent algorithms for non-walk-
summable models, perhaps with restrictions on the order in which walk-sums are computed. Fi-
nally, subgraph preconditioners have been shown to improve the convergence rate of the conjugate-
gradient method; using walk-sum analysis to select such preconditioners is of clear interest.

References
[1] M. Luettgen, W. Carl, and A. Willsky. Efﬁcient multiscale regularization with application to optical ﬂow.

IEEE Transactions on Image Processing, 3(1):41–64, Jan. 1994.

[2] P. Rusmevichientong and B. Van Roy. An Analysis of Turbo Decoding with Gaussian densities.

Advances in Neural Information Processing Systems 12, 2000.

In

[3] J. Pearl. Probabilistic Reasoning in Intelligent Systems. Morgan Kauffman, San Mateo, CA, 1988.
[4] E. Sudderth, M. Wainwright, and A. Willsky. Embedded Trees: Estimation of Gaussian processes on

graphs with cycles. IEEE Transactions on Signal Processing, 52(11):3136–3150, Nov. 2004.

[5] D. Malioutov, J. Johnson, and A. Willsky. Walk-Sums and Belief Propagation in Gaussian Graphical

Models. Journal of Machine Learning Research, 7:2031–2064, Oct. 2006.

[6] R. Varga. Matrix Iterative Analysis. Springer-Verlag, New York, 2000.
[7] R. Bru, F. Pedroche, and D. Szyld. Overlapping Additive and Multiplicative Schwarz iterations for H-

matrices. Linear Algebra and its Applications, 393:91–105, Dec. 2004.

[8] D. Zhou, J. Huang, and B. Scholkopf. Learning from Labeled and Unlabeled data on a directed graph. In

Proceedings of the 22nd International Conference on Machine Learning, 2005.

[9] D. Zhou and C. Burges. Spectral Clustering and Transductive Learning with multiple views. In Proceed-

ings of the 24th International Conference on Machine Learning, 2007.

[10] V. Chandrasekaran, J. Johnson, and A. Willsky. Estimation in Gaussian Graphical Models using Tractable

Subgraphs: A Walk-Sum Analysis. To appear in IEEE Transactions on Signal Processing.

[11] D. Malioutov, J. Johnson, and A. Willsky. GMRF variance approximation using spliced wavelet bases. In

IEEE International Conference on Acoustics, Speech and Signal Processing, 2007.

[12] R. Godement. Analysis I: Convergence, Elementary Functions. Springer-Verlag, New York, 2004.
[13] T. Cormen, C. Leiserson, R. Rivest, and C. Stein. Introduction to Algorithms. MIT Press, 2001.
[14] N. Srebro. Maximum Likelihood Markov Networks: An Algorithmic Approach. Master’s thesis, Mas-

sachusetts Institute of Technology, 2000.

[15] F. Kschischang, B. Frey, and H. Loeliger. Factor graphs and the sum-product algorithm. IEEE Transac-

tions on Information Theory, 47(2):498–519, Feb. 2001.

8

"
110,2007,People Tracking with the Laplacian Eigenmaps Latent Variable Model,"Reliably recovering 3D human pose from monocular video requires constraints that bias the estimates towards typical human poses and motions. We define priors for people tracking using a Laplacian Eigenmaps Latent Variable Model (LELVM). LELVM is a probabilistic dimensionality reduction model that naturally combines the advantages of latent variable models---definining a multimodal probability density for latent and observed variables, and globally differentiable nonlinear mappings for reconstruction and dimensionality reduction---with those of spectral manifold learning methods---no local optima, ability to unfold highly nonlinear manifolds, and good practical scaling to latent spaces of high dimension. LELVM is computationally efficient, simple to learn from sparse training data, and compatible with standard probabilistic trackers such as particle filters. We analyze the performance of a LELVM-based probabilistic sigma point mixture tracker in several real and synthetic human motion sequences and demonstrate that LELVM provides sufficient constraints for robust operation in the presence of missing, noisy and ambiguous image measurements.","People Tracking with the Laplacian Eigenmaps

Latent Variable Model

Zhengdong Lu

CSEE, OGI, OHSU
zhengdon@csee.ogi.edu

Miguel ´A. Carreira-Perpi˜n´an

EECS, UC Merced

Cristian Sminchisescu

University of Bonn

http://eecs.ucmerced.edu

sminchisescu.ins.uni-bonn.de

Abstract

Reliably recovering 3D human pose from monocular video requires models that
bias the estimates towards typical human poses and motions. We construct pri-
ors for people tracking using the Laplacian Eigenmaps Latent Variable Model
(LELVM). LELVM is a recently introduced probabilistic dimensionality reduc-
tion model that combines the advantages of latent variable models—a multimodal
probability density for latent and observed variables, and globally differentiable
nonlinear mappings for reconstruction and dimensionality reduction—with those
of spectral manifold learning methods—no local optima, ability to unfold highly
nonlinear manifolds, and good practical scaling to latent spaces of high dimen-
sion. LELVM is computationally efﬁcient, simple to learn from sparse training
data, and compatible with standard probabilistic trackers such as particle ﬁlters.
We analyze the performance of a LELVM-based probabilistic sigma point mixture
tracker in several real and synthetic human motion sequences and demonstrate that
LELVM not only provides sufﬁcient constraints for robust operation in the pres-
ence of missing, noisy and ambiguous image measurements, but also compares
favorably with alternative trackers based on PCA or GPLVM priors.

Recent research in reconstructing articulated human motion has focused on methods that can exploit
available prior knowledge on typical human poses or motions in an attempt to build more reliable
algorithms. The high-dimensionality of human ambient pose space—between 30-60 joint angles
or joint positions depending on the desired accuracy level, makes exhaustive search prohibitively
expensive. This has negative impact on existing trackers, which are often not sufﬁciently reliable at
reconstructing human-like poses, self-initializing or recovering from failure. Such difﬁculties have
stimulated research in algorithms and models that reduce the effective working space, either us-
ing generic search focusing methods (annealing, state space decomposition, covariance scaling) or
by exploiting speciﬁc problem structure (e.g. kinematic jumps). Experience with these procedures
has nevertheless shown that any search strategy, no matter how effective, can be made signiﬁcantly
more reliable if restricted to low-dimensional state spaces. This permits a more thorough explo-
ration of the typical solution space, for a given, comparatively similar computational effort as a
high-dimensional method. The argument correlates well with the belief that the human pose space,
although high-dimensional in its natural ambient parameterization, has a signiﬁcantly lower percep-
tual (latent or intrinsic) dimensionality, at least in a practical sense—many poses that are possible
are so improbable in many real-world situations that it pays off to encode them with low accuracy.

A perceptual representation has to be powerful enough to capture the diversity of human poses in a
sufﬁciently broad domain of applicability (the task domain), yet compact and analytically tractable
for search and optimization. This justiﬁes the use of models that are nonlinear and low-dimensional
(able to unfold highly nonlinear manifolds with low distortion), yet probabilistically motivated and
globally continuous for efﬁcient optimization. Reducing dimensionality is not the only goal: per-
ceptual representations have to preserve critical properties of the ambient space. Reliable tracking
needs locality: nearby regions in ambient space have to be mapped to nearby regions in latent space.
If this does not hold, the tracker is forced to make unrealistically large, and difﬁcult to predict jumps
in latent space in order to follow smooth trajectories in the joint angle ambient space.

1

In this paper we propose to model priors for articulated motion using a recently introduced proba-
bilistic dimensionality reduction method, the Laplacian Eigenmaps Latent Variable Model (LELVM)
[1]. Section 1 discusses the requirements of priors for articulated motion in the context of proba-
bilistic and spectral methods for manifold learning, and section 2 describes LELVM and shows how
it combines both types of methods in a principled way. Section 3 describes our tracking frame-
work (using a particle ﬁlter) and section 4 shows experiments with synthetic and real human motion
sequences using LELVM priors learned from motion-capture data.
Related work: There is signiﬁcant work in human tracking, using both generative and discrimina-
tive methods. Due to space limitations, we will focus on the more restricted class of 3D generative
algorithms based on learned state priors, and not aim at a full literature review. Deriving com-
pact prior representations for tracking people or other articulated objects is an active research ﬁeld,
steadily growing with the increased availability of human motion capture data. Howe et al. and
Sidenbladh et al. [2] propose Gaussian mixture representations of short human motion fragments
(snippets) and integrate them in a Bayesian MAP estimation framework that uses 2D human joint
measurements, independently tracked by scaled prismatic models [3]. Brand [4] models the human
pose manifold using a Gaussian mixture and uses an HMM to infer the mixture component index
based on a temporal sequence of human silhouettes. Sidenbladh et al. [5] use similar dynamic priors
and exploit ideas in texture synthesis—efﬁcient nearest-neighbor search for similar motion frag-
ments at runtime—in order to build a particle-ﬁlter tracker with observation model based on contour
and image intensity measurements. Sminchisescu and Jepson [6] propose a low-dimensional proba-
bilistic model based on ﬁtting a parametric reconstruction mapping (sparse radial basis function) and
a parametric latent density (Gaussian mixture) to the embedding produced with a spectral method.
They track humans walking and involved in conversations using a Bayesian multiple hypotheses
framework that fuses contour and intensity measurements. Urtasun et al. [7] use a dynamic MAP
estimation framework based on a GPLVM and 2D human joint correspondences obtained from an
independent image-based tracker. Li et al. [8] use a coordinated mixture of factor analyzers within a
particle ﬁltering framework, in order to reconstruct human motion in multiple views using chamfer
matching to score different conﬁguration. Wang et al. [9] learn a latent space with associated dy-
namics where both the dynamics and observation mapping are Gaussian processes, and Urtasun et
al. [10] use it for tracking. Taylor et al. [11] also learn a binary latent space with dynamics (using
an energy-based model) but apply it to synthesis, not tracking. Our work learns a static, generative
low-dimensional model of poses and integrates it into a particle ﬁlter for tracking. We show its
ability to work with real or partially missing data and to track multiple activities.

1 Priors for articulated human pose

We consider the problem of learning a probabilistic low-dimensional model of human articulated
motion. Call y ∈ RD the representation in ambient space of the articulated pose of a person. In this
paper, y contains the 3D locations of anywhere between 10 and 60 markers located on the person’s
joints (other representations such as joint angles are also possible). The values of y have been
normalised for translation and rotation in order to remove rigid motion and leave only the articulated
motion (see section 3 for how we track the rigid motion). While y is high-dimensional, the motion
pattern lives in a low-dimensional manifold because most values of y yield poses that violate body
constraints or are simply atypical for the motion type considered. Thus we want to model y in terms
of a small number of latent variables x given a collection of poses {yn}N
n=1 (recorded from a human
with motion-capture technology). The model should satisfy the following: (1) It should deﬁne a
probability density for x and y, to be able to deal with noise (in the image or marker measurements)
and uncertainty (from missing data due to occlusion or markers that drop), and to allow integration
in a sequential Bayesian estimation framework. The density model should also be ﬂexible enough
to represent multimodal densities. (2) It should deﬁne mappings for dimensionality reduction F :
y → x and reconstruction f : x → y that apply to any value of x and y (not just those in the
training set); and such mappings should be deﬁned on a global coordinate system, be continuous
(to avoid physically impossible discontinuities) and differentiable (to allow efﬁcient optimisation
when tracking), yet ﬂexible enough to represent the highly nonlinear manifold of articulated poses.
From a statistical machine learning point of view, this is precisely what latent variable models
(LVMs) do; for example, factor analysis deﬁnes linear mappings and Gaussian densities, while the
generative topographic mapping (GTM; [12]) deﬁnes nonlinear mappings and a Gaussian-mixture
density in ambient space. However, factor analysis is too limited to be of practical use, and GTM—

2

while ﬂexible—has two important practical problems: (1) the latent space must be discretised to
allow tractable learning and inference, which limits it to very low (2–3) latent dimensions; (2) the
parameter estimation is prone to bad local optima that result in highly distorted mappings.

Another dimensionality reduction method recently introduced, GPLVM [13], which uses a Gauss-
ian process mapping f (x), partly improves this situation by deﬁning a tunable parameter xn for
each data point yn. While still prone to local optima, this allows the use of a better initialisation
n=1 (obtained from a spectral method, see later). This has prompted the application of
for {xn}N
GPLVM for tracking human motion [7]. However, GPLVM has some disadvantages: its training is
very costly (each step of the gradient iteration is cubic on the number of training points N, though
approximations based on using few points exist); unlike true LVMs, it deﬁnes neither a posterior
distribution p(x|y) in latent space nor a dimensionality reduction mapping E {x|y}; and the latent
representation it obtains is not ideal. For example, for periodic motions such as running or walking,
repeated periods (identical up to small noise) can be mapped apart from each other in latent space
because nothing constrains xn and xm to be close even when yn = ym (see ﬁg. 3 and [10]).
There exists a different type of dimensionality reduction methods, spectral methods (such as Isomap,
LLE or Laplacian eigenmaps [14]), that have advantages and disadvantages complementary to those
of LVMs. They deﬁne neither mappings nor densities but just a correspondence (xn, yn) between
points in latent space xn and ambient space yn. However, the training is efﬁcient (a sparse eigen-
value problem) and has no local optima, and often yields a correspondence that successfully models
highly nonlinear, convoluted manifolds such as the Swiss roll. While these attractive properties have
spurred recent research in spectral methods, their lack of mappings and densities has limited their
applicability in people tracking. However, a new model that combines the advantages of LVMs and
spectral methods in a principled way has been recently proposed [1], which we brieﬂy describe next.

2 The Laplacian Eigenmaps Latent Variable Model (LELVM)

min tr(cid:0)XLX⊤(cid:1)

LELVM is based on a natural way of deﬁning an out-of-sample mapping for Laplacian eigenmaps
(LE) which, in addition, results in a density model. In LE, typically we ﬁrst deﬁne a k-nearest-
n=1 and weigh each edge yn ∼ ym by a Gaussian afﬁnity
neighbour graph on the sample data {yn}N
2 k(yn − ym)/σk2). Then the latent points X result from:
function K(yn, ym) = wnm = exp (− 1
(1)
where we deﬁne the matrix XL×N = (x1, . . . , xN ), the symmetric afﬁnity matrix WN ×N , the de-
gree matrix D = diag (PN
n=1 wnm), the graph Laplacian matrix L = D−W, and 1 = (1, . . . , 1)⊤.
The constraints eliminate the two trivial solutions X = 0 (by ﬁxing an arbitrary scale) and
x1 = · · · = xN (by removing 1, which is an eigenvector of L associated with a zero eigenvalue).
The solution is given by the leading u2, . . . , uL+1 eigenvectors of the normalised afﬁnity matrix
N = D− 1
2 with VN ×L = (v2, . . . , vL+1) (an a posteriori trans-
lated, rotated or uniformly scaled X is equally valid).

s.t. X ∈ RL×N , XDX⊤ = I, XD1 = 0

2 , namely X = V⊤D− 1

2 WD− 1

Following [1], we now deﬁne an out-of-sample mapping F(y) = x for a new point y as a semi-
supervised learning problem, by recomputing the embedding as in (1) (i.e., augmenting the graph
Laplacian with the new point), but keeping the old embedding ﬁxed:
K(y)⊤ 1⊤K(y)(cid:17)(cid:16) X⊤

(2)
2 k(y − yn)/σk2) for n = 1, . . . , N is the kernel induced by
where Kn(y) = K(y, yn) = exp (− 1
the Gaussian afﬁnity (applied only to the k nearest neighbours of y, i.e., Kn(y) = 0 if y ≁ yn).
This is one natural way of adding a new point to the embedding by keeping existing embedded
points ﬁxed. We need not use the constraints from (1) because they would trivially determine x, and
the uninteresting solutions X = 0 and X = constant were already removed in the old embedding
anyway. The solution yields an out-of-sample dimensionality reduction mapping x = F(y):

tr(cid:16)( X x )(cid:16) L

x⊤ (cid:17)(cid:17)

min
x∈RL

K(y)

x = F(y) = X K(y)

1⊤K(y) = PN

n=1

K(y,yn)

PN

n′=1 K(y,yn′ )

xn

(3)

N PN

3

applicable to any point y (new or old). This mapping is formally identical to a Nadaraya-Watson
estimator (kernel regression; [15]) using as data {(xn, yn)}N
n=1 and the kernel K. We can take this
a step further by deﬁning a LVM that has as joint distribution a kernel density estimate (KDE):
p(x, y) = 1

n=1 Ky(y, yn)Kx(x, xn) p(y) = 1

n=1 Ky(y, yn) p(x) = 1

N PN

N PN

n=1 Kx(x, xn)

where Ky is proportional to K so it integrates to 1, and Kx is a pdf kernel in x–space. Consequently,
the marginals in observed and latent space are also KDEs, and the dimensionality reduction and
reconstruction mappings are given by kernel regression (the conditional means E {y|x}, E {x|y}):

F(y) = PN
allow the

yn = PN
n=1 p(n|y)xn
bandwidths
ambient
and
latent
2 k(y − yn)/σyk2).
2 k(x − xn)/σxk2) and Ky(y, yn) ∝ exp (− 1

We
Kx(x, xn) ∝ exp (− 1
may be tuned to control the smoothness of the mappings and densities [1].

f (x) = PN
be

different

Kx(x,xn)

n′=1 Kx(x,xn′ )
in

the

n=1

PN

to

n=1 p(n|x)yn.

(4)

spaces:
They

Thus, LELVM naturally extends a LE embedding (efﬁciently obtained as a sparse eigenvalue prob-
lem with a cost O(N 2)) to global, continuous, differentiable mappings (NW estimators) and po-
tentially multimodal densities having the form of a Gaussian KDE. This allows easy computation
of posterior probabilities such as p(x|y) (unlike GPLVM). It can use a continuous latent space of
arbitrary dimension L (unlike GTM) by simply choosing L eigenvectors in the LE embedding. It
has no local optima since it is based on the LE embedding. LELVM can learn convoluted mappings
(e.g. the Swiss roll) and deﬁne maps and densities for them [1]. The only parameters to set are the
graph parameters (number of neighbours k, afﬁnity width σ) and the smoothing bandwidths σx, σy.

3 Tracking framework

We follow the sequential Bayesian estimation framework, where for state variables s and observation
variables z we have the recursive prediction and correction equations:

p(st|z0:t−1) = R p(st|st−1) p(st−1|z0:t−1) dst−1

p(st|z0:t) ∝ p(zt|st) p(st|z0:t−1).

(5)

We deﬁne the state variables as s = (x, d) where x ∈ RL is the low-dim. latent space (for pose)
and d ∈ R3 is the centre-of-mass location of the body (in the experiments our state also includes
the orientation of the body, but for simplicity here we describe only the translation). The observed
variables z consist of image features or the perspective projection of the markers on the camera
plane. The mapping from state to observations is (for the markers’ case, assuming M markers):

x ∈ RL
d ∈ R3

f−−−−→ y ∈ R3M −−→ ⊕ P−−−−−→ z ∈ R2M

(6)

where f is the LELVM reconstruction mapping (learnt from mocap data); ⊕ shifts each 3D marker
by d; and P is the perspective projection (pinhole camera), applied to each 3D point separately. Here
we use a simple observation model p(zt|st): Gaussian with mean given by the transformation (6)
and isotropic covariance (set by the user to control the inﬂuence of measurements in the tracking).
We assume known correspondences and observations that are obtained either from the 3D markers
(for tracking synthetic data) or 2D tracks obtained from a 2D tracker. Our dynamics model is

p(st|st−1) ∝ pd(dt|dt−1) px(xt|xt−1) p(xt)

(7)
where both dynamics models for d and x are random walks: Gaussians centred at the previous
step value dt−1 and xt−1, respectively, with isotropic covariance (set by the user to control the
inﬂuence of dynamics in the tracking); and p(xt) is the LELVM prior. Thus the overall dynamics
predicts states that are both near the previous state and yield feasible poses. Of course, more complex
dynamics models could be used if e.g. the speed and direction of movement are known.

As tracker we use the Gaussian mixture Sigma-point particle ﬁlter (GMSPPF) [16]. This is a par-
ticle ﬁlter that uses a Gaussian mixture representation for the posterior distribution in state space
and updates it with a Sigma-point Kalman ﬁlter. This Gaussian mixture will be used as proposal
distribution to draw the particles. As in other particle ﬁlter implementations, the prediction step
is carried out by approximating the integral (5) with particles and updating the particles’ weights.
Then, a new Gaussian mixture is ﬁtted with a weighted EM algorithm to these particles. This re-
places the resampling stage needed by many particle ﬁlters and mitigates the problem of sample
depletion while also preventing the number of components in the Gaussian mixture from growing
over time. The choice of this particular tracker is not critical; we use it to illustrate the fact that
LELVM can be introduced in any probabilistic tracker for nonlinear, nongaussian models. Given the
corrected distribution p(st|z0:t), we choose its mean as recovered state (pose and location). It is also
possible to choose instead the mode closest to the state at t − 1, which could be found by mean-shift
or Newton algorithms [17] since we are using a Gaussian-mixture representation in state space.

4

4 Experiments

We demonstrate our low-dimensional tracker on image sequences of people walking and running,
both synthetic (ﬁg. 1) and real (ﬁg. 2–3). Fig. 1 shows the model copes well with persistent partial
occlusion and severely subsampled training data (A,B), and quantitatively evaluates temporal recon-
struction (C). For all our experiments, the LELVM parameters (number of neighbors k, Gaussian
afﬁnity σ, and bandwidths σx and σy) were set manually. We mainly considered 2D latent spaces
(for pose, plus 6D for rigid motion), which were expressive enough for our experiments. More
complex, higher-dimensional models are straightforward to construct. The initial state distribution
p(s0) was chosen a broad Gaussian, the dynamics and observation covariance were set manually to
control the tracking smoothness, and the GMSPPF tracker used a 5-component Gaussian mixture
in latent space (and in the state space of rigid motion) and a small set of 500 particles. The 3D
representation we use is a 102-D vector obtained by concatenating the 3D markers coordinates of all
the body joints. These would be highly unconstrained if estimated independently, but we only use
them as intermediate representation; tracking actually occurs in the latent space, tightly controlled
using the LELVM prior. For the synthetic experiments and some of the real experiments (ﬁgs. 2–3)
the camera parameters and the body proportions were known (for the latter, we used the 2D outputs
of [6]). For the CMU mocap video (ﬁg. 2B) we roughly guessed. We used mocap data from several
sources (CMU, OSU). As observations we always use 2D marker positions, which, depending on
the analyzed sequence were either known (the synthetic case), or provided by an existing tracker
[6] or speciﬁed manually (ﬁg. 2B). Alternatively 2D point trackers similar to the ones of [7] can be
used. The forward generative model is obtained by combining the latent to ambient space mapping
(this provides the position of the 3D markers) with a perspective projection transformation. The
observation model is a product of Gaussians, each measuring the probability of a particular marker
position given its corresponding image point track.
Experiments with synthetic data: we analyze the performance of our tracker in controlled condi-
tions (noise perturbed synthetically generated image tracks) both under regular circumstances (rea-
sonable sampling of training data) and more severe conditions with subsampled training points and
persistent partial occlusion (the man running behind a fence, with many of the 2D marker tracks
obstructed). Fig. 1B,C shows both the posterior (ﬁltered) latent space distribution obtained from
our tracker, and its mean (we do not show the distribution of the global rigid body motion; in all
experiments this is tracked with good accuracy). In the latent space plot shown in ﬁg. 1B, the onset
of running (two cycles were used) appears as a separate region external to the main loop. It does not
appear in the subsampled training set in ﬁg. 1B, where only one running cycle was used for training
and the onset of running was removed. In each case, one can see that the model is able to track quite
competently, with a modest decrease in its temporal accuracy, shown in ﬁg. 1C, where the averages
are computed per 3D joint (normalised wrt body height). Subsampling causes some ambiguity in
the estimate, e.g. see the bimodality in the right plot in ﬁg. 1C. In another set of experiments (not
shown) we also tracked using different subsets of 3D markers. The estimates were accurate even
when about 30% of the markers were dropped.
Experiments with real images: this shows our tracker’s ability to work with real motions of differ-
ent people, with different body proportions, not in its latent variable model training set (ﬁgs. 2–3).
We study walking, running and turns. In all cases, tracking and 3D reconstruction are reasonably ac-
curate. We have also run comparisons against low-dimensional models based on PCA and GPLVM
(ﬁg. 3). It is important to note that, for LELVM, errors in the pose estimates are primarily caused
by mismatches between the mocap data used to learn the LELVM prior and the body proportions of
the person in the video. For example, the body proportions of the OSU motion captured walker are
quite different from those of the image in ﬁg. 2–3 (e.g. note how the legs of the stick man are shorter
relative to the trunk). Likewise, the style of the runner from the OSU data (e.g. the swinging of the
arms) is quite different from that of the video. Finally, the interest points tracked by the 2D tracker
do not entirely correspond either in number or location to the motion capture markers, and are noisy
and sometimes missing. In future work, we plan to include an optimization step to also estimate the
body proportions. This would be complicated for a general, unconstrained model because the di-
mensions of the body couple with the pose, so either one or the other can be changed to improve the
tracking error (the observation likelihood can also become singular). But for dedicated prior pose
models like ours these difﬁculties should be signiﬁcantly reduced. The model simply cannot assume
highly unlikely stances—these are either not representable at all, or have reduced probability—and
thus avoids compensatory, unrealistic body proportion estimates.

5

B

0.4

0.3

0.2

0.1

0

−0.1

−0.2

−0.3

−0.4

0.4

1.5

1

0.5

0

−0.5

−1

−1.5

−2

0.4

0.3

0.2

0.1

0

−0.1

−0.2

−0.3

−0.4

0.4

1.5

1

0.5

0

−0.5

−1

−1.5

−2

n = 15

n = 40

n = 65

n = 90

n = 115

n = 140

A

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1

0.5

0

−0.5

1

1

0.5

0.5

0

−0.5

0

−0.5

1

0.5

0

−0.5

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1

0.5

0

−0.5

1

1

0.5

0.5

0

−0.5

0

−0.5

1

0.5

0

−0.5

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1

0.5

0

−0.5

1

1

0.5

0.5

0

−0.5

0

−0.5

1

0.5

0

−0.5

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1

0.5

0

−0.5

1

1

0.5

0.5

0

−0.5

0

−0.5

1

0.5

0

−0.5

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1

0.5

0

−0.5

1

1

0.5

0.5

0

−0.5

0

−0.5

1

0.5

0

−0.5

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1

0.5

0

−0.5

1

1

0.5

0.5

0

−0.5

0

−0.5

1

0.5

0

−0.5

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

−1

−1

−1

−1

−1

−1

−1

−1

−1

−1

−1

−1

−1

−1

−1

−1

−1

−1

−1

−1

−1

−1

−1

−1

−1.5

−1.5

−1.5

−1.5

−1.5

−1

−0.5

0

0.5

1

−1.5

−1

−0.5

0

0.5

1

−1.5

−1

−0.5

0

0.5

1

−1.5

−1

−0.5

0

0.5

1

−1.5

−1.5

−1.5

−1.5

−1.5

−1

−0.5

0

0.5

1

−1.5

−1

−0.5

0

0.5

1

−1.5

−1

−0.5

0

0.5

1

−1.5

−1

−0.5

0

0.5

1

−1.5

−1.5

−1.5

−1.5

−1.5

−1

−0.5

0

0.5

1

−1.5

−1

−0.5

0

0.5

1

−1.5

−1

−0.5

0

0.5

1

−1.5

−1

−0.5

0

0.5

1

−1.5

−1.5

−1.5

−1.5

−1.5

−1

−0.5

0

0.5

1

−1.5

−1

−0.5

0

0.5

1

−1.5

−1

−0.5

0

0.5

1

−1.5

−1

−0.5

0

0.5

1

−1.5

−1.5

−1.5

−1.5

−1.5

−1

−0.5

0

0.5

1

−1.5

−1

−0.5

0

0.5

1

−1.5

−1

−0.5

0

0.5

1

−1.5

−1

−0.5

0

0.5

1

−1.5

−1.5

−1.5

−1.5

−1.5

−1

−0.5

0

0.5

1

−1.5

−1

−0.5

0

0.5

1

−1.5

−1

−0.5

0

0.5

1

−1.5

−1

−0.5

0

0.5

1

n = 1

n = 13

n = 25

n = 37

n = 49

n = 60

0.4

0.3

0.2

0.1

0

−0.1

−0.2

−0.3

−0.4

0.4

1.5

1

0.5

0

−0.5

−1

−1.5

−2

0.4

0.3

0.2

0.1

0

−0.1

−0.2

−0.3

−0.4

0.4

1.5

1

0.5

0

−0.5

−1

−1.5

−2

0.4

0.3

0.2

0.1

0

−0.1

−0.2

−0.3

−0.4

0.4

1.5

1

0.5

0

−0.5

−1

−1.5

−2

0.4

0.3

0.2

0.1

0

−0.1

−0.2

−0.3

−0.4

0.4

1.5

1

0.5

0

−0.5

−1

−1.5

−2

0.6

0.8

1

1.2

1.4

1.6

1.8

2

2.2

2.4

0.6

0.8

1

1.2

1.4

1.6

1.8

2

2.2

2.4

0.6

0.8

1

1.2

1.4

1.6

1.8

2

2.2

2.4

0.6

0.8

1

1.2

1.4

1.6

1.8

2

2.2

2.4

0.6

0.8

1

1.2

1.4

1.6

1.8

2

2.2

2.4

0.6

0.8

1

1.2

1.4

1.6

1.8

2

2.2

2.4

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1

0.5

0

−0.5

1

1

0.5

0.5

0

−0.5

0

−0.5

1

0.5

0

−0.5

1

0.5

0

−0.5

1

1

0.5

0.5

0

−0.5

0

−0.5

1

0.5

0

−0.5

1

0.5

0

−0.5

1

1

0.5

0.5

0

−0.5

0

−0.5

1

0.5

0

−0.5

1

0.5

0

−0.5

1

1

0.5

0.5

0

−0.5

0

−0.5

1

0.5

0

−0.5

1

0.5

0

−0.5

1

1

0.5

0.5

0

−0.5

0

−0.5

1

0.5

0

−0.5

1

0.5

0

−0.5

1

1

0.5

0.5

0

−0.5

0

−0.5

1

0.5

0

−0.5

−1

−1

−1

−1

−1

−1

−1

−1

−1

−1

−1

−1

−1

−1

−1

−1

−1

−1

−1

−1

−1

−1

−1

−1

−1.5

−1.5

−1.5

−1.5

−1.5

−1

−0.5

0

0.5

1

−1.5

−1

−0.5

0

0.5

1

−1.5

−1

−0.5

0

0.5

1

−1.5

−1

−0.5

0

0.5

1

−1.5

−1.5

−1.5

−1.5

−1.5

−1

−0.5

0

0.5

1

−1.5

−1

−0.5

0

0.5

1

−1.5

−1

−0.5

0

0.5

1

−1.5

−1

−0.5

0

0.5

1

−1.5

−1.5

−1.5

−1.5

−1.5

−1

−0.5

0

0.5

1

−1.5

−1

−0.5

0

0.5

1

−1.5

−1

−0.5

0

0.5

1

−1.5

−1

−0.5

0

0.5

1

−1.5

−1.5

−1.5

−1.5

−1.5

−1

−0.5

0

0.5

1

−1.5

−1

−0.5

0

0.5

1

−1.5

−1

−0.5

0

0.5

1

−1.5

−1

−0.5

0

0.5

1

−1.5

−1.5

−1.5

−1.5

−1.5

−1

−0.5

0

0.5

1

−1.5

−1

−0.5

0

0.5

1

−1.5

−1

−0.5

0

0.5

1

−1.5

−1

−0.5

0

0.5

1

−1.5

−1.5

−1.5

−1.5

−1.5

−1

−0.5

0

0.5

1

−1.5

−1

−0.5

0

0.5

1

−1.5

−1

−0.5

0

0.5

1

−1.5

−1

−0.5

0

0.5

1

C

E
S
M
R

0.1

0.08

0.06

0.04

0.02

0

0

0.13

0.12

0.11

0.1

0.09

0.08

0.07

0.06

E
S
M
R

150

0.05

0

10

50

100

time step n

20

30

40

50

60

time step n

OSU running man motion capture data. A: we use 217 datapoints for training LELVM
Figure 1:
(with added noise) and for tracking. Row 1: tracking in the 2D latent space. The contours (very tight
in this sequence) are the posterior probability. Row 2: perspective-projection-based observations
with occlusions. Row 3: each quadruplet (a, a′, b, b′) show the true pose of the running man from
a front and side views (a, b), and the reconstructed pose by tracking with our model (a′, b′). B: we
use the ﬁrst running cycle for training LELVM and the second cycle for tracking. C: RMSE errors
for each frame, for the tracking of A (left plot) and B (middle plot), normalised so that 1 equals the
j=1 kynj − ˆynjk2(cid:1)−1/2 for all 3D locations of the M
height of the stick man. RMSE(n) = (cid:0) 1
markers, i.e., comparison of reconstructed stick man ˆyn with ground-truth stick man yn. Right plot:
multimodal posterior distribution in pose space for the model of A (frame 42).

M PM

Comparison with PCA and GPLVM (ﬁg. 3): for these models, the tracker uses the same GMSPPF
setting as for LELVM (number of particles, initialisation, random-walk dynamics, etc.) but with the
mapping y = f (x) provided by GPLVM or PCA, and with a uniform prior p(x) in latent space
(since neither GPLVM nor the non-probabilistic PCA provide one). The LELVM-tracker uses both
its f (x) and latent space prior p(x), as discussed. All methods use a 2D latent space. We ensured
the best possible training of GPLVM by model selection based on multiple runs. For PCA, the
latent space looks deceptively good, showing non-intersecting loops. However, (1) individual loops
do not collect together as they should (for LELVM they do); (2) worse still, the mapping from 2D
to pose space yields a poor observation model. The reason is that the loop in 102-D pose space
is nonlinearly bent and a plane can at best intersect it at a few points, so the tracker often stays
put at one of those (typically an “average” standing position), since leaving it would increase the
error a lot. Using more latent dimensions would improve this, but as LELVM shows, this is not
necessary. For GPLVM, we found high sensitivity to ﬁlter initialisation: the estimates have high
variance across runs and are inaccurate ≈ 80% of the time. When it fails, the GPLVM tracker often
freezes in latent space, like PCA. When it does succeed, it produces results that are comparable
with LELVM, although somewhat less accurate visually. However, even then GPLVM’s latent space
consists of continuous chunks spread apart and offset from each other; GPLVM has no incentive to
place nearby two xs mapping to the same y. This effect, combined with the lack of a data-sensitive,
realistic latent space density p(x), makes GPLVM jump erratically from chunk to chunk, in contrast
with LELVM, which smoothly follows the 1D loop. Some GPLVM problems might be alleviated
using higher-order dynamics, but our experiments suggest that such modeling sophistication is less

6

n = 1

n = 15

n = 29

n = 43

n = 55

n = 69

100

50

0

−50

−100

100

50

0

−50

−100

100

50

0

−50

−100

100

50

0

−50

−100

100

50

0

−50

−100

100

50

0

−50

−100

−50

−40

−30

−20

−10

0

10

20

30

40

50

50

40

30

20

10

0

−10

−20

−30

−40

−50

−50

−40

−30

−20

−10

0

10

20

30

40

50

50

40

30

20

10

0

−10

−20

−30

−40

−50

−50

−40

−30

−20

−10

0

10

20

30

40

50

50

40

30

20

10

0

−10

−20

−30

−40

−50

−50

−40

−30

−20

−10

0

10

20

30

40

50

50

40

30

20

10

0

−10

−20

−30

−40

−50

−50

−40

−30

−20

−10

0

10

20

30

40

50

50

40

30

20

10

0

−10

−20

−30

−40

−50

−50

−40

−30

−20

−10

0

10

20

30

40

50

50

40

30

20

10

0

−10

−20

−30

−40

−50

n = 4

n = 9

n = 14

n = 19

n = 24

n = 29

20

40

60

80

100

120

140

160

180

200

220

20

40

60

80

100

120

140

160

180

200

220

20

40

60

80

100

120

140

160

180

200

220

20

40

60

80

100

120

140

160

180

200

220

20

40

60

80

100

120

140

160

180

200

220

A

B

20

40

60

80

100

120

140

160

180

200

220

50

100

150

200

250

300

350

50

100

150

200

250

300

350

50

100

150

200

250

300

350

50

100

150

200

250

300

350

50

100

150

200

250

300

350

50

100

150

200

250

300

350

100

50

0

−50

−100

100

50

0

−50

−100

100

50

0

−50

−100

100

50

0

−50

−100

100

50

0

−50

−100

100

50

0

−50

−100

−50

−40

−30

−20

−10

0

10

20

30

40

50

50

40

30

20

10

0

−10

−20

−30

−40

−50

−50

−40

−30

−20

−10

0

10

20

30

40

50

50

40

30

20

10

0

−10

−20

−30

−40

−50

−50

−40

−30

−20

−10

0

10

20

30

40

50

50

40

30

20

10

0

−10

−20

−30

−40

−50

−50

−40

−30

−20

−10

0

10

20

30

40

50

50

40

30

20

10

0

−10

−20

−30

−40

−50

−50

−40

−30

−20

−10

0

10

20

30

40

50

50

40

30

20

10

0

−10

−20

−30

−40

−50

−50

−40

−30

−20

−10

0

10

20

30

40

50

50

40

30

20

10

0

−10

−20

−30

−40

−50

A: tracking of a video from [6] (turning & walking). We use 220 datapoints (3 full walking
Figure 2:
cycles) for training LELVM. Row 1: tracking in the 2D latent space. The contours are the estimated
posterior probability. Row 2: tracking based on markers. The red dots are the 2D tracks and the
green stick man is the 3D reconstruction obtained using our model. Row 3: our 3D reconstruction
from a different viewpoint. B: tracking of a person running straight towards the camera. Notice the
scale changes and possible forward-backward ambiguities in the 3D estimates. We train the LELVM
using 180 datapoints (2.5 running cycles); 2D tracks were obtained by manually marking the video.
In both A–B the mocap training data was for a person different from the video’s (with different body
proportions and motions), and no ground-truth estimate was available for favourable initialisation.

LELVM

GPLVM

PCA

tracking in latent space

38
0.99

0.02

0.015

0.01

0.005

0

−0.005

−0.01

−0.015

−0.02

−0.025

−0.025

−0.02

−0.015

−0.01

−0.005

0

0.005

0.01

0.015

0.02

0.025

2.5

2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

−2.5

tracking in latent space

38

−2

−1

0

1

2

3

30

20

10

0

−10

−20

−30

−80

tracking in latent space

38

−60

−40

−20

0

20

40

60

80

frame 38.

Figure 3: Method compari-
son,
PCA and
GPLVM map consecutive walk-
ing cycles to spatially distinct
latent space regions.
Com-
pounded by a data independent
latent prior, the resulting tracker
gets easily confused:
it jumps
across loops and/or remains put,
trapped in local optima. In con-
trast, LELVM is stable and fol-
lows tightly a 1D manifold (see
videos).

crucial if locality constraints are correctly modeled (as in LELVM). We conclude that, compared to
LELVM, GPLVM is signiﬁcantly less robust for tracking, has much higher training overhead and
lacks some operations (e.g. computing latent conditionals based on partly missing ambient data).

7

5 Conclusion and future work

We have proposed the use of priors based on the Laplacian Eigenmaps Latent Variable Model
(LELVM) for people tracking. LELVM is a probabilistic dim. red. method that combines the advan-
tages of latent variable models and spectral manifold learning algorithms: a multimodal probability
density over latent and ambient variables, globally differentiable nonlinear mappings for reconstruc-
tion and dimensionality reduction, no local optima, ability to unfold highly nonlinear manifolds, and
good practical scaling to latent spaces of high dimension. LELVM is computationally efﬁcient, sim-
ple to learn from sparse training data, and compatible with standard probabilistic trackers such as
particle ﬁlters. Our results using a LELVM-based probabilistic sigma point mixture tracker with sev-
eral real and synthetic human motion sequences show that LELVM provides sufﬁcient constraints
for robust operation in the presence of missing, noisy and ambiguous image measurements. Com-
parisons with PCA and GPLVM show LELVM is superior in terms of accuracy, robustness and
computation time. The objective of this paper was to demonstrate the ability of the LELVM prior
in a simple setting using 2D tracks obtained automatically or manually, and single-type motions
(running, walking). Future work will explore more complex observation models such as silhouettes;
the combination of different motion types in the same latent space (whose dimension will exceed 2);
and the exploration of multimodal posterior distributions in latent space caused by ambiguities.

Acknowledgments

This work was partially supported by NSF CAREER award IIS–0546857 (MACP), NSF IIS–0535140
and EC MCEXT–025481 (CS). CMU data: http://mocap.cs.cmu.edu (created with fund-
ing from NSF EIA–0196217). OSU data: http://accad.osu.edu/research/mocap/mocap
data.htm.

References
[1] M. ´A. Carreira-Perpi˜n´an and Z. Lu. The Laplacian Eigenmaps Latent Variable Model. In AISTATS, 2007.
[2] N. R. Howe, M. E. Leventon, and W. T. Freeman. Bayesian reconstruction of 3D human motion from

single-camera video. In NIPS, volume 12, pages 820–826, 2000.

[3] T.-J. Cham and J. M. Rehg. A multiple hypothesis approach to ﬁgure tracking. In CVPR, 1999.
[4] M. Brand. Shadow puppetry. In ICCV, pages 1237–1244, 1999.
[5] H. Sidenbladh, M. J. Black, and L. Sigal. Implicit probabilistic models of human motion for synthesis

and tracking. In ECCV, volume 1, pages 784–800, 2002.

[6] C. Sminchisescu and A. Jepson. Generative modeling for continuous non-linearly embedded visual infer-

ence. In ICML, pages 759–766, 2004.

[7] R. Urtasun, D. J. Fleet, A. Hertzmann, and P. Fua. Priors for people tracking from small training sets. In

ICCV, pages 403–410, 2005.

[8] R. Li, M.-H. Yang, S. Sclaroff, and T.-P. Tian. Monocular tracking of 3D human motion with a coordinated

mixture of factor analyzers. In ECCV, volume 2, pages 137–150, 2006.

[9] J. M. Wang, D. Fleet, and A. Hertzmann. Gaussian process dynamical models. In NIPS, volume 18, 2006.
[10] R. Urtasun, D. J. Fleet, and P. Fua. Gaussian process dynamical models for 3D people tracking. In CVPR,

pages 238–245, 2006.

[11] G. W. Taylor, G. E. Hinton, and S. Roweis. Modeling human motion using binary latent variables. In

NIPS, volume 19, 2007.

[12] C. M. Bishop, M. Svens´en, and C. K. I. Williams. GTM: The generative topographic mapping. Neural

Computation, 10(1):215–234, January 1998.

[13] N. Lawrence. Probabilistic non-linear principal component analysis with Gaussian process latent variable

models. Journal of Machine Learning Research, 6:1783–1816, November 2005.

[14] M. Belkin and P. Niyogi. Laplacian eigenmaps for dimensionality reduction and data representation.

Neural Computation, 15(6):1373–1396, June 2003.

[15] B. W. Silverman. Density Estimation for Statistics and Data Analysis. Chapman & Hall, 1986.
[16] R. van der Merwe and E. A. Wan. Gaussian mixture sigma-point particle ﬁlters for sequential probabilistic

inference in dynamic state-space models. In ICASSP, volume 6, pages 701–704, 2003.

[17] M. ´A. Carreira-Perpi˜n´an. Acceleration strategies for Gaussian mean-shift image segmentation. In CVPR,

pages 1160–1167, 2006.

8

"
711,2007,The Distribution Family of Similarity Distances,"Assessing similarity between features is a key step in object recognition and scene categorization tasks. We argue that knowledge on the distribution of distances generated by similarity functions is crucial in deciding whether features are similar or not. Intuitively one would expect that similarities between features could arise from any distribution. In this paper, we will derive the contrary, and report the theoretical result that $L_p$-norms --a class of commonly applied distance metrics-- from one feature vector to other vectors are Weibull-distributed if the feature values are correlated and non-identically distributed. Besides these assumptions being realistic for images, we experimentally show them to hold for various popular feature extraction algorithms, for a diverse range of images. This fundamental insight opens new directions in the assessment of feature similarity, with projected improvements in object and scene recognition algorithms.

Erratum: The authors of paper have declared that they have become convinced that the reasoning in the reference is too simple as a proof of their claims. As a consequence, they withdraw their theorems.","The Distribution Family of Similarity Distances

Gertjan J. Burghouts∗

Arnold W. M. Smeulders

Intelligent Systems Lab Amsterdam

Informatics Institute

University of Amsterdam

Jan-Mark Geusebroek †

Abstract

Assessing similarity between features is a key step in object recognition and scene
categorization tasks. We argue that knowledge on the distribution of distances
generated by similarity functions is crucial in deciding whether features are sim-
ilar or not. Intuitively one would expect that similarities between features could
arise from any distribution.
In this paper, we will derive the contrary, and re-
port the theoretical result that Lp-norms –a class of commonly applied distance
metrics– from one feature vector to other vectors are Weibull-distributed if the
feature values are correlated and non-identically distributed. Besides these as-
sumptions being realistic for images, we experimentally show them to hold for
various popular feature extraction algorithms, for a diverse range of images. This
fundamental insight opens new directions in the assessment of feature similarity,
with projected improvements in object and scene recognition algorithms.

1 Introduction

Measurement of similarity is a critical asset of state of the art in computer vision. In all three major
streams of current research - the recognition of known objects [13], assigning an object to a class
[8, 24], or assigning a scene to a type [6, 25] - the problem is transposed into the equality of features
derived from similarity functions. Hence, besides the issue of feature distinctiveness, comparing
two images heavily relies on such similarity functions. We argue that knowledge on the distribution
of distances generated by such similarity functions is even more important, as it is that knowledge
which is crucial in deciding whether features are similar or not.

For example, Nowak and Jurie [21] establish whether one can draw conclusions on two never seen
objects based on the similarity distances from known objects. Where they build and traverse a
randomized tree to establish region correspondence, one could alternatively use the distribution of
similarity distances to establish whether features come from the mode or the tail of the distribution.
Although this indeed only hints at an algorithm, it is likely that knowledge of the distance distribution
will considerably improve or speed-up such tasks.

As a second example, consider the clustering of features based on their distances. Better clustering
algorithms signiﬁcantly boost performance for object and scene categorization [12]. Knowledge
on the distribution of distances aids in the construction of good clustering algorithms. Using this
knowledge allows for the exact distribution shape to be used to determine cluster size and centroid,
where now the Gaussian is often groundlessly assumed. We will show that in general distance
distributions will strongly deviate from the Gaussian probability distribution.

A third example is from object and scene recognition. Usually this is done by measuring invariant
feature sets [9, 13, 24] at a predeﬁned raster of regions in the image or at selected key points in the
image [11, 13] as extensively evaluated [17]. Typically, an image contains a hundred regions or a

∗Dr. Burghouts is now with TNO Defense, The Netherlands, gertjan.burghouts@tno.nl.
†Corresponding author. Email: mark@science.uva.nl.

1

thousand key points. Then, the most expensive computational step is to compare these feature sets
to the feature sets of the reference objects, object classes or scene types. Usually this is done by
going over all entries in the image to all entries in the reference set and select the best matching
pair. Knowledge of the distribution of similarity distances and having established its parameters
enables a remarkable speed-up in the search for matching reference points and hence for matching
images. When verifying that a given reference key-point or region is statistically unlikely to occur
in this image, one can move on to search in the next image. Furthermore, this knowledge can well
be applied in the construction of fast search trees, see e.g. [16].

Hence, apart from obtaining theoretical insights in the general distribution of similarities, the results
derived in this paper are directly applicable in object and scene recognition.

Intuitively one would expect that the set of all similarity values to a key-point or region in an image
would assume any distribution. One would expect that there is no preferred probability density
distribution at stake in measuring the similarities to points or regions in one image. In this paper, we
will derive the contrary. We will prove that under broad conditions the similarity values to a given
reference point or region adhere to a class of distributions known as the Weibull distribution family.
The density function has only three parameters: mean, standard deviation and skewness. We will
verify experimentally that the conditions under which this result from mathematical statistics holds
are present in common sets of images. It appears the theory predicts the resulting density functions
accurately.

Our work on density distributions of similarity values compares to the work by Pekalska and Duin
[23] assuming a Gaussian distribution for similarities. It is based on an original combination of two
facts from statistical physics. An old fact regards the statistics of extreme values [10], as generated
when considering the minima and maxima of many measurements. The major result of the ﬁeld
of extreme value statistics is that the probability density in this case can only be one out of three
different types, independent of the underlying data or process. The second fact is a new result, which
links these extreme value statistics to sums of correlated variables [2, 3]. We exploit these two facts
in order to derive the distribution family of similarity measures.

This paper is structured as follows. In Section 2, we overview literature on similarity distances and
distance distributions. In Section 3, we discuss the theory of distributions of similarity distances
from one to other feature vectors. In Section 4, we validate the resulting distribution experimentally
for image feature vectors. Finally, conclusions are given in Section 5.

2 Related work

2.1 Similarity distance measures

To measure the similarity between two feature vectors, many distance measures have been proposed
[15]. A common metric class of measures is the Lp-norm [1]. The distance from one reference
feature vector s to one other feature vector t can be formalized as:

n

d(s, t) = (

X

|si − ti|p)1/p,

i=1

(1)

where n and i are the dimensionality and indices of the vectors. Let the random variable Dp rep-
resent distances d(s, t) where t is drawn from the random variable T representing feature vectors.
Independent of the reference feature vector s, the probability density function of Lp-distances will
be denoted by f (Dp = d).

2.2 Distance distributions

Ferencz et al. [7] have considered the Gamma distribution to model the L2-distances from image
regions to one reference region: f (D2 = d) = 1
βγ Γ(γ) dγ−1 e−d/β, where γ is the shape parameter,
and β the scale parameter; Γ(·) denotes the Gamma function. In [7], the distance function was
ﬁtted efﬁciently from few examples of image regions. Although the distribution ﬁts were shown to
represent the region distances to some extent, the method lacks a theoretical motivation.

2

Based on the central limit theorem, Pekalska and Duin [23] assumed that Lp-distances between
e−(d2/β2)/2. As the authors argue,
feature vectors are normally distributed: f (Dp = d) = 1√2π β
the use of the central limit theorem is theoretically justiﬁed if the feature values are independent,
identically distributed, and have limited variance. Although feature values generally have limited
variance, unfortunately, they cannot be assumed to be independent and/or identically distributed as
we will show below. Hence, an alternative derivation of the distance distribution function has to be
followed.

2.3 Contribution of this paper

Our contribution is the theoretical derivation of a parameterized distribution for Lp-norm distances
between feature vectors. In the experiments, we establish whether distances to image features adhere
to this distribution indeed. We consider SIFT-based features [17], computed from various interest
region types [18].

3 Statistics of distances between features

In this section, we derive the distribution function family of Lp-distances from a reference feature
vector to other feature vectors. We consider the notation as used in the previous section, with t
a feature vector drawn from the random variable T . For each vector t, we consider the value at
index i, ti, resulting in a random variable Ti. The value of the reference vector at index i, si, can
be interpreted as a sample of the random variable Ti. The computation of distances from one to
other vectors involves manipulations of the random variable Ti resulting in a new random variable:
Xi = |si −Ti|p. Furthermore, the computation of the distances D requires the summation of random
variables, and a reparameterization: D = (PI
i=1 Xi)1/p. In order to derive the distribution of D,
we start with the statistics of the summation of random variables, before turning to the properties of
Xi.

3.1 Statistics of sums

As a starting point to derive the Lp-distance distribution function, we consider a lemma from statis-
tics about the sum of random variables.

Lemma 1 For non-identical and correlated random variables Xi, the sum PN
i=1 Xi, with ﬁnite N,
is distributed according to the generalized extreme value distribution, i.e. the Gumbel, Frechet or
Weibull distribution.

For a proof, see [2, 3]. Note that the lemma is an extension of the central limit theorem to non-
identically distributed random variables. And, indeed, the proof follows the path of the central
limit theorem. Hence, the resulting distribution of sums is different from a normal distribution, and
rather one of the Gumbel, Frechet or Weibull distributions instead. This lemma is important for
our purposes, as later the feature values will turn out to be non-identical and correlated indeed. To
conﬁne the distribution function further, we also need the following lemma.

Lemma 2 If in the above lemma the random variable Xi are upper-bounded, i.e. Xi < max, the
sum of the variables is Weibull distributed (and not Gumbel nor Frechet):

f (Y = y) =

γ
β

(

y
β

)γ−1 e−( y

β )γ

,

(2)

with γ the shape parameter and β the scale parameter.

For a proof, see [10]. Figure 1 illustrates the Weibull distribution for various shape parameters
γ. This lemma is equally important to our purpose, as later the feature values will turn out to be
upper-bounded indeed.

The combination of Lemmas 1 and 2 yields the distribution of sums of non-identical, correlated and
upper-bounded random variables, summarized in the following theorem.

3

p

0.8

0.6

0.4

0.2

shape
parameter
Γ=2

Γ=4

Γ=6

Γ=8

1

2

3

4

5

distance

Figure 1: Examples of the Weibull distribution for various shape parameters γ.

Theorem 1 For non-identical, correlated and upper-bounded random variables Xi, the random
variable Y = PN

i=1 Xi, with ﬁnite N, adheres to the Weibull distribution.

The proof follows trivially from combining the different ﬁndings of statistics as laid down in Lem-
mas 1 and 2. Theorem 1 is the starting point to derive the distribution of Lp-norms from one
reference vector to other feature vectors.

3.2 Lp-distances from one to other feature vectors
Theorem 1 states that Y is Weibull-distributed, given that {Xi = |si − Ti|p}i∈[1,...,I] are non-
identical, correlated and upper-bounded random variables. We transform Y such that it represents
Lp-distances, achieved by the transformation (·)1/p:

N

Y 1/p = (

X

|si − Ti|p)1/p.

i=1

(3)

The consequence of the substitution Z = Y 1/p for the distribution of Y is a change of variables
z = y1/p in Equation 2 [22]: g(Z = z) =
(1/p−1)z(1−p) . This transformation yields a different
distribution still of the Weibull type:

f (zp)

g(Z = z) =

1

(1/p − 1)

γ
β1/p (

z

β1/p )pγ−1 e−(

z

β1/p )pγ

,

(4)

where γ′ = pγ is the new shape parameter and β′ = β1/p is the new scale parameter, respectively.
Thus, also Y 1/p and hence Lp-distances are Weibull-distributed under the assumed case.
We argue that the random variables Xi = |si − Ti|p and Xj (i 6= j) are indeed non-identical,
correlated and upper-bounded random variables when considering a set of values extracted from
feature vectors at indices i and j:

• Xi and Xj are upper-bounded. Features are usually an abstraction of a particular type of
ﬁnite measurements, resulting in a ﬁnite feature. Hence, for general feature vectors, the
values at index i, Ti, are ﬁnite. And, with ﬁnite p, it follows trivially that Xi is ﬁnite.

• Xi and Xj are correlated. The experimental veriﬁcation of this assumption is postponed to

Section 4.1.

• Xi and Xj are non-identically distributed. The experimental veriﬁcation of this assumption

is postponed to Section 4.1.

We have obtained the following result.

Corollary 1 For ﬁnite-length feature vectors with non-identical, correlated and upper-bounded val-
ues, Lp distances, for limited p, from one reference feature vector to other feature vectors adhere to
the Weibull distribution.

4

3.3 Extending the class of features

We extend the class of features for which the distances are Weibull-distributed. From now on, we
allow the possibility that the vectors are preprocessed by a PCA transformation. We denote the PCA
transform g(·) applied to a single feature vector as s′ = g(s). For the random variable Ti, we obtain
T ′i . We are still dealing with upper-bounded variables X′i = |s′i − T ′i |p as PCA is a ﬁnite transform.
The experimental veriﬁcation of the assumption that PCA-transformed feature values T ′i and T ′j,
i 6= j are non-identically distributed is postponed to Section 4.1. Our point here, is that we have
assumed originally correlating feature values, but after the decorrelating PCA transform we are no
longer dealing with correlated feature values T ′i and T ′j. In Section 4.1, we will verify experimentally
whether X′i and X′j correlate. The following observation is hypothesized. PCA translates the data
to the origin, before applying an afﬁne transformation that yields data distributed along orthogonal
axes. The tuples (X′i, X′j) will be in the ﬁrst quadrant due to the absolute value transformation.
Obviously, variances σ(X′i) and σ(X′j) are limited and means µ(X′i) > 0 and µ(X′j) > 0. For
data constrained to the ﬁrst quadrant and distributed along orthogonal axes, a negative covariance is
expected to be observed. Under the assumed case, we have obtained the following result.

Corollary 2 For ﬁnite-length feature vectors with non-identical, correlated and upper-bounded val-
ues, and for PCA-transformations thereof, Lp distances, for limited p, from one to other features
adhere to the Weibull distribution.

3.4 Heterogeneous feature vector data

We extend the corollary to hold also for composite datasets of feature vectors. Consider the com-
posite dataset modelled by random variables {Tt}, where each random variable Tt represents non-
identical and correlated feature values. Hence, from Corollary 2 it follows that feature vectors from
each of the Tt can be ﬁtted by a Weibull function f β,γ(d). However, the distances to each of the
Tt may have a different range and modus, as we will verify by experimentation in Section 4.1. For
heterogeneous distance data {Tt}, we obtain a mixture of Weibull functions [14].

Corollary 3 (Distance distribution) For feature vectors that are drawn from a mixture of datasets,
of which each results in non-identical and correlated feature values, ﬁnite-length feature vectors
with non-identical, correlated and upper-bounded values, and for PCA-transformations thereof, Lp
distances, for limited p, from one reference feature vector to other feature vectors adhere to the
Weibull mixture distribution: f (D = d) = Pc
(d), where fi are the Weibull functions
and ρi are their respective weights such that Pc

i=1 ρi · f βi,γi
i=1 ρi = 1.

i

4 Experiments

In our experiments, we validate assumptions and Weibull goodness-of-ﬁt for the region-based SIFT,
GLOH, SPIN, and PCA-SIFT features on COREL data [5]. We include these features for two
reasons as: a) they are performing well for realistic computer vision tasks and b) they provide
different mechanisms to describe an image region [17]. The region features are computed from
regions detected by the Harris- and Hessian-afﬁne regions, maximally stable regions (MSER), and
intensity extrema-based regions (IBR) [18]. Also, we consider PCA-transformed versions for each
of the detector/feature combinations. For reason of its extensive use, the experimentation is based
on the L2-distance. We consider distances from 1 randomly drawn reference vector to 100 other
randomly drawn feature vectors, which we repeat 1,000 times for generalization. In all experiments,
the features are taken from multiple images, except for the illustration in Section 4.1.2 to show
typical distributions of distances between features taken from single images.

4.1 Validation of the corollary assumptions for image features

4.1.1

Intrinsic feature assumptions

Corollary 2 rests on a few explicit assumptions. Here we will verify whether the assumptions occur
in practice.

5

Differences between feature values are correlated. We consider a set of feature vectors Tj and
the differences at index i to a reference vector s: Xi = |si − Tji|p. We determine the signiﬁcance
of Pearson’s correlation [4] between the difference values Xi and Xj, i 6= j. We establish the
percentage of signiﬁcantly correlating differences at a conﬁdence level of 0.05. We report for each
feature the average percentage of difference values that correlate signiﬁcantly with difference values
at an other feature vector index.

As expected, the feature value differences correlate. For SIFT, 99% of the difference values are
signiﬁcantly correlated. For SPIN and GLOH, we obtain 98% and 96%, respectively. Also PCA-
SIFT contains signiﬁcantly correlating difference values: 95%. Although the feature’s name hints
at uncorrelated values, it does not achieve a decorrelation of the values in practice. For each of the
features, a low standard deviation < 5% is found. This expresses the low variation of correlations
across the random samplings and across the various region types.

We repeat the experiment for PCA-transformed feature values. Although the resulting values are
uncorrelated by construction, their differences are signiﬁcantly correlated. For SIFT, SPIN, GLOH,
and PCA-SIFT, the percentages of signiﬁcantly correlating difference values are: 94%, 86%, 95%,
and 75%, respectively.

Differences between feature values are non-identically distributed. We repeat the same proce-
dure as above, but instead of measuring the signiﬁcance of correlation, we establish the percentage
of signiﬁcantly differently distributed difference values Xi by the Wilcoxon rank sum test [4] at a
conﬁdence level of 0.05. For SIFT, SPIN, GLOH, and PCA-SIFT, the percentages of signiﬁcantly
differently distributed difference values are: 99%, 98%, 92%, and 87%. For the PCA-transformed
versions of SIFT, SPIN, GLOH, and PCA-SIFT, we ﬁnd: 62%, 40%, 64%, and 51%, respectively.
Note that in all cases, correlation is sufﬁcient to fulﬁll the assumptions of Corollary 2. We have
illustrated that feature value differences are signiﬁcantly correlated and signiﬁcantly non-identically
distributed. We conclude that the assumptions of Corollary 2 about properties of feature vectors are
realistic in practice, and that Weibull functions are expected to ﬁt distance distributions well.

4.1.2

Inter-feature assumptions

In Corollary 3, we have assumed that distances from one to other feature vectors are described
well by a mixture of Weibulls, if the features are taken from different clusters in the data. Here,
we illustrate that clusters of feature vectors, and clusters of distances, occur in practice. Figure
2a shows Harris-afﬁne regions from a natural scene which are described by the SIFT feature. The
distances are described well by a single Weibull distribution. The same hold for distances from
one to other regions computed from a man-made object, see Figure 2b. In Figure 2c, we illustrate
the distances of one to other regions computed from a composite image containing two types of
regions. This results in two modalitites of feature vectors hence of similarity distances. The distance
distribution is therefore bimodal, illustrating the general case of multimodality to be expected in
realistic, heterogeneous image data. We conclude that the assumptions of Corollary 3 are realistic
in practice, and that the Weibull function, or a mixture, ﬁts distance distributions well.

4.2 Validation of Weibull-shaped distance distributions

In this experiment, we validate the ﬁtting of Weibull distributions of distances from one reference
feature vector to other vectors. We consider the same data as before. Over 1,000 repetitions we
consider the goodness-of-ﬁt of L2-distances by the Weibull distribution. The parameters of the
Weibull distribution function are obtained by maximum likelihood estimation. The established ﬁt is
assessed by the Anderson-Darling test at a conﬁdence level of α = 0.05 [20]. The Anderson-Darling
test has also proven to be suited to measure the goodness-of-ﬁt of mixture distributions [19].

Table 1 indicates that for most of the feature types computed from various regions, more than 90%
of the distance distributions is ﬁt by a single Weibull function. As expected, distances between each
of the SPIN, SIFT, PCA-SIFT and GLOH features, are ﬁtted well by Weibull distributions. The
exception here is the low number of ﬁts for the SIFT and SPIN features computed from Hessian-
afﬁne regions. The distributions of distances between these two region/feature combinations tend to
have multiple modes. Likewise, there is a low percentage of ﬁts of L2-distance distributions of the

6

0.014

0.012

0.01

0.008

0.006

0.004

0.002

y
t
i
l
i

b
a
b
o
r
p

0.014

0.012

0.01

0.008

0.006

0.004

0.002

y
t
i
l
i

b
a
b
o
r
p

0.014

0.012

0.01

0.008

0.006

0.004

0.002

y
t
i
l
i

b
a
b
o
r
p

0
250

300

350

400

450
500
distances

550

600

650

700

0
250

300

350

400

450
500
distances

550

600

650

700

0
250

300

350

400

450
500
distances

550

600

650

700

(a)

(b)

(c)

Figure 2: Distance distributions from one randomly selected image region to other regions, each
described by the SIFT feature. The distance distribution is described by a single Weibull function
for a natural scene (a) and a man-made object (b). For a composite image, the distance distribution
is bimodal (c). Samples from each of the distributions are shown in the upper images.

Table 1: Accepted Weibull ﬁts for COREL data [5].

c ≤ 2
SIFT
100%
SIFT (g =PCA)
99%
PCA-SIFT
100%
PCA-SIFT (g =PCA)
100%
SPIN
98%
SPIN (g =PCA)
98%
GLOH
100%
GLOH (g =PCA)
100%
Percentages of L2-distance distributions ﬁtted by a Weibull function (c = 1) and a mixture of two Weibull
functions (c ≤ 2) are given.

Hessian-afﬁne
c ≤ 2
c = 1
100% 60%
99%
60%
100% 96%
100% 96%
99%
12%
100% 12%
100% 91%
100% 91%

Harris-afﬁne
c = 1
95%
95%
89%
89%
71%
71%
87%
87%

MSER
c = 1
c ≤ 2
98%
99%
98%
98%
100% 94%
100% 94%
77%
99%
97%
77%
100% 82%
99%
82%

IBR
c ≤ 2
c = 1
100% 92%
100% 92%
100% 95%
100% 95%
45%
99%
45%
99%
86%
99%
99%
86%

SPIN feature computed from IBR regions. Again, multiple modes in the distributions are observed.
For these distributions, a mixture of two Weibull functions provides a good ﬁt (≥ 97%).

5 Conclusion

In this paper, we have derived that similarity distances between one and other image features in
databases are Weibull distributed. Indeed, for various types of features, i.e. the SPIN, SIFT, GLOH
and PCA-SIFT features, and for a large variety of images from the COREL image collection, we
have demonstrated that the similarity distances from one to other features, computed from Lp norms,
are Weibull-distributed. These results are established by the experiments presented in Table 1. Also,
between PCA-transformed feature vectors, the distances are Weibull-distributed. The Malahanobis
distance is very similar to the L2-norm computed in the PCA-transformed feature space. Hence,
we expect Mahalanobis distances to be Weibull distributed as well. Furthermore, when the dataset
is a composition, a mixture of few (typically two) Weibull functions sufﬁces, as established by the
experiments presented in Table 1. The resulting Weibull distributions are distinctively different from
the distributions suggested in literature, as they are positively or negatively skewed while the Gamma
[7] and normal [23] distributions are positively and non-skewed, respectively.

We have demonstrated that the Weibull distribution is the preferred choice for estimating properties
of similarity distances. The assumptions under which the theory is valid are realistic for images. We
experimentally have shown them to hold for various popular feature extraction algorithms, and for a
diverse range of images. This fundamental insight opens new directions in the assessment of feature
similarity, with projected improvements and speed-ups in object/scene recognition algorithms.

7

Acknowledgments

This work is partly sponsored by the EU funded NEST project PERCEPT, by the Dutch BSIK
project Multimedian, and by the EU Network of Excellence MUSCLE.

References
[1] B. G. Batchelor. Pattern Recognition: Ideas in Practice. Plenum Press, New York, 1995.
[2] E. Bertin. Global ﬂuctuations and Gumbel statistics. Physical Review Letters, 95(170601):1–4, 2005.
[3] E. Bertin and M. Clusel. Generalised extreme value statistics and sum of correlated variables. Journal of

Physics A, 39:7607, 2006.

[4] W. J. Conover. Practical nonparametric statistics. Wiley, New York, 1971.
[5] Corel Gallery. www.corel.com.
[6] L. Fei-Fei and P. Perona. A bayesian hierarchical model for learning natural scene categories. In CVPR,

2005.

[7] A. Ferencz, E.G. Learned-Miller, and J. Malik. Building a classiﬁcation cascade for visual identiﬁcation
In Proceedings of the International Conference Computer Vision, pages 286–293.

from one example.
IEEE Computer Society, 2003.

[8] R. Fergus, P. Perona, and A. Zisserman. A sparse object category model for efﬁcient learning and exhaus-

tive recognition. In Proceedings of the Computer Vision and Pattern Recognition. IEEE, 2005.

[9] J. M. Geusebroek, R. van den Boomgaard, A. W. M. Smeulders, and H. Geerts. Color invariance. IEEE

Transactions on Pattern Analysis and Machine Intelligence, 23(12):1338–1350, 2001.

[10] E. J. Gumbel. Statistics of Extremes. Columbia University Press, New York, 1958.
[11] C. Harris and M. Stephans. A combined corner and edge detector. In Proceedings of the 4th Alvey Vision

Conference, pages 189–192, Manchester, 1988.

[12] F. Jurie and B. Triggs. Creating efﬁcient codebooks for visual recognition. In ICCV, pages 604–610,

2005.

[13] D. G. Lowe. Distinctive image features from scale-invariant keypoints. International Journal of Computer

Vision, 60(2):91–110, 2004.

[14] J. M. Marin, M. T. Rodriquez-Bernal, and M. P. Wiper. Using weibull mixture distributions to model

heterogeneous survival data. Communications in statistics, 34(3):673–684, 2005.

[15] R. S. Michalski, R. E. Stepp, and E. Diday. A recent advance in data analysis: Clustering objects into
In L. N. Kanal and A. Rosenfeld, editors, Progress in

classes characterized by conjunctive concepts.
Pattern Recognition, pages 33–56. North-Holland Publishing Co., Amsterdam, 1981.

[16] K. Mikolajczyk, B. Leibe, and B. Schiele. Multiple object class detection with a generative model. In

CVPR, 2006.

[17] K. Mikolajczyk and C. Schmid. A performance evaluation of local descriptors. IEEE Transactions on

Pattern Analysis and Machine Intelligence, 27(10):1615–1630, 2005.

[18] K. Mikolajczyk, T. Tuytelaars, C. Schmid, A. Zisserman, J. Matas, F. Schaffalitzky, T. Kadir, and L. Van
Gool. A comparison of afﬁne region detectors. International Journal of Computer Vision, 65(1/2):43–72,
2005.

[19] K. Mosler. Mixture models in econometric duration analysis. Applied Stochastic Models in Business and

Industry, 19(2):91–104, 2003.

[20] NIST/SEMATECH. e-Handbook of Statistical Methods. NIST, http://www.itl.nist.gov/div898/handbook/,

2006.

[21] E. Nowak and F. Jurie. Learning visual similarity measures for comparing never seen objects. In CVPR,

2007.

[22] A. Papoulis and S. U. Pillai. Probability, Random Variables and Stochastic Processes. McGraw-Hill,

New York, 4 edition, 2002.

[23] E. Pekalska and R. P. W. Duin. Classiﬁers for dissimilarity-based pattern recognition. In Proceedings of

the International Conference on Pattern Recognition, volume 2, page 2012, 2000.

[24] C. Schmid and R. Mohr. Local grayvalue invariants for image retrieval. IEEE Transactions on Pattern

Analysis and Machine Intelligence, 19(5):530–535, 1997.

[25] J.C. van Gemert, J.M. Geusebroek, C.J. Veenman, C.G.M. Snoek, and Arnold W.M. Smeulders. Robust
scene categorization by learning image statistics in context. In CVPR Workshop on Semantic Learning
Applications in Multimedia (SLAM), 2006.

8

"
445,2007,Multi-Task Learning via Conic Programming,"When we have several related tasks, solving them simultaneously is shown to be more effective than solving them individually. This approach is called multi-task learning (MTL) and has been studied extensively. Existing approaches to MTL often treat all the tasks as \emph{uniformly related to each other and the relatedness of the tasks is controlled globally. For this reason, the existing methods can lead to undesired solutions when some tasks are not highly related to each other, and some pairs of related tasks can have significantly different solutions. In this paper, we propose a novel MTL algorithm that can overcome these problems. Our method makes use of a task network, which describes the relation structure among tasks. This allows us to deal with intricate relation structures in a systematic way. Furthermore, we control the relatedness of the tasks locally, so all pairs of related tasks are guaranteed to have similar solutions. We apply the above idea to support vector machines (SVMs) and show that the optimization problem can be cast as a second order cone program, which is convex and can be solved efficiently. The usefulness of our approach is demonstrated through simulations with protein super-family classification and ordinal regression problems.","Multi-Task Learning via Conic Programming

Tsuyoshi Kato(cid:2),◦

, Hisashi Kashima

, Masashi Sugiyama

(cid:2) Graduate School of Frontier Sciences, The University of Tokyo,
◦
Institute for Bioinformatics Research and Development (BIRD),

‡

, Kiyoshi Asai(cid:2),(cid:3)

†

Japan Science and Technology Agency (JST)
†
Tokyo Research Laboratory, IBM Research,

‡

(cid:3)

Department of Computer Science, Tokyo Institute of Technology,

AIST Computational Biology Research Center,

kato-tsuyoshi@cb.k.u-tokyo.ac.jp,

kashi pong@yahoo.co.jp,

sugi@cs.titech.ac.jp,

asai@cbrc.jp

Abstract

When we have several related tasks, solving them simultaneously is shown to be
more effective than solving them individually. This approach is called multi-task
learning (MTL) and has been studied extensively. Existing approaches to MTL
often treat all the tasks as uniformly related to each other and the relatedness of
the tasks is controlled globally. For this reason, the existing methods can lead
to undesired solutions when some tasks are not highly related to each other, and
some pairs of related tasks can have signiﬁcantly different solutions. In this pa-
per, we propose a novel MTL algorithm that can overcome these problems. Our
method makes use of a task network, which describes the relation structure among
tasks. This allows us to deal with intricate relation structures in a systematic way.
Furthermore, we control the relatedness of the tasks locally, so all pairs of related
tasks are guaranteed to have similar solutions. We apply the above idea to sup-
port vector machines (SVMs) and show that the optimization problem can be cast
as a second order cone program, which is convex and can be solved efﬁciently.
The usefulness of our approach is demonstrated through simulations with protein
super-family classiﬁcation and ordinal regression problems.

1 Introduction

In many practical situations, a classiﬁcation task can often be divided into related sub-tasks. Since
the related sub-tasks tend to share common factors, solving them together is expected to be more
advantageous than solving them independently. This approach is called multi-task learning (MTL,
a.k.a. inductive transfer or learning to learn) and has theoretically and experimentally proven to be
useful [4, 5, 8].

Typically, the ‘relatedness’ among tasks is implemented as imposing the solutions of related tasks to
be similar (e.g. [5]). However, the MTL methods developed so far have several limitations. First, it
is often assumed that all sub-tasks are related to each other [5]. However, this may not be always true
in practice—some are related but others may not be. The second problem is that the related tasks
are often imposed to be close in the sense that the sum of the distances between solutions over all
pairs of related tasks is upper-bounded [8] (which is often referred to as the global constraint [10]).
This implies that all the solutions of related tasks are not necessarily close, but some can be quite
different.

In this paper, we propose a new MTL method which overcomes the above limitations. We settle the
ﬁrst issue by making use of a task network that describes the relation structure among tasks. This
enables us to deal with intricate relation structures in a systematic way. We solve the second problem

1

by directly upper-bounding each distance between the solutions of related task pairs (which we call
local constraints).

We apply this ideas in the framework of support vector machines (SVMs) and show that linear SVMs
can be trained via a second order cone program (SOCP) [3] in the primal. An SOCP is a convex
problem and the global solution can be computed efﬁciently. We further show that the kernelized
version of the proposed method can be formulated as a matrix-fractional program (MFP) [3] in the
dual, which can be again cast as an SOCP; thus the optimization problem of the kernelized variant is
still convex and the global solution can be computed efﬁciently. Through experiments with artiﬁcial
and real-world protein super-family classiﬁcation data sets, we show that the proposed MTL method
compares favorably with existing MTL methods.

We further test the performance of the proposed approach in ordinal regression scenarios [9], where
the goal is to predict ordinal class labels such as users’ preferences (‘like’/‘neutral’/‘dislike’) or
students’ grades (from ‘A’ to ‘F’). The ordinal regression problems can be formulated as a set of
one-versus-one classiﬁcation problems, e.g., ‘like’ vs. ‘neutral’ and ‘neutral’ vs. ‘dislike’. In ordinal
regression, the relatedness among tasks is highly structured. That is, the solutions (decision bound-
aries) of adjacent problems are expected to be similar, but others may not be related, e.g., ‘A’ vs. ‘B’
and ‘B’ vs. ‘C’ would be related, but ‘A’ vs. ‘B’ and ‘E’ vs. ‘F’ may not be. Our experiments
demonstrate that the proposed method is also useful in the ordinal regression scenarios and tends to
outperform existing approaches [9, 8]

2 Problem Setting

In this section, we formulate the MTL problem.
Let us consider M binary classiﬁcation tasks, which all share the common input-output space X ×
{±1}. For the time being, we assume X ⊂ Rd for simplicity; later in Section 4, we extend it to
reproducing kernel Hilbert spaces. Let {x t, yt}(cid:3)
t=1 be the training set, where xt ∈ X and yt ∈ {±1}
for t = 1, . . . , (cid:2). Each data sample (xt, yt) has its target task; we denote the set of sample indices
of the i-th task by Ii. We assume that each sample belongs only to a single task, i.e., the index sets
are exclusive:
(cid:4)
The goal is to learn the score function of each classiﬁcation task: f i(x; wi, bi) = w
i x + bi, for
i = 1, . . . , M, where wi ∈ Rd and bi ∈ R are the model parameters of the i-th task. We assume that
a task network is available. The task network describes the relationships among tasks, where each
node represents a task and two nodes are connected by an edge if they are related to each other 1. We
denote the edge set by E ≡ {(ik, jk)}K

(cid:2)M
i=1 |Ii| = (cid:2) and Ii ∩ Ij = null, ∀i (cid:6)= j.

k=1.

3 Local MTL with Task Network: Linear Version

In this section, we propose a new MTL method.

3.1 Basic Idea

When the relation among tasks is not available, we may just solve M penalized ﬁtting problems
individually:

(cid:8)wi(cid:8)2 + Cα

1
2

(cid:3)
t∈Ii

Hinge(fi(xt; wi, bi), yt),

(1)
where Cα ∈ R+ is a regularization constant and Hinge(·,·) is the hinge loss function:
Hinge(f, y) ≡ max(1 − f y, 0). This individual approach tends to perform poorly if the number
of training samples in each task is limited—the performance is expected to be improved if more
training samples are available. Here, we can exploit the information of the task network. A naive

for i = 1, . . . , M,

1More generally, the tasks can be related in an inhomogeneous way, i.e., the strength of the relationship
among tasks can be dependent on tasks. This general setting can be similarly formulated by a weighted network,
where edges are weighted according to the strength of the connections. All the discussions in this paper can be
easily extended to weighted networks, but for simplicity we focus on unweighted networks.

2

idea would be to use the training samples of neighboring tasks in the task network for solving the
target ﬁtting problem. However, this does not fully make use of the network structure since there are
many other indirectly connected tasks via some paths on the network.

To cope with this problem, we take another approach here, which is based on the expectation that
the solutions of related tasks are close to each other. More speciﬁcally, we impose the following
constraint on the optimization problem (1):
− wjk

for ∀k = 1, . . . , K.

(cid:8)2 ≤ ρ,

(cid:8)wik

(2)

1
2

Namely, we upper-bound each difference between the solutions of related task pairs by a positive
scalar ρ ∈ R+. We refer to this constraint as local constraint following [10]. Note that we do
not impose a constraint on the bias parameter b i since the bias could be signiﬁcantly different even
among related tasks. The constraint (2) allows us to implicitly increase the number of training
samples over the task network in a systematic way through the solutions of related tasks.

Following the convention [8], we blend Eqs.(1) and (2) as

M(cid:3)

1
2M

(cid:8)wi(cid:8)2 + Cα

i=1

i=1

M(cid:3)

(cid:3)
t∈Ii

Hinge(fi(xt; θ), yt) + Cρρ,

(3)

where Cρ is a positive trade-off parameter. Then our optimization problem is summarized as follows:
Problem 1.

M(cid:3)

i=1

min

1
2M
(cid:8)wik
1
where w ≡ (cid:6)
2

subj. to

(cid:8)2 ≤ ρ, ∀k,

− wjk
(cid:4)
1 , . . . , w

w

(cid:4)
M

(cid:7)(cid:4)

,

(cid:8)wi(cid:8)2 + Cα(cid:8)ξ(cid:8)1 + Cρρ, wrt. w ∈ R

Md, b ∈ R

(cid:4)

and

and

w

(cid:4)
i xt + bi
yt
ξα ≡ [ξα

1 , . . . , ξα
(cid:3) ]

(cid:4) .

(cid:3)

+, and ρ ∈ R+,

M , ξα ∈ R
(cid:5) ≥ 1 − ξα

t , ∀t ∈ Ii,∀i

(4)

3.2 Primal MTL Learning by SOCP

f

z

n

min

subj. to

wrt z ∈ R

The second order cone program (SOCP) is a class of convex programs of minimizing a linear func-
tion over an intersection of second-order cones [3]: 2
Problem 2.
(cid:4)

(cid:8)Aiz + bi(cid:8) ≤ c
(cid:4)
i z + di,
where f ∈ Rn, Ai ∈ R(ni−1)×n, bi ∈ Rni−1, ci ∈ Rn, di ∈ R.
Linear programs, quadratic programs, and quadratically-constrained quadratic programs are actually
special cases of SOCPs. SOCPs are a sub-class of semideﬁnite programs (SDPs) [3], but SOCPs can
be solved more efﬁciently than SDPs. Successful optimization algorithms for both SDP and SOCP
are interior-point algorithms. The SDP solvers (e.g. [2]) consume O(n 2
i ) time complexity
for solving Problem 2, but the SOCP-specialized solvers that directly solve Problem 2 take only
O(n2
We can show that Problem 1 is cast as an SOCP using hyperbolic constraints [3].
Theorem 1. Problem 1 can be reduced to an SOCP and it can be solved with O((M d+(cid:2)) 2(Kd+(cid:2)))
computation.

(cid:2)
i ni) computation [7]. Thus, SOCPs can be solved more efﬁciently than SDPs.

for i = 1, . . . , N,

(cid:2)

i n2

(5)

4 Local MTL with Task Network: Kernelization

The previous section showed that a linear version of the proposed MTL method can be cast as an
SOCP. In this section, we show how the kernel trick could be employed for obtaining a non-linear
variant.

2More generally, an SOCP can include linear equality constraints, but they can be eliminated, for example,

by some projection method.

3

4.1 Dual Formulation

(cid:8)

Let Kfea be a positive semideﬁnite matrix with the (s, t)-th element being the inner-product of
s,t ≡ (cid:11)xs, xt(cid:12) . This is a kernel matrix of feature vectors. We also
feature vectors xs and xt: Kfea
introduce a kernel among tasks. Using a new K-dimensional non-negative parameter vector λ ∈
RK
+ , we deﬁne the kernel matrix of tasks by
Knet(λ) ≡

where Uλ ≡ (cid:2)K
k=1 λkUk, Uk ≡ Eikik + Ejkjk − Eikjk − Ejkik , and E (i,j) ∈ RM×M is the
sparse matrix whose (i, j)-th element is one and all the others are zero. Note that this is the graph
Laplacian kernel [11], where the k-th edge is weighted according to λ k. Let Z ∈ NM×(cid:3) be the
indicator of a task and a sample such that Z i,t = 1 if t ∈ Ii and Zi,t = 0 otherwise. Then the
information about the tasks are expressed by the (cid:2) × (cid:2) kernel matrix Z
Knet(λ) Z. We integrate
the two kernel matrices Kfea and Z

M IM + Uλ
1

Knet(λ) Z by

(cid:9)−1

(cid:4)

(cid:4)

,

(6)
where ◦ denotes the Hadamard product (a.k.a element-wise product). This parameterized ma-
trix Kint(λ) is guaranteed to be positive semideﬁnite [6].
Based on the above notations, the dual formulation of Problem 1 can be expressed using the param-
eterized integrated kernel matrix K int(λ) as follows:
Problem 3.

Knet(λ) Z

Z

,

Kint(λ) ≡ Kfea ◦(cid:4)

(cid:4)

(cid:5)

min

1
(cid:4)
2 α

diag(y)Kint(λ) diag(y)α − (cid:8)α(cid:8)1,

wrt. α ∈ R

(cid:3)

+, and λ ∈ R

subj. to α ≤ Cα1(cid:3), Z diag(y) α = 0M ,

(cid:8)λ(cid:8)1 ≤ Cρ.

M
+ ,

(7)

We note that the solutions α and λ tend to be sparse due to the (cid:2) 1 norm.
Changing the deﬁnition of Kfea from the linear kernel to an arbitrary kernel, we can extend the
proposed linear MTL method to non-linear domains. Furthermore, we can also deal with non-
vectorial structured data by employing a suitable kernel such as the string kernel and the Fisher
kernel.
In the test stage, a new sample x in the j-th task is classiﬁed by

(cid:3)(cid:3)

M(cid:3)

fj(x) =

αtytkfea(xt, x)knet(i, j)Zi,t + bj,

(8)

where kfea(·,·) and knet(·,·) are the kernel functions over features and tasks, respectively.

t=1

i=1

4.2 Dual MTL Learning by SOCP

Here, we show that the above dual problem can also be reduced to an SOCP. To this end, we ﬁrst
introduce a matrix-fractional program (MFP) [7]:
Problem 4.

p

(cid:4)

P (z)

−1 (F z + g)

+ subj. to P (z) ≡ P0 +

wrt. z ∈ R
+, F ∈ Rn×p, and g ∈ Rn. Here Sn

min (F z + g)
where Pi ∈ Sn
and strictly positive deﬁnite cone of n × n matrices, respectively.
Let us re-deﬁne d as the rank of the feature kernel matrix K fea. We introduce a matrix Vfea ∈ R(cid:3)×d
which decomposes the feature kernel matrix as K fea = VfeaVfea
. Deﬁne the (cid:2)-dimensional vectors
fh ∈ R(cid:3) of the h-th feature as Vfea ≡ [f1,
. . . , fd] ∈ R(cid:3)×d and the matrices Fh ≡ Z diag(fh ◦
(cid:8)
(cid:9)−1
y), for h = 1, . . . , d. Using those variables, the objective function in Problem 3 can be rewritten as

++ denote the positive semideﬁnite cone

ziPi ∈ S

+ and Sn

n
++,

i=1

(cid:4)

p(cid:3)

(cid:4)

(cid:4)
F
h

α

M IM + Uλ
1

Fhα − α

(cid:4)1(cid:3).

(9)

d(cid:3)

h=1

1
2

JD =

4

This implies that Problem 3 can be transformed into the combination of a linear program and d
MFPs.
−ejk, where eik is a unit vector
Let us further introduce the vector v k ∈ RM for each edge: vk = eik
with the ik-th element being one. Let Vlap be a matrix deﬁned by Vlap = [v1, . . . , vK] ∈ R
M×K.
Then we can re-express the graph Lagrangian matrix of tasks as Uλ = V lap diag(λ)Vlap
Given the fact that an MFP can be reduced to an SOCP [7], we can reduce Problem 3 to the following
SOCP:
Problem 5.

(cid:4).

M , uh = [u1,h, . . . , uK,h]

(cid:4) ∈ R

K

∀k, ∀h

(cid:10)(cid:10)(cid:10)(cid:10)

Kλ ≤ Cρ,
(cid:11)
1(cid:4)
2u0,h
s0,h − 1

(cid:12)(cid:10)(cid:10)(cid:10)(cid:10) ≤ s0,h + 1,

∀h

∀k, ∀h

(10)

(11)
(12)
(13)

(14)

(15)

min − 1(cid:4)

(cid:3) α +

1
2

s0,h + s1,h + ··· + sK,h,

d(cid:3)

h=1

wrt

s0,h ∈ R, sk,h ∈ R, u0,h ∈ R
λ ∈ R

+ , α ∈ R

(cid:3)
+,

K

subj. to α ≤ Cα1(cid:3), Z diag(y) α = 0M ,

M−1/2u0,h + Vlapuh = Fhα,
(cid:11)
2uk,h
sk,h − λk

(cid:12)(cid:10)(cid:10)(cid:10)(cid:10) ≤ sk,h + λk

(cid:10)(cid:10)(cid:10)(cid:10)

Consequently, we obtain the following result:
Theorem 2. The dual problem of CoNs learning (Problem 3) can be reduced to the SOCP in Prob-
lem 5 and it can be solved with O((Kd + (cid:2))2((M + K)d + (cid:2))) computation.

5 Discussion

In this section, we discuss the properties of the proposed MTL method and the relation to existing
methods.

MTL with Common Bias A possible variant of the proposed MTL method would be to share the
common bias parameter with all tasks (i.e. b 1 = b2 = ··· = bM ). The idea is expected to be useful
particularly when the number of samples in each task is very small. We can also apply the common
bias idea in the kernelized version just by replacing the constraint Z diag(y)α = 0 M in Problem 3
by y

α = 0.

(cid:4)

− wjk

(cid:2)K
k=1 (cid:8)wik

Global vs. Local Constraints Micchelli and Pontil
[8] have proposed a related MTL
method which upper-bounds the sum of
i.e.,
(cid:8)2 ≤ ρ. We call it the global constraint. This global constraint can also have
1
2
a similar effect to our local constraint (2), i.e., the related task pairs tend to have close solutions.
However, the global constraint can allow some of the distances to be large since only the sum is
upper-bounded. This actually causes a signiﬁcant performance degradation in practice, which will
be experimentally demonstrated in Section 6. We note that the idea of local constraints is also used
in the kernel learning problem [10].

the differences of K related task pairs,

Relation to Standard SVMs By construction, the proposed MTL method includes the standard
SVM learning algorithm a special case. Indeed, when the number of tasks is one, Problem 3 is
reduced to the standard SVM optimization problem. Thus, the proposed method may be regarded
as a natural extension of SVMs.

Ordinal Regression As we mentioned in Section 1, MTL approaches are useful in ordinal regres-
sion problems. Ordinal regression is a task of learning multiple quantiles, which can be formulated
as a set of one-versus-one classiﬁcation problems. A naive approach to ordinal regression is to
individually train M SVMs with score functions f i(x) = (cid:11)wi, x(cid:12) + bi, i = 1, . . . , M . Shashua

5

1

0.5

0

-0.5

1

0.5

0

-0.5

1

0.5

0

-0.5

1

0.5

0

-0.5

1

0.5

0

-0.5

1

0.5

0

-0.5

1

0.5

0

-0.5

1

0.5

0

-0.5

1

0.5

0

-0.5

1

0.5

0

-0.5

-1

-1 -0.5 0

0.5

1

-1

-1 -0.5 0

0.5

1

-1

-1 -0.5 0

0.5

1

-1

-1 -0.5 0

0.5

1

-1

-1 -0.5 0

0.5

1

-1

-1 -0.5 0

0.5

1

-1

-1 -0.5 0

0.5

1

-1

-1 -0.5 0

0.5

1

-1

-1 -0.5 0

0.5

1

-1

-1 -0.5 0

0.5

1

1

0.5

0

-0.5

1

0.5

0

-0.5

1

0.5

0

-0.5

1

0.5

0

-0.5

1

0.5

0

-0.5

1

0.5

0

-0.5

1

0.5

0

-0.5

1

0.5

0

-0.5

1

0.5

0

-0.5

1

0.5

0

-0.5

-1

-1 -0.5 0

0.5

1

-1

-1 -0.5 0

0.5

1

-1

-1 -0.5 0

0.5

1

-1

-1 -0.5 0

0.5

1

-1

-1 -0.5 0

0.5

1

-1

-1 -0.5 0

0.5

1

-1

-1 -0.5 0

0.5

1

-1

-1 -0.5 0

0.5

1

-1

-1 -0.5 0

0.5

1

-1

-1 -0.5 0

0.5

1

(a) True classiﬁcation boundaries

(b) IL-SVMs

1

0.5

0

-0.5

1

0.5

0

-0.5

1

0.5

0

-0.5

1

0.5

0

-0.5

1

0.5

0

-0.5

1

0.5

0

-0.5

1

0.5

0

-0.5

1

0.5

0

-0.5

1

0.5

0

-0.5

1

0.5

0

-0.5

-1

-1 -0.5 0

0.5

1

-1

-1 -0.5 0

0.5

1

-1

-1 -0.5 0

0.5

1

-1

-1 -0.5 0

0.5

1

-1

-1 -0.5 0

0.5

1

-1

-1 -0.5 0

0.5

1

-1

-1 -0.5 0

0.5

1

-1

-1 -0.5 0

0.5

1

-1

-1 -0.5 0

0.5

1

-1

-1 -0.5 0

0.5

1

1

0.5

0

-0.5

1

0.5

0

-0.5

1

0.5

0

-0.5

1

0.5

0

-0.5

1

0.5

0

-0.5

1

0.5

0

-0.5

1

0.5

0

-0.5

1

0.5

0

-0.5

1

0.5

0

-0.5

1

0.5

0

-0.5

-1

-1 -0.5 0

0.5

1

-1

-1 -0.5 0

0.5

1

-1

-1 -0.5 0

0.5

1

-1

-1 -0.5 0

0.5

1

-1

-1 -0.5 0

0.5

1

-1

-1 -0.5 0

0.5

1

-1

-1 -0.5 0

0.5

1

-1

-1 -0.5 0

0.5

1

-1

-1 -0.5 0

0.5

1

-1

-1 -0.5 0

0.5

1

(c) MTL-SVM(global/full)

(d) MTL-SVM(local/network)

Figure 1: Toy multi classiﬁcation tasks. Each subﬁgure contains the 10-th, 30-th, 50-th, 70-th, and
90-th tasks in the top row and the 110-th, 130-th, 150-th, 170-th, and 190-th tasks in the bottom row.

and Levin [9] proposed an ordinal regression method called the support vector ordinal regres-
sion (SVOR), where the weight vectors are shared by all SVMs (i.e. w 1 = w2 = ··· = wM )
and only the bias parameter is learned individually.
The proposed MTL method can be naturally employed in ordinal regression by constraining the
weight vectors as (cid:8)wi − wi+1(cid:8)2 ≤ ρ, i = 1, . . . , M − 1, i.e., the task network only has a weight be-
tween consecutive tasks. This method actually includes the above two ordinal regression approaches
as special cases—Cρ = 0 (i.e., ignoring the task network) yields the independent training of SVMs
and Cρ = ∞ (i.e., the weight vectors of all SVMs agree) is reduced to SVOR. Thus, in the context
of ordinal regression, the proposed method smoothly bridges two extremes and allows us to control
the belief of task constraints.

6 Experiments

In this section, we show the usefulness of the proposed method through experiments.

6.1 Toy Multiple Classiﬁcation Tasks

First, we illustrate how the proposed method behaves using a 2-dimensional toy data set, which
includes 200 tasks (see Figure 1(a)). Each task possesses a circular-shaped classiﬁcation boundary
with different centers and a ﬁxed radius 0.5. The location of the center in the i-th task is (−1 +
0.02(i − 1), 0) for 1 ≤ i ≤ 100 and (0,−1 + 0.02(i − 101)) for 101 ≤ i ≤ 200. For each task,
only two positive and two negative samples are generated following the uniform distribution. We
construct a task network where consecutive tasks are connected in a circular manner, i.e., (1, 2),
(2, 3), . . ., (99, 100), and (100, 1) for the ﬁrst 100 tasks and (101, 102), (102, 103), . . ., (199, 200),
and (200, 1) for the last 100 tasks; we further add (50, 150), which connects the clusters of the ﬁrst
100 and the last 100 nodes.
We compare the following methods: a naive method where 200 SVMs are trained indivisually (in-
dividually learned SVM, ‘IL-SVM’), the MTL-SVM algorithm where the global constraint and the
fully connected task network are used [5] (‘MTL-SVM(global/full)’), and the proposed method which
uses local constraints and the properly deﬁned task network (‘MTL-SVM(local/network)’).

The results are exhibited in Figure 1, showing that IL-SVM can not capture the circular shape due
to the small sample size in each task. MTL-SVM(global/full) can successfully capture closed-loop
boundaries by making use of the information from other tasks. However, the result is still not
so reliable since non-consecutive unrelated tasks heavily damage the solutions. On the other hand,
MTL-SVM(local/network) nicely captures the circular boundaries and the results are highly reliable.
Thus, given an appropriate task network, the proposed MTL-SVM(local/network) can effectively
exploit information of the related tasks.

6

Table 1: The accuracy of each method in the protein super-family classiﬁcation task.

Dataset

IL-SVM

d-f
d-s
d-o
f-s
f-o
s-o

0.908 (0.023)
0.638 (0.067)
0.725 (0.032)
0.891 (0.036)
0.792 (0.046)
0.663 (0.034)

One-SVM
0.941 (0.015)
0.722 (0.030)
0.747 (0.017)
0.886 (0.021)
0.819 (0.029)
0.695 (0.034)

MTL-SVM
(global/full)
0.945 (0.013)
0.698 (0.036)
0.748 (0.021)
0.918 (0.020)
0.834 (0.021)
0.692 (0.050)

MTL-SVM

(global/network)
0.933 (0.017)
0.695 (0.032)
0.749 (0.023)
0.911 (0.022)
0.828 (0.015)
0.663 (0.068)

MTL-SVM

(local/network)
0.952 (0.015)
0.747 (0.020)
0.764 (0.028)
0.918 (0.025)
0.838 (0.018)
0.703 (0.036)

6.2 Protein Super-Family Classiﬁcation

Next, we test the performance of the proposed method with real-world protein super-family classiﬁ-
cation problems.

The input data are amino acid sequences from the SCOP database [1] (not SOCP). We counted
2-mers for extraction of feature vectors. There are 20 kinds of amino acids. Hence, the number
of features is 202 = 400. We use RBF kernels, where the kernel width σ 2
rbf is set to the average
of the squared distances to the ﬁfth nearest neighbors. Each data set consists of two folds. Each
fold is divided into several super-families. We here consider the classiﬁcation problem into the
super-families. A positive class is chosen from one fold, and a negative class is chosen from the
other fold. We perform multi-task learning from all the possible combinations. For example, three
super-families are in DNA/RNA binding, and two in SH3. The number of combinations is 3· 2 = 6.
So the data set d-s has the six binary classiﬁcation tasks. We used four folds: DNA/RNA binding,
Flavodoxin, OB-fold and SH3. From these folds, we generate six data sets: d-f, d-f, d-o, f-o, f-s,
and o-s, where the fold names are abbreviated to d, f, o, and s, respectively.

The task networks are constructed as follows: if the positive super-family or the negative super-
family is common to two tasks, the two tasks are regarded as a related task pair and connected by
an edge. We compare the proposed MTL-SVM(local/network) with IL-SVM, ‘One-SVM’, MTL-
SVM(global/full), and MTL-SVM(global/network). One-SVM regards the multiple tasks as one big
task and learns the big task once by a standard SVM. We set C α = 1 for all the approaches. The
value of the parameter Cρ for three MTL-SVM approaches is determined by cross-validation over
the training set. We randomly pick ten training sequences from each super-family, and use them for
training. We compute the classiﬁcation accuracies of the remaining test sequences. We repeat this
procedure 10 times and take the average of the accuracies.
The results are described in Table 1, showing that the proposed MTL-SVM(local/network) com-
pares favorably with the other methods. In this simulation, the task network is constructed rather
heuristically. Even so, the proposed MTL-SVM(local/network) is shown to signiﬁcantly outperform
MTL-SVM(global/full), which does not use the network structure. This implies that the proposed
method still works well even when the task network contains small errors. It is interesting to note
that MTL-SVM(global/network) actually does not work well in this simulation, implying that the
task relatedness are not properly controlled by the global constraint. Thus the use of the local con-
straints would be effective in MTL scenarios.

6.3 Ordinal Regression

As discussed in Section 5, MTL methods are useful in ordinal regression. Here we create ﬁve ordinal
regression data sets described in Table 2, where all the data sets are originally regression and the out-
put values are divided into ﬁve quantiles. Therefore, the overall task can be divided into four isolated
classiﬁcation tasks, each of which estimates a quantile. We compare MTL-SVM(local/network) with
IL-SVM, SVOR [9] (see Section 5), MTL-SVM(full/network) and MTL-SVM(global/network). The
value of the parameter Cρ for three MTL-SVM approaches is determined by cross-validation over
the training set. We set Cα = 1 for all the approaches. We use RBF kernels, where the parame-
ter σ2
rbf is set to the average of the squared distances to the ﬁfth nearest neighbors. We randomly
picked 200 samples for training. The remaining samples are used for evaluating the classiﬁcation
accuracies.

7

Table 2: The accuracy of each method in ordinal regression tasks.

Data set
pumadyn

stock

0.643 (0.007)
0.894 (0.012)
0.781 (0.003)
bank-8fh
bank-8fm 0.854 (0.004)
calihouse
0.648 (0.003)

IL-SVM

SVOR

MTL-SVM
(global/full)
0.629 (0.025)
0.872 (0.010)
0.772 (0.006)
0.832 (0.013)
0.640 (0.005)

MTL-SVM

(global/network)
0.645 (0.018)
0.888 (0.010)
0.773 (0.006)
0.847 (0.009)
0.646 (0.007)

MTL-SVM

(local/network)
0.661 (0.007)
0.902 (0.007)
0.779 (0.002)
0.854 (0.009)
0.650 (0.004)

0.661 (0.006)
0.878 (0.011)
0.777 (0.006)
0.845 (0.010)
0.642 (0.008)

The averaged performance over ﬁve runs is described in Table 2, showing that the proposed MTL-
SVM(local/network) is also promising in ordinal regression scenarios.

7 Conclusions

In this paper, we proposed a new multi-task learning method, which overcomes the limitation of
existing approaches by making use of a task network and local constraints. We demonstrated through
simulations that the proposed method is useful in multi-task learning scenario; moreover, it also
works excellently in ordinal regression scenarios.

The standard SVMs have a variety of extensions and have been combined with various techniques,
e.g., one-class SVMs, SV regression, and the ν-trick. We expect that such extensions and techniques
can also be applied similarly to the proposed method. Other possible future works include the
elucidation of the entire regularization path and the application to learning from multiple networks;
developing algorithms for learning probabilistic models with a task network is also a promising
direction to be explored.

Acknowledgments

This work was partially supported by a Grant-in-Aid for Young Scientists (B), number 18700287,
from the Ministry of Education, Culture, Sports, Science and Technology, Japan.

References
[1] A. Andreeva, D. Howorth, S. E. Brenner, T. J. P. Hubbard, C. Chothia, and A. G. Murzin. SCOP database
in 2004: reﬁnements integrate structure and sequence family data. Nucl. Acid Res., 32:D226–D229, 2004.
[2] B. Borchers. CSDP, a C library for semideﬁnite programming. Optimization Methods and Software,

11(1):613–623, 1999.

[3] Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press, 2004.
[4] R. Caruana. Multitask learning. Machine Learning, 28(1):41–75, 1997.
[5] T. Evgeniou and M. Pontil. Regularized multitask learning. In Proc. of 17-th SIGKDD Conf. on Knowl-

edge Discovery and Data Mining, 2004.

[6] D. Haussler. Convolution kernels on discrete structures. Technical Report UCSC-CRL-99-10, UC Santa

Cruz, July 1999.

[7] M. Lobo, L. Vandenberghe, S. Boyd, and H. Lebret. Applications of second-order cone programming.

Linear Algebra and its Applications, 284:193–228, 1998.

[8] C. A. Micchelli and M. Pontil. Kernels for multi-task learning. In Lawrence K. Saul, Yair Weiss, and L´eon
Bottou, editors, Advances in Neural Information Processing Systems 17, pages 921–928, Cambridge, MA,
2005. MIT Press.

[9] A. Shashua and A. Levin. Ranking with large margin principle: two approaches. In Advances in Neural

Information Processing Systems 15, pages 937–944, Cambridge, MA, 2003. MIT Press.

[10] K. Tsuda and W.S. Noble. Learning kernels from biological networks by maximizing entropy. Bioinfor-

matics, 20(Suppl. 1):i326–i333, 2004.

[11] X. Zhu, J. Kandola, Z. Ghahramani, and J. Lafferty. Nonparametric transforms of graph kernels for
semi-supervised learning. In Lawrence K. Saul, Yair Weiss, and Lon Bottou, editors, Advances in Neural
Information Processing Systems 17, Cambridge, MA, 2004. MIT Press.

8

"
103,2007,EEG-Based Brain-Computer Interaction: Improved Accuracy by Automatic Single-Trial Error Detection,"Brain-computer interfaces (BCIs), as any other interaction modality based on physiological signals and body channels (e.g., muscular activity, speech and gestures), are prone to errors in the recognition of subject's intent. An elegant approach to improve the accuracy of BCIs consists in a verification procedure directly based on the presence of error-related potentials (ErrP) in the EEG recorded right after the occurrence of an error. Six healthy volunteer subjects with no prior BCI experience participated in a new human-robot interaction experiment where they were asked to mentally move a cursor towards a target that can be reached within a few steps using motor imagination. This experiment confirms the previously reported presence of a new kind of ErrP. These Interaction ErrP"" exhibit a first sharp negative peak followed by a positive peak and a second broader negative peak (~290, ~350 and ~470 ms after the feedback, respectively). But in order to exploit these ErrP we need to detect them in each single trial using a short window following the feedback associated to the response of the classifier embedded in the BCI. We have achieved an average recognition rate of correct and erroneous single trials of 81.8% and 76.2%, respectively. Furthermore, we have achieved an average recognition rate of the subject's intent while trying to mentally drive the cursor of 73.1%. These results show that it's possible to simultaneously extract useful information for mental control to operate a brain-actuated device as well as cognitive states such as error potentials to improve the quality of the brain-computer interaction. Finally, using a well-known inverse model (sLORETA), we show that the main focus of activity at the occurrence of the ErrP are, as expected, in the pre-supplementary motor area and in the anterior cingulate cortex.""","EEG-Based Brain-Computer Interaction: Improved
Accuracy by Automatic Single-Trial Error Detection

Pierre W. Ferrez

IDIAP Research Institute

Centre du Parc

Av. des Pr´es-Beudin 20

1920 Martigny, Switzerland

pierre.ferrez@idiap.ch

Jos´e del R. Mill´an

IDIAP Research Institute

Centre du Parc

Av. des Pr´es-Beudin 20

1920 Martigny, Switzerland
jose.millan@idiap.ch ∗

Abstract

Brain-computer interfaces (BCIs), as any other interaction modality based on
physiological signals and body channels (e.g., muscular activity, speech and ges-
tures), are prone to errors in the recognition of subject’s intent. An elegant ap-
proach to improve the accuracy of BCIs consists in a veriﬁcation procedure di-
rectly based on the presence of error-related potentials (ErrP) in the EEG recorded
right after the occurrence of an error. Six healthy volunteer subjects with no prior
BCI experience participated in a new human-robot interaction experiment where
they were asked to mentally move a cursor towards a target that can be reached
within a few steps using motor imagination. This experiment conﬁrms the previ-
ously reported presence of a new kind of ErrP. These “Interaction ErrP” exhibit a
ﬁrst sharp negative peak followed by a positive peak and a second broader negative
peak (∼290, ∼350 and ∼470 ms after the feedback, respectively). But in order to
exploit these ErrP we need to detect them in each single trial using a short win-
dow following the feedback associated to the response of the classiﬁer embedded
in the BCI. We have achieved an average recognition rate of correct and erroneous
single trials of 81.8% and 76.2%, respectively. Furthermore, we have achieved an
average recognition rate of the subject’s intent while trying to mentally drive the
cursor of 73.1%. These results show that it’s possible to simultaneously extract
useful information for mental control to operate a brain-actuated device as well
as cognitive states such as error potentials to improve the quality of the brain-
computer interaction. Finally, using a well-known inverse model (sLORETA), we
show that the main focus of activity at the occurrence of the ErrP are, as expected,
in the pre-supplementary motor area and in the anterior cingulate cortex.

1 Introduction

People with severe motor disabilities (spinal cord injury (SCI), amyotrophic lateral sclerosis (ALS),
etc.) need alternative ways of communication and control for their everyday life. Over the past two
decades, numerous studies proposed electroencephalogram (EEG) activity for direct brain-computer
interaction [1]-[2]. EEG-based brain-computer interfaces (BCIs) provide disabled people with new
tools for control and communication and are promising alternatives to invasive methods. However,
as any other interaction modality based on physiological signals and body channels (e.g., muscular
activity, speech and gestures), BCIs are prone to errors in the recognition of subject’s intent, and
those errors can be frequent. Indeed, even well-trained subjects rarely reach 100% of success. In
∗This work is supported by the European IST Programme FET Project FP6-003758 and by the Swiss Na-
tional Science Foundation NCCR “IM2”. This paper only reﬂects the authors’ views and funding agencies are
not liable for any use that may be made of the information contained herein.

1

contrast to other interaction modalities, a unique feature of the “brain channel” is that it conveys both
information from which we can derive mental control commands to operate a brain-actuated device
as well as information about cognitive states that are crucial for a purposeful interaction, all this on
the millisecond range. One of these states is the awareness of erroneous responses, which a number
of groups have recently started to explore as a way to improve the performance of BCIs [3]-[6].
In particular, [6] recently reported the presence of a new kind of error potentials (ErrP) elicited by
erroneous feedback provided by a BCI during the recognition of the subject’s intent. In this study
subjects were asked to reach a target by sending repetitive manual commands to pass over several
steps. The system was executing commands with an 80% accuracy, so that at each step there was
a 20% probability that the system delivered an erroneous feedback. The main components of these
“Interaction ErrP” are a negative peak 250 ms after the feedback, a positive peak 320 ms after the
feedback and a second broader negative peak 450 ms after the feedback. To exploit these ErrP for
BCIs, it is mandatory to detect them no more in grand averages but in each single trial using a
short window following the feedback associated to the response of the BCI. The reported average
recognition rates of correct and erroneous single trials are 83.5% and 79.2%, respectively. These
results tend to show that ErrP could be a potential tool to improve the quality of the brain-computer
interaction. However, it is to note that in order to isolate the issue of the recognition of ErrP out of
the more difﬁcult and general problem of a whole BCI where erroneous feedback can be due to non-
optimal performance of both the interface (i.e., the classiﬁer embedded into the interface) and the
user himself, the subject delivered commands manually. The key issue now is to investigate whether
subjects also show ErrP while already engaged in tasks that require a high level of concentration
such as motor imagination, and no more in easy tasks such as pressing a key.
The objective of the present study is to investigate the presence of these ErrP in a real BCI task.
Subjects don’t deliver manual commands anymore, but are focussing on motor imagination tasks
to reach targets randomly selected by the system. In this paper we report new experimental results
recorded with six healthy volunteer subjects with no prior BCI experience during a simple human-
robot interaction that conﬁrm the previously reported existence of a new kind of ErrP [6], which
is satisfactorily recognized in single trials using a short window just after the feedback. Further-
more, using a window just before the feedback, we report a 73.1% accuracy in the recognition of
the subject’s intent during mental control of the BCI. This conﬁrms the fact that EEG conveys si-
multaneously information from which we can derive mental commands as well as information about
cognitive states and shows that both can be sufﬁciently well recognized in each single trials to pro-
vide the subject with an improved brain-computer interaction. Finally, using a well-known inverse
model called sLORETA [7] that non-invasively estimates the intracranial activity from scalp EEG,
we show that the main focus of activity at the occurrence of ErrP seems to be located in the pre-
supplementary motor area (pre-SMA) and in the anterior cingulate cortex (ACC), as expected [8][9].

Figure 1: Illustration of the protocol. (1) The target (blue) appears 2 steps on the left side of the cursor (green).
(2) The subject is imagining a movement of his/her left hand and the cursor moves 1 step to the left. (3) The
subject still focuses on his/her left hand, but the system moves the cursor in the wrong direction. (4) Correct
move to the left, compensating the error. (5) The cursor reaches the target. (6) A new target (red) appears 3
steps on the right side of the cursor, the subject will now imagine a movement of his/her right foot. The system
moved the cursor with an error rate of 20%; i.e., at each step, there was a 20% probability that the robot made
a movement in the wrong direction.

2 Experimental setup

The ﬁrst step to integrate ErrP detection in a BCI is to design a protocol where the subject is fo-
cussing on a mental task for device control and on the feedback delivered by the BCI for ErrP

2

detection. To test the ability of BCI users to concentrate simultaneously on a mental task and to be
aware of the BCI feedback at each single trial, we have simulated a human-robot interaction task
where the subject has to bring the robot to targets 2 or 3 steps either to the left or to the right. This
virtual interaction is implemented by means of a green square cursor that can appear on any of 20
positions along an horizontal line. The goal with this protocol is to bring the cursor to a target that
randomly appears either on the left (blue square) or on the right(red square) of the cursor. The target
is no further away than 3 positions from the cursor (symbolizing the current position of the robot).
This prevents the subject from habituation to one of the stimuli since the cursor reaches the target
within a small number of steps. Figure 1 illustrates the protocol with the target (blue) initially po-
sitioned 2 steps away on the left side of the cursor (green). An error occurred at step 3) so that the
cursor reaches the target in 5 steps. Each target corresponds to a speciﬁc mental task. The subjects
were asked to imagine a movement of their left hand for the left target and to imagine a movement
of their right foot for the right target (note that subject n◦1 selected left foot for the left target and
right hand for the right target). However, since the subjects had no prior BCI experience, the system
was not moving the cursor following the mental commands of the subject, but with an error rate of
20%, to avoid random or totally biased behavior of the cursor.
Six healthy volunteer subjects with no prior BCI experience participated in these experiments. After
the presentation of the target, the subject focuses on the corresponding mental task until the cursor
reached the target. The system moved the cursor with an error rate of 20%; i.e., at each step, there
was a 20% probability that the cursor moved in the opposite direction. When the cursor reached a
target, it brieﬂy turned from green to light green and then a new target was randomly selected by the
system. If the cursor didn’t reach the target after 10 steps, a new target was selected. As shown in
ﬁgure 2, while the subject focuses on a speciﬁc mental task, the system delivers a feedback about ev-
ery 2 seconds. This provides a window just before the feedback for BCI classiﬁcation and a window
just after the feedback for ErrP detection for every single trial. Subjects performed 10 sessions of 3
minutes on 2 different days (the delay between the two days of measurements varied from 1 week
to 1 month), corresponding to ∼75 single trials per session. The 20 sessions were split into 4 groups
of 5, so that classiﬁers were built using a group and tested on the following group. The classiﬁcation
rates presented in Section 3 are therefore the average of 3 prediction performances: classiﬁcation of
group n + 1 using group n to build a classiﬁer. This rule applies for both mental tasks classiﬁcation
and ErrP detection.

Figure 2: Timing of the protocol. The system delivers a feedback about every 2 seconds, this provides a
window just before the feedback for BCI classiﬁcation and a window just after the feedback for ErrP detection
for every single trial. As a new target is presented, the subject focuses on the corresponding mental task until
the target is reached.

EEG potentials were acquired with a portable system (Biosemi ActiveTwo) by means of a cap with
64 integrated electrodes covering the whole scalp uniformly. The sampling rate was 512 Hz and
signals were measured at full DC. Raw EEG potentials were ﬁrst spatially ﬁltered by subtracting
from each electrode the average potential (over the 64 channels) at each time step. The aim of this
re-referencing procedure is to suppress the average brain activity, which can be seen as underlying
background activity, so as to keep the information coming from local sources below each electrode.
Then for off-line mental tasks classiﬁcation, the power spectrum density (PSD) of EEG channels
was estimated over a window of one second just before the feedback. PSD was estimated using
the Welch method resulting in spectra with a 2 Hz resolution from 6 to 44 Hz. The most relevant
EEG channels and frequencies were selected by a simple feature selection algorithm based on the
overlap of the distributions of the different classes. For off-line ErrP detection, we applied a 1-10

3

Hz bandpass ﬁlter as ErrP are known to be a relatively slow cortical potential. EEG signals were
then subsampled from 512 Hz to 64 Hz (i.e., we took one point out of 8) before classiﬁcation,
which was entirely based on temporal features. Indeed the actual input vector for the statistical
classiﬁer described below is a 150 ms window starting 250 ms after the feedback for channels FCz
and Cz. The choice of these channels follows the fact that ErrP are characterized by a fronto-central
distribution along the midline.
For both mental tasks and ErrP classiﬁcation, the two different classes (left or right for mental tasks
and error or correct for ErrP) are recognized by a Gaussian classiﬁer. The output of the statistical
classiﬁer is an estimation of the posterior class probability distribution for a single trial; i.e., the
probability that a given single trial belongs to one of the two classes. In this statistical classiﬁer,
every Gaussian unit represents a prototype of one of the classes to be recognized, and we use several
prototypes per class. During learning, the centers of the classes of the Gaussian units are pulled
towards the trials of the class they represent and pushed away from the trials of the other class. No
artifact rejection algorithm (for removing or ﬁltering out eye or muscular movements) was applied
and all trials were kept for analysis. It is worth noting, however, that after a visual a-posteriori check
of the trials we found no evidence of muscular artifacts that could have contaminated one condition
differently from the other. More details on the Gaussian classiﬁer and the analysis procedure to rule
out ocular/muscular artifacts as the relevant signals for both classiﬁers (BCI itself and ErrP) can be
found in [10].

Figure 3: (Top) Discriminant power (DP) of frequencies. Sensory motor rhythm (12-16 Hz) and some beta
(Bottom) Discriminant power (DP) of electrodes. The most
components are discriminant for all subjects.
relevant electrodes are in the central area (C3, C4 and Cz) according to the ERD/ERD location for hand and
foot movement or imagination.

3 Experimental results

3.1 Mental tasks classiﬁcation

Subject were asked to imagine a movement of their left hand when the left target was proposed and
to imagine a movement of their right foot when the right target was proposed (note that subject n◦1

4

was imagining left foot for the left target and right hand for the right target). The most relevant
EEG channels and frequencies were selected by a simple feature selection algorithm based on the
overlap of the distributions of the different classes. Figure 3 shows the discriminant power (DP) of
frequencies (top) and electrodes (bottom) for the 6 subject. For frequencies, the DP is based on the
best electrode, and for electrodes it is based on the best frequency. Table 1 shows the classiﬁcation
rates for the two mental tasks and the general BCI accuracy for the 6 subjects and the average of
them, it also shows the features (electrodes and frequencies) used for classiﬁcation.
For all 6 subjects, the 12-16 Hz band (sensory motor rhythm (SMR)) appears to be relevant for
classiﬁcation. Subject 1, 3 and 5 show a peak in DP for frequencies around 25 Hz (beta band). For
subject 2 this peak in the beta band is centered at 20 Hz and for subject 6 it is centered at 30 Hz.
Finally subject 4 shows no particular discriminant power in the beta band. Previous studies conﬁrm
these results. Indeed, SMR and beta rhythm over left and/or right sensorimotor cortex have been suc-
cessfully used for BCI control [11]. Event-related de-synchronization (ERD) and synchronization
(ERS) refer to large-scale changes in neural processing. During periods of inactivity, brain areas are
in a kind of idling state with large populations of neurons ﬁring in synchrony resulting in an increase
of amplitude of speciﬁc alpha (8-12 Hz) and beta (12-26 Hz) bands. During activity, populations of
neurons work at their own pace and the power of this idling state is reduced, the cortex has become
de-synchronized. [12]. In our case, the most relevant electrodes for all subjects are in the C3, C4 or
Cz area. These locations conﬁrm previous studies since C3 and C4 areas usually show ERD/ERS
during hands movement or imagination whereas foot movement or imagination are focused in the
Cz area [12].

Table 1: Percentages (mean and standard deviations) of correctly recognized single trials for the 2 motor
imagination tasks for the 6 subjects and the average of them. All subjects show classiﬁcation rates of about
70-75% for motor imagination and the general BCI accuracy is 73%. Features used for classiﬁcation are also
shown.

Electrodes

Frequencies

[Hz]

10 12 14 26

10 12 14 18 20 22

14 16 26

12 14

12 24 26

CPz Cz CP6 CP4

12 14 28 30 32

C3 CP3 CP1 CPz CP2

C4 CP4 P4

C3 C4 C6 CP6 CP4

Cz C2 C4
Cz C4 CP4

# 1*
# 2
# 3
# 4
# 5
# 6
Avg

[%]

Left hand
77.2 ± 3.7
71.8 ± 9.0
76.4 ± 5.8
79.6 ± 1.6
73.5 ± 16.1
77.9 ± 7.4
76.1 ± 2.9

[%]

Right foot
70.4 ± 3.2
80.9 ± 7.1
62.6 ± 6.7
66.3 ± 10.1
71.9 ± 13.3
69.0 ± 13.7
70.2 ± 6.2

[%]

Accuracy
73.8 ± 4.8
76.4 ± 6.4
69.5 ± 9.8
73.0 ± 9.4
72.7 ± 1.1
73.5 ± 6.3
73.1 ± 4.2

* Left foot and Right hand

All 6 subjects show classiﬁcation rates of about 70-75% for motor imagination. These ﬁgures were
achieved with a relatively low number of features (up to 5 electrodes and up to 6 frequencies) and the
general BCI accuracy is 73%. This level of performance can appear relatively low for a 2-class BCI.
However, keeping in mind that ﬁrst all subjects had no prior BCI experience and second that these
ﬁgures were obtained exclusively in prediction (i.e. classiﬁers were always tested on new data), the
performance is satisfactory.

3.2 Error-related potentials

Figure 4 shows the averages of error trials (red curve), of correct trials (green curve) and the dif-
ference error-minus-correct (blue curve) for channel FCz for the six subjects (top). A ﬁrst small
positive peak shows up about ∼230 ms after the feedback (t=0). A negative peak clearly appears
∼290 ms after the feedback for 5 subjects. This negative peak is followed by a positive peak ∼350
ms after the feedback. Finally a second broader negative peak occurs about ∼470 ms after the
feedback. Figure 4 also shows the scalp potentials topographies (right) for the average of the six
subjects, at the occurrence of the four previously described peaks: a ﬁrst fronto-central positivity
appears after ∼230 ms, followed by a fronto-central negativity at ∼290 ms, a fronto-central positiv-
ity at ∼350 ms and a fronto-central negativity at ∼470 ms. All six subjects show similar ErrP time
courses whose amplitudes slightly differ from one subject to the other. These experiments seem to
conﬁrm the existence of a new kind of error-related potentials [6]. Furthermore, the fronto-central

5

focus at the occurrence of the different peaks tends to conﬁrm the hypothesis that ErrP are generated
in a deep brain region called anterior cingulate cortex [8][9] (see also Section 3.3).
Table 2 reports the recognition rates (mean and standard deviations) for the six subjects plus the
average of them. These results show that single-trial recognition of erroneous and correct responses
are above 75% and 80%, respectively. Beside the crucial importance to integrate ErrP in the BCI
in a way that the subject still feels comfortable, for example by reducing as much as possible the
rejection of actually correct commands, a key point for the exploitation of the automatic recognition
of interaction errors is that they translate into an actual improvement of the performance of the BCI.
Table 2 also show the performance of the BCI in terms of bit rate (bits per trial) when detection
of ErrP is used or not and the induced increase of performance (for details see [6]). The beneﬁt of
integrating ErrP detection is obvious since it at least doubles the bit rate for ﬁve of the six subjects
and the average increase is 124%.

Figure 4: (Top) Averages of error trials (red curve), of correct trials (green curve) and the difference error-
minus-correct (blue curve) for channel FCz for the six subjects. All six subjects show similar ErrP time courses
whose amplitudes slightly differ from one subject to the other. (Bottom) Scalp potentials topographies for the
average of the six subjects, at the occurrence of the four described peaks. All focuses are located in fronto-
central areas, over the anterior cingulate cortex (ACC).

Table 2: Percentages (mean and standard deviations) of correctly recognized error trials and correct trials for
the six subjects and the average of them. Table also show the BCI performance in terms of bit rate and its
increase using ErrP detection. Classiﬁcation rates are above 75% and 80% for error trials and correct trials,
respectively. The beneﬁt of integrating ErrP detection is obvious since it at least doubles the bit rate for ﬁve of
the six subjects.

Error
[%]

77.7 ± 13.9
75.4 ± 5.5
74.0 ± 12.9
84.3 ± 7.7
75.3 ± 6.0
70.7 ± 11.4
76.2 ± 4.6

[%]

Correct
76.8 ± 5.4
80.1 ± 7.9
85.9 ± 1.6
80.1 ± 5.5
85.6 ± 5.2
82.2 ± 5.1
81.8 ± 3.5

BCI accuracy [%]

(from Table 1)
73.8 ± 4.8
76.4 ± 6.4
69.5 ± 9.8
73.0 ± 9.4
72.7 ± 1.1
73.5 ± 6.3
73.1 ± 4.2

Bit rate [bits/trial]
(ErrP)
(no ErrP)
0.345
0.385
0.324
0.403
0.371
0.333
0.359

0.170
0.212
0.113
0.159
0.154
0.166
0.160

# 1
# 2
# 3
# 4
# 5
# 6
Avg

Increase

[%]
103
82
187
154
141
101
124

3.3 Estimation of intracranial activity

Estimating the neuronal sources that generate a given potential map at the scalp surface (EEG)
requires the solution of the so-called inverse problem. This inverse problem is always initially
undetermined, i.e.
there is no unique solution since a given potential map at the surface can be

6

generated by many different intracranial activity map. The inverse problem requires supplementary
a priori constraints in order to be univocally solved. The ultimate goal is to unmix the signals
measured at the scalp and to attribute to each brain area its own estimated temporal activity. The
sLORETA inverse model [7] is a standardized low resolution brain electromagnetic tomography.
This software, known for its zero localization error, was used as a localization tool to estimate
the focus of intracranial activity at the occurrence of the four ErrP peaks described in Section 3.2.
Figure 5 shows Talairach slices of localized activity for the grand average of the six subjects at the
occurrence of the four described peaks and at the occurrence of a late positive component showing
up 650 ms after the feedback. As expected, the areas involved in error processing, namely the
pre-supplementary motor area (pre-SMA, Brodmann area 6) and the rostral cingulate zone (RCZ,
Brodmann areas 24 & 32) are systematically activated [8][9]. For the second positive peak (350
ms) and mainly for the late positive component (650 ms), parietal areas are also activated. These
associative areas (somatosensory association cortex, Brodmann areas 5 & 7) could be related to
the fact that the subject becomes aware of the error. It has been proposed that the positive peak
was associated with conscious error recognition in case of error potentials elicited in reaction task
paradigm [13]. In our case, activation of parietal areas after 350 ms after the feedback agrees with
this hypothesis.

Figure 5: Talairach slices of localized activity for the grand average of the six subjects at the occurrence of
the four peaks described in Section 3.2 and at the occurrence of a late positive component showing up 650
ms after the feedback. Supplementary motor cortex and anterior cingulate cortex are systematically activated.
Furthermore, for the second positive peak (350 ms) and mainly for the late positive component (650 ms),
parietal areas are also activated. This parietal activation could reﬂect the fact that the subject is aware of the
error.

4 Discussion

In this study we have reported results on the detection of the neural correlate of error awareness for
improving the performance and reliability of BCI. In particular, we have conﬁrmed the existence of
a new kind of error-related potential elicited in reaction to an erroneous recognition of the subject’s
intention. More importantly, we have shown the feasibility of simultaneously and satisfactorily de-
tecting erroneous responses of the interface and classifying motor imagination for device control at
the level of single trials. However, the introduction of an automatic response rejection strongly inter-
feres with the BCI. The user needs to process additional information which induces higher workload
and may considerably slow down the interaction. These issues have to be investigated when running
online BCI experiments integrating automatic error detection. Given the promising results obtained
in this simulated human-robot interaction, we are currently working in the actual integration of on-
line ErrP detection into our BCI system. The preliminary results are very promising and conﬁrm
that the online detection of errors is a tool of great beneﬁt, especially for subjects with no prior
BCI experience or showing low BCI performance. In parallel, we are exploring how to increase the
recognition rate of single-trial erroneous and correct responses.

7

In this study we have also shown that, as expected, typical cortical areas involved in error process-
ing such as pre-supplementary motor area and anterior cingulate cortex are systematically activated
at the occurrence of the different peaks. The software used for the estimation of the intracranial
activity (sLORETA) is only a localization tool. However, Babiloni et al. [14] have recently devel-
oped the so-called CCD (“cortical current density”) inverse model that estimates the activity of the
cortical mantle. Since ErrP seems to be generated by cortical areas, we plan to use this method to
best discriminate erroneous and correct responses of the interface. As a matter of fact, a key issue to
improve classiﬁcation is the selection of the most relevant current dipoles out of a few thousands. In
fact, the very preliminary results using the CCD inverse model conﬁrm the reported localization in
the pre-supplementary motor area and in the anterior cingulate cortex and thus we may well expect
a signiﬁcant improvement in recognition rates by focusing on the dipoles estimated in those speciﬁc
brain areas.
More generally, the work described here suggests that it could be possible to recognize in real time
high-level cognitive and emotional states from EEG (as opposed, and in addition, to motor com-
mands) such as alarm, fatigue, frustration, confusion, or attention that are crucial for an effective and
purposeful interaction. Indeed, the rapid recognition of these states will lead to truly adaptive in-
terfaces that customize dynamically in response to changes of the cognitive and emotional/affective
states of the user.

References
[1] J.R. Wolpaw, N. Birbaumer, D.J. McFarland, G. Pfurtscheller, and T.M. Vaughan. Brain-computer inter-

faces for communication and control. Clinical Neurophysiology, 113:767–791, 2002.

[2] J. del R. Mill´an, F. Renkens, J. Mouri˜no, and W. Gerstner. Non-invasive brain-actuated control of a mobile

robot by human EEG. IEEE Transactions on Biomedical Engineering, 51:1026–1033, 2004.

[3] G. Schalk, J.R. Wolpaw, D.J. McFarland, and G. Pfurtscheller. EEG-based communication: presence of

and error potential. Clinical Neurophysiology, 111:2138–2144, 2000.

[4] B. Blankertz, G. Dornhege, C. Sch¨afer, R. Krepki, J. Kohlmorgen, K.-R. M¨uller, V. Kunzmann, F. Losch,
and G. Curio. Boosting bit rates and error detection for the classiﬁcation of fast-paced motor commands
based on single-trial EEG analysis. IEEE Transactions on Neural Systems and Rehabilitation Engineer-
ing, 11(2):127–131, 2003.

[5] L.C. Parra, C.D. Spence, A.D. Gerson, and P. Sajda. Response error correction—a demonstration of
improved human-machine performance using real-time EEG monitoring. IEEE Transactions on Neural
Systems and Rehabilitation Engineering, 11(2):173–177, 2003.

[6] P.W. Ferrez and J. del R. Mill´an. You are wrong!—Automatic detection of interaction errors from brain

waves. In Proc. 19th Int. Joint Conf. Artiﬁcial Intelligence, 2005.

[7] R.D. Pascual-Marqui. Standardized low resolution brain electromagnetic tomography (sLORETA): Tech-

nical details. Methods & Findings in Experimental & Clinical Pharmacology, 24D:5–12, 2002.

[8] C.B. Holroyd and M.G.H. Coles. The neural basis of human error processing: Reinforcement learning,

dopamine and the error-related negativity. Psychological Review, 109:679–709, 2002.

[9] K. Fiehler, M. Ullsperger, and Y. von Cramon. Neural correlates of error detection and error correction:
Is there a common neuroanatomical substrate? European Journal of Neuroscience, 19:3081–3087, 2004.
[10] P.W. Ferrez and J. del R. Mill´an. Error-related EEG potentials in brain-computer interfaces. In G. Dorn-
hege, J. del R. Mill´an, T. Hinterberger, D. McFarland, and K.-R. M¨uller, editors, Toward Brain-Computing
Interfacing, pages 291–301. The MIT Press, 2007.

[11] D. McFarland and J.R. Wolpow. Sensorimotor rhythm-based brain-computer interface (BCI): Feature
selection by regression improves performance. IEEE Transactions on Neural Systems and Rehabilitation
Engineering, 13(3):372–379, 2005.

[12] G. Pfurtscheller and F.H. Lopes da Silva. Event-related EEG/MEG synchronization and desynchroniza-

tion: Basic principles. Clinical Neurophysiology, 110:1842–1857, 1999.

[13] S. Nieuwenhuis, K.R. Ridderinkhof, J. Blom, G.P.H. Band, and A. Kok. Error-related brain potentials are
differently related to awareness of response errors: Evidence from an antisaccade task. Psychophysiology,
38:752–760, 2001.

[14] F. Babiloni, C. Babiloni, L. Locche, F. Cincotti, P.M. Rossini, and F. Carducci. High-resolution electro-
encephalogram: Source estimates of laplacian-transformed somatosensory-evoked potentials using realis-
tic subject head model constructed from magnetic resonance imaging. Medical & Biological Engineering
and Computing, 38:512–519, 2000.

8

"
331,2007,The Infinite Gamma-Poisson Feature Model,"We address the problem of factorial learning which associates a set of latent causes or features with the observed data. Factorial models usually assume that each feature has a single occurrence in a given data point. However, there are data such as images where latent features have multiple occurrences, e.g. a visual object class can have multiple instances shown in the same image. To deal with such cases, we present a probability model over non-negative integer valued matrices with possibly unbounded number of columns. This model can play the role of the prior in an nonparametric Bayesian learning scenario where both the latent features and the number of their occurrences are unknown. We use this prior together with a likelihood model for unsupervised learning from images using a Markov Chain Monte Carlo inference algorithm.","The Inﬁnite Gamma-Poisson Feature Model

Michalis K. Titsias

School of Computer Science,
University of Manchester, UK
mtitsias@cs.man.ac.uk

Abstract

We present a probability distribution over non-negative integer valued matrices
with possibly an inﬁnite number of columns. We also derive a stochastic process
that reproduces this distribution over equivalence classes. This model can play
the role of the prior in nonparametric Bayesian learning scenarios where multiple
latent features are associated with the observed data and each feature can have
multiple appearances or occurrences within each data point. Such data arise nat-
urally when learning visual object recognition systems from unlabelled images.
Together with the nonparametric prior we consider a likelihood model that ex-
plains the visual appearance and location of local image patches. Inference with
this model is carried out using a Markov chain Monte Carlo algorithm.

1 Introduction

Unsupervised learning using mixture models assumes that one latent cause is associated with each
data point. This assumption can be quite restrictive and a useful generalization is to consider factorial
representations which assume that multiple causes have generated the data [11]. Factorial models
are widely used in modern unsupervised learning algorithms; see e.g. algorithms that model text
data [2, 3, 4]. Algorithms for learning factorial models should deal with the problem of specifying
the size of the representation. Bayesian learning and especially nonparametric methods such as the
Indian buffet process [7] can be very useful for solving this problem.

Factorial models usually assume that each feature occurs once in a given data point. This is inef-
ﬁcient to model the precise generation mechanism of several data such as images. An image can
contain views of multiple object classes such as cars and humans and each class may have multiple
occurrences in the image. To deal with features having multiple occurrences, we introduce a prob-
ability distribution over sparse non-negative integer valued matrices with possibly an unbounded
number of columns. Each matrix row corresponds to a data point and each column to a feature
similarly to the binary matrix used in the Indian buffet process [7]. Each element of the matrix
can be zero or a positive integer and expresses the number of times a feature occurs in a speciﬁc
data point. This model is derived by considering a ﬁnite gamma-Poisson distribution and taking
the inﬁnite limit for equivalence classes of non-negative integer valued matrices. We also present a
stochastic process that reproduces this inﬁnite model. This process uses the Ewens’s distribution [5]
over integer partitions which was introduced in population genetics literature and it is equivalent to
the distribution over partitions of objects induced by the Dirichlet process [1].

The inﬁnite gamma-Poisson model can play the role of the prior in a nonparametric Bayesian learn-
ing scenario where both the latent features and the number of their occurrences are unknown. Given
this prior, we consider a likelihood model which is suitable for explaining the visual appearance and
location of local image patches. Introducing a prior for the parameters of this likelihood model, we
apply Bayesian learning using a Markov chain Monte Carlo inference algorithm and show results in
some image data.

2 The ﬁnite gamma-Poisson model

Let X = {X1, . . . , XN } be some data where each data point Xn is a set of attributes. In section
4 we specify Xn to be a collection of local image patches. We assume that each data point is
associated with a set of latent features and each feature can have multiple occurrences. Let znk
denote the number of times feature k occurs in the data point Xn. Given K features, Z = {znk} is
a N × K non-negative integer valued matrix that collects together all the znk values so as each row
corresponds to a data point and each column to a feature. Given that znk is drawn from a Poisson
with a feature-speciﬁc parameter λk, Z follows the distribution

P (Z|{λk}) =

NYn=1

KYk=1

λznk
k

exp{−λk}
znk!

=

KYk=1

λmk
k

exp{−N λk}
n=1 znk!

QN

,

(1)

that favors sparsity (in a sense that will be explained shortly):

n=1 znk. We further assume that each λk parameter follows a gamma distribution

where mk =PN

G(λk;

λ

, 1) =

α
K

α
K −1
k

exp{−λk}
Γ( α
K )

.

(2)

The hyperparameter α itself is given a vague gamma prior G(α; α0, β0). Using the above equations
we can easily integrate out the parameters {λk} as follows

P (Z|α) =

KYk=1

Γ(mk + α
K )
K )(N + 1)mk+ α

Γ( α

n=1 znk!

K QN

,

(3)

which shows that given the hyperparameter α the columns of Z are independent. Note that the above
distribution is exchangeable since reordering the rows of Z does not alter the probability. Also as
K increases the distribution favors sparsity. This can be shown by taking the expectation of the sum
n=1 E(znk) and

of all elements of Z. Since the columns are independent this expectation is KPN

E(znk) is given by

E(znk) =

znkN B(znk;

α
K

,

1
2

) =

α
K

,

∞Xznk=0

(4)

where N B(znk; r, p), with r > 0 and 0 < p < 1, denotes the negative binomial distribution over
positive integers

N B(znk; r, p) =

Γ(r + znk)
znk!Γ(r)

pr(1 − p)znk ,

(5)

that has a mean equal to r(1−p)
. Using Equation (4) the expectation of the sum of znks is αN and
is independent of the number of features. As K increases, Z becomes sparser and α controls the
sparsity of this matrix.

p

There is an alternative way of deriving the joint distribution P (Z|α) according to the following
generative process:

(θ1, . . . , θK) ∼ D(cid:16) α
Ln ∼ P oisson(λ), (zn1, . . . , znK) ∼(cid:18)

K(cid:17) , λ ∼ G(λ; α, 1),
zn1 . . . znK(cid:19) KYk=1

Ln

θznk
k

, n = 1, . . . , N,

K ) denotes the symmetric Dirichlet. Marginalizing out θ and λ gives rise to the same
where D( α
distribution P (Z|α). The above process generates a gamma random variable and multinomial pa-
rameters and then samples the rows of Z independently by using the Poisson-multinomial pair. The
connection with the Dirichlet-multinomial pair implies that the inﬁnite limit of the gamma-Poisson
model must be related to the Dirichlet process. In the next section we see how this connection is
revealed through the Ewens’s distribution [5].

Models that combine gamma and Poisson distributions are widely applied in statistics. We point out
that the above ﬁnite model shares similarities with the techniques presented in [3, 4] that model text
data.

3 The inﬁnite limit and the stochastic process

To express the probability distribution in (3) for inﬁnite many features K we need to consider equiv-
alence classes of Z matrices similarly to [7]. The association of columns in Z with features deﬁnes
an arbitrary labelling of the features. Given that the likelihood p(X|Z) is not affected by relabelling
the features, there is an equivalence class of matrices that all can be reduced to the same standard
form after column reordering. We deﬁne the left-ordered form of non-negative integer valued ma-
trices as follows. We assume that for any possible znk holds znk ≤ c − 1, where c is a sufﬁciently
large integer. We deﬁne h = (z1k . . . zN k) as the integer number associated with column k that is
expressed in a numeral system with basis c. The left-ordered form is deﬁned so as the columns of Z
appear from left to right in a decreasing order according to the magnitude of their numbers.

Starting from Equation (3) we wish to deﬁne the probability distribution over matrices constrained in
a left-ordered standard form. Let Kh be the multiplicity of the column with number h; for example
K0 is the number of zero columns. An equivalence class [Z] consists of
different matri-
ces that they are generated from the distribution in (3) with equal probabilities and can be reduced
to the same left-ordered form. Thus, the probability of [Z] is

K!
PcN −1

h=0 Kh!

P ([Z]) =

K!

PcN −1

h=0 Kh!

KYk=1

Γ(mk + α
K )
K )(N + 1)mk+ α

Γ( α

n=1 znk!

K QN

We assume that the ﬁrst K+ features are represented i.e. mk > 0 for k ≤ K+, while the rest K −K+
features are unrepresented i.e. mk = 0 for k > K+. The inﬁnite limit of (6) is derived by following
a similar strategy with the one used for expressing the distribution over partitions of objects as a
limit of the Dirichlet-multinomial pair [6, 9]. The limit takes the following form:

.

(6)

P (Z|α) =

1

h=1 Kh!

PcN −1

αK+

(N + 1)m+α QK+
QK+
k=1QN

k=1(mk − 1)!
n=1 znk!

where m =PK+

k=1 mk. This expression deﬁnes an exchangeable joint distribution over non-negative
integer valued matrices with inﬁnite many columns in a left-ordered form. Next we present a se-
quential stochastic process that reproduces this distribution.

,

(7)

3.1 The stochastic process

The distribution in Equation (7) can be derived from a simple stochastic process that constructs
the matrix Z sequentially so as the data arrive one at each time in a ﬁxed order. The steps of this
stochastic process are discussed below.

When the ﬁrst data point arrives all the features are currently unrepresented. We sample feature
occurrences from the set of unrepresented features as follows. Firstly, we draw an integer number
g1 from the negative binomial N B(g1; α, 1
2 ) which has a mean value equal to α. g1 is the total
number of feature occurrences for the ﬁrst data point. Given g1, we randomly select a partition
(z11, . . . , z1K1 ) of the integer g1 into parts1, i.e. z11 + . . . + z1K1 = g1 and 1 ≤ K1 ≤ g1, by
drawing from Ewens’s distribution [5] over integer partitions which is given by

P (z11, . . . , z1K1 ) = αK1

Γ(α)

g1!

Γ(g1 + α)

z11 × . . . × z1K1

g1Yi=1

1
v(1)

i

!

,

(8)

i

where v(1)
is the multiplicity of integer i in the partition (z11, . . . , z1K1 ). The Ewens’s distribution
is equivalent to the distribution over partitions of objects induced by the Dirichlet process and the
Chinese restaurant process since we can derive the one from the other using simple combinatorics
arguments. The difference between them is that the former is a distribution over integer partitions
while the latter is a distribution over partitions of objects.
Let Kn−1 be the number of represented features when the nth data point arrives. For each feature
k, with k ≤ Kn−1, we choose znk based on the popularity of this feature in the previous n − 1 data

1The partition of a positive integer is a way of writing this integer as a sum of positive integers where order

does not matter, e.g. the partitions of 3 are: (3),(2,1) and (1,1,1).

i=1 zik. Particularly, we draw znk from N B(znk; mk, n

given by mk =Pn−1

points. This popularity is expressed by the total number of occurrences for the feature k which is
n+1 ) which has a mean
value equal to mk
n . Once we have sampled from all represented features we need to consider a
sample from the set of unrepresented features. Similarly to the ﬁrst data point, we ﬁrst draw an
n+1 ), and subsequently we select a partition of that integer by drawing
integer gn from N B(gn; α, n
from the Ewens’s formula. This process produces the following distribution:

P (Z|α) =

1

i=1 v(1)

i

Qg1

! × . . . ×QgN

i=1 v(N )

i

!

αK+

(N + 1)m+α QK+
QK+
k=1QN

k=1(mk − 1)!
n=1 znk!

,

(9)

where {v(n)
i } are the integer-multiplicities for the nth data point which arise when we draw from
the Ewens’s distribution. Note that the above expression does not have exactly the same form as the
distribution in Equation (7) and is not exchangeable since it depends on the order the data arrive.
However, if we consider only the left-ordered class of matrices generated by the stochastic process
then we obtain the exchangeable distribution in Equation (7). Note that a similar situation arises
with the Indian buffet process.

3.2 Conditional distributions

When we combine the prior P (Z|α) with a likelihood model p(X|Z) and we wish to do in-
ference over Z using Gibbs-type sampling, we need to express the conditionals of the form
P (znk|Z−(nk), α) where Z−(nk) = Z \ znk. We can derive such conditionals by taking limits
of the conditionals for the ﬁnite model or by using the stochastic process.
Suppose that for the current value of Z, there exist K+ represented features i.e. mk > 0 for

k ≤ K+. Let m−n,k = Pen6=n zenk. When m−n,k > 0, the conditional of znk is given by

In all different cases, we need a special conditional that samples from
N B(znk; m−n,k, N
new features2 and accounts for all k such that m−n,k = 0. This conditional draws an integer num-
ber from N B(gn; a, N
N +1 ) and then determines the occurrences for the new features by choosing a
partition of the integer gn using the Ewens’s distribution. Finally the conditional p(α|Z), which can
be directly expressed from Equation (7) and the prior of α, is given by

N +1 ).

p(α|Z) ∝ G(α; α0, β0)

αK+

(N + 1)α .

(10)

Typically the likelihood model does not depend on α and thus the above quantity is also the posterior
conditional of α given data and Z.

4 A likelihood model for images

An image can contain multiple objects of different classes. Each object class can have more than
one occurrences, i.e. multiple instances of the class may appear simultaneously in the image. Un-
supervised learning should deal with the unknown number of object classes in the images and also
the unknown number of occurrences of each class in each image separately. If object classes are the
latent features, what we wish to infer is the underlying feature occurrence matrix Z. We consider
an observation model that is a combination of latent Dirichlet allocation [2] and Gaussian mixture
models. Such a combination has been used before [12]. Each image n is represented by dn local
patches that are detected in the image so as Xn = (Yn, Wn) = {(yni, wni), i = 1, . . . , dn}. yni
is the two-dimensional location of patch i and wni is an indicator vector (i.e. is binary and satisﬁes
ni = 1) that points into a set of L possible visual appearances. X, Y , and W denote all
the data the locations and the appearances, respectively. We will describe the probabilistic model
starting from the joint distribution of all variables which is given by

PL

ℓ=1 wℓ

joint = p(α)P (Z|α)p({θk}|Z)×

NYn=1""p(πn|Zn)p(mn, Σn|Zn)

dnYi=1

P (sni|πn)P (wni|sni, {θk})p(yni|sni, mn, Σn)# .

(11)

2Features of this kind are the unrepresented features (k > K+) as well as all the unique features that occur

only in the data point n (i.e. m−n,k = 0, but znk > 0).

Z

α

{θk}

πn

(mn, Σn)

sni

wni

yni

dn

N

Figure 1: Graphical model for the joint distribution in Equation (11).

The graphical representation of this distribution is depicted in Figure 1. We now explain all the
pieces of this joint distribution following the causal structure of the graphical model. Firstly, we
generate α from its prior and then we draw the feature occurrence matrix Z using the inﬁnite
gamma-Poisson prior P (Z|α). The matrix Z deﬁnes the structure for the remaining part of the
model. The parameter vector θk = {θk1, . . . , θkL} describes the appearance of the local patches W
for the feature (object class) k. Each θk is generated from a symmetric Dirichlet so as the whole
k=1 D(θk|γ), where γ is the hyperparameter of
the symmetric Dirichlet and it is common for all features. Note that the feature appearance param-
eters {θk} depend on Z only through the number of represented features K+ which is obtained by
counting the non-zero columns of Z.
The parameter vector πn = {πnkj} deﬁnes the image-speciﬁc mixing proportions for the mixture
model associated with image n. To see how this mixture model arises, notice that a local patch in
image n belongs to a certain occurrence of a feature. We use the double index kj to denote the j

set of {θk} vectors is drawn from p({θk}|Z) =QK+

occurrence of feature k where j = 1, . . . , znk and k ∈ {ek : znek > 0}. This mixture model has
Mn =PK+

k=1 znk components, i.e. as many as the total number of feature occurrences in image n.
The assignment variable sni = {skj
ni}, which takes Mn values, indicates the feature occurrence of
patch i. πn is drawn from a symmetric Dirichlet given by p(πn|Zn) = D(πn|β/Mn), where Zn
denotes the nth row of Z and β is a hyperparameter shared by all images. Notice that πn depends
only on the nth row of Z.
The parameters (mn, Σn) determine the image-speciﬁc distribution for the locations {yni} of the
local patches in image n. We assume that each occurrence of a feature forms a Gaussian cluster
of patch locations. Thus yni follows a image-speciﬁc Gaussian mixture with Mn components. We
assume that the component kj has mean mnkj and covariance Σnkj. mnkj describes object location
and Σnkj object shape. mn and Σn collect all the means and covariances of the clusters in the image
n. Given that any object can be anywhere in the image and have arbitrary scale and orientation,
(mnkj, Σnkj) should be drawn from a quite vague prior. We use a conjugate normal-Wishart prior
for the pair (mnkj, Σnkj) so as

p(mn, Σn|Zn) = Yk:znk>0

znkYj=1

N (mnkj|µ, τ Σnkj)W (Σ−1

nkj|v, V ),

(12)

where (µ, τ, v, V ) are the hyperparameters shared by all features and images. The assignment sni
which determines the allocation of a local patch in a certain feature occurrence follows a multino-
ni. Similarly the observed data pair (wni, yni) of a

j=1(πnkj)skj

mial: P (sni|πn) = Qk:znk>0Qznk

local image patch is generated according to

P (wni|sni, {θk}) =

K+Yk=1

LYℓ=1

θ

ni Pznk
wℓ
kℓ

j=1 skj

ni

and

p(yni|sni, mn, Σn) = Yk:znk>0

znkYj=1

[N (yni|mnkj, Σnkj)]skj
ni .

The hyperparameters (γ, β, µ, τ, v, V ) take ﬁxed values that give vague priors and they are not
depicted in the graphical model shown in Figure 1.

Since we have chosen conjugate priors, we can analytically marginalize out from the joint distri-
bution all the parameters {πn}, {θk}, {mn} and {Σn} and obtain p(X, S, Z, α). Marginalizing
out the assignments S is generally intractable and the MCMC algorithm discussed next produces
samples from the posterior P (S, Z, α|X).

4.1 MCMC inference

Inference with our model involves expressing the posterior P (S, Z, α|X) over the feature occur-
rences Z, the assignments S and the parameter α. Note that the joint P (S, Z, α, X) factorizes
n=1 P (Sn|Zn)p(Yn|Sn, Zn) where Sn denotes the assign-
ments associated with image n. Our algorithm uses mainly Gibbs-type sampling from conditional
posterior distributions. Due to space limitations we brieﬂy discuss the main points of this algorithm.

according to p(α)P (Z|α)P (W |S, Z)QN

nk − zold

nk | ≤ 1. Initially Z is such that Mn =PK+

The MCMC algorithm processes the rows of Z iteratively and updates its values. A single step can
change an element of Z by one so as |znew
k=1 znk ≥
1, for any n which means that at least one mixture component explains the data of each image. The
proposal distribution for changing znks ensures that this constraint is satisﬁed.
Suppose we wish to sample a new value for znk using the joint model p(S, Z, α, X). Simply witting
P (znk|S, Z−(nk), α, X) is not useful since when znk changes the number of states the assignments
Sn can take also changes. This is clear since znk is a structural variable that affects the number of
k=1 znk of the mixture model associated with image n and assignments Sn.
On the other hand the dimensionality of the assignments S−n = S \ Sn of all other images is not
affected when znk changes. To deal with the above we marginalize out Sn and we sample znk from
the marginalized posterior conditional P (znk|S−n, Z−(nk), α, X) which is computed according to

components Mn =PK+
P (znk|S−n, Z−(nk), α, X) ∝ P (znk|Z−(nk), α)XSn

P (W |S, Z)p(Yn|Sn, Zn)P (Sn|Zn),

(13)

where P (znk|Z−n,k, α) for the inﬁnite case is computed as described in section 3.2 while computing
the sum requires an approximation. This sum is a marginal likelihood and we apply importance
sampling using as an importance distribution the posterior conditional P (Sn|S−n, Z, W, Yn) [10].
Sampling from P (Sn|S−n, Z, W, Yn) is carried out by applying local Gibbs sampling moves and
global Metropolis moves that allow two occurrences of different features to exchange their data
clusters. In our implementation we consider a single sample drawn from this posterior distribution
n is a sample accepted
so that the sum is approximated by P (W |S∗
after a burn in period. Additionally to scans that update Z and S we add few Metropolis-Hastings
steps that update the hyperparameter α using the posterior conditional given by Equation (10).

n, S−n, Z)p(Yn|S∗

n, Zn) and S∗

5 Experiments

In the ﬁrst experiment we use a set of 10 artiﬁcial images. We consider four features that have
the regular shapes shown in Figure 2. The discrete patch appearances correspond to pixels and
can take 20 possible grayscale values. Each feature has its own multinomial distribution over the
appearances. To generate an image we ﬁrst decide to include each feature with probability 0.5.
Then for each included feature we randomly select the number of occurrences from the range [1, 3].
For each feature occurrence we select the pixels using the appearance multinomial and place the
respective feature shape in a random location so that feature occurrences do not occlude each other.
The ﬁrst row of Figure 2 shows a training image (left), the locations of pixels (middle) and the
discrete appearances (right). The MCMC algorithm was initialized with K+ = 1, α = 1 and
zn1 = 1, n = 1, . . . , 10. The third row of Figure 2 shows how K+ (left) and the sum of all znks
(right) evolve through the ﬁrst 500 MCMC iterations. The algorithm in the ﬁrst 20 iterations has

training image n

locations Yn

appearances Wn

1 3 3 1

3 2 3 0

0 2 1 2

Figure 2: The ﬁrst row shows a training image (left), the locations of pixels (middle) and the discrete
appearances (right). The second row shows the localizations of all feature occurrences in three
images. Below of each image the corresponding row of Z is also shown. The third row shows how
K+ (left) and the sum of all znks (right) evolve through the ﬁrst 500 MCMC iterations.

Figure 3: The left most plot on the ﬁrst row shows the locations of detected patches and the bounding
boxes in one of the annotated images. The remaining ﬁve plots show examples of detections and
localizations of the three most dominant features (including the car-category) in ﬁve non-annotated
images.

visited the matrix Z that was used to generate the data and then stabilizes. For 86% of the samples
K+ is equal to four. For the state (Z, S) that is most frequently visited, the second row of Figure
2 shows the localizations of all different feature occurrences in three images. Each ellipse is drawn
using the posterior mean values for a pair (mnkj, Σnkj) and illustrates the predicted location and
shape of a feature occurrence. Note that ellipses with the same color correspond to the different
occurrences of the same feature.
In the second experiment we consider 25 real images from the UIUC3 cars database. We used the
patch detection method presented in [8] and we constructed a dictionary of 200 visual appearances
by clustering the SIFT [8] descriptors of the patches using K-means. Locations of detected patches
are shown in the ﬁrst row (left) of Figure 3. We partially labelled some of the images. Particularly,
for 7 out of 25 images we annotated the car views using bounding boxes (Figure 3). This allows
us to specify seven elements of the ﬁrst column of the matrix Z (the ﬁrst feature will correspond
to the car-category). These znks values plus the assignments of all patches inside the boxes do not
change during sampling. Also the patches that lie outside the boxes in all annotated images are not
allowed to be part of car occurrences. This is achieved by applying partial Gibbs sampling updates
and Metropolis moves when sampling the assignments S. The algorithm is initialized with K+ = 1,
after 30 iterations stabilizes and then ﬂuctuates between nine to twelve features. To keep the plots
uncluttered, Figure 3 shows the detections and localizations of only the three most dominant features
(including the car-category) in ﬁve non-annotated images. The red ellipses correspond to different
occurrences of the car-feature, the green ones to a tree-feature and the blue ones to a street-feature.

6 Discussion

We presented the inﬁnite gamma-Poisson model which is a nonparametric prior for non-negative
integer valued matrices with inﬁnite number of columns. We discussed the use of this prior for
unsupervised learning where multiple features are associated with our data and each feature can
have multiple occurrences within each data point. The inﬁnite gamma-Poisson prior can be used for
other purposes as well. For example, an interesting application can be Bayesian matrix factorization
where a matrix of observations is decomposed into a product of two or more matrices with one of
them being a non-negative integer valued matrix.

References

[1] C. Antoniak. Mixture of Dirichlet processes with application to Bayesian nonparametric problems. The

Annals of Statistics, 2:1152–1174, 1974.

[2] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent Dirichlet allocation. JMLR, 3, 2003.
[3] W. Buntime and A. Jakulin. Applying discrete PCA in data analysis. In UAI, 2004.
[4] J. Canny. GaP: A factor model for discrete data. In SIGIR, pages 122–129. ACM Press, 2004.
[5] W. Ewens. The sampling theory of selectively neutral alleles. Theoretical Population Biology, 3:87–112,

1972.

[6] P. Green and S. Richardson. Modelling heterogeneity with and without the Dirichlet process. Scandina-

vian Journal of Statistics, 28:355–377, 2001.

[7] T. Grifﬁths and Z. Ghahramani. Inﬁnite latent feature models and the Indian buffet process. In NIPS 18,

2006.

[8] D. G. Lowe. Distinctive image features from scale-invariant keypoints. International Journal of Computer

Vision, 60(2):91–110, 2004.

[9] R. M. Neal. Bayesian mixture modeling.

In 11th International Workshop on Maximum Entropy and

Bayesian Methods of Statistical Analysis, pages 197–211, 1992.

[10] M. A. Newton and A. E Raftery. Approximate Bayesian inference by the weighted likelihood bootstrap.

Journal of the Royal Statistical Society, Series B, 3:3–48, 1994.

[11] E. Saund. A multiple cause mixture model for unsupervised learning. Neural Computation, 7:51–71,

1995.

[12] E. Sudderth, A. Torralba, W. T. Freeman, and A. Willsky. Describing Visual Scenes using Transformed

Dirichlet Processes. In NIPS 18, 2006.

3available from http://l2r.cs.uiuc.edu/∼cogcomp/Data/Car/.

"
557,2007,Continuous Time Particle Filtering for fMRI,"We construct a biologically motivated stochastic differential model of the neural and hemodynamic activity underlying the observed Blood Oxygen Level Dependent (BOLD) signal in Functional Magnetic Resonance Imaging (fMRI). The model poses a difficult parameter estimation problem, both theoretically due to the nonlinearity and divergence of the differential system, and computationally due to its time and space complexity. We adapt a particle filter and smoother to the task, and discuss some of the practical approaches used to tackle the difficulties, including use of sparse matrices and parallelisation. Results demonstrate the tractability of the approach in its application to an effective connectivity study.","Continuous Time Particle Filtering for fMRI

Lawrence Murray
School of Informatics
University of Edinburgh

lawrence.murray@ed.ac.uk

Amos Storkey

School of Informatics
University of Edinburgh
a.storkey@ed.ac.uk

Abstract

We construct a biologically motivated stochastic differential model of the neu-
ral and hemodynamic activity underlying the observed Blood Oxygen Level De-
pendent (BOLD) signal in Functional Magnetic Resonance Imaging (fMRI). The
model poses a difﬁcult parameter estimation problem, both theoretically due to the
nonlinearity and divergence of the differential system, and computationally due to
its time and space complexity. We adapt a particle ﬁlter and smoother to the task,
and discuss some of the practical approaches used to tackle the difﬁculties, includ-
ing use of sparse matrices and parallelisation. Results demonstrate the tractability
of the approach in its application to an effective connectivity study.

1 Introduction
Functional Magnetic Resonance Imaging (fMRI) poses a large-scale, noisy and altogether difﬁcult
problem for machine learning algorithms. The Blood Oxygen Level Dependent (BOLD) signal,
from which fMR images are produced, is a measure of hemodynamic activity in the brain – only an
indirect indicator of the neural processes which are of primary interest in most cases.
For studies of higher level patterns of activity, such as effective connectivity [1], it becomes neces-
sary to strip away the hemodynamic activity to reveal the underlying neural interactions. In the ﬁrst
instance, this is because interactions between regions at the neural level are not necessarily evident at
the hemodynamic level [2]. In the second, analyses increasingly beneﬁt from the temporal qualities
of the data, and the hemodynamic response itself is a form of temporal blurring.
We are interested in the application of machine learning techniques to reveal meaningful patterns
of neural activity from fMRI. In this paper we construct a model of the processes underlying the
BOLD signal that is suitable for use in a ﬁltering framework. The model proposed is close to that
of Dynamic Causal Modelling (DCM) [3]. The main innovation over these deterministic models is
the incorporation of stochasticity at all levels of the system. This is important; under ﬁxed inputs,
DCM reduces to a generative model with steady state equilibrium BOLD activity and independent
noise at each time point. Incorporating stochasticity allows proper statistical characterisation of the
dependence between brain regions, rather than relying on relating decay rates1.
Our work has involved applying a number of ﬁltering techniques to estimate the parameters of
the model, most notably the Unscented Kalman Filter [4] and various particle ﬁltering techniques.
This paper presents the application of a simple particle ﬁlter. [5] take a similar ﬁltering approach,
applying a local linearisation ﬁlter [6] to a model of individual regions. In contrast, the approach
here is applied to multiple regions and their interactions, not single regions in isolation.
Other approaches to this type of problem are worth noting. Perhaps the most commonly used tech-
nique to date is Structural Equation Modelling (SEM) [7; 8] (e.g. [9; 10; 11]). SEM is a multivariate
1A good analogy is the fundamental difference between modelling time series data yt using an exponentially
decaying curve with observational noise xt = axt−1 +c, yt = xt +t, and using a much more ﬂexible Kalman
ﬁlter xt = axt−1 + c + ωt, yt = xt + t (where xt is a latent variable, a a decay constant, c a constant and 
and ω Gaussian variables).

1

regression technique where each dependent variable may be a linear combination of both indepen-
dent and other dependent variables. Its major limitation is that it is static, assuming that all observa-
tions are temporally independent and that interactions are immediate and wholly evident within each
single observation. Furthermore, it does not distinguish between neural and hemodynamic activity,
and in essence identiﬁes interactions only at the hemodynamic level.
The major contributions of this paper are establishing a stochastic model of latent neural and hemo-
dynamic activity, formulating a ﬁltering and smoothing approach for inference in this model, and
overcoming the basic practical difﬁculties associated with this. The estimated neural activity relates
to the domain problem and is temporally consistent with the stimulus. The approach is also able to
establish connectivity relationships.
The ability of this model to establish such connectivity relationships on the basis of stochastic tem-
poral relationships is signiﬁcant. One problem in using structural equation models for effective
connectivity analysis is the statistical equivalence of different causal models. By presuming a tem-
poral causal order, temporal models of this form have no such equivalence problems. Any small
amount of temporal connectivity information available in fMRI data is of signiﬁcant beneﬁt, as it
can disambiguate between statically equivalent models.
Section 2 outlines the basis of the hemodynamic model that is used. This is combined with neural,
input and measurement models in Section 3 to give the full framework. Inference and parameter
estimation are discussed in Section 4, before experiments and analysis in Sections 5 and 6.
2 Hemodynamics
Temporal analysis of fMRI is signiﬁcantly confounded by the fact that it does not measure brain
activity directly, but instead via hemodynamic activity, which (crudely) temporally smooths the
activity signal. The quality of temporal analysis therefore depends signiﬁcantly on the quality of
model used to relate neural and hemodynamic activity.
This relationship may be described using the now well established Balloon model [12]. This models
a venous compartment as a balloon using Windkessel dynamics. The state of the compartment is
represented by its blood volume normalised to the volume at rest, v = V /V0 (blood volume V , rest
volume V0), and deoxyhemoglobin (dHb) content normalised to the content at rest, q = Q/Q0 (dHb
content Q, rest content Q0). The compartment receives inﬂow of fully oxygenated arterial blood
fin(t), extracts oxygen from the blood, and expels partially deoxygenated blood fout(t). The full
dynamics may be represented by the differential system:
fin(t) E(t)
E0
[fin(t) − fout(v)]

− fout(v) q
v

(cid:184)

(cid:183)

(1)

(2)

=

1
τ0
1
τ0

dq
dt
dv
dt
E(t) ≈ 1 − (1 − E0) 1
fout(v) ≈ v

=

1
α

fin(t)

(3)
(4)

where τ0 and α are constants, and E0 is the oxygen extraction fraction at rest.
This base model is driven by the independent input fin(t). It may be further extended to couple in
neural activity z(t) via an abstract vasodilatory signal s [13]:

df
dt
ds
dt

= s
= z(t) − s
τs

− (f − 1)

τf

.

(5)

(6)

The complete system deﬁned by Equations 1-6, with fin(t) = f, is now driven by the independent
input z(t). From the balloon model, the relative BOLD signal change over the baseline S at any
time may be predicted using [12]:

(cid:179)

(cid:180)

(cid:105)

(cid:104)

∆S
S

= V0

k1(1 − q) + k2

1 − q
v

+ k3(1 − v)

.

(7)

Figure 1 illustrates the system dynamics. Nominal values for constants are given in Table 1.

2

Figure 1: Response of the balloon model to a 1s burst of neural activity at magnitude 1 (time on x
axis, response on y axis).

3 Model
We deﬁne a model of the neural and hemodynamic interactions between M regions of interest. A
region consists of neural tissue and a venous compartment. The state xi(t) of region i at time t is
given by:



xi(t) =

neural activity

zi(t)
fi(t) normalised blood ﬂow into the venous compartment
si(t)
qi(t)
vi(t)

vasodilatory signal
normalised dHb content of the venous compartment
normalised blood volume of the venous compartment

Input model

The complete state at time t is given by x(t) = (x1(t)T , . . . , xM (t)T )T .
We construct a model of the interactions between regions in four parts – the input model, the neural
model, the hemodynamic model and the measurement model.
3.1
The input model represents the stimulus associated with the experimental task during an fMRI ses-
sion. In general this is a function u(t) with U dimensions. For a simple block design paradigm a
one-dimensional box-car function is sufﬁcient.
3.2 Neural model
Neural interactions between the regions are given by:

dz = Az dt + Cu dt + c + Σz dW,

(8)
where dW is the M-dimensional standard (zero mean, unit variance) Wiener process, A an M × M
matrix of efﬁcacies between regions, C an M × U matrix of efﬁcacies between inputs and regions,
c an M-dimensional vector of constant terms and Σz an M × M diagonal diffusion matrix with
σz1, . . . , σzM along the diagonal.
This is similar to the deterministic neural model of DCM expressed as a stochastic differential equa-
tion, but excludes the bilinear components allowing modulation of connections between seeds. In
theory these can be added, we simply limit ourselves to a simpler model for this early work. In
addition, and unlike DCM, nonlinear interactions between regions could also be included to account
for modulatory activity. Again it seems sensible to keep the simplest linear case at this stage of
the work, but the potential for nonlinear generalisation is one of the longer term beneﬁts of this
approach.
3.3 Hemodynamic model
Within each region, the variables fi, si, qi, vi and zi interact according to a stochastic extension of
the balloon model (c.f. Equations 1-6). It is assumed that regions are sufﬁciently separate that their

Constant
Value

τ0
0.98

τf
1/0.65

τs
1/0.41

α
0.32


0.8

V0 E0
0.4

0.018

k1
7E0

k2
2

k3
2E0 − 0.2

Table 1: Nominal values for constants of the balloon model [12; 13].

3

 0.84 1.04 0 30q 0.9 1.35 0 30v 0.8 1.9 0 30f-0.3 0.7 0 30s-0.2 1 0 30BOLD (%)hemodynamic activity is independent given neural activity[14]. Noise in the form of the Wiener
process is introduced to si and the log space of fi, qi and vi, in the latter three cases to ensure
positivity:

d ln fi =

dsi =

d ln qi =

d ln vi =

(cid:183)

(cid:184)

1
si dt + σfi dW
fi
− (f − 1)
zi − s
τs
1 − (1 − E0) 1
(cid:105)

(cid:34)
(cid:104)

E0

τf

fi

fi

fi − v

1
α
i

dt + σvi dW.

dt + σsi dW

(cid:35)

− v

1

α−1

i

qi

dt + σqi dW

1
qiτ0
1
viτ0

(cid:183)

(9)

(10)

(11)

(12)

(14)

3.4 Measurement model
The relative BOLD signal change at any time for a particular region is given by (c.f. Equation 7):

(cid:181)

(cid:182)

(cid:184)

∆yi = V0

k1(1 − qi) + k2

1 − qi
vi

+ k3(1 − vi)

.

(13)

This may be converted to an absolute measurement y∗
using the baseline signal bi for each seed and an independent noise source ξ ∼ N (0, 1):

i for comparison with actual observations by

y∗
i = bi(1 + ∆yi) + σyi ξ.

4 Estimation
The model is completely deﬁned by Equations 8 to 14. This ﬁts nicely into a ﬁltering framework,
whereby the input, neural and hemodynamic models deﬁne state transitions, and the measurement
model predicted observations. For i = 1, . . . , M, σzi, σfi, σsi, σqi and σvi deﬁne the system noise
and σyi the measurement noise. Parameters to estimate are the elements of A, C, c and b.
For a sequence of time points t1, . . . , tT , we are given observations y(t1), . . . , y(tT ), where
y(t) = (y1(t), . . . , yM (t))T . We seek to exploit the data as much as possible by estimating
P (x(tn)| y(t1), . . . , y(tT )) for n = 1, . . . , T – the distribution over the state at each time point
given all the data.
Because of non-Gaussianity and nonlinearity of the transitions and measurements, a two-pass parti-
cle ﬁlter is proposed to solve the problem. The forward pass is performed using a sequential impor-
tance resampling technique similar to CONDENSATION [15], obtaining P (x(tn)| y(t1), . . . , y(tn))
for n = 1, . . . , T . Resampling at each step is handled using a deterministic resampling method [16].
The transition of particles through the differential system uses a 4th/5th order Runge-Kutta-Fehlberg
method, the adaptive step size maintaining ﬁxed error bounds.
The backwards pass is substantially more difﬁcult. Naively, we can simply negate the derivatives of
the differential system and step backwards to obtain P (x(tn)| y(tn+1), . . . , y(tT )), then fuse these
with the results of the forwards pass to obtain the desired posterior. Unfortunately, such a backwards
model is divergent in q and v, so that the accumulated numerical errors of the Runge-Kutta can
easily cause an explosion to implausible values and a tip-toe adaptive step size to maintain error
bounds. This can be mitigated by tightening the error bounds, but the task becomes computationally
prohibitive well before the system is tamed.
An alternative is a two-pass smoother that reuses particles from the forwards pass [17], reweighting
them on the backwards pass so that no explicit backwards dynamics are required. This sidesteps the
divergence issue completely, but is computationally and spatially expensive and requires computa-
tion of p(x(tn) = s(i)
tn−1. This imposes some
limitations, but is nevertheless the method used here.
The forwards pass provides a weighted sample set {(s(i)
t )} at each time point t = t1, . . . , tT
for i = 1, . . . , P . Initialising with ψtT = πtT , the backwards step to calculate weights at time tn is

tn−1) for particular particles s(i)

tn | x(tn−1) = s(j)

tn and s(j)

, π(i)

t

4

as follows [17]2:

α(i,j)

tn+1 | x(tn) = s(j)

= p(x(tn+1) = s(i)

tn ) for i, j = 1, . . . , P
tn
γtn = αtnπtn
tn(ψtn+1 ﬁ γtn) where ﬁ is element-wise division,
δtn = αT
(cid:80)
ψtn = πtn ⊗ δtn where ⊗ is element-wise multiplication.
tn , ψ(i)

tn = 1 and the smoothed result {(s(i)
ψ(i)

tn )} for i = 1, . . . , P

tn+1 | x(tn) = s(j)

These are then normalised so that
is stored.
There are numerous means of propagating particles through the forwards pass that accommodate the
resampling step and propagation of the Wiener noise through the nonlinearity. These include var-
ious stochastic Runge-Kutta methods, the Unscented Transformation [4] or a simple Euler scheme
using ﬁxed time steps and adding an appropriate portion of noise after each step. The requirement to
efﬁciently make P 2 density calculations of p(x(tn+1) = s(i)
tn ) during the backwards
pass is challenging with such approaches, however. To keep things simple, we instead simply prop-
agate particles noiselessly through the transition function, and add noise from the Wiener process
only at times t1, . . . , tT as if the transition were linear. This reasonably approximates the noise of
the system while keeping the density calculations very simple – transition s(j)
tn noiselessly to obtain
the mean value of a Gaussian with covariance equal to that of the system noise, then calculate the
density of this Gaussian at s(i)
Observe that if system noise is sufﬁciently tight, αtn becomes sparse as negligibly small densities
round to zero. Implementing αtn as a sparse matrix can provide signiﬁcant time and space savings.
Propagation of particles through the transition function and density calculations can be performed
in parallel. This applies during both passes. For the backwards pass, each particle at tn need only
be transitioned once to produce a Gaussian from which the density of all particles at tn+1 can be
calculated, ﬁlling in one column of αtn.
Finally, the parameters A, C, c and b may be estimated by adding them to the state with artiﬁcial
dynamics (c.f. [18]), applying a broad prior and small system noise to suggest that they are generally
constant. The same applies to parameters of the balloon model, which may be included to allow
variation in the hemodynamic response across the brain.

tn+1.

5 Experiments
We apply the model to data collected during a simple ﬁnger tapping exercise. Using a Siemens
Vision at 2T with a TR of 4.1s, a healthy 23-year-old right-handed male was scanned on 33 separate
days over a period of two months. In each session, 80 whole volumes were taken, with the ﬁrst
two discarded to account for T1 saturation effects. The experimental paradigm consists of alternat-
ing 6TR blocks of rest and tapping of the right index ﬁnger at 1.5Hz, where tapping frequency is
provided by a constant audio cue, present during both rest and tapping phases.
All scans across all sessions were realigned using SPM5 [19] and a two-level random effects analysis
performed, from which 13 voxels were selected to represent regions of interest. No smoothing or
normalisation was applied to the data. Of the 13 voxels, four are selected for use in this experiment
– located in the left posterior parietal cortex, left M1, left S1 and left premotor cortex. The mean
of all sessions is used as the measurement y(t), which consists of M = 4 elements, one for each
region.
We set t1 = 1TR = 4.1s, . . . , tT = 78TR = 319.8s as the sequence of times, corresponding to
the times at which measurements are taken after realignment. The experimental input function u(t)
is plotted in Figure 2, taking a value of 0 at rest and 1 during tapping. The error bounds on the
Runge-Kutta are set to 10−4. Measurement noise is set to σyi = 2 for i = 1, . . . , M and the prior
and system noise as in Table 2. With the elements of A, C, c and b included in the state, the state
size is 48. P = 106 particles are used for the forwards pass, downsampling to 2.5 × 104 particles
for the more expensive backwards pass.

2We have expressed this in matrix notation rather than the original notation in [17]

5

Figure 2: Experimental input u(t), x axis is
time t expressed in TRs.

Figure 3: Number of nonzero elements in αtn
for n = 1, . . . , 77.

Ai,i
Ai,j
Ci,1
zi
fi, si, qi, vi, ci
bi

i = 1, . . . , N
i, j = 1, . . . , N, i 6= j
i = 1, . . . , N
i = 1, . . . , N
i = 1, . . . , N
i = 1, . . . , N

Prior Noise
σ
10−2
10−2
10−2
10−1
10−2
10−2

σ
1/2
1/2
1/2
1/2
1/2
10

µ
−1
0
0
0
0
¯yi

Table 2: Prior and system noise.

The experiment is run on the Eddie cluster of the Edinburgh Compute and Data Facility (ECDF) 3
over 200 nodes, taking approximately 10 minutes real time. The particle ﬁlter and smoother are
distributed across nodes and run in parallel using the dysii Dynamic Systems Library 4.
After application of the ﬁlter, the predicted neural activity is given in Figure 4 and parameter esti-
mates in Figures 6 and 7. The predicted output obtained from the model is in Figure 5, where it is
compared to actual measurements acquired during the experiment to assess model ﬁt.
6 Discussion
The model captures the expected underlying form for neural activity, with all regions distinctly
correlated with the experimental stimulus. Parameter estimates are generally constant throughout
the length of the experiment and some efﬁcacies are signiﬁcant enough in magnitude to provide
biological insight. The parameters found typically match those expected for this form of ﬁnger
tapping task. However, as the focus of this paper is the development of the ﬁltering approach we
will reserve a real analysis of the results for a future paper, and focus on the issues surrounding the
ﬁlter and its capabilities and deﬁciencies. A number of points are worth making in this regard.
Particles stored during the forwards pass do not necessarily support the distributions obtained during
the backwards pass. This is particularly obvious towards the extreme left of Figure 4, where the
smoothed results appear to become erratic, essentially due to degeneracy in the backwards pass.
Furthermore, while the smooth weighting of particles in the forwards pass is informative, that of
the backwards pass is often not, potentially relying on heavy weighting of outlying particles and
shedding little light on the actual nature of the distributions involved.
Figure 3 provides empirical results as to the sparseness of αtn. At worst at least 25% of elements
are zero, demonstrating the advantages of a sparse matrix implementation in this case.
The particle ﬁlter is able to establish consistent neural activity and parameter estimates across runs.
These estimates also come with distributions in the form of weighted sample sets which enable the
uncertainty of the estimates to be understood. This certainly shows the stochastic model and particle
ﬁlter to be a promising approach for systematic connectivity analysis.

3http://www.is.ed.ac.uk/ecdf/
4http://www.indii.org/software/dysii/

6

-1 0 1 2061218 01x1082x1083x1084x108 0 77Figure 4: Neural activity predictions z (y axis)
over time (x axis). Forwards pass results as
shaded histogram, smoothed results as solid line
with 2σ error.

Figure 5: Measurement predictions y∗ (y axis)
over time (x axis). Forwards pass results as
shaded histogram, smoothed results as solid line
with 2σ error, circles actual measurements.

Figure 6: Parameter estimates A (y axis) over time (x axis). Forwards pass results as shaded his-
togram, smoothed results as solid line with 2σ error.

The authors would like to thank David McGonigle for helpful discussions and detailed information
regarding the data set.

7

 0 0.14-1 0 1 0 0.14-1 0 1 0 0.14-1 0 1 0 0.14 0 319.8-1 0 1 0 0.06 180 190 200 210 0 0.06 180 190 200 210 0 0.06 180 190 200 210 0 0.06 0 319.8 180 190 200 210-2-1 0 1-2-1 0 1-2-1 0 1-2-1 0 1 0 0.2 0 0.2 0 0.2 0 0.2Figure 7: Parameter estimates of C (y axis) over time (x axis). Forwards pass results as shaded
histogram, smoothed results as solid line with 2σ error.

References
[1] Friston, K. and Buchel, C. (2004) Human Brain Function, chap. 49, pp. 999–1018. Elsevier.
[2] Gitelman, D. R., Penny, W. D., Ashburner, J., and Friston, K. J. (2003) Modeling regional and psy-
chophysiologic interactions in fMRI: the importance of hemodynamic deconvolution. NeuroImage, 19,
200–207.

[3] Friston, K., Harrison, L., and Penny, W. (2003) Dynamic causal modelling. NeuroImage, 19, 1273–1302.
[4] Julier, S. J. and Uhlmann, J. K. (1997) A new extension of the Kalman ﬁlter to nonlinear systems. The
Proceedings of AeroSense: The 11th International Symposium on Aerospace/Defense Sensing, Simulation
and Controls, Multi Sensor Fusion, Tracking and Resource Management.

[5] Riera, J. J., Watanabe, J., Kazuki, I., Naoki, M., Aubert, E., Ozaki, T., and Kawashim, R. (2004) A
state-space model of the hemodynamic approach: nonlinear ﬁltering of BOLD signals. NeuroImage, 21,
547–567.

[6] Ozaki, T. (1993) A local linearization approach to nonlinear ﬁltering. International Journal on Control,

57, 75–96.

45, 289–307.

[7] Bentler, P. M. and Weeks, D. G. (1980) Linear structural equations with latent variables. Psychometrika,

[8] McArdle, J. J. and McDonald, R. P. (1984) Some algebraic properties of the reticular action model for

moment structures. British Journal of Mathematical and Statistical Psychology, 37, 234–251.

[9] Schlosser, R., Gesierich, T., Kaufmann, B., Vucurevic, G., Hunsche, S., Gawehn, J., and Stoeterb, P.
(2003) Altered effective connectivity during working memory performance in schizophrenia: a study
with fMRI and structural equation modeling. NeuroImage, 19, 751–763.

[10] Au Duong, M., et al. (2005) Modulation of effective connectivity inside the working memory network in

patients at the earliest stage of multiple sclerosis. NeuroImage, 24, 533–538.

[11] Storkey, A. J., Simonotto, E., Whalley, H., Lawrie, S., Murray, L., and McGonigle, D. (2007) Learning

structural equation models for fMRI. Advances in Neural Information Processing Systems, 19.

[12] Buxton, R. B., Wong, E. C., and Frank, L. R. (1998) Dynamics of blood ﬂow and oxygenation changes

during brain activation: The balloon model. Magnetic Resonance in Medicine, 39, 855–864.

[13] Friston, K. J., Mechelli, A., Turner, R., and Price, C. J. (2000) Nonlinear responses in fMRI: The balloon

model, Volterra kernels, and other hemodynamics. NeuroImage, 12, 466–477.

[14] Zarahn, E. (2001) Spatial localization and resolution of BOLD fMRI. Current Opinion in Neurobiology,

11, 209–212.

[15] Isard, M. and Blake, A. (1998) Condensation – conditional density propagation for visual tracking. Inter-

national Journal of Computer Vision, 29, 5–28.

[16] Kitagawa, G. (1996) Monte Carlo ﬁlter and smoother for non-Gaussian nonlinear state space models.

Journal of Computational and Graphical Statistics, 5, 1–25.

[17] Isard, M. and Blake, A. (1998) A smoothing ﬁlter for condensation. Proceedings of the 5th European

Conference on Computer Vision, 1, 767–781.

[18] Kitagawa, G. (1998) A self-organising state-space model. Journal of the American Statistical Association,

[19] Wellcome Department of Imaging Neuroscience (2006), Statistical parametric mapping. Online at

93, 1203–1215.

www.ﬁl.ion.ucl.ac.uk/spm/.

8

-1 0 1 0 0.2"
807,2007,Computing Robust Counter-Strategies,"Adaptation to other initially unknown agents often requires computing an effective counter-strategy. In the Bayesian paradigm, one must find a good counter-strategy to the inferred posterior of the other agents' behavior. In the experts paradigm, one may want to choose experts that are good counter-strategies to the other agents' expected behavior. In this paper we introduce a technique for computing robust counter-strategies for adaptation in multiagent scenarios under a variety of paradigms. The strategies can take advantage of a suspected tendency in the decisions of the other agents, while bounding the worst-case performance when the tendency is not observed. The technique involves solving a modified game, and therefore can make use of recently developed algorithms for solving very large extensive games. We demonstrate the effectiveness of the technique in two-player Texas Hold'em. We show that the computed poker strategies are substantially more robust than best response counter-strategies, while still exploiting a suspected tendency. We also compose the generated strategies in an experts algorithm showing a dramatic improvement in performance over using simple best responses.","Computing Robust Counter-Strategies

Michael Johanson

johanson@cs.ualberta.ca

Martin Zinkevich

maz@cs.ualberta.ca

Computing Science Department

University of Alberta

Michael Bowling

Edmonton, AB Canada T6G2E8
bowling@cs.ualberta.ca

Abstract

Adaptation to other initially unknown agents often requires computing an effec-
tive counter-strategy. In the Bayesian paradigm, one must ﬁnd a good counter-
strategy to the inferred posterior of the other agents’ behavior.
In the experts
paradigm, one may want to choose experts that are good counter-strategies to
the other agents’ expected behavior. In this paper we introduce a technique for
computing robust counter-strategies for adaptation in multiagent scenarios under
a variety of paradigms. The strategies can take advantage of a suspected tendency
in the decisions of the other agents, while bounding the worst-case performance
when the tendency is not observed. The technique involves solving a modiﬁed
game, and therefore can make use of recently developed algorithms for solving
very large extensive games. We demonstrate the effectiveness of the technique in
two-player Texas Hold’em. We show that the computed poker strategies are sub-
stantially more robust than best response counter-strategies, while still exploiting
a suspected tendency. We also compose the generated strategies in an experts al-
gorithm showing a dramatic improvement in performance over using simple best
responses.

1 Introduction

Many applications for autonomous decision making (e.g., assistive technologies, electronic com-
merce, interactive entertainment) involve other agents interacting in the same environment. The
agents’ choices are often not independent, and good performance may necessitate adapting to the
behavior of the other agents. A number of paradigms have been proposed for adaptive decision
making in multiagent scenarios. The agent modeling paradigm proposes to learn a predictive model
of other agents’ behavior from observations of their decisions. The model is then used to compute
or select a counter-strategy that will perform well given the model. An alternative paradigm is the
mixture of experts. In this approach, a set of expert strategies is identiﬁed a priori. These experts
can be thought of as counter-strategies for the range of expected tendencies in the other agents’
behavior. The decision maker, then, chooses amongst the counter-strategies based on their online
performance, commonly using techniques for regret minimization (e.g., UCB1 [ACBF02]). In either
approach, ﬁnding counter-strategies is an important subcomponent.
The most common approach to choosing a counter-strategy is best response: the performance maxi-
mizing strategy if the other agents’ behavior is known [Rob51, CM96]. In large domains where best
response computations are not tractable, they are often approximated with “good responses” from a
computationally tractable set, where performance maximization remains the only criterion [RV02].
The problem with this approach is that best response strategies can be very brittle. While max-

1

imizing performance against the model, they can (and often do) perform poorly when the model
is wrong. The use of best response counter-strategies, therefore, puts an impossible burden on a
priori choices, either the agent model bias or the set of expert counter-strategies. McCracken and
Bowling [MB04] proposed -safe strategies to address this issue. Their technique chooses the best
performance maximizing strategy from the set of strategies that don’t lose more than  in the worst-
case. The strategy balances exploiting the agent model with a safety guarantee in case the model is
wrong. Although conceptually appealing, it is computationally infeasible even for moderately sized
domains and has only been employed in the simple game of Ro-Sham-Bo.
In this paper, we introduce a new technique for computing robust counter-strategies. The counter-
strategies, called restricted Nash responses, balance performance maximization against the model
with reasonable performance even when the model is wrong. The technique involves computing a
Nash equilibrium of a modiﬁed game, and therefore can exploit recent advances in solving large
extensive games [GHPS07, ZBB07, ZJBP08]. We demonstrate the practicality of the approach in
the challenging domain of poker. We begin by reviewing the concepts of extensive form games,
best responses, and Nash equilibria, as well as describing how these concepts apply in the poker
domain. We then describe a technique for computing an approximate best response to an arbitrary
poker strategy, and show that this, indeed, produces brittle counter-strategies. We then introduce
restricted Nash responses, describe how they can be computed efﬁciently, and show that they are
signiﬁcantly more robust while still being effective counter-strategies. Finally, we demonstrate that
these strategies can be used in an experts algorithm to make a more effective adaptive player than
when using simple best response.

2 Background

A perfect information extensive game consists of a tree of game states. At each game state, an
action is made either by nature, or by one of the players, or the state is a terminal state where each
player receives a ﬁxed utility. A strategy for a player consists of a distribution over actions for every
game state. In an imperfect information extensive game, the states where a player makes an action
are divided into information sets. When a player chooses an action, it does not know the state of
the game, only the information set, and therefore its strategy is a mapping from information sets
to distributions over actions. A common restriction on imperfect information extensive games is
perfect recall, where two states can only be in the same information set for a player if that player
took the same actions from the same information sets to reach the two game states. In the remainder
of the paper, we will be considering imperfect information extensive games with perfect recall.
Let σi be a strategy for player i where σi(I, a) is the probability that strategy assigns to action a in
information set I. Let Σi be the set of strategies for player i, and deﬁne ui(σ1, σ2) to be the expected
utility of player i if player 1 uses σ1 ∈ Σ1 and player 2 uses σ2 ∈ Σ2. Deﬁne BR(σ2) ⊆ Σ1 to be
the set of best responses to σ2, i.e.:

BR(σ2) = argmax
σ1∈Σ1

u1(σ1, σ2)

(1)

and deﬁne BR(σ1) ⊆ Σ2 similarly. If σ1 ∈ BR(σ2) and σ2 ∈ BR(σ1), then (σ1, σ2) is a Nash
equilibrium. A zero-sum extensive game is an extensive game where u1 = −u2. In this type of
game, for any two equilibria (σ1, σ2) and (σ0
2) (as well as
(σ0
1, σ2)) are also equilibria. Deﬁne the value of the game to player 1 (v1) to be the expected utility
of player 1 in equilibrium. In a zero-sum extensive game, the exploitability of a strategy σ1 ∈ Σ1
is:

2), u1(σ1, σ2) = u1(σ0

2) and (σ1, σ0

1, σ0

1, σ0

(v1 − u1(σ1, σ2)).

ex(σ1) = max
σ2∈Σ2

(2)
The value of the game to player 2 (v2) and the exploitability of a strategy σ2 ∈ Σ2 are deﬁned
similarly. A strategy which can be exploited for no more than  is -safe. An -Nash equilibrium
in a zero-sum extensive game is a strategy pair where both strategies are -safe.
In the remainder of the work, we will be dealing with mixing two strategies. Informally, one can
think of mixing two strategies as performing the following operation: ﬁrst, ﬂip a (possibly biased)
coin; if it comes up heads, use the ﬁrst strategy, otherwise use the second strategy. Formally, deﬁne
πσi(I) to be the probability that player i when following strategy σi chooses the actions necessary to

2

make information set I reachable from the root of the game tree. Given σ1, σ0
deﬁne mixp(σ1, σ0

1) ∈ Σ1 such that for any information set I of player 1, for all actions a:

1 ∈ Σ1 and p ∈ [0, 1],

mixp(σ1, σ0

1)(I, a) = p × πσ1(I)σ1(I, a) + (1 − p) × πσ0
p × πσ1(I) + (1 − p) × πσ0

1(I)σ1(I, a)
1(I)

.

(3)

Given an event E, deﬁne Prσ1,σ2[E] to be the probability of the event E given player 1 uses σ1,
1 ∈ Σ1, all
and player 2 uses σ2. Given the above deﬁnition of mix, it is the case that for all σ1, σ0
σ2 ∈ Σ2, all p ∈ [0, 1], and all events E:

Pr
mixp(σ1,σ0

1),σ2

[E] = p Pr
σ1,σ2

[E] + (1 − p) Pr
σ0
1,σ2

[E]

(4)

So probabilities of outcomes can simply be combined linearly. As a result the utility of a mixture of
strategies is just u(mixp(σ1, σ0

1), σ2) = pu(σ1, σ2) + (1 − p)u(σ0

1, σ2).

3 Texas Hold’Em

While the techniques in this paper apply to general extensive games, our empirical results will focus
on the domain of poker. In particular, we look at heads-up limit Texas Hold’em, the game used in
the AAAI Computer Poker Competition [ZL06]. A single hand of this poker variant consists of two
players each being dealt two private cards, followed by ﬁve community cards being revealed. Each
player tries to form the best ﬁve-card poker hand from the community cards and her private cards:
if the hand goes to a showdown, the player with the best ﬁve-card hand wins the pot. The key to
good play is on average to have more chips in the pot when you win than are in the pot when you
lose. The players’ actions control the pot size through betting. After the private cards are dealt, a
round of betting occurs, followed by additional betting rounds after the third (ﬂop), fourth (turn),
and ﬁfth (river) community cards are revealed. Betting rounds involve players alternately deciding
to either fold (letting the other player win the chips in the pot), call (matching the opponent’s chips
in the pot), or raise (matching, and then adding an additional ﬁxed amount into the pot). No more
than four raises are allowed in a single betting round. Notice that heads-up limit Texas Hold’em is
an example of a ﬁnite imperfect information extensive game with perfect recall. When evaluating
the results of a match (several hands of poker) between two players, we ﬁnd it convenient to state
the result in millibets won per hand. A millibet is one thousandth of a small-bet, the ﬁxed magnitude
of bets used in the ﬁrst two rounds of betting. To provide some intuition for these numbers, a player
that always folds will lose 750 mb/h while a typical player that is 10 mb/h stronger than another
would require over one million hands to be 95% certain to have won overall.

Abstraction. While being a relatively small variant of poker, the game tree for heads-up limit
Texas Hold’em is still very large, having approximately 9.17×1017 states. Fundamental operations,
such as computing a best response strategy or a Nash equilibrium as described in Section 2, are
intractable on the full game. Common practice is to deﬁne a more reasonably sized abstraction by
merging information sets (e.g., by treating certain hands as indistinguishable). If the abstraction
involves the same betting structure, a strategy for an abstract game can be played directly in the full
game. If the abstraction is small enough Nash equilibria and best response computations become
feasible. Finding an approximate Nash equilibrium in an abstract game has proven to be an effective
way to construct a strong program for the full game [BBD+03, GS06]. Recent solution techniques
have been able to compute approximate Nash equilibria for abstractions with as many as 1010 game
states [ZBB07, GHPS07]. Given a strategy deﬁned in a small enough abstraction, it is also possible
to compute a best response to the strategy in the abstract game. This can be done in time linear in the
size of the extensive game. The abstraction used in this paper has approximately 6.45 × 109 game
states, and is described in an accompanying technical report [JZB07].
The Competitors. Since this work focuses on adapting to other agents’ behavior, our experiments
make use of a battery of different poker playing programs. We give a brief description of these
programs here. PsOpti4 [BBD+03] is one of the earliest successful near equilibrium programs
for poker and is available as “Sparbot” in the commercial title Poker Academy. PsOpti6 is a later
and weaker variant, but whose weaknesses are thought to be less obvious to human players. To-
gether, PsOpti4 and PsOpti6 formed Hyperborean, the winner of the AAAI 2006 Computer Poker
Competition. S1239, S1399, and S2298 are similar near equilibrium strategies generated by a new

3

equilibrium computation method [ZBB07] using a much larger abstraction than is used in PsOpti4
and PsOpti6. A60 and A80 are two past failed attempts at generating interesting exploitive strategies,
and are highly exploitable for over 1000 mb/h. CFR5 is a new near Nash equilibrium [ZJBP08],
and uses the abstraction described in the accompanying technical report [JZB07]. We will also ex-
periment with two programs Bluffbot and Monash, who placed second and third respectively in the
AAAI 2006 Computer Poker Competition’s bankroll event [ZL06].

4 Frequentist Best Response

In the introduction, we described best response counter-strategies as brittle, performing poorly when
playing against a different strategy from the one which they were computed to exploit. In this sec-
tion, we examine this claim empirically in the domain of poker. Since a best response computation
is intractable in the full game, we ﬁrst describe a technique, called frequentist best response, for
ﬁnding a “good response” using an abstract game. As described in the previous section, given a
strategy in an abstract game we can compute a best response to that strategy within the abstraction.
The challenge is that the abstraction used by an arbitrary opponent is not known. In addition, it may
be beneﬁcial to ﬁnd a best response in an alternative, possible more powerful, abstraction.
Suppose we want to ﬁnd a “good response” to some strategy P. The basic idea of frequentist best
response (FBR) is to observe P playing the full game of poker, construct a model of it in an abstract
game (unrelated to that P’s own abstraction), and then compute a best-response in this abstraction.
FBR ﬁrst needs many examples of the strategy playing the full, unabstracted game. It then iterates
through every one of P’s actions for every hand. It ﬁnds the action’s associated information set in
the abstract game and increments a counter associated with that information set and action. After
observing a sufﬁcient number of hands, we can construct a strategy in the abstract game based on
the frequency counts. At each information set, we set the strategy’s probability for performing each
action to be the number of observations of that action being chosen from that information set, divided
by the total number of observations in the information set. If an information set was never observed,
the strategy defaults to the call action. Since this strategy is deﬁned in a known abstraction, FBR
can simply calculate a best response to this frequentist strategy.
P’s opponent in the observed games greatly affects the quality of the model. We have found it
most effective to have P play against a trivial strategy that calls and raises with equal probability.
This provides with us the most observations of P’s decisions that are well distributed throughout
the possible betting sequences. Observing P in self-play or against near equilibrium strategies has
shown to require considerably more observed hands. We typically use 5 million hands of training
data to compute the model strategy, although reasonable responses can still be computed with as few
as 1 million hands.

Evaluation. We computed frequentist best response strategies against seven different opponents.
We played the resulting responses both against the opponent it was designed to exploit as well as the
other six opponents and an approximate equilibrium strategy computed using the same abstraction.
The results of this tournament are shown as a crosstable in Table 1. Positive numbers (appearing
with a green background) are in favor of the row player (FBR strategies, in this case).
The ﬁrst thing to notice is that FBR is very successful at exploiting the opponent it was designed to
exploit, i.e., the diagonal of the crosstable is positive and often large. In some cases, FBR identiﬁed
strategies exploiting the opponent for more than previously known to be possible, e.g., PsOpti4 had
only previously been exploited for 75 mb/h [Sch06], while FBR exploits it for 137 mb/h. The second
thing to notice is that when FBR strategies play against other opponents their performance is poor,
i.e., the off-diagonal of the crosstable is generally negative and occasionally by a large amount. For
example, A60 is not a strong program. It is exploitable for over 2000 mb/h (note that always fold
only loses 750 mb/h) and an approximate equilibrium strategy defeats it by 93 mb/h. Yet, every FBR
strategy besides the one trained on it, loses to it, sometimes by a substantial amount. These results
give evidence that best response is, in practice, a brittle computation, and can perform poorly when
the model is wrong.
One exception to this trend is play within the family of S-bots. In particular, consider S1399 and
S1239, which are very similar programs, using the same technique for equilibrium computation with
the same abstract game. They only differ in the number of iterations the algorithm was afforded. The

4

FBR-PsOpti4
FBR-PsOpti6
FBR-A60
FBR-A80
FBR-S1239
FBR-S1399
FBR-S2298
CFR5
Max

PsOpti4
137
-79
-442
-312
-20
-43
-39
36
137

PsOpti6
-163
330
-499
-281
105
38
51
123
330

A60
-227
-68
2170
-557
-89
-48
-50
93
2170

Opponents
A80
-231
-89
-701
1048
-42
-77
-26
41
1048

S1239
-106
-36
-359
-251
106
75
42
70
106

S1399
-85
-23
-305
-231
91
118
50
68
118

S2298
-144
-48
-377
-266
-32
-46
33
17
33

CFR5 Average
-129
-210
-14
-97
-620
-142
-148
-331
3
-87
-11
-109
2
-41
0
56
0

Table 1: Results of frequentist best responses (FBR) against a variety of opponent programs in full
Texas Hold’em, with winnings in mb/h for the row player. Results involving PsOpti4 or PsOpti6
used 10 duplicate matches of 10,000 hands and are signiﬁcant to 20 mb/h. Other results used 10
duplicate matches of 500,000 hands and are signiﬁcant to 2 mb/h.

results show they do share weaknesses as FBR-S1399 does beat S1239 by 75 mb/h. However, this
is 30% less than 106 mb/h, the amount that FBR-S1239 beats the same opponent. Considering the
similarity of these opponents, even this apparent exception is actually suggestive that best response
is not robust to even slight changes in the model.
Finally, consider the performance of the approximate equilibrium player, CFR5. As it was computed
from a relatively large abstraction it performs comparably well, not losing to any of the seven oppo-
nents. However, it also does not win by the margins of the correct FBR strategy. As noted, against
the highly exploitable A60, it wins by a mere 93 mb/h. What we really want is a compromise.
We would like a strategy that can exploit an opponent successfully like FBR, but without the large
penalty when playing against a different opponent. The remainder of the paper examines Restricted
Nash Response, a technique for creating such strategies.

5 Restricted Nash Response

Imagine that you had a model of your opponent, but did not believe that this model was perfect.
The model may capture the general idea of the adversary you expect to face, but most likely is not
identical. For example, maybe you have played a previous version of the same program, have a
model of its play, but suspect that the designer is likely to have made some small improvements in
the new version. One way to explicitly deﬁne our situation is that with the new version we might
expect that 75 percent of the hands will be played identically to the old version. The other 25 percent
is some new modiﬁcation, for which we want to be robust. This, in itself, can be thought of as a
game for which we can apply the usual game theoretic machinery of equilibria.
Let our model of our opponent be some strategy σﬁx ∈ Σ2. Deﬁne Σp,σﬁx
the form mixp(σﬁx, σ0
responses to σ1 ∈ Σ1 to be:

to be those strategies of
2 is an arbitrary strategy in Σ2. Deﬁne the set of restricted best

2), where σ0

2

BRp,σﬁx(σ1) = argmax
σ2∈Σp,σﬁx

2

u2(σ1, σ2)

(5)

1 ∈ BR(σ∗

2). In this pair, the strategy σ∗

2 ∈ BRp,σﬁx(σ∗
A (p, σﬁx) restricted Nash equilibrium is a pair of strategies (σ∗
1)
and σ∗
1 is a p-restricted Nash response (RNR) to σﬁx. We
propose these RNRs would be ideal counter-strategies for σﬁx, where p provides a balance between
exploitation and exploitability. This concept is closely related to -safe best responses [MB04].
⊆ Σ1 to be the set of all strategies which are -safe (with an exploitability less than
Deﬁne Σ-safe
). Then the set of -safe best responses are:

1

1, σ∗

2) where σ∗

BR-safe(σ2) = argmax
σ1∈Σ-safe

u1(σ1, σ2)

(6)

Theorem 1 For all σ2 ∈ Σ2, for all p ∈ (0, 1], if σ1 is a p-RNR to σ2, then there exists an  such
that σ1 is an -safe best response to σ2.

5

(a) Versus PsOpti4

(b) Versus A80

Figure 1: The tradeoff between  and utility. For each opponent, we varied p ∈ [0, 1] for the RNR.
The labels at each datapoint indicate the value of p used.

The proof of Theorem 1 is in an accompanying technical report [JZB07]. The signiﬁcance of The-
orem 1 is that, among all strategies that are at most  suboptimal, the RNR strategies are among the
best responses. Thus, if we want a strategy that is at most  suboptimal, we can vary p to produce a
strategy that is the best response among all such -safe strategies.
Unlike safe best responses, a RNR can be computed by just solving a modiﬁcation of the original
abstract game. For example, if using a sequence form representation of linear programming then
one just needs to add lower bound constraints for the restricted player’s realization plan probabili-
ties. In our experiments we use a recently developed solution technique based on regret minimiza-
tion [ZJBP08] with a modiﬁed game that starts with an unobserved chance node deciding whether
the restricted player is forced to use strategy σﬁx on the current hand. The RNRs used in our experi-
ments were computed with less than a day of computation on a 2.4Ghz AMD Opteron.

Choosing p.
In order to compute a RNR we have to choose a value of p. By varying the value
p ∈ [0, 1], we can produce poker strategies that are closer to a Nash equilibrium (when p is near 0) or
are closer to the best response (when p is near 1). When producing an RNR to a particular opponent,
it is useful to consider the tradeoff between the utility of the response against that opponent and
the exploitability of the response itself. We explore this tradeoff in Figure 1. In 1a we plot the
results of using RNR with various values of p against the model of PsOpti4. The x-axis shows the
exploitability of the response, . The y-axis shows the exploitation of the model by the response
in the abstract game. Note that the actual exploitation and exploitability in the full game may be
different, as we explore later. Figure 1b shows this tradeoff against A80.
Notice that by selecting values of p, we can control the tradeoff between  and the response’s ex-
ploitation of the strategy. More importantly, the curves are highly concave meaning that dramatic
reductions in exploitability can be achieved with only a small sacriﬁce in the ability to exploit the
model.

Evaluation. We used RNR to compute a counter-strategy to the same seven opponents used in the
FBR experiments, with the p value used for each opponent selected such that the resulting  is close
to 100 mb/h. The RNR strategies were played against these seven opponents and the equilibrium
CFR5 in the full game of Texas Hold’em. The results of this tournament are displayed as a crosstable
in Table 2.
The ﬁrst thing to notice is that RNR is capable of exploiting the opponent for which it was designed
as a counter-strategy, while still performing well against the other opponents. In other words, not
only is the diagonal positive and large, most of the crosstable is positive. For the highly exploitable
opponents, such as A60 and A80, the degree of exploitation is much reduced from FBR, which is a
consequence of choosing p such that  is at most 100 mb/h. Notice, though, that it does exploit these
opponents signiﬁcantly more than the approximate Nash strategy (CFR5).

6

 80 100 120 140 160 180 200 220 240 260 0 1000 2000 3000 4000 5000 6000 7000 8000Exploitation (mb/h)Exploitability (mb/h)(0.00)(0.50)(0.75)(0.82)(0.85)(0.90)(0.95)(0.99)(1.00) 0 100 200 300 400 500 600 700 800 900 1000 1100 0 1000 2000 3000 4000 5000 6000 7000 8000 9000Exploitation (mb/h)Exploitability (mb/h)(0.00)(0.25)(0.40)(0.45)(0.50)(0.55)(0.60)(0.80)(0.90)(0.95)(1.00)RNR-PsOpti4
RNR-PsOpti6
RNR-A60
RNR-A80
RNR-S1239
RNR-S1399
RNR-S2298
CFR5
Max

PsOpti4
85
26
-17
-7
38
31
21
36
85

PsOpti6 A60 A80
9
34
-22
293
31
29
30
41
293

112
234
63
66
130
136
137
123
234

39
72
582
22
68
66
72
93
582

Opponents

S1239
63
59
37
11
111
105
77
70
111

S1399
61
59
39
12
106
112
76
68
112

S2298
-1
1
-9
0
9
6
31
17
31

CFR5 Average
43
57
78
46
59
58
54
56

-23
-28
-45
-29
-20
-24
-11
0
0

Table 2: Results of restriced Nash response (RNR) against a variety of opponent programs in full
Texas Hold’em, with winnings in mb/h for the row player. See the caption of Table 1 for match
details.

Figure 2: Performance of FBR-experts, RNR-experts, and a near Nash equilibrium strategy (CFR5)
against “training” opponents and “hold out” opponents in 50 duplicate matches of 1000 hands.

Revisiting the family of S-bots, we notice that the known similarity of S1239 and S1399 is more
apparent with RNR. The performance of RNR with the correct model against these two players is
close to that of FBR, while the performance with the similar model is only a 6mb/h drop. Essentially,
RNR is forced to exploit only the weaknesses that are general and is robust to small changes. Overall,
RNR offers a similar degree of exploitation to FBR, but with far more robustness.

6 Restricted Nash Experts

We have shown that RNR can be used to ﬁnd robust counter-strategies. In this section we investigate
their use in an adaptive poker program. We generated four counter-strategies based on the opponents
PsOpti4, A80, S1399, and S2298, and then used these as experts which UCB1 [ACBF02] (a regret
minimizing algorithm) selected amongst. The FBR-experts algorithm used a FBR to each opponent,
and the RNR-experts used RNR to each opponent. We then played these two expert mixtures in
1000 hand matches against both the four programs used to generate the counter strategies as well as
two programs from the 2006 AAAI Computer Poker Competition, which have an unknown origin
and were developed independently of the other programs. We call the ﬁrst four programs “training
opponents” and the other two programs “holdout opponents”, as they are similar to training error
and holdout error in supervised learning.
The results of these matches are shown in Figure 2. As expected, when the opponent matches one of
the training models, FBR-experts and RNR-experts perform better, on average, than a near equilib-
rium strategy (see “Training Average” in Figure 2). However, if we look at the break down against
individual opponents, we see that all of FBR’s performance comes from its ability to signiﬁcantly
exploit one single opponent. Against the other opponents, it actually performs worse than the non-
adaptive near equilibrium strategy. RNR does not exploit A80 to the same degree as FBR, but also
does not lose to any opponent.

7

-100 0 100 200 300 400 500 600 700 800HoldoutAverageMonashBluffBotTrainingAverageS2298S1399Attack80Opti4Performance (mb/h)FBR ExpertsRNR Experts5555hs2The comparison with the holdout opponents, though, is more realistic and more telling. Since it
is unlikely a player will have a model of the exact program its likely to face in a competition,
it is important for its counter-strategies to exploit general weaknesses that might be encountered.
Our holdout programs have no explicit relationship to the training programs, yet the RNR counter-
strategies are still effective at exploiting these programs as demonstrated by the expert mixture
being able to exploit these programs by more than the near equilibrium strategy. The FBR counter-
strategies, on the other hand, performed poorly outside of the training programs, demonstrating once
again that RNR counter-strategies are both more robust and more suitable as a basis for adapting
behavior to other agents in the environment.

7 Conclusion

We proposed a new technique for generating robust counter-strategies in multiagent scenarios. The
restricted Nash responses balance exploiting suspected tendencies in other agents’ behavior, while
bounding the worst-case performance when the tendency is not observed. The technique involves
computing an approximate equilibrium to a modiﬁcation of the original game, and therefore can
make use of recently developed algorithms for solving very large extensive games. We demon-
strated the technique in the domain of poker, showing it to generate more robust counter-strategies
than traditional best response. We also showed that a simple mixture of experts algorithm based on
restricted Nash response counter-strategies was far superior to using best response counter-strategies
if the exact opponent was not used in training. Further, the restricted Nash experts algorithm outper-
formed a static non-adaptive near equilibrium at exploiting the previously unseen programs.

References
[ACBF02] P. Auer, N. Cesa-Bianchi, and P. Fischer. Finite time analysis of the multiarmed bandit problem.

Machine Learning, 47:235–256, 2002.

[BBD+03] D. Billings, N. Burch, A. Davidson, R. Holte, J. Schaeffer, T. Schauenberg, and D. Szafron. Ap-
proximating game-theoretic optimal strategies for full-scale poker. In International Joint Confer-
ence on Artiﬁcial Intelligence, pages 661–668, 2003.
David Carmel and Shaul Markovitch. Learning models of intelligent agents. In Proceedings of the
Thirteenth National Conference on Artiﬁcial Intelligence, Menlo Park, CA, 1996. AAAI Press.

[CM96]

[GHPS07] A. Gilpin, S. Hoda, J. Pena, and T. Sandholm. Gradient-based algorithms for ﬁnding nash equilibria
In Proceedings of the Eighteenth International Conference on Game

in extensive form games.
Theory, 2007.
A. Gilpin and T. Sandholm. A competitive texas hold’em poker player via automated abstraction
and real-time equilibrium computation. In National Conference on Artiﬁcial Intelligence, 2006.

[GS06]

[JZB07] Michael Johanson, Martin Zinkevich, and Michael Bowling. Computing robust counter-strategies.

[MB04]

[Rob51]
[RV02]

[Sch06]

Technical Report TR07-15, Department of Computing Science, University of Alberta, 2007.
Peter McCracken and Michael Bowling. Safe strategies for agent modelling in games. In AAAI
Fall Symposium on Artiﬁcial Multi-agent Learning, October 2004.
Julia Robinson. An iterative method of solving a game. Annals of Mathematics, 54:296–301, 1951.
Patrick Riley and Manuela Veloso. Planning for distributed execution through use of probabilis-
tic opponent models. In Proceedings of the Sixth International Conference on AI Planning and
Scheduling, pages 77–82, April 2002.
T.C. Schauenberg. Opponent modelling and search in poker. Master’s thesis, University of Alberta,
2006.

[ZBB07] M. Zinkevich, M. Bowling, and N. Burch. A new algorithm for generating strong strategies in mas-
sive zero-sum games. In Proceedings of the Twenty-Seventh Conference on Artiﬁcial Intelligence
(AAAI), 2007. To Appear.

[ZJBP08] M. Zinkevich, M. Johanson, M. Bowling, and C. Piccione. Regret minimization in games with

incomplete information. In Neural Information Processing Systems 21, 2008.
M. Zinkevich and M. Littman. The AAAI computer poker competition. Journal of the International
Computer Games Association, 29, 2006. News item.

[ZL06]

8

"
329,2007,Random Sampling of States in Dynamic Programming,We combine two threads of research on approximate dynamic programming: random sampling of states and using local trajectory optimizers to globally optimize a policy and associated value function. This combination allows us to replace a dense multidimensional grid with a much sparser adaptive sampling of states. Our focus is on finding steady state policies for the deterministic time invariant discrete time control problems with continuous states and actions often found in robotics. In this paper we show that we can now solve problems we couldn't solve previously with regular grid-based approaches.,"Random Sampling of States in Dynamic

Programming

Christopher G. Atkeson and Benjamin Stephens

Robotics Institute, Carnegie Mellon University

cga@cmu.edu, bstephens@cmu.edu

www.cs.cmu.edu/∼cga, www.cs.cmu.edu/∼bstephe1

Abstract

We combine three threads of research on approximate dynamic programming:
sparse random sampling of states, value function and policy approximation using
local models, and using local trajectory optimizers to globally optimize a policy
and associated value function. Our focus is on ﬁnding steady state policies for
deterministic time invariant discrete time control problems with continuous states
and actions often found in robotics. In this paper we show that we can now solve
problems we couldn’t solve previously.

1 Introduction

Optimal control provides a potentially useful methodology to design nonlinear control laws (poli-
cies) u = u(x) which give the appropriate action u for any state x. Dynamic programming provides
a way to ﬁnd globally optimal control laws, given a one step cost (a.k.a. “reward” or “loss”) function
and the dynamics of the problem to be optimized. We focus on control problems with continuous
states and actions, deterministic time invariant discrete time dynamics xk+1 = f(xk, uk), and a time
invariant one step cost function L(x, u). Policies for such time invariant problems will also be time
invariant. We assume we know the dynamics and one step cost function. Future work will address
simultaneously learning a dynamic model, ﬁnding a robust policy, and performing state estimation
with an erroneous partially learned model. One approach to dynamic programming is to approximate
the value function V (x) (the optimal total future cost from each state V (x) = ∑∞
k=0 L(xk, uk)), and to
repeatedly solve the Bellman equation V (x) = minu(L(x, u) + V (f(x, u))) at sampled states x until
the value function estimates have converged to globally optimal values. We explore approximating
the value function and policy using many local models.

An example problem: We use one link pendulum swingup as an example problem in this intro-
duction to provide the reader with a visualizable example of a value function and policy. In one
link pendulum swingup a motor at the base of the pendulum swings a rigid arm from the downward
stable equilibrium to the upright unstable equilibrium and balances the arm there (Figure 1). What
makes this challenging is that the one step cost function penalizes the amount of torque used and
the deviation of the current position from the goal. The controller must try to minimize the total
cost of the trajectory. The one step cost function for this example is a weighted sum of the squared
position errors (θ: difference between current angles and the goal angles) and the squared torques
τ: L(x, u) = 0.1θ2T + τ2T where 0.1 weights the position error relative to the torque penalty, and
T is the time step of the simulation (0.01s). There are no costs associated with the joint velocity.
Figure 2 shows the value function and policy generated by dynamic programming.

One important thread of research on approximate dynamic programming is developing representa-
tions that adapt to the problem being solved and extend the range of problems that can be solved
with a reasonable amount of memory and time. Random sampling of states has been proposed by a
number of researchers [1, 2, 3, 4, 5, 6, 7]. In our case we add new randomly selected states as we

1

1

0.8

0.6

0.4

0.2

0

−0.2

−0.4

−0.6

−0.8

−1

1

0.8

0.6

0.4

0.2

0

−0.2

−0.4

−0.6

−0.8

−1

1

0.8

0.6

0.4

0.2

0

−0.2

−0.4

−0.6

−0.8

−1

1

0.8

0.6

0.4

0.2

0

−0.2

−0.4

−0.6

−0.8

−1

1

0.8

0.6

0.4

0.2

0

−0.2

−0.4

−0.6

−0.8

−1

1

0.8

0.6

0.4

0.2

0

−0.2

−0.4

−0.6

−0.8

−1

1

0.8

0.6

0.4

0.2

0

−0.2

−0.4

−0.6

−0.8

−1

1

0.8

0.6

0.4

0.2

0

−0.2

−0.4

−0.6

−0.8

−1

1

0.8

0.6

0.4

0.2

0

−0.2

−0.4

−0.6

−0.8

−1

1

0.8

0.6

0.4

0.2

0

−0.2

−0.4

−0.6

−0.8

−1

Figure 1: Conﬁgurations from the simulated one link pendulum optimal trajectory every half a second and at
the end of the trajectory.

solve the problem, allowing the “grid” that results to reﬂect the local complexity of the value func-
tion as we generate it. Figure 2:right shows such a randomly generated set of states superimposed
on a contour plot of the value function for one link swingup.

Another important thread in our work on applied dynamic programming is developing ways for grids
or random samples to be as sparse as possible. One technique that we apply here is to represent full
trajectories from each sampled state to the goal, and to reﬁne each trajectory using local trajectory
optimization [8]. Figure 2:right shows a set of optimized trajectories from the sampled states to the
goal. One key aspect of the local trajectory optimizer we use is that it provides a local quadratic
model of the value function and a local linear model of the policy at the sampled state. These local
models help our function approximators handle sparsely sampled states. To obtain globally optimal
solutions, we incorporate exchange of information between non-neighboring sampled states.

On what problems will the proposed approach work? We believe our approach can discover
underlying simplicity in many typical problems. An example of a problem that appears complex but
is actually simple is a problem with linear dynamics and a quadratic one step cost function. Dy-
namic programming can be done for linear quadratic regulator (LQR) problems even with hundreds
of dimensions and it is not necessary to build a grid of states [9]. The cost of representing the value
function is quadratic in the dimensionality of the state. The cost of performing a “sweep” or update
of the value function is at most cubic in the state dimensionality. Continuous states and actions
are easy to handle. Perhaps many problems, such as the examples in this paper, have simplifying
characteristics similar to LQR problems. For example, problems that are only “slightly” nonlinear
and have a locally quadratic cost function may be solvable with quite sparse representations. One
goal of our work is to develop methods that do not immediately build a hugely expensive represen-
tation if it is not necessary, and attempt to harness simple and inexpensive parallel local planning
to solve complex planning problems. Another goal of our work is to develop methods that can take
advantage of situations where only a small amount of global interaction is necessary to enable local
planners capable of solving local problems to ﬁnd globally optimal solutions.

2 Related Work

Random state selection: Random grids and random sampling are well known in numerical inte-
gration, ﬁnite element methods, and partial differential equations. Rust applied random sampling
of states to dynamic programming [1, 10]. He showed that random sampling of states can avoid
the curse of dimensionality for stochastic dynamic programming problems with a ﬁnite set of dis-
crete actions. This theoretical result focused on the cost of computing the expectation term in the
stochastic version of the Bellman equation. [11] claim the assumptions used in [1] are unrealistically
restrictive, and [12] point out that the complexity of Rust’s approach is proportional to the Lipschitz
constant of the problem data, which often increases exponentially with increasing dimensions. The
practicality and usefulness of random sampling of states in deterministic dynamic programming with
continuous actions (the focus of our paper) remains an open question. We note that deterministic
problems are usually more difﬁcult to solve since the random element in the stochastic dynamics
smooths the dynamics and makes them easier to sample. Alternatives to random sampling of states
are irregular or adaptive grids [13], but in our experience they still require too many representational
resources as the problem dimensionality increases.

In reinforcement learning random sampling of states is sometimes used to provide training data for
function approximation of the value function. Reinforcement learning also uses random exploration
for several purposes. In model-free approaches exploration is used to ﬁnd actions and states that lead
to better outcomes. This process is somewhat analogous to the random state sampling described in
this paper for model-based approaches.
In model-based approaches, exploration is also used to
improve the model of the task. In our paper it is assumed a model of the task is available, so this
type of exploration is not necessary.

2

Value function for one link example

Policy for one link example

random initial states and trajectories for one link example

e
u
a
v

l

20

10

0

3

2

1

)

m
N

(
 

e
u
q
r
o

t

10

0

−10

3

2

1

0

−1

−2

−3

−4

−5

position (r)

−6

20

15

−20

−15

−10

−5

0

5

10

velocity (r/s)

0

−1

−2

−3

−4

−5

position (r)

−6

20

15

−20

−15

−10

−5

0

5

10

velocity (r/s)

10

8

6

4

2

0

−2

−4

−6

−8

)
s
/
r
(
 
y
t
i
c
o
e
v

l

−10

−6

−5

−4

−3

−2

−1

0

1

2

3

position (r)

Figure 2: Left and Middle: The value function and policy for a one link pendulum swingup. The optimal
trajectory is shown as a yellow line in the value function plot, and as a black line with a yellow border in the
policy plot. The value function is cut off above 20 so we can see the details of the part of the value function that
determines the optimal trajectory. The goal is at the state (0,0). Right: Random states (dots) and trajectories
(black lines) used to plan one link swingup, superimposed on a contour map of the value function.

In the ﬁeld of Partially Observable Markov Decision Processes (POMDPs) there has been some
work on randomly sampling belief states, and also using local models of the value function and its
ﬁrst derivative at each randomly sampled belief state (for example [2, 3, 4, 5, 6, 7]). Thrun explored
random sampling of belief states where the underlying states and actions were continuous [7]. He
used a nearest neighbor scheme to perform value function interpolation, and a coverage test to decide
whether to accept a new random state (is a new random state far enough from existing states?) rather
than a surprise test (is the value of the new random state predicted incorrectly?).

In robot planning for obstacle avoidance random sampling of states is now quite popular [14]. Proba-
bilistic Road Map (PRM) methods build a graph of plans between randomly selected states. Rapidly
Exploring Random Trees (RRTs) grow paths or trajectories towards randomly selected states. In
general it is difﬁcult to modify PRM and RRT approaches to ﬁnd optimal paths, and the resulting
algorithms based on RRTs are very similar to A* search.

3 Combining Random State Sampling With Local Optimization

The process of using the Bellman equation to update a representation of the value function by mini-
mizing over all actions at a state is referred to as value iteration. Standard value iteration represents
the value function and associated policy using multidimensional tables, with each entry in the table
corresponding to a particular state. In our approach we randomly select states, and associate with
each state a local quadratic model of the value function and a local linear model of the policy. Our
approach generalizes value iteration, and has the following components: 1. There is a “global”
function approximator for both the value function and the policy. In our current implementation the
value function and policy are represented through a combination of sampled and parametric repre-
sentations, building global approximations by combining local models. 2. It is possible to estimate
the value of a state in two ways. The ﬁrst is to use the approximated value function. The second is
our analog of using the Bellman equation: use the cost of a trajectory starting from the state under
consideration and following the current global policy. The trajectory is optimized using local tra-
jectory optimization. 3. As in a Bellman update, there is a way to globally optimize the value of
a state by considering many possible “actions”. In our approach we consider many local policies
associated with different stored states.

Taking advantage of goal states: For problems with goal states there are several ways to speed
up convergence. In cases where LQR techniques apply [9], we use the policy obtained by solving
the corresponding LQR control problem at the goal as the default policy everywhere, to which the
policy computed by dynamic programming is added. [15] plots an example of a default policy and
the policy generated by dynamic programming for comparison. We limit the outputs of this default
policy. In setting up the goal LQR controller, a radius is established and tested within which the
goal LQR controller always works and achieves close to the predicted optimal cost. This has the
effect of making of enlarging the goal. If the dynamic programming process can get within the LQR
radius of the goal, it can use only the default policy to go the rest of the way. If it is not possible to
create a goal LQR controller due to a hard nonlinearity, or if there is no goal state, it does not have
to be done as the goal controller merely accelerates the solution process. The proposed technique
can be generalized in a straightforward way to use any default goal policy. In this paper the swingup

3

problems use an LQR default policy, which was limited for each action dimension to ±5Nm. For
the balance problem we did not use a default policy. We note that for the swingup problems shown
here the default LQR policy is capable of balancing the inverted pendulum at the goal, but is not
capable of swinging up the pendulum to the goal.

We also initially only generate the value function and policy in the region near the goal. This solved
region is gradually increased in size by increasing a value function threshold. Examples of regions
bounded by a constant value are shown by the value function contours in Figure 2. [16] describes
how to handle periodic tasks which have no goal states, and also discontinuities in the dynamics.

Local models of the value function and policy: We need to represent value functions as sparsely
as possible. We propose a hybrid tabular and parametric approach: parametric local models of the
value function and policy are represented at sampled locations. This representation is similar to
using many Taylor series approximations of a function at different points. At each sampled state xp
the local quadratic model for the value function is:

V p(x) ≈ V p

0 + V p

x ˆx +

ˆxTV p

xx ˆx

1

(1)

2
where ˆx = x − xp is the vector from the stored state xp, V p
0 is the constant term of the local model,
V p
xx is the second
x
derivative of the local model (and the value function) at xp. The local linear model for the policy is:

is the ﬁrst derivative of the local model (and the value function) at xp, and V p

where up
also the gain matrix for a local linear controller.

0 is the constant term of the local policy, and K p is the ﬁrst derivative of the local policy and

up(x) = up
0

− K p ˆx

(2)

Creating the local model: These local models of the value function can be created using Dif-
ferential Dynamic Programming (DDP) [17, 18, 8, 16]. This local trajectory optimization process
is similar to linear quadratic regulator design in that a local model of the value function is pro-
duced. In DDP, value function and policy models are produced at each point along a trajectory.
Suppose at a point (xi, ui) we have 1) a local second order Taylor series approximation of the opti-
mal value function: V i(x) ≈ V i
xx ˆx where ˆx = x − xi. 2) a local second order Taylor
series approximation of the robot dynamics, which can be learned using local models of the dy-
namics (fi
u correspond to A and B of the linear plant model used in linear quadratic regulator
(LQR) design): xk+1 = fi(x, u) ≈ fi
uu ˆu where ˆu = u − ui, and
3) a local second order Taylor series approximation of the one step cost, which is often known
analytically for human speciﬁed criteria (Lxx and Luu correspond to Q and R of LQR design):
Li(x, u) ≈ Li

xx ˆx + ˆxTfi

xu ˆu + 1

x and fi

2 ˆxTV i

x ˆx + 1

u ˆu + 1

0 + V i

x ˆx + fi

2 ˆuTfi

2 ˆxTfi

0 + fi

xx ˆx + ˆxTLi

xu ˆu + 1

x ˆx + Li

u ˆu + 1

2 ˆxTLi

2 ˆuTLi

0 + Li

uu ˆu

Given a trajectory, one can integrate the value function and its ﬁrst and second spatial derivatives
backwards in time to compute an improved value function and policy. We utilize the “Q function”
notation from reinforcement learning: Q(x, u) = L(x, u) + V (f(x, u)). The backward sweep takes
the following form (in discrete time):

Qi

xx = Li

xx + V i
x fi

xx + (fi

x)TV i

xxfi

x; Qi

uu = Li

uu + V i
x fi

uu + (fi

u)TV i

Qi

x = Li
ux = Li

x + V i
x fi

x; Qi
ux + V i
x fi

u = Li

u

x fi
u + V i
xxfi
x; Qi

ux + (fi

u)TV i

∆ui = (Qi
V i−1
x = Qi

uu)−1Qi
− Qi

u; Ki = (Qi
xx = Qi

uKi; V i−1

uu)−1Qi
− Qi

xx

x

ux

xuKi

(3)

xxfi
u
(4)
(5)

(6)

where subscripts indicate derivatives and superscripts indicate the trajectory index. After the back-
ward sweep, forward integration can be used to update the trajectory itself: ui
new = ui − ∆ui −
− xi). We note that the cost of this approach grows at most cubically rather than expo-
Ki(xi
nentially with respect to the dimensionality of the state.

new

In problems that have a goal state, we can generate a trajectory from each stored state all the way to
the goal. The cost of this trajectory is an upper bound on the true value of the state, and is used to
bound the estimated value for the old state.

Utilizing the local models: For the purpose of explaining our algorithm, let’s assume we already
have a set of sampled states, each of which has a local model of the value function and the policy.

4

How should we use these multiple local models? The simplest approach is to just use the predictions
of the nearest sampled state, which is what we currently do. We use a kd-tree to efﬁciently ﬁnd
nearest neighbors, but there are many other approaches that will ﬁnd nearby stored states efﬁciently.
In the future we will investigate using other methods to combine local model predictions from nearby
stored states: distance weighted averaging (kernel regression), linear locally weighted regression,
and quadratic locally weighted regression for value functions.

Creating new random states: For tasks with a goal state, we initialize the set of stored states by
storing the goal state itself. We have explored a number of distributions to select additional states
from: uniform within bounds on the states; Gaussian with the mean at the goal; sampling near
existing states; and sampling from an underlying low resolution regular grid. The uniform approach
is a useful default approach, which we use in the swingup examples, the Gaussian approach provides
a simple way to tune the distribution, sampling near existing states provides a way to efﬁciently
sample while growing the solved region in high dimensions, and sampling from an underlying low
resolution grid seems to perform well when only a small number of stored states are used (similar to
using low dispersion sequences [1, 14]). A key point of our approach is that we do not generate the
random states in advance but instead select them as the algorithm progresses. This allows us to apply
an acceptance criteria to candidate states, which we describe in the next paragraph. We have also
explored changing the distribution we generate candidate states from as the algorithm progresses,
for example using a mixture of Gaussians with the Gaussians centered on existing stored states.
Another reasonable hybrid approach would be to initially sample from a grid, and then bias more
general sampling to regions of higher value function approximation error.

Acceptance criteria for candidate states: We have several criteria to accept or reject states to be
permanently stored. In the future we will explore “forgetting” or removing stored states, but at this
point we apply all memory control techniques at the storage event. To focus the search and limit the
volume considered, a steadily increasing value limit is maintained (Vlimit ), which is increased slightly
after each use. The approximated value function is used to predict the value of the candidate state.
If the prediction is above Vlimit , the candidate state is rejected. Otherwise, a trajectory is created
from the candidate state using the current approximated policy, and then locally optimized. If the
value of that trajectory is above Vlimit , the candidate state is rejected. If the value of the trajectory is
within 10% of the predicted value, the candidate state is rejected. Only “surprises” are stored. For
problems with a goal state, if the trajectory does not reach the goal the candidate state is rejected.
Other criteria such as an A* like criteria (cost-to-go(x) + cost-from-start(x) > threshold) can be
used to reject candidate states. All of the thresholds mentioned can be changed as the algorithm
progresses. For example, Vlimit is gradually increased during the solution process, to increase the
volume considered by the algorithm. We currently use a 10% “surprise” threshold. In future work
we will explore starting with a larger threshold and decreasing this threshold with time, to further
reduce the number of samples accepted and stored while improving convergence.
It is possible
to take the distance to the nearest sampled state into account in the acceptance criteria for new
samples. The common approach of accepting states beyond a distance threshold enforces a minimum
resolution, and leads to potentially severe curse of dimensionality effects. Rejecting states that are
too close to existing states will increase the error in representing the value function, but may be a
way for preventing too many samples near complex regions of the value functions that have little
practical effect. For example, we often do not need much accuracy in representing the value function
near policy discontinuities where the value function has discontinuities in its spatial derivative and
“creases”.
In these areas the trajectories typically move away from the discontinuities, and the
details of the value function have little effect.

In the current implementation, after a candidate state is accepted, the state in the database whose
local model was used to make the prediction is re-optimized including information from the newly
added point, since the prediction was wrong and the new point’s policy may lead to a better value
for that state.

Creating a trajectory from a state: We create a trajectory from a candidate state or reﬁne a trajec-
tory from a stored state in the same way. The ﬁrst step is to use the current approximated policy until
the goal or a time limit is reached. In the current implementation this involves ﬁnding the stored
state nearest to the current state in the trajectory and using its locally linear policy to compute the
action on each time step. The second step is to locally optimize the trajectory. We use Differential
Dynamic Programming (DDP) in the current implementation [17, 18, 8, 16]. In the current imple-
mentation we do not save the trajectory but only the local models from its start. If the cost of the

5

1

0.8

0.6

0.4

0.2

0

−0.2

−0.4

−0.6

−0.8

−1

1

0.8

0.6

0.4

0.2

0

−0.2

−0.4

−0.6

−0.8

−1

1

0.8

0.6

0.4

0.2

0

−0.2

−0.4

−0.6

−0.8

−1

1

0.8

0.6

0.4

0.2

0

−0.2

−0.4

−0.6

−0.8

−1

1

0.8

0.6

0.4

0.2

0

−0.2

−0.4

−0.6

−0.8

−1

1

0.8

0.6

0.4

0.2

0

−0.2

−0.4

−0.6

−0.8

−1

1

0.8

0.6

0.4

0.2

0

−0.2

−0.4

−0.6

−0.8

−1

1

0.8

0.6

0.4

0.2

0

−0.2

−0.4

−0.6

−0.8

−1

1

0.8

0.6

0.4

0.2

0

−0.2

−0.4

−0.6

−0.8

−1

1

0.8

0.6

0.4

0.2

0

−0.2

−0.4

−0.6

−0.8

−1

1

0.8

0.6

0.4

0.2

0

−0.2

−0.4

−0.6

−0.8

−1

1

0.8

0.6

0.4

0.2

0

−0.2

−0.4

−0.6

−0.8

−1

Figure 3: Conﬁgurations from the simulated two link pendulum optimal swing up trajectory every ﬁfth of a
second and the end of the trajectory.

trajectory is more than the currently stored value for the state, we reject the new value, as the values
all come from actual trajectories and are upper bounds for the true value. We always keep the lowest
upper bound.

Combining parallel greedy local optimizers to perform global optimization: As currently de-
scribed, the algorithm ﬁnds a locally optimal policy, but not necessarily a globally optimal policy.
For example, the stored states could be divided into two sets of nearest neighbors. One set could
have a suboptimal policy, but have no interaction with the other set of states that had a globally
optimal policy since no nearest neighbor relations joined the two sets. We expect the locally optimal
policies to be fairly good because we 1) gradually increase the solved volume and 2) use local op-
timizers. Given local optimization of actions, gradually increasing the solved volume will result in
a globally optimal policy if the boundary of this volume never touches a non adjacent section of it-
self. Figures 2 and 2 show the creases in the value function (discontinuities in the spatial derivative)
and corresponding discontinuities in the policy that typically result when the constant cost contour
touches a non adjacent section of itself as Vlimit is increased.

In theory, the approach we have described will produce a globally optimal policy if it has inﬁnite
resolution and all the stored states form a densely connected set in terms of nearest neighbor rela-
tions [8]. By enforcing consistency of the local value function models across all nearest neighbor
pairs, we can create a globally consistent value function estimate. Consistency means that any state’s
local model correctly predicts values of nearby states. If the value function estimate is consistent
everywhere, the Bellman equation is solved and we have a globally optimal policy. We can en-
force consistency of nearest neighbor value functions by 1) using the policy of one state of a pair
to reoptimize the trajectory of the other state of the pair and vice versa, and 2) adding more stored
states in between nearest neighbors that continue to disagree [8]. This approach is similar to using
the method of characteristics to solve partial differential equations and ﬁnding value functions for
games.

In practice, we cannot achieve inﬁnite resolution. To increase the likelihood of ﬁnding a globally
optimal policy with a limited resolution of stored states, we need an analog to exploration and to
global minimization with respect to actions found in the Bellman equation. We approximate this
process by periodically reoptimizing each stored state using the policies of other stored states. As
more and more states are stored, and many alternative stored states are considered in optimizing any
given stored state, a wide range of actions are considered for each state. We run a reoptimization
phase of the algorithm after every N (typically 100) states have been stored. There are several ways
to design this reoptimization phase. Each state could use the policy of a nearest neighbor, or a
randomly chosen neighbor with the distribution being distance dependent, or just choosing another
state randomly with no consideration of distance (what we currently do). [8] describes how to follow
a policy of another stored state if its trajectory is stored, or can be recomputed as needed. In this
work we explored a different approach that does not require each stored state to save its trajectory
or recompute it. To “follow” the policy of another state, we follow the locally linear policy for that
state until the trajectory begins to go away from the state. At that point we switch to following the
globally approximated policy. Since we apply this reoptimization process periodically with different
randomly selected policies, over time we explore using a wide range of actions from each state.

6

1

0.8

0.6

0.4

0.2

0

−0.2

−0.4

−0.6

−0.8

−1

1

0.8

0.6

0.4

0.2

0

−0.2

−0.4

−0.6

−0.8

−1

1

0.8

0.6

0.4

0.2

0

−0.2

−0.4

−0.6

−0.8

−1

1

0.8

0.6

0.4

0.2

0

−0.2

−0.4

−0.6

−0.8

−1

1

0.8

0.6

0.4

0.2

0

−0.2

−0.4

−0.6

−0.8

−1

1

0.8

0.6

0.4

0.2

0

−0.2

−0.4

−0.6

−0.8

−1

1

0.8

0.6

0.4

0.2

0

−0.2

−0.4

−0.6

−0.8

−1

1

0.8

0.6

0.4

0.2

0

−0.2

−0.4

−0.6

−0.8

−1

1

0.8

0.6

0.4

0.2

0

−0.2

−0.4

−0.6

−0.8

−1

1

0.8

0.6

0.4

0.2

0

−0.2

−0.4

−0.6

−0.8

−1

1

0.8

0.6

0.4

0.2

0

−0.2

−0.4

−0.6

−0.8

−1

1

0.8

0.6

0.4

0.2

0

−0.2

−0.4

−0.6

−0.8

−1

Figure 4: Conﬁgurations from the simulated three link pendulum optimal trajectory every tenth of a second
and at the end of the trajectory.

4 Results

In addition to the one link swingup example presented in the introduction, we present results on
two link swingup (4 dimensional state) and three link swingup (6 dimensional state). A companion
paper using these techniques to explore how multiple balance strategies can be generated from one
optimization criterion is [19]. Further results, including some for a four link (8 dimensional state)
standing robot are presented.

One link pendulum swingup: For the one link swingup case, the random state approach found
a globally optimal trajectory (the same trajectory found by our grid based approaches [15]) after
adding only 63 random states. Figure 2:right shows the distribution of states and their trajectories
superimposed on a contour map of the value function for one link swingup.

Two link pendulum swingup: For the two link swingup case, the random state approach ﬁnds
what we believe is a globally optimal trajectory (the same trajectory found by our grid based ap-
proaches [15]) after storing an average of 12000 random states (Figure 3). In this case the state has
four dimensions (a position and velocity for each joint) and a two dimensional action (a torque at
each joint). The one step cost function was a weighted sum of the squared position errors and the
squared torques: L(x, u) = 0.1(θ2
2)T. 0.1 weights the position errors relative to
the torque penalty, T is the time step of the simulation (0.01s), and there were no costs associated
with joint velocities. The approximately 12000 sampled states should be compared to the millions
of states used in grid-based approaches. A 60x60x60x60 grid with almost 13 million states failed
to ﬁnd a trajectory as good as this one, while a 100x100x100x100 grid with 100 million states did
ﬁnd the same trajectory. In 13 runs with different random number generator seeds, the mean number
of states stored at convergence is 11430. All but two of the runs converged after storing less than
13000 states, and all runs converged after storing 27000 states.

2)T + (τ2

1 + θ2

1 + τ2

Three link pendulum swingup: For the three link swingup case, the random state approach found
a good trajectory after storing less than 22000 random states (Figure 4). We have not yet solved
this problem a sufﬁcient number of times to be convinced this is a global optimum, and we do not
have a solution based on a regular grid available for comparison. We were not able to solve this
problem using regular grid-based approaches due to limited state resolution: 22x22x22x22x38x44
= 391,676,032 states ﬁlled our largest memory. As in the previous examples, the one step cost
function was a weighted sum of the squared position errors and the squared torques: L(x, u) =
0.1(θ2

1 + θ2

2 + θ2

3)T + (τ2

1 + τ2

2 + τ2

3)T.

5 Conclusion

We have combined random sampling of states and local trajectory optimization to create a promis-
ing approach to practical dynamic programming for robot control problems. We are able to solve
problems we couldn’t solve before due to memory limitations. Future work will optimize aspects
and variants of this approach.

7

Acknowledgments

This material is based upon work supported in part by the DARPA Learning Locomotion Program
and the National Science Foundation under grants CNS-0224419, DGE-0333420, ECS-0325383,
and EEC-0540865.

References

[1] J. Rust. Using randomization to break the curse of dimensionality. Econometrica, 65(3):487–

516, 1997.

[2] M. Hauskrecht. Incremental methods for computing bounds in partially observable Markov
decision processes. In Proceedings of the 14th National Conference on Artiﬁcial Intelligence
(AAAI-97), pages 734–739, Providence, Rhode Island, 1997. AAAI Press / MIT Press.

[3] N.L. Zhang and W. Zhang. Speeding up the convergence of value iteration in partially observ-

able Markov decision processes. JAIR, 14:29–51, 2001.

[4] J. Pineau, G. Gordon, and S. Thrun. Point-based value iteration: An anytime algorithm for

POMDPs. In International Joint Conference on Artiﬁcial Intelligence (IJCAI), 2003.

[5] T. Smith and R. Simmons. Heuristic search value iteration for POMDPs. In Uncertainty in

Artiﬁcial Intelligence, 2004.

[6] M.T.J. Spaan and Nikos V. A point-based POMDP algorithm for robot planning. In Proceed-
ings of the IEEE International Conference on Robotics and Automation, pages 2399–2404,
New Orleans, Louisiana, April 2004.

[7] S. Thrun. Monte Carlo POMDPs. In S.A. Solla, T.K. Leen, and K.-R. M¨uller, editors, Advances

in Neural Information Processing 12, pages 1064–1070. MIT Press, 2000.

[8] C. G. Atkeson. Using local trajectory optimizers to speed up global optimization in dynamic
programming. In Jack D. Cowan, Gerald Tesauro, and Joshua Alspector, editors, Advances in
Neural Information Processing Systems, volume 6, pages 663–670. Morgan Kaufmann Pub-
lishers, Inc., 1994.

[9] F. L. Lewis and V. L. Syrmos. Optimal Control, 2nd Edition. Wiley-Interscience, 1995.

[10] C. Szepesv´ari. Efﬁcient approximate planning in continuous space Markovian decision prob-

lems. AI Communications, 13(3):163–176, 2001.

[11] J. N. Tsitsiklis and Van B. Roy. Regression methods for pricing complex American-style

options. IEEE-NN, 12:694–703, July 2001.

[12] V. D. Blondel and J. N. Tsitsiklis. A survey of computational complexity results in systems

and control, 2000.

[13] R. Munos and A. W. Moore. Variable resolution discretization in optimal control. Machine

Learning Journal, 49:291–323, 2002.

[14] S. M. LaValle. Planning Algorithms. Cambridge University Press, 2006.
[15] C. G. Atkeson. Randomly sampling actions in dynamic programming.

In 2007 IEEE In-
ternational Symposium on Approximate Dynamic Programming and Reinforcement Learning
(ADPRL), 2007.

[16] C. G. Atkeson and J. Morimoto. Nonparametric representation of a policies and value func-
tions: A trajectory based approach. In Advances in Neural Information Processing Systems 15.
MIT Press, 2003.

[17] P. Dyer and S. R. McReynolds. The Computation and Theory of Optimal Control. Academic

Press, New York, NY, 1970.

[18] D. H. Jacobson and D. Q. Mayne. Differential Dynamic Programming. Elsevier, New York,

NY, 1970.

[19] C. G. Atkeson and B. Stephens. Multiple balance strategies from one optimization criterion.

In Humanoids, 2007.

8

"
1074,2007,Predicting human gaze using low-level saliency combined with face detection,"Under natural viewing conditions, human observers shift their gaze to allocate processing resources to subsets of the visual input. Many computational models have aimed at predicting such voluntary attentional shifts. Although the importance of high level stimulus properties (higher order statistics, semantics) stands undisputed, most models are based on low-level features of the input alone. In this study we recorded eye-movements of human observers while they viewed photographs of natural scenes. About two thirds of the stimuli contained at least one person. We demonstrate that a combined model of face detection and low-level saliency clearly outperforms a low-level model in predicting locations humans fixate. This is reflected in our finding fact that observes, even when not instructed to look for anything particular, fixate on a face with a probability of over 80% within their first two fixations (500ms). Remarkably, the model's predictive performance in images that do not contain faces is not impaired by spurious face detector responses, which is suggestive of a bottom-up mechanism for face detection. In summary, we provide a novel computational approach which combines high level object knowledge (in our case: face locations) with low-level features to successfully predict the allocation of attentional resources.","Predicting human gaze using low-level saliency

combined with face detection

Moran Cerf

Computation and Neural Systems
California Institute of Technology

Pasadena, CA 91125

moran@klab.caltech.edu

Jonathan Harel

Electrical Engineering

California Institute of Technology

Pasadena, CA 91125

harel@klab.caltech.edu

Wolfgang Einh¨auser

Institute of Computational Science

Swiss Federal Institute of Technology (ETH)

Zurich, Switzerland

wolfgang.einhaeuser@inf.ethz.ch

koch@klab.caltech.edu

Christof Koch

Computation and Neural Systems
California Institute of Technology

Pasadena, CA 91125

Abstract

Under natural viewing conditions, human observers shift their gaze to allocate
processing resources to subsets of the visual input. Many computational mod-
els try to predict such voluntary eye and attentional shifts. Although the impor-
tant role of high level stimulus properties (e.g., semantic information) in search
stands undisputed, most models are based on low-level image properties. We here
demonstrate that a combined model of face detection and low-level saliency sig-
niﬁcantly outperforms a low-level model in predicting locations humans ﬁxate on,
based on eye-movement recordings of humans observing photographs of natural
scenes, most of which contained at least one person. Observers, even when not in-
structed to look for anything particular, ﬁxate on a face with a probability of over
80% within their ﬁrst two ﬁxations; furthermore, they exhibit more similar scan-
paths when faces are present. Remarkably, our model’s predictive performance in
images that do not contain faces is not impaired, and is even improved in some
cases by spurious face detector responses.

1 Introduction

Although understanding attention is interesting purely from a scientiﬁc perspective, there are nu-
merous applications in engineering, marketing and even art that can beneﬁt from the understanding
of both attention per se, and the allocation of resources for attention and eye movements. One ac-
cessible correlate of human attention is the ﬁxation pattern in scanpaths [1], which has long been of
interest to the vision community [2]. Commonalities between different individuals’ ﬁxation patterns
allow computational models to predict where people look, and in which order [3]. There are several
models for predicting observers’ ﬁxations [4], some of which are inspired by putative neural mech-
anisms. A frequently referenced model for ﬁxation prediction is the Itti et al. saliency map model
(SM) [5]. This “bottom-up” approach is based on contrasts of intrinsic images features such as
color, orientation, intensity, ﬂicker, motion and so on, without any explicit information about higher
order scene structure, semantics, context or task-related (“top-down”) factors, which may be cru-
cial for attentional allocation [6]. Such a bottom-up saliency model works well when higher order
semantics are reﬂected in low-level features (as is often the case for isolated objects, and even for
reasonably cluttered scenes), but tends to fail if other factors dominate: e.g., in search tasks [7, 8],
strong contextual effects [9], or in free-viewing of images without clearly isolated objects, such as

1

forest scenes or foliage [10]. Here, we test how images containing faces - ecologically highly rel-
evant objects - inﬂuence variability of scanpaths across subjects. In a second step, we improve the
standard saliency model by adding a “face channel” based on an established face detector algorithm.
Although there is an ongoing debate regarding the exact mechanisms which underlie face detec-
tion, there is no argument that a normal subject (in contrast to autistic patients) will not interpret
a face purely as a reddish blob with four lines, but as a much more signiﬁcant entity ([11, 12]. In
fact, there is mounting evidence of infants’ preference for face-like patterns before they can even
consciously perceive the category of faces [13], which is crucial for emotion and social processing
([13, 14, 15, 16]).
Face detection is a well investigated area of machine vision. There are numerous computer-vision
models for face detection with good results ([17, 18, 19, 20]). One widely used model for face
recognition is the Viola & Jones [21] feature-based template matching algorithm (VJ). There have
been previous attempts to incorporate face detection into a saliency model. However, they have
either relied on biasing a color channel toward skin hue [22] - and thus being ineffective in many
cases nor being face-selective per se - or they have suffered from lack of generality [23]. We here
propose a system which combines the bottom-up saliency map model of Itti et al. [5] with the Viola
& Jones face detector.
The contributions of this study are: (1) Experimental data showing that subjects exhibit signiﬁcantly
less variable scanpaths when viewing natural images containing faces, marked by a strong tendency
to ﬁxate on faces early. (2) A novel saliency model which combines a face detector with intensity,
color, and orientation information. (3) Quantitative results on two versions of this saliency model,
including one extended from a recent graph-based approach, which show that, compared to previous
approaches, it better predicts subjects’ ﬁxations on images with faces, and predicts as well otherwise.

2 Methods

2.1 Experimental procedures
Seven subjects viewed a set of 250 images (1024 × 768 pixels) in a three phase experiment. 200 of
the images included frontal faces of various people; 50 images contained no faces but were otherwise
identical, allowing a comparison of viewing a particular scene with and without a face. In the ﬁrst
(“free-viewing”) phase of the experiment, 200 of these images (the same subset for each subject)
were presented to subjects for 2 s, after which they were instructed to answer “How interesting was
the image?” using a scale of 1-9 (9 being the most interesting). Subjects were not instructed to
look at anything in particular; their only task was to rate the entire image. In the second (“search”)
phase, subjects viewed another 200 image subset in the same setup, only this time they were initially
presented with a probe image (either a face, or an object in the scene: banana, cell phone, toy car,
etc.) for 600 ms after which one of the 200 images appeared for 2 s. They were then asked to
indicate whether that image contained the probe. Half of the trials had the target probe present. In
half of those the probe was a face. Early studies suggest that there should be a difference between
free-viewing of a scene, and task-dependent viewing of it [2, 4, 6, 7, 24]. We used the second
task to test if there are any differences in the ﬁxation orders and viewing patterns between free-
viewing and task-dependent viewing of images with faces. In the third phase, subjects performed a
100 images recognition memory task where they had to answer with y/n whether they had seen the
image before. 50 of the images were taken from the experimental set and 50 were new. Subjects’
mean performance was 97.5% correct, verifying that they were indeed alert during the experiment.
The images were introduced as “regular images that one can expect to ﬁnd in an everyday personal
photo album”. Scenes were indoors and outdoors still images (see examples in Fig. 1). Images
included faces in various skin colors, age groups, and positions (no image had the face at the center
as this was the starting ﬁxation location in all trials). A few images had face-like objects (see balloon
in Fig. 1, panel 3), animal faces, and objects that had irregular faces in them (masks, the Egyptian
sphinx face, etc.). Faces also vary in size (percentage of the entire image). The average face was
5% ± 1% (mean ± s.d.) of the entire image - between 1◦ to 5◦ of the visual ﬁeld; we also varied the
number of faces in the image between 1-6, with a mean of 1.1 ± 0.48. Image order was randomized
throughout, and subjects were na¨ıve to the purpose of the experiment. Subjects ﬁxated on a cross in
the center before each image onset. Eye-position data were acquired at 1000 Hz using an Eyelink
1000 (SR Research, Osgoode, Canada) eye-tracking device. The images were presented on a CRT

2

screen (120 Hz), using Matlab’s Psychophysics and eyelink toolbox extensions ([25, 26]). Stimulus
luminance was linear in pixel values. The distance between the screen and the subject was 80 cm,
giving a total visual angle for each image of 28◦ × 21◦. Subjects used a chin-rest to stabilize their
head. Data were acquired from the right eye alone. All subjects had uncorrected normal eyesight.

Figure 1: Examples of stimuli during the “free-viewinng” phase. Notice that faces have neu-
tral expressions. Upper 3 panels include scanpaths of one individual. The red triangle marks
the ﬁrst and the red square the last ﬁxation, the yellow line the scanpath, and the red circles
the subsequent ﬁxations. Lower panels show scanpaths of all 7 subjects. The trend of visiting
the faces ﬁrst - typically within the 1st or 2nd ﬁxation - is evident. All images are available at
http://www.klab.caltech.edu/˜moran/db/faces/.

2.2 Combining face detection with various saliency algorithms

We tried to predict the attentional allocation via ﬁxation patterns of the subjects using various
saliency maps.
In particular, we computed four different saliency maps for each of the images
in our data set: (1) a saliency map based on the model of [5] (SM), (2) a graph-based saliency map
according to [27] (GBSM), (3) a map which combines SM with face-detection via VJ (SM+VJ), and
(4) a saliency map combining the outputs of GBSM and VJ (GBSM+VJ). Each saliency map was
represented as a positive valued heat map over the image plane.
SM is based on computing feature maps, followed by center-surround operations which highlight lo-
cal gradients, followed by a normalization step prior to combining the feature channels. We used the
“Maxnorm” normalization scheme which is a spatial competition mechanism based on the squared
ratio of global maximum over average local maximum. This promotes feature maps with one con-
spicuous location to the detriment of maps presenting numerous conspicuous locations. The graph-
based saliency map model (GBSM) employs spectral techniques in lieu of center surround subtrac-
tion and “Maxnorm” normalization, using only local computations. GBSM has shown more robust
correlation with human ﬁxation data compared with standard SM [27].
For face detection, we used the Intel Open Source Computer Vision Library (“OpenCV”) [28] im-
plementation of [21]. This implementation rapidly processes images while achieving high detection
rates. An efﬁcient classiﬁer built using the Ada-Boost learning algorithm is used to select a small
number of critical visual features from a large set of potential candidates. Combining classiﬁers
in a cascade allows background regions of the image to be quickly discarded, so that more cycles
process promising face-like regions using a template matching scheme. The detection is done by
applying a classiﬁer to a sliding search window of 24x24 pixels. The detectors are made of three
joined black and white rectangles, either up-right or rotated by 45◦. The values at each point are
calculated as a weighted sum of two components: the pixel sum over the black rectangles and the
sum over the whole detector area. The classiﬁers are combined to make a boosted cascade with
classiﬁers going from simple to more complex, each possibly rejecting the candidate window as
“not a face” [28]. This implementation of the facedetect module was used with the standard default
training set of the original model. We used it to form a “Faces conspicuity map”, or “Face channel”

3

by convolving delta functions at the (x,y) detected facial centers with 2D Gaussians having standard
deviation equal to estimated facial radius. The values of this map were normalized to a ﬁxed range.
For both SM and GBSM, we computed the combined saliency map as the mean of the normalized
color (C), orientation (O), and intensity (I) maps [5]:

1
3

(N(I) + N(C) + N(O))

And for SM+VJ and GBSM+VJ, we incorporated the normalized face conspicuity map (F) into this
mean (see Fig 2):

1
4

(N(I) + N(C) + N(O) + N(F ))

This is our combined face detector/saliency model. Although we could have explored the space of
combinations which would optimize predictive performance, we chose to use this simplest possible
combination, since it is the least complicated to analyze, and also provides us with ﬁrst intuition for
further studies.

Figure 2: Modiﬁed saliency model. An image is processed through standard [5] color, orientation
and intensity multi-scale channels, as well as through a trained template-matching face detection
mechanism. Face coordinates and radius from the face detector are used to form a face conspicuity
map (F), with peaks at facial centers. All four maps are normalized to the same dynamic range, and
added with equal weights to a ﬁnal saliency map (SM+VJ, or GBSM+VJ). This is compared to a
saliency map which only uses the three bottom-up features maps (SM or GBSM).

3 Results

3.1 Psychophysical results

To evaluate the results of the 7 subjects’ viewing of the images, we manually deﬁned minimally
sized rectangular regions-of-interest (ROIs) around each face in the entire image collection. We
ﬁrst assessed, in the “free-viewing” phase, how many of the ﬁrst ﬁxations went to a face, how
many of the second, third ﬁxations and so forth. In 972 out of the 1050 (7 subjects x 150 images
with faces) trials (92.6%), the subject ﬁxated on a face at least once. In 645/1050 (61.4%) trials, a

4

Face detectionColorIntensityOrientationFalseFalse positiveSaliency MapSaliency Map with face  detectionface was ﬁxated on within the ﬁrst ﬁxation, and of the remaining 405 trials, a face was ﬁxated on
in the second ﬁxation in 71.1% (288/405), i.e. after two ﬁxations a face was ﬁxated on in 88.9%
(933/1050) of trials (Fig. 3). Given that the face ROIs were chosen very conservatively (i.e. ﬁxations
just next to a face do not count as ﬁxations on the face), this shows that faces, if present, are typically
ﬁxated on within the ﬁrst two ﬁxations (327 ms ± 95 ms on average). Furthermore, in addition to
ﬁnding early ﬁxations on faces, we found that inter-subject scanpath consistency on images with
faces was higher. For the free-viewing task, the mean minimum distance to another’s subject’s
ﬁxation (averaged over ﬁxations and subjects) was 29.47 pixels on images with faces, and a greater
34.24 pixels on images without faces (different with p < 10−6). We found similar results using a
variety of different metrics (ROC, Earth Mover’s Distance, Normalized Scanpath Saliency, etc.). To
verify that the double spatial bias of photographer and observer ([29] for discussion of this issue)
did not artiﬁcially result in high fractions of early ﬁxations on faces, we compared our results to an
unbiased baseline: for each subject, the fraction of ﬁxations from all images which fell in the ROIs
of one particular image. The null hypothesis that we would see the same fraction of ﬁrst ﬁxations
on a face at random is rejected at p < 10−20 (t-test).
To test for the hypothesis that face saliency is not due to top-down preference for faces in the absence
of other interesting things, we examined the results of the “search” task, in which subjects were
presented with a non-face target probe in 50% of the trials. Provided the short amount of time for
the search (2 s), subjects should have attempted to tune their internal saliency weights to adjust color,
intensity, and orientation optimally for the searched target [30]. Nevertheless, subjects still tended
to ﬁxate on the faces early. A face was ﬁxated on within the ﬁrst ﬁxation in 24% of trials, within the
ﬁrst two ﬁxations in 52% of trials, and within the three ﬁxations in 77% of the trials. While this is
weaker than in free-viewing, where 88.9% was achieved after just two ﬁxations, the difference from
what would be expected for random ﬁxation selection (unbiased baseline as above) is still highly
signiﬁcant (p < 10−8).
Overall, we found that in both experimental conditions (“free-viewing” and “search”), faces were
powerful attractors of attention, accounting for a strong majority of early ﬁxations when present.
This trend allowed us to easily improve standard saliency models, as discussed below.

Figure 3: Extent of ﬁxation on face regions-of-interest (ROIs) during the “free-viewing” phase .
Left: image with all ﬁxations (7 subjects) superimposed. First ﬁxation marked in blue, second in
cyan, remaining ﬁxations in red. Right: Bars depict percentage of trials, which reach a face the ﬁrst
time in the ﬁrst, second, third, . . . ﬁxation. The solid curve depicts the integral, i.e. the fraction of
trials in which faces were ﬁxated on at least once up to and including the nth ﬁxation.

3.2 Assessing the saliency map models

We ran VJ on each of the 200 images used in the free viewing task, and found at least one face
detection on 176 of these images, 148 of which actually contained faces (only two images with
faces were missed). For each of these 176 images, we computed four saliency maps (SM, GBSM,
SM+VJ, GBSM+VJ) as discussed above, and quantiﬁed the compatibility of each with our scan-
path recordings, in particular ﬁxations, using the area under an ROC curve. The ROC curves were
generated by sweeping over saliency value thresholds, and treating the fraction of non-ﬁxated pixels

5

on a map above threshold as false alarms, and the fraction of ﬁxated pixels above threshold as hits
[29, 31]. According to this ROC ﬁxation “prediction” metric, for the example image in Fig. 4, all
models predict above chance (50%): SM performs worst, and GBSM+VJ best, since including the
face detector substantially improves performance in both cases.

Figure 4: Comparison of the area-under-the-curve (AUC) for an image (chosen arbitrarily. Subjects’
scanpaths shown on the left panels of ﬁgure 1). Top panel: image with the 49 ﬁxations of the 7
subjects (red). First central ﬁxations for each subject were excluded. From left to right, saliency map
model of Itti et al. (SM), saliency map with the VJ face detection map (SM+VJ), the graph-based
saliency map (GBSM), and the graph-based saliency map with face detection channel (GBSM+VJ).
Red dots correspond to ﬁxations. Lower panels depict ROC curves corresponding to each map.
Here, GBSM+VJ predicts ﬁxations best, as quantiﬁed by the highest AUC.

Across all 176 images, this trend prevails (Fig. 5): ﬁrst, all models perform better than chance,
even over the 28 images without faces. The SM+VJ model performed better than the SM model for
154/176 images. The null hypothesis to get this result by chance can be rejected at p < 10−22 (using
a coin-toss sign-test for which model does better, with uniform null-hypothesis, neglecting the size
of effects). Similarly, the GBSM+VJ model performed better than the GBSM model for 142/176
images, a comparably vast majority (p < 10−15) (see Fig. 5, right). For the 148/176 images with
faces, SM+VJ was better than SM alone for 144/148 images (p < 10−29), whereas VJ alone (equal
to the face conspicuity map) was better than SM alone for 83/148 images, a fraction that fails to
reach signiﬁcance. Thus, although the face conspicuity map was surprisingly predictive on its own,
ﬁxation predictions were much better when it was combined with the full saliency model. For the
28 images without faces, SM (better than SM+VJ for 18) and SM+VJ (better than SM for 10) did
not show a signiﬁcant difference, nor did GBSM vs. GBSM+VJ (better on 15/28 compared to
13/28, respectively. However, in a recent follow-up study with more non-face images, we found
preliminary results indicating that the mean ROC score of VJ-enhanced saliency maps is higher on
such non-face images, although the median is slightly lower, i.e. performance is much improved
when improved at all indicating that VJ false positives can sometimes enhance saliency maps.
In summary, we found that adding a face detector channel improves ﬁxation prediction in images
with faces dramatically, while it does not impair prediction in images without faces, even though the
face detector has false alarms in those cases.

4 Discussion

First, we demonstrated that in natural scenes containing frontal shots of people, faces were ﬁxated on
within the ﬁrst few ﬁxations, whether subjects had to grade an image on interest value or search it for
a speciﬁc possibly non-face target. This powerful trend motivated the introduction of a new saliency

6

60

0

0.9

0.8

0.7

0.6

0.5

)
J
V
+
M
S

(
 

C
U
A

1

5

4

***

2

2

0

6
0

image with face
image without face

1

4

2

***

3

4

0

6
0

70

0
1

0.9

0.8

0.7

0.6

)
J
V
+
M
S
B
G

(
 

C
U
A

0.5

0.6

0.7
AUC (SM)

0.8

0.9

0.5

0.5

0.6

0.8
0.7
AUC (GBSM)

0.9

1

Figure 5: Performance of SM compared to SM+VJ and GBSM compared to GBSM+VJ. Scatter-
plots depict the area under ROC curves (AUC) for the 176 images in which VJ found a face. Each
point represents a single image. Points above the diagonal indicate better prediction of the model
including face detection compared to the models without face channel. Blue markers denote images
with faces; red markers images without faces (i.e. false positives of the VJ face detector). His-
tograms of the SM and SM+VJ (GBSM and GBSM+VJ) are depicted to the top and left (binning:
0.05); colorcode as in scatterplots.

model, which combined the “bottom-up” feature channels of color, orientation, and intensity, with a
special face-detection channel, based on the Viola & Jones algorithm. The combination was linear
in nature with uniform weight distribution for maximum simplicity. In attempting to predict the
ﬁxations of human subjects, we found that this additional face channel improved the performance of
both a standard and a more recent graph-based saliency model (almost all blue points in Fig. 5 are
above the diagonal) in images with faces. In the few images without faces, we found that the false
positives represented in the face-detection channel did not signiﬁcantly alter the performance of the
saliency maps – although in a preliminary follow-up on a larger image pool we found that they boost
mean performance. Together, these ﬁndings point towards a specialized “face channel” in our vision
system, which is subject to current debate in the attention literature [11, 12, 32].
In conclusion, inspired by biological understanding of human attentional allocation to meaningful
objects - faces - we presented a new model for computing an improved saliency map which is more
consistent with gaze deployment in natural images containing faces than previously studied models,
even though the face detector was trained on standard sets. This suggests that faces always attract
attention and gaze, relatively independent of the task. They should therefore be considered as part
of the bottom-up saliency pathway.

References

[1] G. Rizzolatti, L. Riggio, I. Dascola, and C. Umilta. Reorienting attention across the horizontal and vertical
meridians: evidence in favor of a premotor theory of attention. Neuropsychologia, 25(1A):31–40, 1987.
[2] G.T. Buswell. How People Look at Pictures: A Study of the Psychology of Perception in Art. The

University of Chicago press, 1935.

[3] M. Cerf, D. R. Cleary, R. J. Peters, and C. Koch. Observers are consistent when rating image conspicuity.

Vis Res, 47(25):3017–3027, 2007.

[4] S.J. Dickinson, H.I. Christensen, J. Tsotsos, and G. Olofsson. Active object recognition integrating atten-

tion and viewpoint control. Computer Vision and Image Understanding, 67(3):239–260, 1997.

[5] L. Itti, C. Koch, E. Niebur, et al. A model of saliency-based visual attention for rapid scene analysis.

IEEE Transactions on Pattern Analysis and Machine Intelligence, 20(11):1254–1259, 1998.

[6] A.L. Yarbus. Eye Movements and Vision. Plenum Press New York, 1967.

7

[7] J.M. Henderson, J.R. Brockmole, M.S. Castelhano, and M. Mack. Visual Saliency Does Not Account
for Eye Movements during Visual Search in Real-World Scenes. Eye Movement Research: Insights into
Mind and Brain, R. van Gompel, M. Fischer, W. Murray, and R. Hill, Eds., 1997.

[8] Gregory Zelinsky, Wei Zhang, Bing Yu, Xin Chen, and Dimitris Samaras. The role of top-down and
In Y. Weiss, B. Sch¨olkopf, and
bottom-up processes in guiding eye movements during visual search.
J. Platt, editors, Advances in Neural Information Processing Systems 18, pages 1569–1576. MIT Press,
Cambridge, MA, 2006.

[9] A. Torralba, A. Oliva, M.S. Castelhano, and J.M. Henderson. Contextual guidance of eye movements and
attention in real-world scenes: the role of global features in object search. Psych Rev, 113(4):766–786,
2006.

[10] W. Einh¨auser and P. K¨onig. Does luminance-contrast contribute to a saliency map for overt visual atten-

tion? Eur. J Neurosci, 17(5):1089–1097, 2003.

[11] O. Hershler and S. Hochstein. At ﬁrst sight: a high-level pop out effect for faces. Vision Res, 45(13):1707–

24, 2005.

[12] R. Vanrullen. On second glance: Still no high-level pop-out effect for faces. Vision Res, 46(18):3017–

3027, 2006.

[13] C. Simion and S. Shimojo. Early interactions between orienting, visual sampling and decision making in

facial preference. Vision Res, 46(20):3331–3335, 2006.

[14] R. Adolphs. Neural systems for recognizing emotion. Curr. Op. Neurobiol., 12(2):169–177, 2002.
[15] A. Klin, W. Jones, R. Schultz, F. Volkmar, and D. Cohen. Visual Fixation Patterns During Viewing of

Naturalistic Social Situations as Predictors of Social Competence in Individuals With Autism, 2002.

[16] JJ Barton. Disorders of face perception and recognition. Neurol Clin, 21(2):521–48, 2003.
[17] K.K. Sung and T. Poggio. Example-based learning for view-based human face detection. IEEE Transac-

tions on Pattern Analysis and Machine Intelligence, 20(1):39–51, 1998.

[18] H.A. Rowley, S. Baluja, and T. Kanade. Neural network-based face detection. IEEE Transactions on

Pattern Analysis and Machine Intelligence, 20(1):23–38, 1998.

[19] H. Schneiderman and T. Kanade. Statistical method for 3 D object detection applied to faces and cars.

Computer Vision and Pattern Recognition, 1:746–751, 2000.

[20] D. Roth, M. Yang, and N. Ahuja. A snow-based face detection. In S. A. Solla, T. K. Leen, and K. R.
Muller, editors, Advances in Neural Information Processing Systems 13, pages 855–861. MIT Press,
Cambridge, MA, 2000.

[21] P. Viola and M. Jones. Rapid object detection using a boosted cascade of simple features. Computer

Vision and Pattern Recognition, 1:511–518, 2001.

[22] D. Walther. Interactions of visual attention and object recognition: computational modeling, algorithms,

and psychophysics. PhD thesis, California Institute of Technology, 2006.

[23] C. Breazeal and B. Scassellati. A context-dependent attention system for a social robot. 1999 International

Joint Conference on Artiﬁcial Intelligence, pages 1254–1259, 1999.

[24] V. Navalpakkam and L. Itti. Search Goal Tunes Visual Features Optimally. Neuron, 53(4):605–617, 2007.
[25] D.H. Brainard. The psychophysics toolbox. Spat Vis, 10(4):433–436, 1997.
[26] F.W. Cornelissen, E.M. Peters, and J. Palmer. The Eyelink Toolbox: Eye tracking with MATLAB and the

Psychophysics Toolbox. Behav Res Meth Instr Comput, 34(4):613–617, 2002.

[27] J. Harel, C. Koch, and P. Perona. Graph-based visual saliency. In B. Sch¨olkopf, J. Platt, and T. Hoffman,
editors, Advances in Neural Information Processing Systems 19, pages 545–552. MIT Press, Cambridge,
MA, 2007.

[28] G. Bradski, A. Kaehler, and V. Pisarevsky. Learning-based computer vision with Intels open source

computer vision library. Intel Technology Journal, 9(1), 2005.

[29] B.W. Tatler, R.J. Baddeley, and I.D. Gilchrist. Visual correlates of ﬁxation selection: effects of scale and

time. Vision Res, 45(5):643–59, 2005.

[30] V. Navalpakkam and L. Itti. Search goal tunes visual features optimally. Neuron, 53(4):605–617, 2007.
[31] R.J. Peters, A. Iyer, L. Itti, and C. Koch. Components of bottom-up gaze allocation in natural images.

Vision Res, 45(18):2397–2416, 2005.

[32] O. Hershler and S. Hochstein. With a careful look: Still no low-level confound to face pop-out Authors’

reply. Vis Res, 46(18):3028–3035, 2006.

8

"
781,2007,Local Algorithms for Approximate Inference in Minor-Excluded Graphs,"We present a new local approximation algorithm for computing MAP and log-partition function for arbitrary exponential family distribution represented by a finite-valued pair-wise Markov random field (MRF), say G. Our algorithm is based on decomposing G into appropriately chosen small components; computing estimates locally in each of these components and then producing a good global solution. We prove that the algorithm can provide approximate solution within arbitrary accuracy when $G$ excludes some finite sized graph as its minor and G has bounded degree: all Planar graphs with bounded degree are examples of such graphs. The running time of the algorithm is $\Theta(n)$ (n is the number of nodes in G), with constant dependent on accuracy, degree of graph and size of the graph that is excluded as a minor (constant for Planar graphs). Our algorithm for minor-excluded graphs uses the decomposition scheme of Klein, Plotkin and Rao (1993). In general, our algorithm works with any decomposition scheme and provides quantifiable approximation guarantee that depends on the decomposition scheme.","Local Algorithms for Approximate Inference in

Minor-Excluded Graphs

Kyomin Jung

Dept. of Mathematics, MIT

kmjung@mit.edu

Devavrat Shah

Dept. of EECS, MIT

devavrat@mit.edu

Abstract

We present a new local approximation algorithm for computing MAP and log-
partition function for arbitrary exponential family distribution represented by a
ﬁnite-valued pair-wise Markov random ﬁeld (MRF), say G. Our algorithm is
based on decomposing G into appropriately chosen small components; computing
estimates locally in each of these components and then producing a good global
solution. We prove that the algorithm can provide approximate solution within
arbitrary accuracy when G excludes some ﬁnite sized graph as its minor and G
has bounded degree: all Planar graphs with bounded degree are examples of such
graphs. The running time of the algorithm is Θ(n) (n is the number of nodes in
G), with constant dependent on accuracy, degree of graph and size of the graph
that is excluded as a minor (constant for Planar graphs).
Our algorithm for minor-excluded graphs uses the decomposition scheme of
Klein, Plotkin and Rao (1993). In general, our algorithm works with any decom-
position scheme and provides quantiﬁable approximation guarantee that depends
on the decomposition scheme.

1 Introduction

Markov Random Field (MRF) based exponential family of distribution allows for representing dis-
tributions in an intuitive parametric form. Therefore, it has been successful for modeling in many
applications Speciﬁcally, consider an exponential family on n random variables X = (X1, . . . , Xn)
represented by a pair-wise (undirected) MRF with graph structure G = (V, E), where vertices
V = {1, . . . , n} and edge set E ⊂ V × V . Each Xi takes value in a ﬁnite set Σ (e.g. Σ = {0, 1}).
The joint distribution of X = (Xi): for x = (xi) ∈ Σn,

Pr[X = x] ∝ exp
X

i∈V

φi(xi) + X

(i,j)∈E

ψij(xi, xj)
 .

(1)

: Σ → R+ 4

= {x ∈ R : x ≥ 0}, and ψij

Here, functions φi
: Σ2 → R+ are as-
sumed to be arbitrary non-negative (real-valued) functions.1 The two most important computa-
tional questions of interest are: (i) ﬁnding maximum a-posteriori (MAP) assignment x∗, where
x∗ = arg maxx∈Σn Pr[X = x]; and (ii) marginal distributions of variables, i.e. Pr[Xi =
for x ∈ Σ, 1 ≤ i ≤ n. MAP is equivalent to a minimal energy assignment (or ground state)
x];
where energy, E(x), of state x ∈ Σn is deﬁned as E(x) = −H(x) + Constant, where H(x) =

Pi∈V φi(xi)+P(i,j)∈E ψij (xi, xj ). Similarly, computing marginal is equivalent to computing log-
partition function, deﬁned as log Z = log(cid:16)Px∈Σn exp(cid:16)Pi∈V φi(xi) +P(i,j)∈E ψij (xi, xj )(cid:17)(cid:17) .

In this paper, we will ﬁnd ε-approximation solutions of MAP and log-partition function: that is, ˆx
and log ˆZ such that: (1 − ε)H(x∗) ≤ H(ˆx) ≤ H(x∗),
(1 − ε) log Z ≤ log ˆZ ≤ (1 + ε) log Z.

1Here, we assume the positivity of φi’s and ψij’s for simplicity of analysis.

1

Previous Work. The question of ﬁnding MAP (or ground state) comes up in many important appli-
cation areas such as coding theory, discrete optimization, image denoising.Similarly, log-partition
function is used in counting combinatorial objects loss-probability computation in computer net-
works, etc. Both problems are NP-hard for exact and even (constant) approximate computation for
arbitrary graph G. However, applications require solving this problem using very simple algorithms.
A plausible approach is as follows. First, identify wide class of graphs that have simple algorithms
for computing MAP and log-partition function. Then, try to build system (e.g. codes) so that such
good graph structure emerges and use the simple algorithm or else use the algorithm as a heuristic.

Such an approach has resulted in many interesting recent results starting the Belief Propagation
(BP) algorithm designed for Tree graph [1].Since there a vast literature on this topic, we will recall
only few results. Two important algorithms are the generalized belief propagation (BP) [2] and the
tree-reweighted algorithm (TRW) [3,4].Key properties of interest for these iterative procedures are
the correctness of ﬁxed points and convergence. Many results characterizing properties of the ﬁxed
points are known starting from [2]. Various sufﬁcient conditions for their convergence are known
starting [5]. However, simultaneous convergence and correctness of such algorithms are established
for only speciﬁc problems, e.g. [6].

Finally, we discuss two relevant results. The ﬁrst result is about properties of TRW. The TRW
algorithm provides provable upper bound on log-partition function for arbitrary graph [3]However,
to the best of authors’ knowledge the error is not quantiﬁed. The TRW for MAP estimation has
a strong connection to speciﬁc Linear Programming (LP) relaxation of the problem [4]. This was
made precise in a sequence of work by Kolmogorov [7], Kolmogorov and Wainwright [8] for binary
MRF. It is worth noting that LP relaxation can be poor even for simple problems.

The second is an approximation algorithm proposed by Globerson and Jaakkola [9] to compute
log-partition function using Planar graph decomposition (PDC). PDC uses techniques of [3] in con-
junction with known result about exact computation of partition function for binary MRF when G is
Planar and the exponential family has speciﬁc form. Their algorithm provides provable upper bound
for arbitrary graph. However, they do not quantify the error incurred. Further, their algorithm is
limited to binary MRF.

Contribution. We propose a novel local algorithm for approximate computation of MAP and log-
partition function. For any ε > 0, our algorithm can produce an ε-approximate solution for MAP
and log-partition function for arbitrary MRF G as long as G excludes a ﬁnite graph as a minor
(precise deﬁnition later). For example, Planar graph excludes K3,3, K5 as a minor. The running
time of the algorithm is Θ(n), with constant dependent on ε, the maximum vertex degree of G and
the size of the graph that is excluded as minor. Speciﬁcally, for a Planar graph with bounded degree,
it takes ≤ C(ε)n time to ﬁnd ε-approximate solution with log log C(ε) = O(1/ε). In general, our
algorithm works for any G and we can quantify bound on the error incurred by our algorithm. It is
worth noting that our algorithm provides a provable lower bound on log-partition function as well
unlike many of previous works.

The precise results for minor-excluded graphs are stated in Theorems 1 and 2. The result concerning
general graphs are stated in the form of Lemmas 2-3-4 for log-partition and Lemmas 5-6-7 for MAP.

Techniques. Our algorithm is based on the following idea: First, decompose G into small-size
connected components say G1, . . . , Gk by removing few edges of G. Second, compute estimates
(either MAP or log-partition) in each of Gi separately. Third, combine these estimates to produce a
global estimate while taking care of the effect induced by removed edges. We show that the error in
the estimate depends only on the edges removed. This error bound characterization is applicable for
arbitrary graph.

Klein, Plotkin and Rao [10]introduced a clever and simple decomposition method for minor-
excluded graphs to study the gap between max-ﬂow and min-cut for multicommodity ﬂows. We
use their method to obtain a good edge-set for decomposing minor-excluded G so that the error
induced in our estimate is small (can be made as small as required).

In general, as long as G allows for such good edge-set for decomposing G into small components,
our algorithm will provide a good estimate. To compute estimates in individual components, we
use dynamic programming. Since each component is small, it is not computationally burdensome.

2

However, one may obtain further simpler heuristics by replacing dynamic programming by other
method such as BP or TRW for computation in the components.

2 Preliminaries
Here we present useful deﬁnitions and previous results about decomposition of minor-excluded
graphs from [10,11].
Deﬁnition 1 (Minor Exclusion) A graph H is called minor of G if we can transform G into H
through an arbitrary sequence of the following two operations: (a) removal of an edge; (b) merge
two connected vertices u, v: that is, remove edge (u, v) as well as vertices u and v; add a new vertex
and make all edges incident on this new vertex that were incident on u or v. Now, if H is not a minor
of G then we say that G excludes H as a minor.

The explanation of the following statement may help understand the deﬁnition: any graph H with
r nodes is a minor of Kr, where Kr is a complete graph of r nodes. This is true because one may
obtain H by removing edges from Kr that are absent in H. More generally, if G is a subgraph of
G0 and G has H as a minor, then G0 has H as its minor. Let Kr,r denote a complete bipartite graph
with r nodes in each partition. Then Kr is a minor of Kr,r. An important implication of this is as
follows: to prove property P for graph G that excludes H, of size r, as a minor, it is sufﬁcient to
prove that any graph that excludes Kr,r as a minor has property P. This fact was cleverly used by
Klein et. al. [10] to obtain a good decomposition scheme described next. First, a deﬁnition.
Deﬁnition 2 ((δ, ∆)-decomposition) Given graph G = (V, E), a randomly chosen subset of edges
B ⊂ E is called (δ, ∆) decomposition of G if the following holds: (a) For any edge e ∈ E,
Pr(e ∈ B) ≤ δ. (b) Let S1, . . . , SK be connected components of graph G0 = (V, E\B) obtained by
removing edges of B from G. Then, for any such component Sj, 1 ≤ j ≤ K and any u, v ∈ Sj the
shortest-path distance between (u, v) in the original graph G is at most ∆ with probability 1.

The existence of (δ, ∆)-decomposition implies that it is possible to remove δ fraction of edges so
that graph decomposes into connected components whose diameter is small. We describe a simple
and explicit construction of such a decomposition for minor excluded class of graphs. This scheme
was proposed by Klein, Plotkin, Rao [10] and Rao [11].

DeC(G, r, ∆)

(0) Input is graph G = (V, E) and r, ∆ ∈ N. Initially, i = 0, G0 = G, B = ∅.
(1) For i = 0, . . . , r − 1, do the following.

(a) Let Si
(b) For each Si

1, . . . , Si
ki

be the connected components of Gi.

j, 1 ≤ j ≤ ki, pick an arbitrary node vj ∈ Si
j.
j rooted at vj in Si
j.

◦ Create a breadth-ﬁrst search tree T i
◦ Choose a number Li
◦ Let Bi
◦ Update B = B ∪ki

j be the set of edges at level Li

j.
j=1 Bi

j uniformly at random from {0, . . . , ∆ − 1}.

j, ∆ + Li

j, 2∆ + Li

j, . . . in T i
j .

(c) set i = i + 1.

(3) Output B and graph G0 = (V, E\B).

As stated above, the basic idea is to use the following step recursively (upto depth r of recursion):
in each connected component, say S, choose a node arbitrarily and create a breadth-ﬁrst search tree,
say T . Choose a number, say L, uniformly at random from {0, . . . , ∆ − 1}. Remove (add to B) all
edges that are at level L + k∆, k ≥ 0 in T . Clearly, the total running time of such an algorithm is
O(r(n + |E|)) for a graph G = (V, E) with |V | = n; with possible parallel implementation across
different connected components.
The algorithm DeC(G, r, ∆) is designed to provide a good decomposition for class of graphs that
exclude Kr,r as a minor. Figure 1 explains the algorithm for a line-graph of n = 9 nodes, which
excludes K2,2 as a minor. The example is about a sample run of DeC(G, 2, 3) (Figure 1 shows the
ﬁrst iteration of the algorithm).

3

G0

L1

1





2






3






4






5





6






7





8






9






5











4






3

2




1



6

7





8

9



T1

G1



       

1

2

3

4

5

6

7

8

9

S1

S2

S3

S4

S5

Figure 1: The ﬁrst of two iterations in execution of DeC(G, 2, 3) is shown.

Lemma 1 If G excludes Kr,r as a minor,
(r/∆, O(∆))-decomposition of G.

then algorithm DeC(G, r, ∆) outputs B which is

It is known that Planar graph excludes K3,3 as a minor. Hence, Lemma 1 implies the following.
Corollary 1 Given a planar graph G,
decomposition for any ∆ ≥ 1.

the algorithm DeC(G, 3, ∆) produces (3/∆, O(∆))-

3 Approximate log Z
Here, we describe algorithm for approximate computation of log Z for any graph G. The algorithm
uses a decomposition algorithm as a sub-routine. In what follows, we use term DECOMP for a
generic decomposition algorithm. The key point is that our algorithm provides provable upper and
lower bound on log Z for any graph; the approximation guarantee and computation time depends on
the property of DECOMP. Speciﬁcally, for Kr,r minor excluded G (e.g. Planar graph with r = 3),
we will use DeC(G, r, ∆) in place of DECOMP. Using Lemma 1, we show that our algorithm based
on DeC provides approximation upto arbitrary multiplicative accuracy by tuning parameter ∆.

LOG PARTITION(G)

(1) Use DECOMP(G) to obtain B ⊂ E such that

(a) G0 = (V, E\B) is made of connected components S1, . . . , SK.

(2) For each connected component Sj, 1 ≤ j ≤ K, do the following:

(a) Compute partition function Zj restricted to Sj by dynamic programming(or exhaus-

(3) Let ψL

tive computation).
ij = min(x,x0)∈Σ2 ψij(x, x0), ψU
log Zj + X

log ˆZLB =

KX

j=1

(i,j)∈B

ij = max(x,x0)∈Σ2 ψij (x, x0). Then
log Zj + X

log ˆZUB =

KX

ψL
ij ;

j=1

(i,j)∈B

ψU
ij.

(4) Output: lower bound log ˆZLB and upper bound log ˆZUB.

In words, LOG PARTITION(G) produces upper and lower bound on log Z of MRF G as follows:
decompose graph G into (small) components S1, . . . , SK by removing (few) edges B ⊂ E using
DECOMP(G). Compute exact log-partition function in each of the components. To produce bounds
log ˆZLB, log ˆZUB take the summation of thus computed component-wise log-partition function along
with minimal and maximal effect of edges from B.
Analysis of LOG PARTITION for General G : Here, we analyze performance of LOG PARTI-
TION for any G. In the next section, we will specialize our analysis for minor excluded G when
LOG PARTITION uses DeC as the DECOMP algorithm.
Lemma 2 Given an MRF G described by (1), the LOG PARTITION produces log ˆZLB, log ˆZUB such
that

log ˆZLB ≤ log Z ≤ log ˆZUB,

log ˆZUB − log ˆZLB = X

ij(cid:1) .
(cid:0)ψU
ij − ψL

(i,j)∈B

4

It takes O(cid:0)|E|KΣ|S ∗|(cid:1) + TDECOMP time to produce this estimate, where |S∗| = maxK
j=1 |Sj| with
iji .
ij − ψL

DECOMP producing decomposition of G into S1, . . . , SK in time TDECOMP .
Lemma 3 If G has maximum vertex degree D then, log Z ≥ 1

D+1 hP(i,j)∈E ψU

Lemma 4 If G has maximum vertex degree D and the DECOMP(G) produces B that is (δ, ∆)-
decomposition, then

Ehlog ˆZUB − log ˆZLBi ≤ δ(D + 1) log Z,
w.r.t. the randomness in B, and LOG PARTITION takes time O(nD|Σ|D∆

) + TDECOMP .

Analysis of LOG PARTITION for Minor-excluded G : Here, we specialize analysis of LOG PAR-
TITIONfor minor exclude graph G. For G that exclude minor Kr,r, we use algorithm DeC(G, r, ∆).
Now, we state the main result for log-partition function computation.
Theorem 1 Let G exclude Kr,r as minor and have D as maximum vertex degree. Given ε > 0, use
LOG PARTITION algorithm with DeC(G, r, ∆) where ∆ = d r(D+1)

e. Then,

log ˆZLB ≤ log Z ≤ log ˆZUB;

ε

Ehlog ˆZUB − log ˆZLBi ≤ ε log Z.

Further, algorithm takes (nC(D, |Σ|, ε)), where constant C(D, |Σ|, ε) = D|Σ|DO(rD/ε)

.

We obtain the following immediate implication of Theorem 1.
Corollary 2 For any ε > 0, the LOG PARTITION algorithm with DeC algorithm for constant degree
Planar graph G based MRF, produces log ˆZLB, log ˆZUB so that

(1 − ε) log Z ≤ log ˆZLB ≤ log Z ≤ log ˆZUB ≤ (1 + ε) log Z,

in time O(nC(ε)) where log log C(ε) = O(1/ε).

4 Approximate MAP
Now, we describe algorithm to compute MAP approximately. It is very similar to the LOG PAR-
TITION algorithm: given G, decompose it into (small) components S1, . . . , SK by removing (few)
edges B ⊂ E. Then, compute an approximate MAP assignment by computing exact MAP restricted
to the components. As in LOG PARTITION, the computation time and performance of the algorithm
depends on property of decomposition scheme. We describe algorithm for any graph G; which will
be specialized for Kr,r minor excluded G using DeC(G, r, ∆).

MODE(G)

(1) Use DECOMP(G) to obtain B ⊂ E such that

(a) G0 = (V, E\B) is made of connected components S1, . . . , SK.

(2) For each connected component Sj, 1 ≤ j ≤ K, do the following:

(a) Through dynamic programming (or exhaustive computation) ﬁnd exact MAP x∗,j for

component Sj, where x∗,j = (x∗,j

i

)i∈Sj .

(3) Produce output cx∗, which is obtained by assigning values to nodes using x∗,j, 1 ≤ j ≤ K.

Analysis of MODE for General G : Here, we analyze performance of MODE for any G. Later,
we will specialize our analysis for minor excluded G when it uses DeC as the DECOMP algorithm.

Lemma 5 Given an MRF G described by (1), the MODE algorithm produces outputs cx∗ such that
H(x∗) − P(i,j)∈B(cid:0)ψU
ij(cid:1) ≤ H(cx∗) ≤ H(x∗). It takes O(cid:0)|E|KΣ|S ∗|(cid:1) + TDECOMP time to
j=1 |Sj| with DECOMP producing decomposition of G into

ij − ψL

produce this estimate, where |S∗| = maxK
S1, . . . , SK in time TDECOMP .
Lemma 6 If G has maximum vertex degree D, then

H(x∗) ≥

1

D + 1


 X

(i,j)∈E


 ≥

1

D + 1


 X

(i,j)∈E

ψU

ij − ψL

ij


 .

ψU
ij

5

Lemma 7 If G has maximum vertex degree D and the DECOMP(G) produces B that is (δ, ∆)-
decomposition, then

EhH(x∗) − H(cx∗)i ≤ δ(D + 1)H(x∗),

where expectation is w.r.t. the randomness in B. Further, MODE takes time O(nD|Σ|D∆

)+TDECOMP .

Analysis of MODE for Minor-excluded G : Here, we specialize analysis of MODE for minor
exclude graph G. For G that exclude minor Kr,r, we use algorithm DeC(G, r, ∆). Now, we state
the main result for MAP computation.
Theorem 2 Let G exclude Kr,r as minor and have D as the maximum vertex degree. Given ε > 0,
use MODE algorithm with DeC(G, r, ∆) where ∆ = d r(D+1)

e. Then,

ε

Further, algorithm takes n · C(D, |Σ|, ε) time, where constant C(D, |Σ|, ε) = D|Σ|DO(rD/ε)

(1 − ε)H(x∗) ≤ H(cx∗) ≤ H(x∗).

.

We obtain the following immediate implication of Theorem 2.
Corollary 3 For any ε > 0, the MODE algorithm with DeC algorithm for constant degree Planar

graph G based MRF, produces estimate cx∗ so that

in time O(nC(ε)) where log log C(ε) = O(1/ε).

(1 − ε)H(x∗) ≤ H(cx∗) ≤ H(x∗),

5 Experiments
Our algorithm provides provably good approximation for any MRF with minor excluded graph
structure, with planar graph as a special case. In this section, we present experimental evaluation of
our algorithm for popular synthetic model.
Setup 1.2 Consider binary (i.e. Σ = {0, 1}) MRF on an n × n lattice G = (V, E):

Pr(x) ∝ exp
X

i∈V

θixi + X

(i,j)∈E

θijxixj


 ,

for x ∈ {0, 1}n2

.

Figure 2 shows a lattice or grid graph with n = 4 (on the left side). There are two scenarios for
choosing parameters (with notation U[a, b] being uniform distribution over interval [a, b]):
(1) Varying interaction. θi is chosen independently from distribution U[−0.05, 0.05] and θij chosen
independent from U[−α, α] with α ∈ {0.2, 0.4, . . . , 2}.
(2) Varying ﬁeld. θij is chosen independently from distribution U[−0.5, 0.5] and θi chosen indepen-
dently from U[−α, α] with α ∈ {0.2, 0.4, . . . , 2}.
The grid graph is planar. Hence, we run our algorithms LOG PARTITION and MODE, with decom-
position scheme DeC(G, 3, ∆), ∆ ∈ {3, 4, 5}. We consider two measures to evaluate performance:
n2 |H(xalg − H(x∗)|.
error in log Z, deﬁned as 1
We compare our algorithm for error in log Z with the two recently very successful algorithms –
Tree re-weighted algorithm (TRW) and planar decomposition algorithm (PDC). The comparison is
plotted in Figure 3 where n = 7 and results are averages over 40 trials. The Figure 3(A) plots
error with respect to varying interaction while Figure 3(B) plots error with respect to varying ﬁeld
strength. Our algorithm, essentially outperforms TRW for these values of ∆ and perform very
competitively with respect to PDC.

n2 | log Zalg − log Z|; and error in H(x∗), deﬁned as 1

The key feature of our algorithm is scalability. Speciﬁcally, running time of our algorithm with a
given parameter value ∆ scales linearly in n, while keeping the relative error bound exactly the
same. To explain this important feature, we plot the theoretically evaluated bound on error in log Z

2Though this setup has φi, ψij taking negative values, they are equivalent to the setup considered in the
paper as the function values are lower bounded and hence afﬁne shift will make them non-negative without
changing the distribution.

6

in Figure 4 with tags (A), (B) and (C). Note that error bound plot is the same for n = 100 (A) and
n = 1000 (B). Clearly, actual error is likely to be smaller than these theoretically plotted bounds.
We note that these bounds only depend on the interaction strengths and not on the values of ﬁelds
strengths (C).

Results similar to of LOG PARTITION are expected from MODE. We plot the theoretically evaluated
bounds on the error in MAP in Figure 4 with tags (A), (B) and (C). Again, the bound on MAP relative
error for given ∆ parameter remains the same for all values of n as shown in (A) for n = 100 and
(B) for n = 1000. There is no change in error bound with respect to the ﬁeld strength (C).
Setup 2. Everything is exactly the same as the above setup with the only difference that grid graph
is replaced by cris-cross graph which is obtained by adding extra four neighboring edges per node
(exception of boundary nodes). Figure 2 shows cris-cross graph with n = 4 (on the right side).
We again run the same algorithm as above setup on this graph. For cris-cross graph, we obtained
its graph decomposition from the decomposition of its grid sub-graph. graph Though the cris-cross
graph is not planar, due to the structure of the cris-cross graph it can be shown (proved) that the
running time of our algorithm will remain the same (in order) and error bound will become only 3
times weaker than that for the grid graph ! We compute these theoretical error bounds for log Z and
MAP which is plotted in Figure 5. This ﬁgure is similar to the Figure 4 for grid graph. This clearly
exhibits the generality of our algorithm even beyond minor excluded graphs.
References
[1] J. Pearl, “Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference,” San Francisco,
CA: Morgan Kaufmann, 1988.
[2] J. Yedidia, W. Freeman and Y. Weiss, “Generalized Belief Propagation,” Mitsubishi Elect. Res. Lab., TR-
2000-26, 2000.
[3] M. J. Wainwright, T. Jaakkola and A. S. Willsky, “Tree-based reparameterization framework for analysis of
sum-product and related algorithms,” IEEE Trans. on Info. Theory, 2003.
[4] M. J. Wainwright, T. S. Jaakkola and A. S. Willsky, “MAP estimation via agreement on (hyper)trees:
Message-passing and linear-programming approaches,” IEEE Trans. on Info. Theory, 51(11), 2005.
[5] S. C. Tatikonda and M. I. Jordan, “Loopy Belief Propagation and Gibbs Measure,” Uncertainty in Artiﬁcial
Intelligence, 2002.
[6] M. Bayati, D. Shah and M. Sharma, “Maximum Weight Matching via Max-Product Belief Propagation,”
IEEE ISIT, 2005.
[7] V. Kolmogorov, “Convergent Tree-reweighted Message Passing for Energy Minimization,” IEEE Transac-
tions on Pattern Analysis and Machine Intelligence, 2006.
[8] V. Kolmogorov and M. Wainwright, “On optimality of tree-reweighted max-product message-passing,”
Uncertainty in Artiﬁcial Intelligence, 2005.
[9] A. Globerson and T. Jaakkola, “Bound on Partition function through Planar Graph Decomposition,” NIPS,
2006.
[10] P. Klein, S. Plotkin and S. Rao, “Excluded minors, network decomposition, and multicommodity ﬂow,”
ACM STOC, 1993.
[11] S. Rao, “Small distortion and volume preserving embeddings for Planar and Euclidian metrics,” ACM
SCG, 1999.

























































































Grid

























































































Cris

Figure 2: Example of grid graph (left) and cris-cross graph (right) with n = 4.

7

(1-A) Grid, N=7

(1-B) Gird, n=7

TRW

PDC

3(cid:32)(cid:39)

4(cid:32)(cid:39)

5(cid:32)(cid:39)

(cid:865)(cid:863)(cid:868)

(cid:865)(cid:863)(cid:867)(cid:870)

(cid:865)(cid:863)(cid:867)

 

Z
E
r
r
o
r

(cid:865)(cid:863)(cid:866)(cid:870)

(cid:865)(cid:863)(cid:866)

(cid:865)(cid:863)(cid:865)(cid:870)

(cid:865)

TRW

PDC

3(cid:32)(cid:39)

4(cid:32)(cid:39)

5(cid:32)(cid:39)

 

Z
E
r
r
o
r

(cid:865)(cid:863)(cid:865)(cid:868)(cid:870)

(cid:865)(cid:863)(cid:865)(cid:868)

(cid:865)(cid:863)(cid:865)(cid:867)(cid:870)

(cid:865)(cid:863)(cid:865)(cid:867)

(cid:865)(cid:863)(cid:865)(cid:866)(cid:870)

(cid:865)(cid:863)(cid:865)(cid:866)

(cid:865)(cid:863)(cid:865)(cid:865)(cid:870)

(cid:865)

(cid:865)(cid:863)(cid:867)

(cid:865)(cid:863)(cid:869)

(cid:865)(cid:863)(cid:871)

(cid:865)(cid:863)(cid:873)

(cid:866)

(cid:866)(cid:863)(cid:867)

(cid:866)(cid:863)(cid:869)

(cid:866)(cid:863)(cid:871)

(cid:866)(cid:863)(cid:873)

(cid:867)

(cid:865)(cid:863)(cid:867)

(cid:865)(cid:863)(cid:869)

(cid:865)(cid:863)(cid:871)

(cid:865)(cid:863)(cid:873)

Interaction Strength

(cid:866)

(cid:866)(cid:863)(cid:867)
Field Strength

(cid:866)(cid:863)(cid:869)

(cid:866)(cid:863)(cid:871)

(cid:866)(cid:863)(cid:873)

(cid:867)

Figure 3: Comparison of TRW, PDC and our algorithm for grid graph with n = 7 with respect to error in log Z. Our algorithm outperforms TRW and is
competitive with respect to PDC.

(2-A) Grid, n=100

(2-B) Grid, n=1000

(2-C) Grid, n=1000

5(cid:32)(cid:39)

10(cid:32)(cid:39)

20(cid:32)(cid:39)

 

Z
E

r
r
o
r
 

B
o
u
n
d

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

 

Z
E

r
r
o
r
 

B
o
u
n
d

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0.2

0.4

0.6

0.8

1

1.2

1.4

1.6

1.8

2

0.2

0.4

0.6

0.8

1

1.2

1.4

1.6

1.8

2

(3-A) Grid, n=100

Interaction Strength

(3-B) Grid, n=1000

Interaction Strength

M
A
P
E

 

r
r
o
r
 

B
o
u
n
d

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

5(cid:32)(cid:39)

10(cid:32)(cid:39)

20(cid:32)(cid:39)

0.2

0.4

0.6

M
A
P
E

 

r
r
o
r
 

B
o
u
n
d

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

 

Z
E

r
r
o
r
 

B
o
u
n
d

M
A
P
E

 

r
r
o
r
 

B
o
u
n
d

2.5

2

1.5

1

0.5

0

2.5

2

1.5

1

0.5

0

0.2

0.4

0.6

0.8

1

1.2

1.4

1.6

1.8

2

(3-C) Grid, n=1000

Field Strength

1

1.2

0.8
1.4
Interaction Strength

1.6

1.8

2

0.2

0.4

0.6

0.8

1

1.2

1.4

1.6

1.8

2

0.2

0.4

0.6

0.8

1

1.2

1.4

1.6

1.8

2

Interaction Strength

Field Strength

Figure 4: The theoretically computable error bounds for log Z and MAP under our algorithm for grid with n = 100 and n = 1000 under varying
interaction and varying ﬁeld model. This clearly shows scalability of our algorithm.

(4-A) Cris Cross, n=100

5(cid:32)(cid:39)

10(cid:32)(cid:39)

20(cid:32)(cid:39)

(4-B) Cris Cross, n=1000

 

Z
E

r
r
o
r
 

B
o
u
n
d

2.5

2

1.5

1

0.5

0

0.2

0.4

0.6

0.8

1

1.2

1.4

1.6

1.8

2

0.2

0.4

0.6

0.8

1

1.2

1.4

1.6

1.8

2

(5-A) Criss Cross, n=100

Interaction Strength

5(cid:32)(cid:39)

10(cid:32)(cid:39)

20(cid:32)(cid:39)

(5-B) Cris Cross, n=1000

Interaction Strength

M
A
P
E

 

r
r
o
r
 

B
o
u
n
d

2.5

2

1.5

1

0.5

0

2.5

2

 

Z
E

r
r
o
r
 

B
o
u
n
d

1.5

1

0.5

0

2.5

2

1.5

1

0.5

0

M
A
P
E

 

r
r
o
r
 

B
o
u
n
d

 

Z
E

r
r
o
r
 

B
o
u
n
d

M
A
P
E

 

r
r
o
r
 

B
o
u
n
d

0.6

0.5

0.4

0.3

0.2

0.1

0

0.6

0.5

0.4

0.3

0.2

0.1

0

(4-C) Cris Cross, n=1000

0.2

0.4

0.6

0.8

(5-C) Cris Cross, n=1000

1

1.2
Field Strength

1.4

1.6

1.8

2

0.2

0.4

0.6

0.8

1

1.2

1.4

1.6

1.8

2

0.2

0.4

0.6

0.8

1

1.2

1.4

1.6

1.8

2

0.2

0.4

0.6

0.8

Interaction Strength

Interaction Strength

1.4

1.6

1.8

2

1
1.2
Field Strength

Figure 5: The theoretically computable error bounds for log Z and MAP under our algorithm for cris-cross with n = 100 and n = 1000 under varying
interaction and varying ﬁeld model. This clearly shows scalability of our algorithm and robustness to graph structure.

8

"
67,2007,Variational inference for Markov jump processes,"Markov jump processes play an important role in a large number of application domains. However, realistic systems are analytically intractable and they have traditionally been analysed using simulation based techniques, which do not provide a framework for statistical inference. We propose a mean field approximation to perform posterior inference and parameter estimation. The approximation allows a practical solution to the inference problem, {while still retaining a good degree of accuracy.} We illustrate our approach on two biologically motivated systems.","Variational inference for Markov jump processes

Department of Computer Science

Technische Universit¤at Berlin
D-10587 Berlin, Germany

Manfred Opper

opperm@cs.tu-berlin.de

Guido Sanguinetti

Department of Computer Science

University of Shef(cid:2)eld, U.K.
guido@dcs.shef.ac.uk

Abstract

Markov jump processes play an important role in a large number of application
domains. However, realistic systems are analytically intractable and they have tra-
ditionally been analysed using simulation based techniques, which do not provide
a framework for statistical inference. We propose a mean (cid:2)eld approximation to
perform posterior inference and parameter estimation. The approximation allows
a practical solution to the inference problem, while still retaining a good degree of
accuracy. We illustrate our approach on two biologically motivated systems.

Introduction

Markov jump processes (MJPs) underpin our understanding of many important systems in science
and technology. They provide a rigorous probabilistic framework to model the joint dynamics of
groups (species) of interacting individuals, with applications ranging from information packets in
a telecommunications network to epidemiology and population levels in the environment. These
processes are usually non-linear and highly coupled, giving rise to non-trivial steady states (often
referred to as emerging properties). Unfortunately, this also means that exact statistical inference is
unfeasible and approximations must be made in the analysis of these systems.
A traditional approach, which has been very successful throughout the past century, is to ignore
the discrete nature of the processes and to approximate the stochastic process with a deterministic
process whose behaviour is described by a system of non-linear, coupled ODEs. This approximation
relies on the stochastic (cid:3)uctuations being negligible compared to the average population counts.
There are many important situations where this assumption is untenable: for example, stochastic
(cid:3)uctuations are reputed to be responsible for a number of important biological phenomena, from
cell differentiation to pathogen virulence [1]. Researchers are now able to obtain accurate estimates
of the number of macromolecules of a certain species within a cell [2, 3], prompting a need for
practical statistical tools to handle discrete data.
Sampling approaches have been extensively used to simulate the behaviour of MJPs. Gillespie’s
algorithm and its generalisations [4, 5] form the basis of many simulators used in systems biology
studies. The simulations can be viewed as individual samples taken from a completely speci(cid:2)ed
MJP, and can be very useful to reveal possible steady states. However, it is not clear how observed
data can be incorporated in a principled way, which renders this approach of limited use for posterior
inference and parameter estimation. A Markov chain Monte Carlo (MCMC) approach to incorpo-
rate observations has been recently proposed by Boys et al. [6]. While this approach holds a lot of
promise, it is computationally very intensive. Despite several simplifying approximations, the cor-
relations between samples mean that several millions of MCMC iterations are needed even in simple
examples. In this paper we present an alternative, deterministic approach to posterior inference and
parameter estimation in MJPs. We extend the mean-(cid:2)eld (MF) variational approach ([cf. e.g. 7])
to approximate a probability distribution over an (in(cid:2)nite dimensional) space of discrete paths, rep-
resenting the time-evolving state of the system. In this way, we replace the couplings between the

1

different species by their average, mean-(cid:2)eld (MF) effect. The result is an iterative algorithm that
allows parameter estimation and prediction with reasonable accuracy and very contained computa-
tional costs.
The rest of this paper is organised as follows: in sections 1 and 2 we review the theory of Markov
jump processes and introduce our general strategy to obtain a MF approximation.
In section 3
we introduce the Lotka-Volterra model which we use as an example to describe how our approach
works. In section 4 we present experimental results on simulated data from the Lotka-Volterra model
and from a simple gene regulatory network. Finally, we discuss the relationship of our study to other
stochastic models, as well as further extensions and developments of our approach.

1 Markov jump processes

We start off by establishing some notation and basic de(cid:2)nitions. A D-dimensional discrete stochas-
tic process is a family of D-dimensional discrete random variables x(t) indexed by the continuous
time t. In our examples, the values taken by x(t) will be restricted to the non-negative integers
0 . The dimensionality D represents the number of (molecular) species present in the system; the
ND
components of the vector x (t) then represent the number of individuals of each species present at
time t. Furthermore, the stochastic processes we will consider will always be Markovian, i.e. given
any sequence of observations for the state of the system (xt1 ; : : : ; xtN ), the conditional probability
of the state of the system at a subsequent time xtN +1 depends only on the last of the previous obser-
vations. A discrete stochastic process which exhibits the Markov property is called a Markov jump
process (MJP).
A MJP is characterised by its process rates f (x
interval (cid:14)t, the quantity f (x
a transition from state x at time t to state x

6= x; in an in(cid:2)nitesimal time
0jx) (cid:14)t represents the in(cid:2)nitesimal probability that the system will make

0 at time t + (cid:14)t. Explicitly,

0jx), de(cid:2)ned 8x

0

0

0

p (x

0jx)

x + (cid:14)tf (x

0jx) ’ (cid:14)x

where (cid:14)x

(1) implies by normalisation that f (xjx) = (cid:0)Px

(1)
x is the Kronecker delta and the equation becomes exact in the limit (cid:14)t ! 0. Equation
0jx). The interpretation of the process
rates as in(cid:2)nitesimal transition probabilities highlights the simple relationship between the marginal
distribution pt (x) and the process rates. The probability of (cid:2)nding the system in state x at time
t + (cid:14)t will be given by the probability that the system was already in state x at time t, minus the
probability that the system was in state x at time t and jumped to state x
0, plus the probability that
00 at time t and then jumped to state x. In formulae, this is given
the system was in a different state x
by

06=x f (x

0jx) (cid:14)t3

f (x

5 + Xx

06=x

pt (x

0) f (xjx

0) (cid:14)t:

pt+(cid:14)t (x) = pt (x)2

06=x

41 (cid:0) Xx
= Xx

06=x

dpt (x)

dt

Taking the limit for (cid:14)t ! 0 we obtain the (forward) Master equation for the marginal probabilities

[(cid:0)pt (x) f (x

0jx) + pt (x

0) f (xjx

0)] :

(2)

2 Variational approximate inference

Let us assume that we have noisy observations yl
l = 1; : : : ; N of the state of the system at a dis-
crete number of time points; the noise model is speci(cid:2)ed by a likelihood function ^p (yljx (tl)). We
can combine this likelihood with the prior process to obtain a posterior process. As the observations
happen at discrete time points, the posterior process is clearly still a Markov jump process. Given
the Markovian nature of the processes, one could hope to obtain the posterior rate functions g(x
0jx)
by a forward-backward procedure similar to the one used for Hidden Markov Models. While this
is possible in principle, the computations would require simultaneously solving a very large system
of coupled linear ODEs (the number of equations is of order S D, S being the number of states
accessible to the system), which is not feasible even in simple systems.

2

In the following, we will use the variational mean (cid:2)eld (MF) approach to approximate the posterior
process by a factorizing process, minimising the Kullback - Leibler (KL) divergence between pro-
cesses. The inference process is then reduced to the solution of D one - dimensional Master and
backward equations of size S. This is still nontrivial because the KL divergence requires the joint
probabilities of variables x(t) at in(cid:2)nitely many different times t, i.e. probabilities over entire paths
of a process rather than the simpler marginals pt(x). We will circumvent this problem by working
with time discretised trajectories and then passing on to the continuum time limit. We denote such
a trajectory as x0:K = (x (t0) ; : : : ; x (t0 + K(cid:14)t)) where (cid:14)t is a small time interval and K is very
large. Hence, we write the joint posterior probability as

ppost(x0:K) =

1
Z

pprior(x0:K) (cid:2)

N

Yl=1

^p (yljx (tl)) with pprior(x0:K ) = p(x0)

p(xk+1jxk)

K(cid:0)1

Yk=0

with Z = p(y1; : : : ; yN ). Note that x (tl) 2 x0:K. In the rest of this section, we will show how to
compute the posterior rates and marginals by minimising the KL divergence. We notice in passing
that a similar framework for continuous stochastic processes was proposed recently in [8].

2.1 KL divergence between MJPs

The KL divergence between two MJPs de(cid:2)ned by their path probabilities p(x0:K ) and q(x0:K ) is

q(x0:K ) ln

q(x0:K )
p(x0:K )

=

K(cid:0)1

Xk=0 Xxk

q(xk) Xxk+1

q(xk+1jxk) ln

q(xk+1jxk)
p(xk+1jxk)

+ K0

KL [q; p] = Xx0:K
and where K0 =Px0
KL [q; p] =Z T

0
0jx) and g(x

p(x0) will be set to zero in the following. We can now use equation
(1) for the conditional probabilities; letting (cid:14)t ! 0 and simultaneously K ! 1 so that K(cid:14)t ! T ,
we obtain

q (x0) log q(x0)

dt Xx

qt(x) Xx

0:x

06=x(cid:26)g(x

0jx) ln

0jx)
g(x
f (x0jx)

+ f (x

0jx) (cid:0) g(x

0jx)(cid:27)

(3)

where f (x
0jx) are the rates of the p and q process respectively. Notice that we have
swapped from the path probabilities framework to an expression that depends solely on the process
rates and marginals.

2.2 MF approximation to posterior MJPs

We will now consider the case where p is a posterior MJP and q is an approximating process. The
prior process will be denoted as pprior and its rates will be denoted by f. The KL divergence then is

N

KL(q; ppost) = ln Z + KL(q; pprior) (cid:0)

Eq [ln ^p (yljx (tl))] :

To obtain a tractable inference problem, we will assume that, in the approximating process q, the
joint path probability for all the species factorises into the product of path probabilities for individ-
ual species. This gives the following equations for the species probabilities and transition rates

qt (x) =

D

Yi=1

qit (xi)

gt (x

0jx) =

D

Xi=1Yj6=i

(cid:14)x0

j ;xj git (x0

ijxi) :

(4)

Notice that we have emphasised that the process rates for the approximating process may depend
explicitly on time, even if the process rates of the original process do not. Exploiting these assump-
tions, we obtain that the KL divergence between the approximating process and the posterior process
is given by

N

Xl=1

KL [q; ppost] = ln Z (cid:0)

Eq [ln ^p (yljx (tl))] +

Xl=1

Z T

0

dtXi Xx

qit(x) Xx0:x06=x(git (x0jx) ln

git (x0jx)
^fi (x0jx)

+ ~fi (x0jx) (cid:0) git (x0jx))

(5)

3

where we have de(cid:2)ned

^fi (x0jx) = exp(cid:0)Exni[ln fi(cid:0)x
~fi (x0jx) = Exni[fi(cid:0)x

0jx : x0

0jx : x0

j = xj; 8j 6= i(cid:1)](cid:1)

j = xj; 8j 6= i(cid:1)]

and Exni[: : :] denotes an expectation over all components of x except xi (using the measure q). In
order to (cid:2)nd the MF approximation to the posterior process we must optimise the KL divergence (5)
with respect to the marginals qit(x) and the rates git (x0jx). These, however, are not independent
but ful(cid:2)ll the Master equation (2).
We will take care of this constraint by using a Lagrange multiplier function (cid:21)i(x; t) and compute
the stationary values of the Lagrangian

(6)

(7)

(8)

(9)

L =KL (q; ppost)

(cid:0)Xi Z T

0

dtXx

(cid:21)i (x; t)0

@@tqit (x) (cid:0) Xx06=x

fgit (xjx0) qit (x0) (cid:0) git (x0jx) qit (x)g1
A :

We can now compute functional derivatives of (7) to obtain

(cid:14)L

(cid:14)qit(x)

= Xx06=x""git (x0jx) ln
Xx0

git (x0jx)
^fi (x0jx)

(cid:0) git (x0jx) + ~fi (x0jx)# + @t(cid:21)i (x; t) +

git (x0jx) f(cid:21)i (x0; t) (cid:0) (cid:21)i (x; t)g (cid:0)Xl

ln ^p (yljx (t)) (cid:14) (t (cid:0) tl) = 0

(cid:14)L

(cid:14)git (x0jx)

= qit (x) ln

git (x0jx)
^fi (x0jx)

+ (cid:21)i (x0; t) (cid:0) (cid:21)i (x; t)! = 0

De(cid:2)ning ri(x; t) = e(cid:0)(cid:21)i(x;t) and inserting (9) into (8), we arrive at the linear differential equation

dri(x; t)

dt

= Xx06=x(cid:16) ~fi (x0jx) ri (x; t) (cid:0) ^fi (x0jx) ri (x0; t)(cid:17)

(10)

valid for all times outside of the observations. To include the observations, we assume for simplicity
8l. Then

that the noise model factorises across the species, so that ^p (yljx(t)) =Qi ^pi (yiljxi(tl))

equation (8) yields

lim
t!t(cid:0)
l

ri (x; t) = ^pi (yiljxi(tl)) lim
t!t+
l

ri (x; t) :

We can then optimise the Lagrangian (7) using an iterative strategy. Starting with an initial guess for
qt(x) and selecting a species i, we can compute ^fi (x0jx) and ~fi (x0jx). Using these, we can solve
equation (10) backwards starting from the condition ri (x; T ) = 18x (i.e., the constraint becomes
void at the end of the time under consideration). This allows us to update our estimate of the rates
git (x0jx) using equation (9), which can then be used to solve the master equation (2) and update our
guess of qit(x). This procedure can be followed sequentially for all the species; as each step leads
to a decrease in the value of the Lagrangian, this guarantees that the algorithm will converge to a
(local) minimum.

2.3 Parameter estimation

Since KL [q; ppost] (cid:21) 0, we obtain as useful by-product of the MF approximation a tractable varia-
tional lower bound on the log - likelihood of the data log Z = log p(y1; : : : ; yN ) from (5). As usual
[e.g 7] such a bound can be used in order to optimise model parameters using a variational E-M
algorithm.

4

3 Example: the Lotka-Volterra process

The Lotka-Volterra (LV) process is often used as perhaps the simplest non-trivial MJP [6, 4]. Intro-
duced independently by Alfred J. Lotka in 1925 and Vito Volterra in 1926, it describe the dynamics
of a population composed of two interacting species, traditionally referred to as predator and prey.
The process rates for the LV system are given by

fprey (x + 1jx; y) = (cid:11)x
fpredator (y + 1jx; y) = (cid:14)xy

fprey (x (cid:0) 1jx; y) = (cid:12)xy
fprey (y (cid:0) 1jx; y) = (cid:13)y

(11)

where x is the number of preys and y is the number of predators. All other rates are zero: individuals
can only be created or destroyed one at the time. Rate sparsity is a characteristic of very many
processes, including all chemical kinetic processes (indeed, the LV model can be interpreted as a
chemical kinetic model). An immediate dif(cid:2)culty in implementing our strategy is that some of the
process rates are identically zero when one of the species is extinct (i.e. its numbers have reached
zero); this will lead to in(cid:2)nities when computing the expectation of the logarithm of the rates in
equation (6). To avoid this, we will (cid:147)regularise(cid:148) the process by adding a small constant to the f (1j0);
it can be proved that on average over the data generating process the variational approximation to
the regularised process still optimises a bound analogous to (3) on the original process [9].
The variational estimates for the parameters of the LV process are obtained by inserting the process
rates (11) into the MF bound and taking derivatives w.r.t. the parameters. Setting them to zero, we
obtain a set of (cid:2)xed point equations

(cid:11) = R T
(cid:13) = R T

0 hgpreyt (x + 1jx)ipreyt

;

0 dt hxipreyt

0 hgpredatort (y (cid:0) 1jy)ipredatort

;

0 dt hyipredatort

R T
R T

(cid:12) = R T
R T

0 hgpreyt (x (cid:0) 1jx)ipreyt
0 dt hxipreyt hyipredatort
0 hgpredatort (y + 1jy)ipredatort

0 dt hyipredatort hxipreyt

;

:

(12)

(cid:14) = R T

R T

Equations (12) have an appealing intuitive meaning in terms of the physics of the process: for
example, (cid:11) is given by the average total increase rate of the approximating process divided by the
average total number of preys.
We generated 15 counts of predator and prey numbers at regular intervals from a LV process with
parameters (cid:11) = 5 (cid:2) 10(cid:0)4, (cid:12) = 1 (cid:2) 10(cid:0)4, (cid:13) = 5 (cid:2) 10(cid:0)4 and (cid:14) = 1 (cid:2) 10(cid:0)4, starting from initial
population levels of seven predators and nineteen preys. These counts were then corrupted according
to the following noise model

^pi (yiljxi (tl)) /(cid:20)

1

2jyil(cid:0)xi(tl)j + 10(cid:0)6(cid:21) ;

(13)

where xi (tl) is the (discrete) count for species i at time tl before the addition of noise. Notice that,
since population numbers are constrained to be positive, the noise model is not symmetric. The
original count is placed at the mode, rather than the mean, of the noise model. This asymmetry is
unavoidable when dealing with quantities that are constrained positive.
While in theory each species can have an arbitrarily large number of individuals, in order to solve the
differential equations (2) and (10) we have to truncate the process. While the truncation threshold
could be viewed as another parameter and optimised variationally, in these experiments we took a
more heuristic approach and limited the maximum number of individuals of each species to 200.
This was justi(cid:2)ed by considering that an exponential growth pattern (cid:2)tted to the available data led to
an estimate of approximately 90 individuals in the most abundant species, well below the truncation
threshold.
The results of the inference are shown in Figure 1. The solid line is the mean of the approximating
distribution, the dashed lines are the 90% con(cid:2)dence intervals, the dotted line is the true path from
which the data was obtained. The diamonds are the noisy observations. The parameter values
inferred are reasonably close to the real parameter values: (cid:11) = 1:35 (cid:2) 10(cid:0)3, (cid:12) = 2:32 (cid:2) 10(cid:0)4,

5

y

25

20

15

10

5

0
0

x

25

20

15

10

5

0
0

500

1000

1500

(a)

2000

2500

3000
t

500

1000

1500

(b)

2000

2500

3000
t

Figure 1: MF approximation to posterior LV process: (a) predator population and (b) prey pop-
ulation. Diamonds are the (noisy) observed data points, solid line the mean, dashed lines 90%
con(cid:2)dence intervals, dotted lines the true path from which the data was sampled.

(cid:13) = 1:57 (cid:2) 10(cid:0)3 and (cid:14) = 1:78 (cid:2) 10(cid:0)4. While the process is well approximated in the area where
data is present, the free-form prediction is less good, especially for the predator population. This
might be due to the inaccuracies in the estimates of the parameters. The approximate posterior
displays nontrivial emerging properties: for example, we predict that there is a 10% chance that the
prey population will become extinct at the end of the period of interest. These results were obtained
in approximately (cid:2)fteen minutes on an Intel Pentium M 1.7GHz laptop computer.
To check the reliability of our inference results and the rate with which the estimated parameter
values converge to the true values, we repeated our experiments for 5, 10, 15 and 20 available data
points. For each sample size, we drew (cid:2)ve independent samples from the same LV process. Figure
2(a) shows the average and standard deviation of the mean squared error (MSE) in the estimate of
the parameters as a function of the number of observations N; as expected, this decreases uniformly
with the sample size.

4 Example: gene autoregulatory network

As a second example we consider a gene autoregulatory network. This simple network motif is
one of the most important building blocks of the transcriptional regulatory networks found in cells
because of its ability to increase robustness in the face of (cid:3)uctuation in external signals [10]. Because
of this, it is one of the best studied systems, both at the experimental and at the modelling level
[11, 3]. The system consists again of two species, mRNA and protein; the process rates are given by

fRN A (x (cid:0) 1jx; y) = (cid:12)x
fp (y (cid:0) 1jx; y) = (cid:14)y

fRN A(x + 1jx; y) = (cid:11) (1 (cid:0) 0:99 (cid:2) (cid:2) (y (cid:0) yc))
fp (y + 1jx; y) = (cid:13)x

(14)
where (cid:2) is the Heavyside step function, y the protein number and x the mRNA number. The intuitive
meaning of these equations is simple: both protein and mRNA decay exponentially. Proteins are
produced through translation of mRNA with a rate proportional to the mRNA abundance. On the
other hand, mRNA production depends on protein concentration levels through a logical function:
as soon as protein numbers increase beyond a certain critical parameter yc, mRNA production drops
dramatically by a factor 100.
The optimisation of the variational bound w.r.t. the parameters (cid:11), (cid:12), (cid:13) and (cid:14) is straightforward and
yields (cid:2)xed point equations similar to the ones for the LV process. The dependence of the MF bound
on the critical parameter yc is less straightforward and is given by

Lyc = const +(2Z T

dt(cid:22)gh (yc) + log""1 (cid:0) 0:99
where (cid:22)g = hgRN A (x + 1jx)iqRN A and h (yc) = Py(cid:21)yc

qp (y). A plot of this function obtained
during the inference task below can be seen in Figure 2(b). We can determine the minimum of (15)
by searching over the possible (discrete) values of yc.

h (yc) dt#Z T

dt(cid:22)g)

(15)

1

T Z T

0

0

0

6

x 10−4

2

M SE

1.5

1

L

2.5

2

1.5

1

0.5

0

0.5
4

6

8

10

12

14

(a)

16

18

20

22

N

−0.5
0

20

40

60

80

100

(b)

120

140

160

180

200

yc

Figure 2: (a) Mean squared error (MSE) in the estimate of the parameters as a function of the
number of observations N for the LV process. (b) Negative variational likelihood bound for the
gene autoregulatory network as a function of the critical parameter yc.

y

30

28

26

24

22

20

18

16

14
0

x

18

17

16

15

14

13

12

11

10

500

1000

(a)

1500

2000
t

9
0

500

1000

(b)

1500

2000
t

Figure 3: MF approximation to posterior autoregulatory network process: (a) protein population and
(b) mRNA population. Diamonds are the (noisy) observed data points, solid line the mean, dashed
lines 90% con(cid:2)dence intervals, dotted lines the true path from which the data was taken.

Again, we generated data by simulating the process with parameter values yc = 20, (cid:11) = 2 (cid:2) 10(cid:0)3,
(cid:12) = 6 (cid:2) 10(cid:0)5, (cid:13) = 5 (cid:2) 10(cid:0)4 and (cid:14) = 7 (cid:2) 10(cid:0)5. Fifteen counts were generated for both mRNA
and proteins, with initial count of 17 protein and 12 mRNA molecules. These were then corrupted
with noise generated from the distribution shown in equation (13). The results of the approximate
posterior inference are shown in Figure 3. The inferred parameter values are in good agreement
with the true values: yc = 19, (cid:11) = 2:20 (cid:2) 10(cid:0)3, (cid:12) = 1:84 (cid:2) 10(cid:0)5, (cid:13) = 4:01 (cid:2) 10(cid:0)4 and
(cid:14) = 1:54 (cid:2) 10(cid:0)4. Interestingly, if the data is such that the protein count never exceeds the critical
parameter yc, this becomes unidenti(cid:2)able (the likelihood bound is optimised by yc = 1 or yc = 0),
as may be expected. The likelihood bound loses its sharp optimum evident from Figure 2(b) (results
not shown).

5 Discussion

In this contribution we have shown how a MF approximation can be used to perform posterior in-
ference in MJPs from discretely observed noisy data. The MF approximation has been shown to
perform well and to retain much of the richness of these complex systems. The proposed approach
is conceptually very different from existing MCMC approaches [6]. While these focus on sampling
from the distribution of reactions happening in a small interval in time, we compute an approxi-
mation to the probability distribution over possible paths of the system. This allows us to easily
factorise across species; by contrast, sampling the number of reactions happening in a certain time

7

interval is dif(cid:2)cult, and not amenable to simple techniques such as Gibbs sampling. While it is
possible that future developments will lead to more ef(cid:2)cient sampling strategies, our approach out-
strips current MCMC based methods in terms of computational ef(cid:2)ciency, A further strength of our
approach is the ease with which it can be scaled to more complex systems involving larger numbers
of species. The factorisation assumption implies that the computational complexity grows linearly
in the number of species D; it is unclear how MCMC would scale to larger systems.
An alternative suggestion, proposed in [11], was somehow to seek a middle way between a MJP
and a deterministic, ODE based approach by approximating the MJP with a continuous stochastic
process, i.e. by using a diffusion approximation. While these authors show that this approximation
works reasonably well for inference purposes, it is worth pointing out that the population sizes
in their experimental results were approximately one order of magnitude larger than in ours.
It
is arguable that a diffusion approximation might be suitable for population sizes as low as a few
hundreds, but it cannot be expected to be reasonable for population sizes of the order of 10.
The availability of a practical tool for statistical inference in MJPs opens a number of important
possible developments for modelling. It would be of interest, for example, to develop mixed mod-
els where one species with low counts interacts with another species with high counts that can be
modelled using a deterministic or diffusion approximation. This situation would be of particular im-
portance for biological applications, where different proteins can have very different copy numbers
in a cell but still be equally important. Another interesting extension is the possibility of introducing
a spatial dimension which in(cid:3)uences how likely interactions are. Such an extension would be very
important, for example, in an epidemiological study. All of these extensions rely centrally on the
possibility of estimating posterior probabilities, and we expect that the availability of a practical tool
for the inference task will be very useful to facilitate this.

References
[1] Harley H. McAdams and Adam Arkin. Stochastic mechanisms in gene expression. Proceed-

ings of the National Academy of Sciences USA, 94:814(cid:150)819, 1997.

[2] Long Cai, Nir Friedman, and X. Sunney Xie. Stochastic protein expression in individual cells

at the single molecule level. Nature, 440:580(cid:150)586, 2006.

[3] Yoshito Masamizu, Toshiyuki Ohtsuka, Yoshiki Takashima, Hiroki Nagahara, Yoshiko Take-
naka, Kenichi Yoshikawa, Hitoshi Okamura, and Ryoichiro Kageyama. Real-time imaging of
the somite segmentation clock: revelation of unstable oscillators in the individual presomitic
mesoderm cells. Proceedings of the National Academy of Sciences USA, 103:1313(cid:150)1318,
2006.

[4] Daniel T. Gillespie. Exact stochastic simulation of coupled chemical reactions. Journal of

Physical Chemistry, 81(25):2340(cid:150)2361, 1977.

[5] Eric Mjolsness and Guy Yosiphon. Stochastic process semantics for dynamical grammars. to

appear in Annals of Mathematics and Arti(cid:2)cial Intelligence, 2006.

[6] Richard J. Boys, Darren J. Wilkinson, and Thomas B. L. Kirkwood.

Bayesian
from

available

inference
http://www.staff.ncl.ac.uk/d.j.wilkinson/pub.html, 2004.

a discretely observed stochastic kinetic model.

for

[7] Manfred Opper and David Saad (editors). Advanced Mean Field Methods. MIT press, Cam-

bridge,MA, 2001.

[8] Cedric Archambeau, Dan Cornford, Manfred Opper, and John Shawe-Taylor. Gaussian process
approximations of stochastic differential equations. Journal of Machine Learning Research
Workshop and Conference Proceedings, 1(1):1(cid:150)16, 2007.

[9] Manfred Opper and David Haussler. Bounds for predictive errors in the statistical mechanics

of supervised learning. Physical Review Letters, 75:3772(cid:150)3775, 1995.

[10] Uri Alon. An introduction to systems biology. Chapman and Hall, London, 2006.
[11] Andrew Golightly and Darren J. Wilkinson. Bayesian inference for stochastic kinetic models

using a diffusion approximation. Biometrics, 61(3):781(cid:150)788, 2005.

8

"
429,2007,Optimal ROC Curve for a Combination of Classifiers,"We present a new analysis for the combination of binary classifiers. We propose a theoretical framework based on the Neyman-Pearson lemma to analyze combinations of classifiers. In particular, we give a method for finding the optimal decision rule for a combination of classifiers and prove that it has the optimal ROC curve. We also show how our method generalizes and improves on previous work on combining classifiers and generating ROC curves.","Optimal ROC Curve for a Combination of Classiﬁers

Marco Barreno

Alvaro A. C´ardenas

Computer Science Division

University of California at Berkeley

Berkeley, California 94720

J. D. Tygar

{barreno,cardenas,tygar}@cs.berkeley.edu

Abstract

We present a new analysis for the combination of binary classiﬁers. Our analysis
makes use of the Neyman-Pearson lemma as a theoretical basis to analyze combi-
nations of classiﬁers. We give a method for ﬁnding the optimal decision rule for a
combination of classiﬁers and prove that it has the optimal ROC curve. We show
how our method generalizes and improves previous work on combining classiﬁers
and generating ROC curves.

1 Introduction

We present an optimal way to combine binary classiﬁers in the Neyman-Pearson sense: for a given
upper bound on false alarms (false positives), we ﬁnd the set of combination rules maximizing the
detection rate (true positives). This forms the optimal ROC curve of a combination of classiﬁers.

This paper makes the following original contributions: (1) We present a new method for ﬁnding
the meta-classiﬁer with the optimal ROC curve. (2) We show how our framework can be used to
interpret, generalize, and improve previous work by Provost and Fawcett [1] and Flach and Wu [2].
(3) We present experimental results that show our method is practical and performs well, even when
we must estimate the distributions with insufﬁcient data.

In addition, we prove the following results: (1) We show that the optimal ROC curve is composed
in general of 2n + 1 different decision rules and of the interpolation between these rules (over the
space of 22n possible Boolean rules). (2) We prove that our method is optimal in this space. (3) We
prove that the Boolean AND and OR rules are always part of the optimal set for the special case of
independent classiﬁers (though in general we make no independence assumptions). (4) We prove a
sufﬁcient condition for Provost and Fawcett’s method to be optimal.

2 Background

Consider classiﬁcation problems where examples from a space of inputs X are associated with
binary labels {0, 1} and there is a ﬁxed but unknown probability distribution P(x, c) over examples
(x, c) ∈ X × {0, 1}. H0 and H1 denote the events that c = 0 and c = 1, respectively.
A binary classiﬁer is a function f : X → {0, 1} that predicts labels on new inputs. When we use
the term “classiﬁer” in this paper we mean binary classiﬁer. We address the problem of combining
results from n base classiﬁers f1, f2, . . . , fn. Let Yi = fi(X) be a random variable indicating the
output of classiﬁer fi and Y ∈ {0, 1}n = (Y1, Y2, . . . , Yn). We can characterize the performance of
classiﬁer fi by its detection rate (also true positives, or power) PDi = Pr[Yi = 1|H1] and its false
alarm rate (also false positives, or test size) PF i = Pr[Yi = 1|H0]. In this paper we are concerned
with proper classiﬁers, that is, classiﬁers where PDi > PF i. We sometimes omit the subscript i.

1

The Receiver Operating Characteristic (ROC) curve plots PF on the x-axis and PD on the y-axis
(ROC space). The point (0, 0) represents always classifying as 0, the point (1, 1) represents always
classifying as 1, and the point (0, 1) represents perfect classiﬁcation. If one classiﬁer’s curve has no
points below another, it weakly dominates the latter. If no points are below and at least one point
is strictly above, it dominates it. The line y = x describes a classiﬁer that is no better than chance,
and every proper classiﬁer dominates this line. When an ROC curve consists of a single point, we
connect it with straight lines to (0, 0) and (1, 1) in order to compare it with others (see Lemma 1).
In this paper, we focus on base classiﬁers that occupy a single point in ROC space. Many classiﬁers
have tunable parameters and can produce a continuous ROC curve; our analysis can apply to these
cases by choosing representative points and treating each one as a separate classiﬁer.

2.1 The ROC convex hull

Provost and Fawcett [1] give a seminal result on the use of ROC curves for combining classiﬁers.
They suggest taking the convex hull of all points of the ROC curves of the classiﬁers. This ROC
convex hull (ROCCH) combination rule interpolates between base classiﬁers f1, f2, . . . , fn, select-
ing (1) a single best classiﬁer or (2) a randomization between the decisions of two classiﬁers for
every false alarm rate [1]. This approach, however, is not optimal: as pointed out in later work by
Fawcett, the Boolean AND and OR rules over classiﬁers can perform better than the ROCCH [3].

AND and OR are only 2 of 22n possible Boolean rules over the outputs of n base classiﬁers (n
classiﬁers ⇒ 2n possible outcomes ⇒ 22n rules over outcomes). We address ﬁnding optimal rules.

2.2 The Neyman-Pearson lemma

In this section we introduce Neyman-Pearson theory from the framework of statistical hypothesis
testing [4, 5], which forms the basis of our analysis.
We test a null hypothesis H0 against an alternative H1. Let the random variable Y have probability
distributions P (Y|H0) under H0 and P (Y|H1) under H1, and deﬁne the likelihood ratio ℓ(Y) =
P (Y|H1)/P (Y|H0). The Neyman-Pearson lemma states that the likelihood ratio test

D(Y) =( 1

γ
0

if ℓ(Y) > τ
if ℓ(Y) = τ
if ℓ(Y) < τ

,

(1)

for some τ ∈ (0, ∞) and γ ∈ [0, 1], is a most powerful test for its size: no other test has higher
PD = Pr[D(Y) = 1|H1] for the same bound on PF = Pr[D(Y) = 1|H0]. (When ℓ(Y) = τ ,
D = 1 with probability γ and 0 otherwise.) Given a test size α, we maximize PD subject to PF ≤ α
by choosing τ and γ as follows. First we ﬁnd the smallest value τ ∗ such that Pr[ℓ(Y) > τ ∗|H0] ≤
α. To maximize PD, which is monotonically nondecreasing with PF , we choose the highest value
γ ∗ that satisﬁes Pr[D(Y) = 1|H0] = Pr[ℓ(Y) > τ ∗|H0] + γ ∗ Pr[ℓ(Y) = τ ∗|H0] ≤ α, ﬁnding
γ ∗ = (α − Pr[ℓ(Y) > τ ∗|H0])/ Pr[ℓ(Y) = τ ∗|H0].

3 The optimal ROC curve for a combination of classiﬁers

We characterize the optimal ROC curve for a decision based on a combination of arbitrary
classiﬁers—for any given bound α on PF , we maximize PD. We frame this problem as a Neyman-
Pearson hypothesis test parameterized by the choice of α. We assume nothing about the classiﬁers
except that each produces an output in {0, 1}. In particular, we do not assume the classiﬁers are
independent or related in any way.

Before introducing our method we analyze the one-classiﬁer case (n = 1).

Lemma 1 Let f1 be a classiﬁer with performance probabilities PD1 and PF 1. Its optimal ROC
curve is a piecewise linear function parameterized by a free parameter α bounding PF : for α <
PF 1, PD(α) = (PD1/PF 1)α, and for α > PF 1, PD(α) = [(1 − PD1)/(1 − PF 1)](α − PF 1) + PD1.

Proof. When α < PF 1, we can obtain a likelihood ratio test by setting τ ∗ = ℓ(1) and γ ∗ = α/PF 1,
and for α > PF 1, we set τ ∗ = ℓ(0) and γ ∗ = (α − PF 1)/(1 − PF 1).

2

2

The intuitive interpretation of this result is that to decrease or increase the false alarm rate of the
classiﬁer, we randomize between using its predictions and always choosing 1 or 0. In ROC space,
this forms lines interpolating between (PF 1, PD1) and (1, 1) or (0, 0), respectively.
To generalize this result for the combination of n classiﬁers, we require the distributions P (Y|H0)
and P (Y|H1). With this information we then compute and sort the likelihood ratios ℓ(y) for all
outcomes y ∈ {0, 1}n. Let L be the list of likelihood ratios ranked from low to high.

Lemma 2 Given any 0 ≤ α ≤ 1, the ordering L determines parameters τ ∗ and γ ∗ for a likelihood
ratio test of size α.

Lemma 2 sets up a classiﬁcation rule for each interval between likelihoods in L and interpolates
between them to create a test with size exactly α. Our meta-classiﬁer does this for any given bound
on its false positive rate, then makes predictions according to Equation 1. To ﬁnd the ROC curve for
our meta-classiﬁer, we plot PD against PF for all 0 ≤ α ≤ 1. In particular, for each y ∈ {0, 1}n
we can compute Pr[ℓ(Y) > ℓ(y)|H0], which gives us one value for τ ∗ and a point in ROC space
(PF and PD follow directly from L and P ). Each τ ∗ will turn out to be the slope of a line segment
between adjacent vertices, and varying γ ∗ interpolates between the vertices. We call the ROC curve
obtained in this way the LR-ROC.

Theorem 1 The LR-ROC weakly dominates the ROC curve of any possible combination of Boolean
functions g : {0, 1}n → {0, 1} over the outputs of n classiﬁers.

Proof. Let α′ be the probability of false alarm PF for g. Let τ ∗ and γ ∗ be chosen for a test of
size α′. Then our meta-classiﬁer’s decision rule is a likelihood ratio test. By the Neyman-Pearson
lemma, no other test has higher power for any given size. Since ROC space plots power on the
y-axis and size on the x-axis, this means that the PD for g at PF = α′ cannot be higher than that of
the LR-ROC. Since this is true at any α′, the LR-ROC weakly dominates the ROC curve for g. 2

3.1 Practical considerations

To compute all likelihood ratios for the classiﬁer outcomes we need to know the probability distri-
butions P (Y|H0) and P (Y|H1). In practice these distributions need to be estimated. The simplest
method is to run the base classiﬁers on a training set and count occurrences of each outcome. It is
likely that some outcomes will not occur in the training, or will occur only a small number of times.
Our initial approach to deal with small or zero counts when estimating was to use add-one smooth-
ing. In our experiments, however, simple special-case treatment of zero counts always produced
better results than smoothing, both on the training set and on the test set. See Section 5 for details.

Furthermore, the optimal ROC curve may have a different likelihood ratio for each possible outcome
from the n classiﬁers, and therefore a different point in ROC space, so optimal ROC curves in general
have up to 2n points. This implies an exponential (in the number of classiﬁers) lower bound on the
running time of any algorithm to compute the optimal ROC curve for a combination of classiﬁers.
For a handful of classiﬁers, such a bound is not problematic, but it is impractical to compute the
optimal ROC curve for dozens or hundreds of classiﬁers. (However, by computing and sorting the
likelihood ratios we avoid a 22n-time search over all possible classiﬁcation functions.)

4 Analysis

4.1 The independent case

In this section we take an in-depth look at the case of two binary classiﬁers f1 and f2 that are
conditionally independent given the input’s class, so that P (Y1, Y2|Hc) = P (Y1|Hc)P (Y2|Hc) for
c ∈ {0, 1} (this section is the only part of the paper in which we make any independence assump-
tions). Since Y1 and Y2 are conditionally independent, we do not need the full joint distribution; we
need only the probabilities PD1, PF 1, PD2, and PF 2 to ﬁnd the combined PD and PF . For example,
ℓ(01) = ((1 − PD1)PD2)/((1 − PF 1)PF 2).
The assumption that f1 and f2 are conditionally independent and proper deﬁnes a partial ordering
on the likelihood ratio: ℓ(00) < ℓ(10) < ℓ(11) and ℓ(00) < ℓ(01) < ℓ(11). Without loss of

3

Table 1: Two probability distributions.

Class 1 (H1)

Class 0 (H0)

Class 1 (H1)

Class 0 (H0)

Y1

1

0.375
0.325

0
0.2
0.1

Y2
0
1

Y2
0
1

0
0.5
0.3

Y1

1
0.1
0.1

(a)

Y2
0
1

0
0.2
0.2

Y1

1
0.1
0.5

Y2
0
1

0
0.1
0.5

Y1

1
0.3
0.1

(b)

generality, we assume ℓ(00) < ℓ(01) < ℓ(10) < ℓ(11). This ordering breaks the likelihood ratio’s
range (0, ∞) into ﬁve regions; choosing τ in each region deﬁnes a different decision rule.
The trivial cases 0 ≤ τ < ℓ(00) and ℓ(11) < τ < ∞ correspond to always classifying as
1 and 0, respectively. PD and PF are therefore both equal to 1 and both equal to 0, respec-
tively. For the case ℓ(00) ≤ τ < ℓ(01), Pr [ℓ(Y) > τ ] = Pr [Y = 01 ∨ Y = 10 ∨ Y = 11] =
Pr [Y1 = 1 ∨ Y2 = 1] . Thresholds in this range deﬁne an OR rule for the classiﬁers, with PD =
PD1 + PD2 − PD1PD2 and PF = PF 1 + PF 2 − PF 1PF 2. For the case ℓ(01) ≤ τ < ℓ(10), we
have Pr [ℓ(Y) > τ ] = Pr [Y = 10 ∨ Y = 11] = Pr [Y1 = 1] . Therefore the performance proba-
bilities are simply PD = PD1 and PF = PF 1. Finally, the case ℓ(10) ≤ τ < ℓ(11) implies that
Pr [ℓ(Y) > τ ] = Pr [Y = 11] , and therefore thresholds in this range deﬁne an AND rule, with
PD = PD1PD2 and PF = PF 1PF 2. Figure 1a illustrates this analysis with an example.
The assumption of conditional independence is a sufﬁcient condition for ensuring that the AND and
OR rules improve on the ROCCH for n classiﬁers, as the following result shows.

Theorem 2 If the distributions of the outputs of n proper binary classiﬁers Y1, Y2, . . . , Yn are con-
ditionally independent given the instance class, then the points in ROC space for the rules AND
(Y1 ∧ Y2 ∧ · · · ∧ Yn) and OR (Y1 ∨ Y2 ∨ · · · ∨ Yn) are strictly above the convex hull of the ROC
curves of the base classiﬁers f1, . . . , fn. Furthermore, these Boolean rules belong to the LR-ROC.

Proof.
The likelihood ratio of the case when AND outputs 1 is given by ℓ(11 · · · 1) =
(PD1PD2 · · · PDn)/(PF 1PF 2 · · · PF n). The likelihood ratio of the case when OR does not output 1
is given by ℓ(00 · · · 0) = [(1 − PD1)(1 − PD2) · · · (1 − PDn)]/[(1 − PF 1)(1 − PF 2) · · · (1 − PF n)].
Now recall that for proper classiﬁers fi, PDi > PF i and thus (1 − PDi)/(1 − PF i) < 1 < PDi/PF i.
It is now clear that ℓ(00 · · · 0) is the smallest likelihood ratio and ℓ(11 · · · 1) is the largest likelihood
ratio, since others are obtained only by swapping P(F,D)i and (1 − P(F,D)i), and therefore the OR
and AND rules will always be part of the optimal set of decisions for conditionally independent clas-
siﬁers. These rules are strictly above the ROCCH: because ℓ(11 · · · 1) > PD1/PD2, and PD1/PD2
is the slope of the line from (0, 0) to the ﬁrst point in the ROCCH (f1), the AND point must be
above the ROCCH. A similar argument holds for OR since ℓ(00 · · · 0) < (1 − PDn)/(1 − PF n). 2

4.2 Two examples

We return now to the general case with no independence assumptions. We present two example
distributions for the two-classiﬁer case that demonstrate interesting results.

The ﬁrst distribution appears in Table 1a. The likelihood ratio values are ℓ(00) = 0.4, ℓ(10) = 3.75,
ℓ(01) = 1/3, and ℓ(11) = 3.25, giving us ℓ(01) < ℓ(00) < ℓ(11) < ℓ(10). The three non-trivial
rules correspond to the Boolean functions Y1 ∨ ¬Y2, Y1, and Y1 ∧ ¬Y2. Note that Y2 appears only
negatively despite being a proper classiﬁer, and both the AND and OR rules are sub-optimal.

The distribution for the second example appears in Table 1b. The likelihood ratios of the outcomes
are ℓ(00) = 2.0, ℓ(10) = 1/3, ℓ(01) = 0.4, and ℓ(11) = 5, so ℓ(10) < ℓ(01) < ℓ(00) < ℓ(11)
and the three points deﬁning the optimal ROC curve are ¬Y1 ∨ Y2, ¬(Y1 ⊕ Y2), and Y1 ∧ Y2 (see
Figure 1b). In this case, an XOR rule emerges from the likelihood ratio analysis.

These examples show that for true optimal results it is not sufﬁcient to use weighted voting rules
w1Y1 + w2Y2 + · · · + wnYn ≥ τ , where w ∈ (0, ∞) (like some ensemble methods). Weighted
voting always has AND and OR rules in its ROC curve, so it cannot always express optimal rules.

4

1

0.8

Y1 ∨ Y2

Y2

 

0.6

Y1 ∧ Y2

D
P

0.4

0.2

0
 
0

Y1

ROC of f
1
ROC of f
2
LR−ROC

0.2

0.4

PF

0.6

0.8

1

(a)

D
P

1

0.8

0.6

0.4

0.2

0
 
0

¬Y1 ∨ Y2

 

¬(Y1 ⊕ Y2)

Y1 ∧ Y2

Y2

Y1

ROC of f
1
ROC of f
2
LR−ROC

0.2

0.4

PF

0.6

0.8

1

(b)

D
P

1

0.8

0.6

0.4

0.2

0
 
0

f ′
2

f1

f ′
1

 

f ′
3

f3

f2

Original ROC
LR−ROC

0.2

0.4

PF

0.6

0.8

1

(c)

Figure 1: (a) ROC for two conditionally independent classiﬁers. (b) ROC curve for the distributions
in Table 1b. (c) Original ROC curve and optimal ROC curve for example in Section 4.4.

4.3 Optimality of the ROCCH

We have seen that in some cases, rules exist with points strictly above the ROCCH. As the following
result shows, however, there are conditions under which the ROCCH is optimal.

Theorem 3 Consider n classiﬁers f1, . . . , fn. The convex hull of points (PF i, PDi) with (0, 0) and
(1, 1) (the ROCCH) is an optimal ROC curve for the combination if (Yi = 1) ⇒ (Yj = 1) for i < j
and the following ordering holds: ℓ(00 · · · 0) < ℓ(00 · · · 01) < ℓ(00 · · · 011) < · · · < ℓ(1 · · · 1).

Proof. The condition (Yi = 1) ⇒ (Yj = 1) for i < j implies that we only need to consider n + 2
points in the ROC space (the two extra points are (0, 0) and (1, 1)) rather than 2n. It also implies the
following conditions on the joint distribution: Pr[Y1 = 0 ∧ · · · ∧ Yi = 0 ∧ Yi+1 = 1 ∧ · · · ∧ Yn =
1|H0] = PF i+1 − PF i, and Pr[Y1 = 1 ∧ · · · ∧ Yn = 1|H0] = PF 1. With these conditions
and the ordering condition on the likelihood ratios, we have Pr[ℓ(Y) > ℓ(1 · · · 1)|H0] = 0, and
1 · · · 1)|H0] = PF i. Therefore, ﬁnding the optimal threshold of the likelihood
Pr[ℓ(Y) > ℓ(0 · · · 0

ratio test for PF i−1 ≤ α < PF i, we get τ ∗ = ℓ(0 · · · 0
i−1

1 · · · 1), and for PF i ≤ α < PF i+1,

τ ∗ = ℓ(0 · · · 0

1 · · · 1). This change in τ ∗ implies that the point PF i is part of the LR-ROC. Setting

| {z }

α = PF i (thus τ ∗ = ℓ(0 · · · 0

1 · · · 1) and γ ∗=0) implies Pr[ℓ(Y) > τ ∗|H1] = PDi.

2

| {z }i

| {z }i

| {z }i

The condition Yi = 1 ⇒ Yj = 1 for i < j is the same inclusion condition Flach and Wu use
for repairing an ROC curve [2]. It intuitively represents the performance in ROC space of a single
classiﬁer with different operating points. The next section explores this relationship further.

4.4 Repairing an ROC curve

Flach and Wu give a voting technique to repair concavities in an ROC curve that generates operating
points above the ROCCH [2]. Their intuition is that points underneath the convex hull can be
mirrored to appear above the convex hull in much the same way as an improper classiﬁer can be
negated to obtain a proper classiﬁer. Although their algorithm produces better ROC curves, their
solution will often yield curves with new concavities (see for example Flach and Wu’s Figure 4 [2]).
Their algorithm has a similar purpose to ours, but theirs is a local greedy optimization technique,
while our method performs a global search in order to ﬁnd the best ROC curve.

Figure 1c shows an example comparing their method to ours. Consider the following probabil-
ity distribution on a random variable Y ∈ {0, 1}2: P ((00, 10, 01, 11)|H1) = (0.1, 0.3, 0.0, 0.6),
P ((00, 10, 01, 11)|H0) = (0.5, 0.001, 0.4, 0.099). Flach and Wu’s method assumes the original
ROC curve to be repaired has three models, or operating points: f1 predicts 1 when Y ∈ {11}, f2
predicts 1 when Y ∈ {11, 01}, and f3 predicts 1 when Y ∈ {11, 01, 10}. If we apply Flach and
Wu’s repair algorithm, the point f2 is corrected to the point f ′
2; however, the operating points of f1
and f3 remain the same.

5

d
P

d
P

0

.

1

8

.

0

6

.

0

4

.

0

2

.

0

0

.

0

0
1

.

8
0

.

6
0

.

4
0

.

2
0

.

0
0

.

d
P

0

.

1

8

.

0

6

.

0

4

.

0

2

.

0

0

.

0

Meta (train)
Base (train)
Meta (test)
Base (test)
PART

Meta (train)
Base (train)
Meta (test)
Base (test)
PART

0.00

0.05

0.10

Pfa

(a) adult

0.15

0.20

0.000

0.005

0.010

0.015

Pfa

(b) hypothyroid

d
P

0
1

.

8
0

.

6
0

.

4
0

.

2
0

.

0
0

.

Meta (train)
Base (train)
Meta (test)
Base (test)
PART

Meta (train)
Base (train)
Meta (test)
Base (test)
PART

0.00

0.05

0.10

0.15

0.00

0.02

0.04

0.06

0.08

0.10

Pfa

(c) sick-euthyroid

Pfa

(d) sick

Figure 2: Empirical ROC curves for experimental results on four UCI datasets.

Our method improves on this result by ordering the likelihood ratios ℓ(01) < ℓ(00) < ℓ(11) < ℓ(10)
and using that ordering to make three different rules: f ′
2 predicts 1
when Y ∈ {10, 11}, and f ′

1 predicts 1 when Y ∈ {10}, f ′

3 predicts 1 when Y ∈ {10, 11, 00}.

5 Experiments

We ran experiments to test the performance of our combining method on the adult, hypothyroid,
sick-euthyroid, and sick datasets from the UCI machine learning repository [6]. We chose ﬁve base
classiﬁers from the YALE machine learning platform [7]: PART (a decision list algorithm), SMO
(Sequential Minimal Optimization), SimpleLogistic, VotedPerceptron, and Y-NaiveBayes. We used
default settings for all classiﬁers. The adult dataset has around 30,000 training points and 15,000
test points and the sick dataset has around 2000 training points and 700 test points. The others each
have around 2000 points that we split randomly into 1000 training and 1000 test.

For each experiment, we estimate the joint distribution by training the base classiﬁers on a training
set and counting the outcomes. We compute likelihood ratios for all outcomes and order them. When
outcomes have no examples, we treat ·/0 as near-inﬁnite and 0/· as near-zero and deﬁne 0/0 = 1.

6

We derive a sequence of decision rules from the likelihood ratios computed on the training set. We
can compute an optimal ROC curve for the combination by counting the number of true positives
and false positives each rule achieves. In the test set we use the rules learned on the training set.

5.1 Results

The ROC graphs for our four experiments appear in Figure 2. The ROC curves in these experiments
all rise very quickly and then ﬂatten out, so we show only the range of PF 1 for which the values
are interesting. We can draw some general conclusions from these graphs. First, PART clearly
outperforms the other base classiﬁers in three out of four experiments, though it seems to overﬁt
on the hypothyroid dataset. The LR-ROC dominates the ROC curves of the base classiﬁers on both
training and test sets. The ROC curves for the base classiﬁers are all strictly below the LR-ROC
in results on the test sets. The results on training sets seem to imply that the LR-ROC is primarily
classifying like PART with a small boost from the other classiﬁers; however, the test set results (in
particular, Figure 2b) demonstrate that the LR-ROC generalizes better than the base classiﬁers.

The robustness of our method to estimation errors is uncertain. In our experiments we found that
smoothing did not improve generalization, but undoubtedly our method would beneﬁt from better
estimation of the outcome distribution and increased robustness.

We ran separate experiments to test how many classiﬁers our method could support in practice.
Estimation of the joint distribution and computation of the ROC curve ﬁnished within one minute
for 20 classiﬁers (not including time to train the individual classiﬁers). Unfortunately, the inherent
exponential structure of the optimal ROC curve means we cannot expect to do signiﬁcantly better
(at the same rate, 30 classiﬁers would take over 12 hours and 40 classiﬁers almost a year and a half).

6 Related work

Our work is loosely related to ensemble methods such as bagging [8] and boosting [9] because
it ﬁnds meta-classiﬁcation rules over a set of base classiﬁers. However, bagging and boosting each
take one base classiﬁer and train many times, resampling or reweighting the training data to generate
classiﬁer diversity [10] or increase the classiﬁcation margin [11]. The decision rules applied to
the generated classiﬁers are (weighted) majority voting. In contrast, our method takes any binary
classiﬁers and ﬁnds optimal combination rules from the more general space of all binary functions.

Ranking algorithms, such as RankBoost [12], approach the problem of ranking points by score or
preference. Although we present our methods in a different light, our decision rule can be interpreted
as a ranking algorithm. RankBoost, however, is a boosting algorithm and therefore fundamentally
different from our approach. Ranking can be used for classiﬁcation by choosing a cutoff or threshold,
and in fact ranking algorithms tend to optimize the common Area Under the ROC Curve (AUC)
metric. Although our method may have the side effect of maximizing the AUC, its formulation is
different in that instead of optimizing a single global metric, it is a constrained optimization problem,
maximizing PD for each PF .
Another more similar method for combining classiﬁers is stacking [13]. Stacking trains a meta-
learner to combine the predictions of several base classiﬁers; in fact, our method might be consid-
ered a stacking method with a particular meta-classiﬁer. It can be difﬁcult to show the improvement
of stacking in general over selecting the best base-level classiﬁer [14]; however, stacking has a use-
ful interpretation as generalized cross-validation that makes it practical. Our analysis shows that our
combination method is the optimal meta-learner in the Neyman-Pearson sense, but incorporating the
model validation aspect of stacking would make an interesting extension to our work.

7 Conclusion

In this paper we introduce a new way to analyze a combination of classiﬁers and their ROC curves.
We give a method for combining classiﬁers and prove that it is optimal in the Neyman-Pearson
sense. This work raises several interesting questions.

Although the algorithm presented in this paper avoids checking the whole doubly exponential num-
ber of rules, the exponential factor in running time limits the number of classiﬁers that can be

7

combined in practice. Can a good approximation algorithm approach optimality while having lower
time complexity? Though in general we make no assumptions about independence, Theorem 2
shows that certain simple rules are optimal when we do know that the classiﬁers are independent.
Theorem 3 proves that the ROCCH can be optimal when only n output combinations are possible.
Perhaps other restrictions on the distribution of outcomes can lead to useful special cases.

Acknowledgments

This work was supported in part by TRUST (Team for Research in Ubiquitous Secure Technology),
which receives support from the National Science Foundation (NSF award number CCF-0424422)
and the following organizations: AFOSR (#FA9550-06-1-0244), Cisco, British Telecom, ESCHER,
HP, IBM, iCAST, Intel, Microsoft, ORNL, Pirelli, Qualcomm, Sun, Symantec, Telecom Italia, and
United Technologies; and in part by the UC Berkeley-Taiwan International Collaboration in Ad-
vanced Security Technologies (iCAST) program. The opinions expressed in this paper are solely
those of the authors and do not necessarily reﬂect the opinions of any funding agency or the U.S. or
Taiwanese governments.

References

[1] Foster Provost and Tom Fawcett. Robust classiﬁcation for imprecise environments. Machine Learning

Journal, 42(3):203–231, March 2001.

[2] Peter A. Flach and Shaomin Wu. Repairing concavities in ROC curves.

In Proceedings of the 19th

International Joint Conference on Artiﬁcial Intelligence (IJCAI’05), pages 702–707, August 2005.

[3] Tom Fawcett. ROC graphs: Notes and practical considerations for data mining researchers. Technical

Report HPL-2003-4, HP Laboratories, Palo Alto, CA, January 2003. Updated March 2004.

[4] J. Neyman and E. S. Pearson. On the problem of the most efﬁcient tests of statistical hypotheses. Philo-
sophical Transactions of the Royal Society of London, Series A, Containing Papers of a Mathematical or
Physical Character, 231:289–337, 1933.

[5] Vincent H. Poor. An Introduction to Signal Detection and Estimation. Springer-Verlag, second edition,

1988.

[6] D. J. Newman, S. Hettich, C. L. Blake, and C. J. Merz. UCI repository of machine learning databases,

1998. http://www.ics.uci.edu/∼mlearn/MLRepository.html.

[7] I. Mierswa, M. Wurst, R. Klinkenberg, M. Scholz, and T. Euler. YALE: Rapid prototyping for com-
plex data mining tasks. In Proceedings of the ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining (KDD), 2006.

[8] L. Breiman. Bagging predictors. Machine Learning, 24(2):123–140, 1996.
[9] Y. Freund and R. E. Schapire. Experiments with a new boosting algorithm. In Thirteenth International

Conference on Machine Learning, pages 148–156, Bari, Italy, 1996. Morgan Kaufmann.

[10] Thomas G. Dietterich. Ensemble methods in machine learning. Lecture Notes in Computer Science,

1857:1–15, 2000.

[11] Robert E. Schapire, Yoav Freund, Peter Bartlett, and Wee Sun Lee. Boosting the margin: A new ex-
planation for the effectiveness of voting methods. The Annals of Statistics, 26(5):1651–1686, October
1998.

[12] Yoav Freund, Raj Iyer, Robert E. Schapire, and Yoram Singer. An efﬁcient boosting algorithm for com-

bining preferences. Journal of Machine Learning Research (JMLR), 4:933–969, 2003.

[13] D. H. Wolpert. Stacked generalization. Neural Networks, 5:241–259, 1992.
[14] Saso D˘zeroski and Bernard ˘Zenko. Is combining classiﬁers with stacking better than selecting the best

one? Machine Learning, 54:255–273, 2004.

8

"
587,2007,Modeling homophily and stochastic equivalence in symmetric relational data,"This article discusses a latent variable model for inference and prediction of symmetric relational data. The model, based on the idea of the eigenvalue decomposition, represents the relationship between two nodes as the weighted inner-product of node-specific vectors of latent characteristics. This ``eigenmodel'' generalizes other popular latent variable models, such as latent class and distance models: It is shown mathematically that any latent class or distance model has a representation as an eigenmodel, but not vice-versa. The practical implications of this are examined in the context of three real datasets, for which the eigenmodel has as good or better out-of-sample predictive performance than the other two models.","Modeling homophily and stochastic equivalence in

symmetric relational data

Peter D. Hoff

Departments of Statistics and Biostatistics

University of Washington
Seattle, WA 98195-4322.

hoff@stat.washington.edu

Abstract

This article discusses a latent variable model for inference and prediction of sym-
metric relational data. The model, based on the idea of the eigenvalue decomposi-
tion, represents the relationship between two nodes as the weighted inner-product
of node-speciﬁc vectors of latent characteristics. This “eigenmodel” generalizes
other popular latent variable models, such as latent class and distance models: It is
shown mathematically that any latent class or distance model has a representation
as an eigenmodel, but not vice-versa. The practical implications of this are exam-
ined in the context of three real datasets, for which the eigenmodel has as good or
better out-of-sample predictive performance than the other two models.

1 Introduction
Let {yi,j : 1 ≤ i < j ≤ n} denote data measured on pairs of a set of n objects or nodes. The
examples considered in this article include friendships among people, associations among words
and interactions among proteins. Such measurements are often represented by a sociomatrix Y ,
which is a symmetric n × n matrix with an undeﬁned diagonal. One of the goals of relational data
analysis is to describe the variation among the entries of Y , as well as any potential covariation of
Y with observed explanatory variables X = {xi,j, 1 ≤ i < j ≤ n}.
To this end, a variety of statistical models have been developed that describe yi,j as some function
of node-speciﬁc latent variables ui and uj and a linear predictor βT xi,j.
In such formulations,
{u1, . . . , un} represent across-node variation in the yi,j’s and β represents covariation of the yi,j’s
with the xi,j’s. For example, Nowicki and Snijders [2001] present a model in which each node i
is assumed to belong to an unobserved latent class ui, and a probability distribution describes the
relationships between each pair of classes (see Kemp et al. [2004] and Airoldi et al. [2005] for recent
extensions of this approach). Such a model captures stochastic equivalence, a type of pattern often
seen in network data in which the nodes can be divided into groups such that members of the same
group have similar patterns of relationships.
An alternative approach to representing across-node variation is based on the idea of homophily, in
which the relationships between nodes with similar characteristics are stronger than the relationships
between nodes having different characteristics. Homophily provides an explanation to data patterns
often seen in social networks, such as transitivity (“a friend of a friend is a friend”), balance (“the
enemy of my friend is an enemy”) and the existence of cohesive subgroups of nodes. In order to
represent such patterns, Hoff et al. [2002] present a model in which the conditional mean of yi,j is a
function of β0xi,j − |ui − uj|, where {u1, . . . , un} are vectors of unobserved, latent characteristics
in a Euclidean space. In the context of binary relational data, such a model predicts the existence
of more transitive triples, or “triangles,” than would be seen under a random allocation of edges
among pairs of nodes. An important assumption of this model is that two nodes with a strong

1

Figure 1: Networks exhibiting homophily (left panel) and stochastic equivalence (right panel).

relationship between them are also similar to each other in terms of how they relate to other nodes:
A strong relationship between i and j suggests |ui − uj| is small, but this further implies that
|ui − uk| ≈ |uj − uk|, and so nodes i and j are assumed to have similar relationships to other nodes.
The latent class model of Nowicki and Snijders [2001] and the latent distance model of Hoff et al.
[2002] are able to identify, respectively, classes of nodes with similar roles, and the locational prop-
erties of the nodes. These two items are perhaps the two primary features of interest in social network
and relational data analysis. For example, discussion of these concepts makes up more than half of
the 734 pages of main text in Wasserman and Faust [1994]. However, a model that can represent
one feature may not be able to represent the other: Consider the two graphs in Figure 1. The graph
on the left displays a large degree of transitivity, and can be well-represented by the latent distance
model with a set of vectors {u1, . . . , un} in two-dimensional space, in which the probability of an
edge between i and j is decreasing in |ui − uj|. In contrast, representation of the graph by a latent
class model would require a large number of classes, none of which would be particularly cohesive
or distinguishable from the others. The second panel of Figure 1 displays a network involving three
classes of stochastically equivalent nodes, two of which (say A and B) have only across-class ties,
and one (C) that has both within- and across-class ties. This graph is well-represented by a latent
class model in which edges occur with high probability between pairs having one member in each
of A and B or in B and C, and among pairs having both members in C (in models of stochastic
equivalence, nodes within each class are not differentiated). In contrast, representation of this type
of graph with a latent distance model would require the dimension of the latent characteristics to be
on the order of the class membership sizes.
Many real networks exhibit combinations of structural equivalence and homophily in varying de-
grees. In these situations, use of either the latent class or distance model would only be representing
part of the network structure. The goal of this paper is to show that a simple statistical model based
on the eigenvalue decomposition can generalize the latent class and distance models: Just as any
symmetric matrix can be approximated with a subset of its largest eigenvalues and corresponding
eigenvectors, the variation in a sociomatrix can be represented by modeling yi,j as a function of
i Λuj, where {u1, . . . , un} are node-speciﬁc factors and Λ is a diagonal matrix. In this
β0xi,j + uT
article, we show mathematically and by example how this eigenmodel can represent both stochastic
equivalence and homophily in symmetric relational data, and thus is more general than the other two
latent variable models.
The next section motivates the use of latent variables models for relational data, and shows mathe-
matically that the eigenmodel generalizes the latent class and distance models in the sense that it can
compactly represent the same network features as these other models but not vice-versa. Section 3
compares the out-of-sample predictive performance of these three models on three different datasets:
a social network of 12th graders; a relational dataset on word association counts from the ﬁrst chap-
ter of Genesis; and a dataset on protein-protein interactions. The ﬁrst two networks exhibit latent
homophily and stochastic equivalence respectively, whereas the third shows both to some degree.

2

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllIn support of the theoretical results of Section 2, the latent distance and class models perform well
for the ﬁrst and second datasets respectively, whereas the eigenmodel performs well for all three.
Section 4 summarizes the results and discusses some extensions.

2 Latent variable modeling of relational data

2.1 Justiﬁcation of latent variable modeling

The use of probabilistic latent variable models for the representation of relational data can be moti-
vated in a natural way: For undirected data without covariate information, symmetry suggests that
any probability model we consider should treat the nodes as being exchangeable, so that

Pr({yi,j : 1 ≤ i < j ≤ n} ∈ A) = Pr({yπi,πj : 1 ≤ i < j ≤ n} ∈ A)

for any permutation π of the integers {1, . . . , n} and any set of sociomatrices A. Results of Hoover
[1982] and Aldous [1985, chap. 14] show that if a model satisﬁes the above exchangeability condi-
tion for each integer n, then it can be written as a latent variable model of the form

yi,j = h(µ, ui, uj, i,j)

(1)
for i.i.d. latent variables {u1, . . . , un}, i.i.d. pair-speciﬁc effects {i,j : 1 ≤ i < j ≤ n} and some
function h that is symmetric in its second and third arguments. This result is very general - it says
that any statistical model for a sociomatrix in which the nodes are exchangeable can be written as a
latent variable model.
Difference choices of h lead to different models for y. A general probit model for binary network
data can be put in the form of (1) as follows:

{i,j : 1 ≤ i < j ≤ n} ∼ i.i.d. normal(0, 1)

{u1, . . . , un} ∼ i.i.d. f(u|ψ)

yi,j = h(µ, ui, uj, i,j) = δ(0,∞)(µ + α(ui, uj) + i,j),

where µ and ψ are parameters to be estimated, and α is a symmetric function, also potentially
involving parameters to be estimated. Covariation between Y and an array of predictor variables
X can be represented by adding a linear predictor βT xi,j to µ. Finally, integrating over i,j we
obtain Pr(yi,j = 1|xi,j, ui, uj) = Φ[µ + βT xi,j + α(ui, uj)]. Since the i,j’s can be assumed to be
independent, the conditional probability of Y given X and {u1, . . . , un} can be expressed as

Pr(yi,j = 1|xi,j, ui, uj) ≡ θi,j = Φ[µ + βT xi,j + α(ui, uj)]

(2)

Pr(Y |X, u1, . . . , un) = Y

i,j (1 − θi,j)yi,j
θyi,j

i<j

Many relational datasets have ordinal, non-binary measurements (for example, the word association
data in Section 3.2). Rather than “thresholding” the data to force it to be binary, we can make use of
the full information in the data with an ordered probit version of (2):
Pr(yi,j = y|xi,j, ui, uj) ≡ θ(y)

i,j = Φ[µy + βT xi,j + α(ui, uj)] − Φ[µy+1 + βT xi,j + α(ui, uj)]

Pr(Y |X, u1, . . . , un) = Y

θ(yi,j )
i,j

,

where {µy} are parameters to be estimated for all but the lowest value y in the sample space.

i<j

2.2 Effects of nodal variation

The latent variable models described in the Introduction correspond to different choices for the
symmetric function α:

Latent class model:

α(ui, uj) = mui,uj
ui ∈ {1, . . . , K}, i ∈ {1, . . . , n}

3

M a K × K symmetric matrix

Latent distance model:

α(ui, uj) = −|ui − uj|
ui ∈ RK, i ∈ {1, . . . , n}

Latent eigenmodel:

i Λuj

α(ui, uj) = uT
ui ∈ RK, i ∈ {1, . . . , n}
Λ a K × K diagonal matrix.

Interpretations of the latent class and distance models were given in the Introduction. An inter-
pretation of the latent eigenmodel is that each node i has a vector of unobserved characteristics
ui = {ui,1, . . . , ui,K}, and that similar values of ui,k and uj,k will contribute positively or nega-
tively to the relationship between i and j, depending on whether λk > 0 or λk < 0. In this way,
the model can represent both positive or negative homophily in varying degrees, and stochastically
equivalent nodes (nodes with the same or similar latent vectors) may or may not have strong rela-
tionships with one another.
We now show that the eigenmodel generalizes the latent class and distance models: Let Sn be the
set of n × n sociomatrices, and let

CK = {C ∈ Sn : ci,j = mui,uj , ui ∈ {1, . . . , K}, M a K × K symmetric matrix};
DK = {D ∈ Sn : di,j = −|ui − uj|, ui ∈ RK};
EK = {E ∈ Sn : ei,j = uT

i Λuj, ui ∈ RK, Λ a K × K diagonal matrix}.

In other words, CK is the set of possible values of {α(ui, uj), 1 ≤ i < j ≤ n} under a K-
dimensional latent class model, and similarly for DK and EK.
EK generalizes CK: Let C ∈ CK and let ˜C be a completion of C obtained by setting ci,i = mui,ui.
There are at most K unique rows of ˜C and so ˜C is of rank K at most. Since the set EK contains
all sociomatrices that can be completed as a rank-K matrix, we have CK ⊆ EK. Since EK includes
matrices with n unique rows, CK ⊂ EK unless K ≥ n in which case the two sets are equal.
EK+1 weakly generalizes DK: Let D ∈ DK. Such a (negative) distance matrix will generally be
of full rank, in which case it cannot be represented exactly by an E ∈ EK for K < n. However,
what is critical from a modeling perspective is whether or not the order of the entries of each D can
be matched by the order of the entries of an E. This is because the probit and ordered probit model
we are considering include threshold variables {µy : y ∈ Y} which can be adjusted to accommodate
monotone transformations of α(ui, uj). With this in mind, note that the matrix of squared distances
among a set of K-dimensional vectors {z1, . . . , zn} is a monotonic transformation of the distances,
nzn] − 2ZZ T ) and so is
nzn]T 1T + 1[z0
is of rank K + 2 or less (as D2 = [z0
i zi) ∈ RK+1 for each i ∈ {1, . . . , n}, we have

izj +p(r2 − |ui|2)(r2 − |uj|2). For large r this is approximately r2−|zi− zj|2/2, which

in EK+2. Furthermore, letting ui = (zi,pr2 − zT

u0
iuj = z0
is an increasing function of the negative distance di,j. For large enough r the numerical order of the
entries of this E ∈ EK+1 is the same as that of D ∈ DK.
DK does not weakly generalize E1: Consider E ∈ E1 generated by Λ = 1, u1 = 1 and ui =
r < 1 for i > 1. Then r = e1,i1 = e1,i2 > ei1,i2 = r2 for all i1, i2 6= 1. For which K is such an
ordering of the elements of D ∈ DK possible? If K = 1 then such an ordering is possible only if
n = 3. For K = 2 such an ordering is possible for n ≤ 6. This is because the kissing number in
R2, or the number of non-overlapping spheres of unit radius that can simultaneously touch a central
sphere of unit radius, is 6. If we put node 1 at the center of the central sphere, and 6 nodes at the
centers of the 6 kissing spheres, then we have d1,i1 = d1,i2 = di1,i2 for all i1, i2 6= 1. We can only
have d1,i1 = d1,i2 > di1,i2 if we remove one of the non-central spheres to allow for more room
between those remaining, leaving one central sphere plus ﬁve kissing spheres for a total of n = 6.
Increasing n increases the necessary dimension of the Euclidean space, and so for any K there are
n and E ∈ E1 that have entry orderings that cannot be matched by those of any D ∈ DK.

1z1, . . . , z0

1z1, . . . , z0

4

A less general positive semi-deﬁnite version of the eigenmodel has been studied by Hoff [2005],
in which Λ was taken to be the identity matrix. Such a model can weakly generalize a distance
model, but cannot generalize a latent class model, as the eigenvalues of a latent class model could
be negative.

3 Model comparison on three different datasets

3.1 Parameter estimation

Bayesian parameter estimation for the three models under consideration can be achieved via Markov
chain Monte Carlo (MCMC) algorithms, in which posterior distributions for the unknown quantities
are approximated with empirical distributions of samples from a Markov chain. For these algo-
rithms, it is useful to formulate the probit models described in Section 2.1 in terms of an additional
latent variable zi,j ∼ normal[β0xi,j + α(ui, uj)], for which yi,j = y if µy < zi,j < µy+1. Using
conjugate prior distributions where possible, the MCMC algorithms proceed by generating a new
state φ(s+1) = {Z (s+1), µ(s+1), β(s+1), u(s+1)

} from a current state φ(s) as follows:
1. For each {i, j}, sample zi,j from its (constrained normal) full conditional distribution.
2. For each y ∈ Y, sample µy from its (normal) full conditional distribution.
3. Sample β from its (multivariate normal) full conditional distribution.
4. Sample u1, . . . , un and their associated parameters:

, . . . , u(s+1)

n

1

• For the latent distance model, propose and accept or reject new values of the ui’s with
the Metropolis algorithm, and then sample the population variances of the ui’s from
their (inverse-gamma) full conditional distributions.
• For the latent class model, update each class variable ui from its (multinomial) con-
ditional distribution given current values of Z,{uj : j 6= i} and the variance of the
elements of M (but marginally over M to improve mixing). Then sample the elements
of M from their (normal) full conditional distributions and the variance of the entries
of M from its (inverse-gamma) full conditional distribution.
• For the latent vector model, sample each ui from its (multivariate normal) full con-
ditional distribution, sample the mean of the ui’s from their (normal) full conditional
distributions, and then sample Λ from its (multivariate normal) full conditional distri-
bution.

To facilitate comparison across models, we used prior distributions in which the level of prior vari-
ability in α(ui, uj) was similar across the three different models (further details and code to imple-
ment these algorithms are available at my website).

3.2 Cross validation

To compare the performance of these three different models we evaluated their out-of-sample pre-
dictive performance under a range of dimensions (K ∈ {3, 5, 10}) and on three different datasets
exhibiting varying combinations of homophily and stochastic equivalence. For each combination of
dataset, dimension and model we performed a ﬁve-fold cross validation experiment as follows:

(cid:1) data values into 5 sets of roughly equal size, letting si,j be the set

1. Randomly divide the(cid:0)n

to which pair {i, j} is assigned.

2

2. For each s ∈ {1, . . . , 5}:

(a) Obtain posterior distributions of the model parameter conditional on {yi,j : si,j 6= s},
(b) For pairs {k, l} in set s, let ˆyk,l = E[yk,l|{yi,j : si,j 6= s}], the posterior predictive

the data on pairs not in set s.

mean of yk,l obtained using data not in set s.

This procedure generates a sociomatrix ˆY , in which each entry ˆyi,j represents a predicted value
obtained from using a subset of the data that does not include yi,j. Thus ˆY is a sociomatrix of
out-of-sample predictions of the observed data Y .

5

Table 1: Cross validation results and area under the ROC curves.

K

3
5
10

dist
0.82
0.81
0.76

Add health

class
0.64
0.70
0.69

eigen
0.75
0.78
0.80

Genesis
class
0.82
0.82
0.82

eigen
0.82
0.82
0.82

dist
0.62
0.66
0.74

Protein interaction
eigen
dist
0.83
0.88
0.90
0.84
0.85
0.90

class
0.79
0.84
0.86

Figure 2: Social network data and unscaled ROC curves for the K = 3 models.

3.3 Adolescent Health social network

The ﬁrst dataset records friendship ties among 247 12th-graders, obtained from the National Longi-
tudinal Study of Adolescent Health (www.cpc.unc.edu/projects/addhealth). For these data,
yi,j = 1 or 0 depending on whether or not there is a close friendship tie between student i and j
(as reported by either i or j). These data are represented as an undirected graph in the ﬁrst panel of
Figure 2. Like many social networks, these data exhibit a good deal of transitivity. It is therefore not
surprising that the best performing models considered (in terms of area under the ROC curve, given
in Table 1) are the distance models, with the eigenmodels close behind. In contrast, the latent class
models perform poorly, and the results suggest that increasing K for this model would not improve
its performance.

3.4 Word neighbors in Genesis

The second dataset we consider is derived from word and punctuation counts in the ﬁrst chapter
of the King James version of Genesis (www.gutenberg.org/dirs/etext05/bib0110.txt).
There are 158 unique words and punctuation marks in this chapter, and for our example we take
yi,j to be the number of times that word i and word j appear next to each other (a model extension,
appropriate for an asymmetric version of this dataset, is discussed in the next section). These data
can be viewed as a graph with weighted edges, the unweighted version of which is shown in the
ﬁrst panel of Figure 3. The lack of a clear spatial representation of these data is not unexpected,
as text data such as these do not have groups of words with strong within-group connections, nor
do they display much homophily: a given noun may appear quite frequently next to two different
verbs, but these verbs will not appear next to each other. A better description of these data might be
that there are classes of words, and connections occur between words of different classes. The cross
validation results support this claim, in that the latent class model performs much better than the
distance model on these data, as seen in the second panel of Figure 3 and in Table 1. As discussed in
the previous section, the eigenmodel generalizes the latent class model and performs equally well.

6

lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0500015000250000100200300400false positivestrue positivesdistanceclassvectorFigure 3: Relational text data from Genesis and unscaled ROC curves for the K = 3 models.

We note that parameter estimates for these data were obtained using the ordered probit versions of
the models (as the data are not binary), but the out-of-sample predictive performance was evaluated
based on each model’s ability to predict a non-zero relationship.

3.5 Protein-protein interaction data

Our last example is the protein-protein interaction data of Butland et al. [2005], in which yi,j = 1
if proteins i and j bind and yi,j = 0 otherwise. We analyze the large connected component of this
graph, which includes 230 proteins and is displayed in the ﬁrst panel of 4. This graph indicates
patterns of both stochastic equivalence and homophily: Some nodes could be described as “hubs”,
connecting to many other nodes which in turn do not connect to each other. Such structure is better
represented by a latent class model than a distance model. However, most nodes connecting to hubs
generally connect to only one hub, which is a feature that is hard to represent with a small number
of latent classes. To represent this structure well, we would need two latent classes per hub, one for
the hub itself and one for the nodes connecting to the hub. Furthermore, the core of the network
(the nodes with more than two connections) displays a good degree of homophily in the form of
transitive triads, a feature which is easiest to represent with a distance model. The eigenmodel is
able to capture both of these data features and performs better than the other two models in terms of
out-of-sample predictive performance. In fact, the K = 3 eigenmodel performs better than the other
two models for any value of K considered.

4 Discussion

Latent distance and latent class models provide concise, easily interpreted descriptions of social
networks and relational data. However, neither of these models will provide a complete picture of
relational data that exhibit degrees of both homophily and stochastic equivalence. In contrast, we
have shown that a latent eigenmodel is able to represent datasets with either or both of these data
patterns. This is due to the fact that the eigenmodel provides an unrestricted low-rank approximation
to the sociomatrix, and is therefore able to represent a wide array of patterns in the data.
The concept behind the eigenmodel is the familiar eigenvalue decomposition of a symmetric ma-
trix. The analogue for directed networks or rectangular matrix data would be a model based on the
singular value decomposition, in which data yi,j could be modeled as depending on uT
i Dvj, where
ui and vj represent vectors of latent row and column effects respectively. Statistical inference using
the singular value decomposition for Gaussian data is straightforward. A model-based version of

7

,;:.aaboveabundantlyafterairallalsoandappearbebearingbeastbeginningbeholdblessedbringbroughtcalledcattlecreatedcreaturecreepethcreepingdarknessdaydaysdeepdividedivideddominiondryeartheveningeveryfacefemalefifthfillfinishedfirmamentfirstfishflyforformforthfourthfowlfromfruitfruitfulgatheredgatheringgivegivengodgoodgrassgreatgreatergreenhadhathhaveheheavenheavensherbhimhishostiimageinisititselfkindlandlesserletlifelightlightslikenesslivingmademakemalemanmaymeatmidstmorningmovedmovethmovingmultiplynightofoneopenouroverownplacereplenishrulesaidsawsayingseaseasseasonssecondseedsetshallsignssixthsospiritstarssubduethatthetheirthemtherethingthirdthustotogethertreetwounderuntouponusveryvoidwaswaterswerewhaleswhereinwhichwhosewingedwithoutyearsyieldingyou040008000120000100200300400false positivestrue positivesdistanceclassvectorFigure 4: Protein-protein interaction data and unscaled ROC curves for the K = 3 models.

the approach for binary and other non-Gaussian relational datasets could be implemented using the
ordered probit model discussed in this paper.

Acknowledgment

This work was partially funded by NSF grant number 0631531.

References
Edoardo Airoldi, David Blei, Eric Xing, and Stephen Fienberg. A latent mixed membership model
In LinkKDD ’05: Proceedings of the 3rd international workshop on Link
for relational data.
discovery, pages 82–89, New York, NY, USA, 2005. ACM Press. ISBN 1-59593-215-1. doi:
http://doi.acm.org/10.1145/1134271.1134283.

David J. Aldous. Exchangeability and related topics. In ´Ecole d’´et´e de probabilit´es de Saint-Flour,

XIII—1983, volume 1117 of Lecture Notes in Math., pages 1–198. Springer, Berlin, 1985.

G. Butland, J. M. Peregrin-Alvarez, J. Li, W. Yang, X. Yang, V. Canadien, A. Starostine, D. Richards,
B. Beattie, N. Krogan, M. Davey, J. Parkinson, J. Greenblatt, and A. Emili. Interaction network
containing conserved and essential protein complexes in escherichia coli. Nature, 433:531–537,
2005.

Peter D. Hoff. Bilinear mixed-effects models for dyadic data. J. Amer. Statist. Assoc., 100(469):

286–295, 2005. ISSN 0162-1459.

Peter D. Hoff, Adrian E. Raftery, and Mark S. Handcock. Latent space approaches to social network

analysis. J. Amer. Statist. Assoc., 97(460):1090–1098, 2002. ISSN 0162-1459.

D. N. Hoover. Row-column exchangeability and a generalized model for probability. In Exchange-
ability in probability and statistics (Rome, 1981), pages 281–291. North-Holland, Amsterdam,
1982.

Charles Kemp, Thomas L. Grifﬁths, and Joshua B. Tenenbaum. Discovering latent classes in rela-

tional data. AI Memo 2004-019, Massachusetts Institute of Technology, 2004.

Krzysztof Nowicki and Tom A. B. Snijders. Estimation and prediction for stochastic blockstructures.

J. Amer. Statist. Assoc., 96(455):1077–1087, 2001. ISSN 0162-1459.

Stanley Wasserman and Katherine Faust. Social Network Analysis: Methods and Applications.

Cambridge University Press, Cambridge, 1994.

8

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0500015000250000100300500700false positivestrue positivesdistanceclassvector"
1048,2007,On Sparsity and Overcompleteness in Image Models,"Computational models of visual cortex, and in particular those based on sparse coding, have enjoyed much recent attention. Despite this currency, the question of how sparse or how over-complete a sparse representation should be, has gone without principled answer. Here, we use Bayesian model-selection methods to address these questions for a sparse-coding model based on a Student-t prior. Having validated our methods on toy data, we find that natural images are indeed best modelled by extremely sparse distributions; although for the Student-t prior, the associated optimal basis size is only modestly overcomplete.","On Sparsity and Overcompleteness in Image Models

Pietro Berkes, Richard Turner, and Maneesh Sahani

Gatsby Computational Neuroscience Unit, UCL

Alexandra House, 17 Queen Square, London WC1N 3AR

Abstract

Computational models of visual cortex, and in particular those based on sparse
coding, have enjoyed much recent attention. Despite this currency, the question
of how sparse or how over-complete a sparse representation should be, has gone
without principled answer. Here, we use Bayesian model-selection methods to ad-
dress these questions for a sparse-coding model based on a Student-t prior. Hav-
ing validated our methods on toy data, we ﬁnd that natural images are indeed best
modelled by extremely sparse distributions; although for the Student-t prior, the
associated optimal basis size is only modestly over-complete.

1 Introduction

Computational models of visual cortex, and in particular those based on sparse coding, have re-
cently enjoyed much attention. The basic assumption behind sparse coding is that natural scenes are
composed of structural primitives (edges or lines, for example) and, although there are a potentially
large number of these primitives, typically only a few are active in a single natural scene (hence the
term sparse, [1, 2]). The claim is that cortical processing uses these statistical regularities to shape
a representation of natural scenes, and in particular converts the pixel-based representation at the
retina to a higher-level representation in terms of these structural primitives.

Traditionally, research has focused on determining the characteristics of the structural primitives and
comparing their representational properties with those of V1. This has been a successful enterprise,
but as a consequence other important questions have been neglected. The two we focus on here
are: How large is the set of structural primitives best suited to describe all natural scenes (how
over-complete), and how many primitives are active in a single scene (how sparse)? We will also be
interested in the coupling between sparseness and over-completeness. The intuition is that, if there
are a great number of structural primitives, they can be very speciﬁc and only a small number will
be active in a visual scene. Conversely if there are a small number they have to be more general and
a larger number will be active on average. We attempt to map this coupling by evaluating models
with different over-completenesses and sparsenesses and discover where natural scenes live along
this trade-off (see Fig. 1).

In order to test the sparse coding hypothesis it is necessary to build algorithms that both learn the
primitives and decompose natural scenes in terms of them. There have been many ways to derive
such algorithms, but one of the more successful is to regard the task of building a representation
of natural scenes as one of probabilistic inference. More speciﬁcally, the unknown activities of the
structural primitives are viewed as latent variables that must be inferred from the natural scene data.
Commonly the inference is carried out by writing down a generative model (although see [3] for an
alternative), which formalises the assumptions made about the data and latent variables. The rules
of probability are then used to derive inference and learning algorithms.

Unfortunately the assumption that natural scenes are composed of a small number of structural
primitives is not sufﬁcient to build a meaningful generative model. Other assumptions must therefore
be made and typically these are that the primitives occur independently, and combine linearly. These

1

s
s
e
n
e

t

l

e
p
m
o
c
r
e
v
o

10

8

6

4

2

0

sparsity

Figure 1: Schematic showing the space of possible sparse coding models in terms of sparseness (increasing in
the direction of the arrow) and over-completeness. For reference, complete models lie along the dashed black
line. Ideally every model could be evaluated (e.g. via their marginal likelihood or cross-validation) and the
grey contours illustrate what we might expect to discover if this were possible: The solid black line illustrates
the hypothesised trade-off between over-completeness and sparsity, whilst the star shows the optimal point in
this trade-off.

are drastic approximations and it is an open question to what extent this affects the results of sparse
coding. The distribution over the latent variables xt,k is chosen to be sparse and typical choices
are Student-t, a Mixture of Gaussians (with zero means), and the Generalised Gaussian (which
includes the Laplace distribution). The output yt is then given by a linear combination of the K,
D-dimensional structural primitives gk, weighted by their activities, plus some additive Gaussian
noise (the model reduces to independent components analysis in the absence of this noise [4]),

p(xt,k|α) = psparse(α)

p(yt|xt, G) = Nyt(Gxt, Σy) .

(1)
(2)
The goal of this paper will be to learn the optimal dimensionality of the latent variables (K) and
the optimal sparseness of the prior (α). In order to do this a notion of optimality has to be deﬁned.
One option is to train many different sparse-coding models and ﬁnd the one which is most “similar”
to visual processing. (Indeed this might be a fair characterisation of much of the current activity in
ﬁeld.) However, this is fraught with difﬁculty not least as it is unclear how recognition models map
to neural processes. We believe the more consistent approach is, once again, to use the Bayesian
framework and view this as a problem of probabilistic inference. In fact, if the hypothesis is that the
visual system is implementing an optimal generative model, then questions of over-completeness
and sparsity should be addressed in this context.

Unfortunately, this is not a simple task and quite sophisticated machine-learning algorithms have
to be harnessed in order to answer these seemingly simple questions. In the ﬁrst part of this paper
we describe these algorithms and then validate them using artiﬁcial data. Finally, we present results
concerning the optimal sparseness and over-completeness for natural image patches in the case that
the prior is a Student-t distribution.

2 Model

As discussed earlier, there are many variants of sparse-coding. Here, we focus on the Student-t prior
for the latent variables xt,k:

p(xt,k|α, λ) =

Γ(cid:0) α+1
2 (cid:1)
λ√απ Γ(cid:0) α
2(cid:1)

(cid:18)1 +

1

α (cid:16) xt,k

λ (cid:17)2(cid:19)−

α+1

2

(3)

There are two main reasons for this choice: The ﬁrst is that this is a widely used model [1]. The
second is that by implementing the Student-t prior using an auxiliary variable, all the distributions in
the generative model become members of the exponential family [5]. This means it is easy to derive
efﬁcient approximate inference schemes like variational Bayes and Gibbs sampling.

The auxiliary variable method is based on the observation that a Student-t distribution is a continuous
mixture of zero-mean Gaussians, whose mixing proportions are given by a Gamma distribution over

2

the precisions. This indicates that we can exchange the Student-t prior for a two-step prior in which
we ﬁrst draw a precision from a Gamma distribution and then draw an activation from a Gaussian
with that precision,

2

,

αλ2(cid:19) ,
p(ut,k|α, λ) = Gut,k(cid:18) α
p(xt,k|ut,k) = Nxt,k(cid:16)0, u−1
t,k(cid:17) ,
p(yt|xt, G) = Nyt(Gxt, Σy) ,

2

Σy := diag(cid:0)σ2
y(cid:1) .

(4)

(5)

(6)
(7)

This model produces data which are often near zero, but occasionally highly non-zero. These non-
zero elements form star-like patterns, where the points of the star are determined by the direction of
the weights (e.g., Fig. 2).

One of the major technical difﬁculties posed by sparse-coding is that, in the over-complete regime,
the posterior distribution of the latent variables p(X|Y, θ) is often complex and multi-modal. Ap-
proximation schemes are therefore required, but we must be careful to ensure that the scheme we
choose does not bias the conclusions we are trying to draw. This is true for any application of sparse
coding, but is particularly pertinent for our problem as we will be quantitatively comparing different
sparse-coding models.

3 Bayesian Model Comparison

A possible strategy for investigating the sparseness/over-completeness coupling would be to tile
the space with models and learn the parameters at each point (as schematised in Fig. 1). A model
comparison criterion could then be used to rank the models, and to ﬁnd the optimal sparseness/over-
completeness. One such criterion would be to use cross validation and evaluate the likelihoods on
some held-out test data. Another is to use (approximate) Bayesian Model Comparison, and it is on
this method that we focus.
To evaluate the plausibility of two alternative versions of a model M, each with a different setting
of the hyperparameters Ξ1 and Ξ2, in the light of some data Y , we compute the evidence [6]:

p(M, Ξ1|Y )
p(M, Ξ2|Y )

=

p(Y |M, Ξ1) P (M, Ξ1)
p(Y |M, Ξ2) P (M, Ξ2)

.

(8)

Since we do not have any reason a priori to prefer one particular conﬁguration of hyperparameters
to another, we take the prior terms P (M, Ξi) to be equal, which leaves us with the ratio of the
marginal-likelihoods (or Bayes Factor),

P (Y |M, Ξ1)
P (Y |M, Ξ2)

,

(9)

The marginal-likelihoods themselves are hard to compute, being formed from high dimensional
integrals over the latent variables V and parameters Θ,

p(Y |M, Ξi) = Z dV dΘ p(Y, V, Θ|M, Ξi)

= Z dV dΘ p(Y, V |Θ,M, Ξi)p(Θ|M, Ξi) .

(10)

(11)

One concern in model comparison might be that the more complex models (those which are more
over-complete) have a larger number parameters and therefore ‘ﬁt’ any data set better. However, the
Bayes factor (Eq. 9) implicitly implements a probabilistic version of Occam’s razor that penalises
more complex models and mitigates this effect [6]. This makes the Bayesian method appealing for
determining the over-completeness of a sparse-coding model.

Unfortunately computing the marginal-likelihood is computationally intensive, and this precludes
tiling the sparseness/over-completeness space. However, an alternative is to learn the optimal over-
completeness at a given sparseness using automatic relevance determination (ARD) [7, 8]. The

3

advantage of ARD is that it changes a hard and lengthy model comparison problem (i.e., computing
the marginal-likelihood for many models of differing dimensionalities) into a much simpler infer-
ence problem. In a nutshell, the idea is to equip the model with many more components than are
believed to be present in the data, and to let it prune out the weights which are unnecessary. Prac-
tically this involves placing a (Gaussian) prior over the components which favours small weights,
and then inferring the scale of this prior. In this way the scale of the superﬂuous weights is driven to
zero, removing them from the model. The necessary ARD hyper-priors are

p(gk|γk) = Ngk(cid:0)0, γ−1
k (cid:1) ,
p(γk) = Gγk(θk, lk) .

(12)
(13)

4 Determining the over-completeness: Variational Bayes

In the previous two sections we described a generative model for sparse coding that is theoretically
able to learn the optimal over-completeness of natural scenes. We have two distinct uses for this
model: The ﬁrst, and computationally more demanding task, is to learn the over-completeness at a
variety of different, ﬁxed, sparsenesses (that is, to ﬁnd the optimal over-completeness in a vertical
slice through Fig. 1); The second is to determine the optimal point on this trade-off by evaluating
the (approximate) marginal-likelihood (that is, evaluating points along the trade-off line in Fig. 1 to
ﬁnd the optimal model - the star). It turns out that no single method is able to solve both these tasks,
but that it is possible to develop a pair of approximate algorithms to solve them separately. The
ﬁrst approximation scheme is Variational Bayes (VB), and it excels at the ﬁrst task, but is severely
biased in the case of the second. The second scheme is Annealed Importance Sampling (AIS) which
is prohibitively slow for the ﬁrst task, but much more accurate on the second. We describe them in
turn, starting with VB.

The quantity required for learning is the marginal-likelihood,

log p(Y |M, Ξ) = log Z dV dΘ p(Y, V, Θ|M, Ξ).

(14)

Computing this integral is intractable (for reasons similar to those given in Sec. 2), but a lower-
bound can be constructed by introducing any distribution over the latent variables and parameters,
q(V, Θ), and using Jensen’s inequality,

log p(Y |M, Ξ) ≥ Z dV dΘ q(V, Θ) log

p(Y, V, Θ|M, Ξ)

q(V, Θ)

=: F(q(V, Θ))

= log p(Y |M, Ξ) − KL(q(V, Θ)||p(V, Θ|Y ))

(15)

(16)

This lower-bound is called the free-energy, and the idea is to repeatedly optimise it with respect
to the distribution q(V, Θ) so that it becomes as close to the true marginal likelihood as possible.
Clearly the optimal choice for q(V, Θ) is the (intractable) true posterior. However, by constraining
this distribution headway can be made. In particular if we assume that the set of parameters and
set of latent variables are independent in the posterior, so that q(V, Θ) = q(V )q(Θ) then we can
sequentially optimise the free-energy with respect to each of these distributions. For large hierar-
chical models, including the one described in this paper, it is often necessary to introduce further
factorisations within these two distributions in order to derive the updates. Their general form is,

q(Vi) ∝ exphlog p(V, Θ)iq(Θ) Qj6=i q(Vi)
q(Θi) ∝ exphlog p(V, Θ)iq(V ) Qj6=i q(Θi) .

(17)

(18)

As the Bayesian Sparse Coding model is composed of distributions from the exponential family, the
functional form of these updates is the same as the corresponding priors. So, for example the latent
variables have the following form: q(xt) is Gaussian and q(ut,k) is Gamma distributed.
Although this approximation is good at discovering the over-completeness of data at ﬁxed sparsities,
it provides an estimate of the marginal-likelihood (the free-energy) which is biased toward regions of
low sparsity. The reason is simple to understand. The difference between the free energy and the true
likelihood is given by the KL divergence between the approximate and true posterior. Thus, the free-
energy bound is tightest in regions where q(V, Θ) is a good match to the true posterior, and loosest in

4

regions where it is a poor match. At high sparsities, the true posterior is multimodal and highly non-
Gaussian. In this regime q(V, Θ) – which is always uni-modal – is a poor approximation. At low-
sparsities the prior becomes Gaussian-like and the posterior also becomes a uni-modal Gaussian.
In this regime q(V, Θ) is an excellent approximation. This leads to a consistent bias in the peak of
the free-energy toward regions of low sparsity. One might also be concerned with another potential
source of bias: The number of modes in the posterior increases with the number of components
in the model, which gives a worse match to the variational approximation for more over-complete
models. However, because of the sparseness of the prior distribution, most of the modes are going
to be very shallow for typical inputs, so that this effect should be small. We verify this claim on
artiﬁcial data in Section 6.2.

5 Determining the sparsity: AIS

An approximation scheme is required to estimate the marginal-likelihood, but without a sparsity-
dependent bias. Any scheme which uses a uni-modal approximation to the posterior will inevitably
fall victim to such biases. This rules out many alternate variational schemes, as well as methods
like the Laplace approximation, or Expectation Propagation. One alternative might be to use a
variational method which has a multi-modal approximating distribution (e.g. a mixture model). The
approach taken here is to use Annealed Importance Sampling (AIS) [9] which is one of the few
methods for evaluating normalising constants of intractable distributions. The basic idea behind
AIS is to estimate the marginal-likelihood using importance sampling. The twist is that the proposal
distribution for the importance sampler is itself generated using an MCMC method. Brieﬂy, this
inner loop starts by drawing samples from the model’s prior distribution and continues to sample
as the prior is deformed into the posterior, according to an annealing schedule. Both the details of
this schedule, and having a quick-mixing MCMC method, are critical for good results. In fact it is
simple to derive a quick-mixing Gibbs sampler for our application and this makes AIS particularly
appealing.

6 Results

Before tackling natural images, it is necessary to verify that the approximations can discover the
correct degree of over-completeness and sparsity in the case where the data are drawn from the
forward model. This is done in two stages: Firstly we focus on a very simple, low-dimensional
example that is easy to visualise and which helps explicate the learning algorithms, allowing them
to be tuned; Secondly, we turn to a larger scale example designed to be as similar to the tests on
natural data as possible.

6.1 Veriﬁcation using simple artiﬁcal data

In the ﬁrst experiment the training data are produced as follows: Two-dimensional observations
are generated by three Student-t sources with degree of freedom chosen to be 2.5. The generative
weights are ﬁxed to be 60 degrees apart from one another, as shown in Figure 2.

A series of VB simulations were then run, differing only in the sparseness level (as measured by
the degrees of freedom of the Student-t distribution over xt). Each simulation consisted of 500 VB
iterations performed on a set of 3000 data points randomly generated from the model. We initialised
the simulations with K = 7 components. To improve convergence, we started the simulations with
weights near the origin (drawn from a normal distribution with mean 0 and standard deviation 10−8)
and a relatively large input noise variance, and annealed the noise variance between the iterations
y = 0.3 for 100 iterations,
of VBEM. The annealing schedule was as following: we started with σ2
reduced this linearly down to σ2
y = 0.01 in a further 50
iterations. During the annealing process, the weights typically grew from the origin and spread in all
directions to cover the input space. After an initial growth period, where the representation usually
became as over-complete as allowed by the model, some of the weights rapidly shrank again and
collapsed to the origin. At the same time, the corresponding precision hyperparameters grew and
effectively pruned the unnecessary components. We performed 7 blocks of simulations at different
sparseness levels. In every block we performed 3 runs of the algorithm and retained the result with
the highest free energy.

y = 0.1 in 100 iterations, and ﬁnally to σ2

5

2024
4





4





2

0

2

4

−8000

−8200

−8400

−8600

−8800

−9000

−9200

−9400

−9600

)

S
T
A
N

(
 
y
g
r
e
n
e

 

e
e
r
f

)

S
T
A
N

(
 
)

(

Y
P
g
o

 

l

2.1

2.2

2.4

α

2.5

3.0

3.5

−6500

−6550

−6600

−6650

−6700

−6750

−6800

−6850

2.1

2.2

2.4

α

2.5

3.0

3.5

Figure 2: Left: Test data drawn from the simple artiﬁcial model. Centre: Free energy of the models learned by
VBEM in the artiﬁcial data case. Right: Estimated log marginal likelihood. Error bars are 3 times the estimated
standard deviation.

The marginal likelihoods of the selected results were then estimated using AIS. We derived the
importance weights using a ﬁxed data set with 2500 data points, 250 samples, and 300 intermediate
distributions. Following the recommendations in [9], the annealing schedule was chosen to be linear
initially (with 50 inverse temperatures spaced uniformly from 0 to 0.01), followed by a geometric
section (250 inverse temperatures spaced geometrically from 0.01 to 1). This mean that there were
a total of 300 distributions between the prior and posterior.

The results indicate that the combination of the two methods is successful at learning both the over-
completeness and sparseness. In particular the VBEM algorithm was able to recover the correct
dimensionality for all sparseness levels, except for the sparsest case α = 2.1, where it preferred a
model with 5 signiﬁcant components. As expected, however, ﬁgure 2 shows that the maximum free
energy is biased toward the more Gaussian models. In contrast to this, the marginal likelihood esti-
mated by AIS (Fig. 2), which is strictly greater than the free-energy as expected, favours sparseness
levels close to the true value.

6.2 Veriﬁcation using complex artiﬁcial data

Although it is necessary that the inference scheme should pass simple tests like that in the previous
section, they are not sufﬁcient to give us conﬁdence that it will perform successfully on natural
data. One pertinent criticism is that the regime in which we tested the algorithms in the previous
section (two dimensional observations, and three hidden latents) is quite different from that required
to model natural data. To that end, in this section we ﬁrst learn a sparse model for natural images
with ﬁxed over-completeness levels using a Maximum A Posteriori (MAP) algorithm [2] (degree
of freedom 2.5). These solutions are then used to generate artiﬁcial data as in the previous section.
The goal is to validate the model on data which has a content and scale similar to the natural images
case, but with a controlled number of generative components.
The image data comprised patches of size 9 × 9 pixels, taken at random positions from 36 natural
images randomly selected from the van Hateren database (preprocessed as described in [10]). The
patches were whitened and their dimensionality reduced from 81 to 36 by principal component
analysis. The MAP solution was trained for 500 iterations, with every iteration performed on a new
batch of 1440 patches (100 patches per image).

The model was initialised with a 3-times over-complete number of components (K = 108). As
above, the weights were initialised near the origin, and the input noise was annealed linearly from
σd = 0.5 to σd = 0.2 in the ﬁrst 300 iterations, remaining constant thereafter. Every run consisted
of 500 VBEM iterations, with every iteration performed on 3600 patches generated from the MAP
solution. We performed several simulations for over-completeness levels between 0.5 and 4.5, and
retained the solutions with the highest free energy.

The results are summarised in Figure 3: The model is able to recover the underlying dimensionality
for data between 0.5 and 2 times over-complete, and correctly saturates to 3 times over-complete
(the maximum attainable level here) when the data over-completeness exceeds 3. In the regime
between 2.5 and 3 times over-complete data, the model returns solutions with a smaller number of
components, which is possibly due to the bias described at the end of Section 5. However, these

6

3

2.5

2

1.5

1

s
s
e
n
e

t

l

e
p
m
o
c
r
e
v
O
d
e
r
r
e
n

f

 

I

0.5

0.5

1

1.5

2

3.5

4

4.5

2.5

3

True Overcompleteness

Figure 3: True versus inferred over-completeness from data drawn from the forward model trained on natural
images. If inference was perfect, the true over-completeness would be recovered (black line). This straight
line saturates when we hit the number of latent variables with which ARD was initialised (three times over-
complete). The results using multiple runs of ARD are close to this line (open circles, simulations with the
highest free-energy are shown as closed circles). The maximal and best over-completeness inferred from natural
scenes is shown by the dotted line, and lies well below the over-completenesses we are able to infer.

values are still far above the highest over-completeness learned from natural images (see section
6.3), so that we believe that the bias does not invalidate our conclusions.

6.3 Natural images

Having established that the model performs as expected, at least when the data is drawn from the
forward model, we now turn to natural image data and examine the optimal over-completeness ratio
and sparseness degree for natural scene statistics.

The image data for this simulation and the model initialisation and annealing procedure are identical
to the ones in the experiments in the preceeding section. We performed 20 simulations with different
sparseness levels, especially concentrated on the more sparse values. Every run comprised 500
VBEM iterations, with every iteration performed on a new batch of 3600 patches.

As shown in Figure 4, the free energy increased almost monotonically until α = 5 and then stabilised
and started to decrease for more Gaussian models. The algorithm learnt models that were only
slightly over-complete: the over-completeness ratio was distributed between 1 and 1.3, with a trend
for being more over-complete at high sparseness levels (Fig. 4). Although this general trend accords
with the intuition that sparseness and over-completeness are coupled, both the magnitude of the
effect and the degree of over-completeness is smaller than might have been anticipated. Indeed, this
result suggests that highly over-complete models with a Student-t prior may very well be overﬁtting
the data.

Finally we performed AIS using the same annealing schedule as in Section 6.1, using 250 samples
for the ﬁrst 6 sparseness levels and 50 for the successive 14. The estimates obtained for the log
marginal likelihood, shown in Figure 4, were monotonically increasing with increasing sparseness
(decreasing α). This indicates that sparse models are indeed optimal for natural scenes. Note that
this is exactly the opposite trend to that of the free energy, indicating that it is also biased for natural
scenes. Figure 4 shows the basis vectors learned in the simulation with α = 2.09, which had
maximal marginal likelihood. The weights resemble the Gabor wavelets, typical of sparse codes for
natural images [1].

7 Discussion

Our results suggest that the optimal sparse-coding model for natural scenes is indeed one which
is very sparse, but only modestly over-complete. The anticipated coupling between the degree of
sparsity and the over-completeness in the model is visible, but is weak.

One crucial question is how far these results will generalise to other prior distributions; and indeed,
which of the various possible sparse-coding priors is best able to capture the structure of natural
scenes. One indication that the Student-t might not be optimal, is its behaviour as the degree-of-

7

a)

x 104

−8.8

b)

x 104

−8.1

)

S
T
A
N

(
 
)

(

Y
P
g
o

 

4

6
α

8

10

l

d)

−8.2

−8.3

−8.4

−8.5

−8.6

2

3

4

5

α

6

7

8

9

)

S
T
A
N

(
 
y
g
r
e
n
e
 
e
e
r
f

−8.9

−9

−9.1

−9.2
2

c)

o

i
t

t

a
r
 
s
s
e
n
e
e
p
m
o
c
r
e
v
o

l

1.4

1.3

1.2

1.1

1

2

4

6

α

8

Figure 4: Natural images results. a) Free energy b) Marginal likelihood c) Estimated over-completeness d)
Basis vectors

freedom parameter moves towards sparser values. The distribution puts a very small amount of
mass at a very great distance from the mean (for example, the kurtosis is undeﬁned for α < 4). It
is not clear that data with such extreme values will be encountered in typical data sets, and so the
model may become distorted at high sparseness values.

Future work will be directed towards more general prior distributions. The formulation of the
Student-t in terms of a random precision Gaussian is computationally helpful. While no longer
within the exponential family, other distributions on the precision (such as a uniform one) may be
approximated using a similar approach.

Acknowledgements

This work has been supported by the Gatsby Charitable Foundation. We thank Yee Whye Teh, Iain
Murray, and David McKay for fruitful discussions.

References

[1] B.A. Olshausen and D.J. Field. Emergence of simple-cell receptive ﬁeld properties by learning a sparse

code for natural images. Nature, 381(6583):607–609, 1996.

[2] B.A. Olshausen and D.J. Field. Sparse coding with an overcomplete basis set: A strategy employed by

V1? Vision Research, 37:3311–3325, 1997.

[3] Y.W Teh, M. Welling, S. Osindero, and G.E. Hinton. Energy-based models for sparse overcomplete

representations. Journal of Machine Learning Research, 4:1235–1260, 2003.

[4] A.J. Bell and T.J. Sejnowski. The ‘independent components’ of natural scenes are edge ﬁlters. Vision

Research, 37(23):3327–3338, 1997.

[5] S. Osindero, M. Welling, and G.E. Hinton. Topographic product models applied to natural scene statistics.

Neural Computation, 18:381–344, 2006.

[6] D.J.C. McKay. Bayesian interpolation. Neural Comput, 4(3):415–447, 1992.
[7] C.M. Bishop. Variational principal components. In ICANN 1999 Proceedings, pages 509–514, 1999.
[8] M.J. Beal. Variational Algorithms for Approximate Bayesian Inference. PhD thesis, Gatsby Computa-

tional Neuroscience Unit, University College London, 2003.

[9] R.M. Neal. Annealed importance sampling. Statistics and Computing, 11:125–139, 2001.
[10] J.H. van Hateren and A. van der Schaaf. Independent component ﬁlters of natural images compared with

simple cells in primary visual cortex. Proc. R. Soc. Lond. B, 265:359–366, 1998.

8

"
887,2007,A Probabilistic Approach to Language Change,"We present a probabilistic approach to language change in which word forms are represented by phoneme sequences that undergo stochastic edits along the branches of a phylogenetic tree. Our framework combines the advantages of the classical comparative method with the robustness of corpus-based probabilistic models. We use this framework to explore the consequences of two different schemes for defining probabilistic models of phonological change, evaluating these schemes using the reconstruction of ancient word forms in Romance languages. The result is an efficient inference procedure for automatically inferring ancient word forms from modern languages, which can be generalized to support inferences about linguistic phylogenies.","A Probabilistic Approach to Language Change

Alexandre Bouchard-Cˆot´e∗

Percy Liang∗

∗Computer Science Division

University of California at Berkeley

Berkeley, CA 94720

Thomas L. Grifﬁths†
†Department of Psychology

Dan Klein∗

Abstract

We present a probabilistic approach to language change in which word forms
are represented by phoneme sequences that undergo stochastic edits along the
branches of a phylogenetic tree. This framework combines the advantages of
the classical comparative method with the robustness of corpus-based probabilis-
tic models. We use this framework to explore the consequences of two differ-
ent schemes for deﬁning probabilistic models of phonological change, evaluating
these schemes by reconstructing ancient word forms of Romance languages. The
result is an efﬁcient inference procedure for automatically inferring ancient word
forms from modern languages, which can be generalized to support inferences
about linguistic phylogenies.

1 Introduction

Languages evolve over time, with words changing in form, meaning, and the ways in which they can
be combined into sentences. Several centuries of linguistic analysis have shed light on some of the
key properties of this evolutionary process, but many open questions remain. A classical example is
the hypothetical Proto-Indo-European language, the reconstructed common ancestor of the modern
Indo-European languages. While the existence and general characteristics of this proto-language are
widely accepted, there is still debate regarding its precise phonology, the original homeland of its
speakers, and the date of various events in its evolution. The study of how languages change over
time is known as diachronic (or historical) linguistics (e.g., [4]).
Most of what we know about language change comes from the comparative method, in which words
from different languages are compared in order to identify their relationships. The goal is to identify
regular sound correspondences between languages and use these correspondences to infer the forms
of proto-languages and the phylogenetic relationships between languages. The motivation for basing
the analysis on sounds is that phonological changes are generally more systematic than syntactic or
morphological changes. Comparisons of words from different languages are traditionally carried
out by hand, introducing an element of subjectivity into diachronic linguistics. Early attempts to
quantify the similarity between languages (e.g., [15]) made drastic simplifying assumptions that
drew strong criticism from diachronic linguists.
In particular, many of these approaches simply
represent the appearance of a word in two languages with a single bit, rather than allowing for
gradations based on correspondences between sequences of phonemes.
We take a quantitative approach to diachronic linguistics that alleviates this problem by operating
at the phoneme level. Our approach combines the advantages of the classical, phoneme-based,
comparative method with the robustness of corpus-based probabilistic models. We focus on the
case where the words are etymological cognates across languages, e.g. French faire and Spanish
hacer from Latin facere (to do). Following [3], we use this information to estimate a contextualized
model of phonological change expressed as a probability distribution over rules applied to individual
phonemes. The model is fully generative, and thus can be used to solve a variety of problems. For
example, we can reconstruct ancestral word forms or inspect the rules learned along each branch of

1

a phylogeny to identify sound laws. Alternatively, we can observe a word in one or more modern
languages, say French and Spanish, and query the corresponding word form in another language,
say Italian. Finally, models of this kind can potentially be used as a building block in a system for
inferring the topology of phylogenetic trees [3].
In this paper, we use this general approach to evaluate the performance of two different schemes for
deﬁning probability distributions over rules. The ﬁrst scheme, used in [3], treats these distributions
as simple multinomials and uses a Dirichlet prior on these multinomials. This approach makes it
difﬁcult to capture rules that apply at different levels of granularity.
Inspired by the prevalence
of multi-scale rules in diachronic phonology and modern phonological theory, we develop a new
scheme in which rules possess a set of features, and a distribution over rules is deﬁned using a log-
linear model. We evaluate both schemes in reconstructing ancient word forms, showing that the new
linguistically-motivated change can improve performance signiﬁcantly.

2 Background and previous work

Most previous computational approaches to diachronic linguistics have focused on the reconstruc-
tion of phylogenetic trees from a Boolean matrix indicating the properties of words in different
languages [10, 6, 14, 13]. These approaches descend from glottochronology [15], which measures
the similarity between languages (and the time since they diverged) using the number of words in
those languages that belong to the same cognate set. This information is obtained from manually
curated cognate lists such as the data of [5]. The modern instantiations of this approach rely on so-
phisticated techniques for inferring phylogenies borrowed from evolutionary biology (e.g., [11, 7]).
However, they still generally use cognate sets as the basic data for evaluating the similarity between
languages (although some approaches incorporate additional manually constructed features [14]).
As an example of a cognate set encoding, consider the meaning “eat”. There would be one column
for the cognate set which appears in French as manger and Italian as mangiare since both descend
from the Latin mandere (to chew). There would be another column for the cognate set which appears
in both Spanish and Portuguese as comer, descending from the Latin comedere (to consume). If
these were the only data, algorithms based on this data would tend to conclude that French and Italian
were closely related and that Spanish and Portuguese were equally related. However, the cognate
set representation has several disadvantages: it does not capture the fact that the cognate is closer
between Spanish and Portuguese than between French and Spanish, nor do the resulting models let
us conclude anything about the regular processes which caused these languages to diverge. Also,
curating cognate data can be expensive. In contrast, each word in our work is tracked using an
automatically obtained cognate list. While these cognates may be noisier, we compensate for this
by modeling phonological changes rather than Boolean mutations in cognate sets.
Another line of computational work has explored using phonological models as a way to capture
the differences between languages. [16] describes an information theoretic measure of the distance
between two dialects of Chinese. They use a probabilistic edit model, but do not consider the recon-
struction of ancient word forms, nor do they present a learning algorithm for such models. There
have also been several approaches to the problem of cognate prediction in machine translation (es-
sentially transliteration), e.g., [12]. Compared to our work, the phenomena of interest, and therefore
the models, are different. [12] presents a model for learning “sound laws,” general phonological
changes governing two completely observed aligned cognate lists. This model can be viewed as a
special case of ours using a simple two-node topology.

3 A generative model of phonological change

In this section, we outline the framework for modeling phonological change that we will use through-
out the paper. Assume we have a ﬁxed set of word types (cognate sets) in our vocabulary V and a set
of languages L. Each word type i has a word form wil in each language l ∈ L, which is represented
as a sequence of phonemes which might or might not be observed. The languages are arranged
according to some tree topology T (see Figure 2(a) for examples). It is possible to also induce the
topology or cognate set assignments, but in this paper we assume that the topology is ﬁxed and
cognates have already been identiﬁed.

2

Figure 1: (a) A description of the generative model.
(b) An example of edits that were used to transform
the Latin word focus (/fokus/) into the Italian word fuoco (/fwOko/) (ﬁre) along with the context-speciﬁc rules
that were applied. (c) The graphical model representation of our model: θ are the parameters specifying the
stochastic edits e, which govern how the words w evolve.

consisting of n phonemes x1 ··· xn is generated with probability plm(x1) =(cid:81)n

The probabilistic model speciﬁes a distribution over the word forms {wil} for each word type i ∈ V
and each language l ∈ L via a simple generative process (Figure 1(a)). The generative process
starts at the root language and generates all the word forms in each language in a top-down manner.
The w ∼ LanguageModel distribution is a simple bigram phoneme model. A root word form w
j=2 plm(xj | xj−1),
where plm is the distribution of the language model. The stochastic edit model w(cid:48) ∼ Edit(w, θ)
describes how a single old word form w = x1 ··· xn changes along one branch of the phylogeny
with parameters θ to produce a new word form w(cid:48). This process is parametrized by rule probabilities
θk→l, which are speciﬁc to branch (k → l).
The generative process used in the edit model is as follows: for each phoneme xi in the old word
form, walking from left to right, choose a rule to apply. There are three types of rules: (1) deletion
of the phoneme, (2) substitution with some phoneme (possibly the same one), or (3) insertion of
another phoneme, either before or after the existing one. The probability of applying a rule depends
on the context (xi−1, xi+1). Context-dependent rules are often used to characterize phonological
changes in diachronic linguistics [4]. Figure 1(b) shows an example of the rules being applied. The
context-dependent form of these rules allows us to represent phenomena such as the likely deletion
of s in word-ﬁnal positions.

4 Deﬁning distributions over rules
In the model deﬁned in the previous section, each branch (k → l) ∈ T has a collection of context-
dependent rule probabilities θk→l. Speciﬁcally, θk→l speciﬁes a collection of multinomial distribu-
tions, one for each C = (cl, x, cr), where cl is left phoneme, x is the old phoneme, cr is the right
phoneme. Each multinomial distribution is over possible right-hand sides α of the rule, which could
consist of 0, 1, or 2 phonemes. We write θk→l(C, α) for the probability of rule x → α / c1
Previous work using this probabilistic framework simply placed independent Dirichlet priors on
each of the multinomial distributions [3]. While this choice results in a simple estimation procedure,
it has some severe limitations. Sound changes happen at many granularities. For example, from
Latin to Vulgar Latin, u → o occurs in many contexts while s → ∅ occurs only in word-ﬁnal con-
texts. Using independent Dirichlets forces us to commit to a single context granularity for C. Since
the different multinomial distributions are not tied together, generalization becomes very difﬁcult,
especially as data is limited. It is also difﬁcult to interpret the learned rules, since the evidence
for a coarse phenomenon such as u → o would be unnecessarily fragmented across many different

c2.

3

Foreachwordi∈V:wiROOT∼LanguageModelForeachbranch(k→l)∈T:θk→l∼Rules(σ2)[chooseeditparameters]Foreachwordi∈V:wil∼Edit(wik,θk→l)[samplewordform](a)Generativedescription#CVCVC##fokus##fwOko##CVVCV#f→f/#Vo→wO/CCk→k/VVu→o/CCs→/V#EditsappliedRulesused(b)Exampleofedits···wiAwiBwiCwiD······wordtypei=1...|V|eiA→BθA→BeiB→CθB→CeiB→DθB→D(c)Graphicalmodelcontext-dependent rules. We would like to ideally capture a phenomenon using a single rule or fea-
ture. We could relate the rule probabilities via a simple hierarchical Bayesian model, but we would
still have to deﬁne a single hierarchy of contexts. This restriction might be inappropriate given that
sound changes often depend on different contexts that are not necessarily nested.
For these reasons, we propose using a feature-based distribution over the rule probabilities. Let
F (C, α) be a feature vector that depends on the context-dependent rule (C, α), and λk→l be the
log-linear weights for branch (k → l). We use a Normal prior on the log-linear weights, λk→l ∼
N (0, σ2I). The rule probabilities are then deterministically related to the weights via the softmax
function:

(cid:80)

eλT
α(cid:48) eλT

k→lF (C,α)

k→lF (C,α(cid:48))

θk→l(C, α; λk→l) =

.

(1)

For each rule x → α / cl
cr, we deﬁned features based on whether x = α (i.e. self-substitution),
and whether |α| = n for each n = 0, 1, 2 (corresponding to deletion, substitution, and insertion).
We also deﬁned sets of features using three partitions of phonemes c into “natural classes”. These
correspond to looking at the place of articulation (denoted A2(c)), testing whether c is a vowel,
consonant, or boundary symbol (A1(c)), and the trivial wildcard partition (A0(c)), which allows
rules to be insensitive to c. Using these partitions, the ﬁnal set of features corresponded to whether
Akl(cl) = al and Akr(cr) = ar for each type of partitioning kl, kr ∈ {0, 1, 2} and natural classes
al, ar.
The move towards using a feature-based scheme for deﬁning rule probabilities is not just motivated
by the greater expressive capacity of this scheme. It also provides a connection with contemporary
phonological theory. Recent work in computational linguistics on probabilistic forms of optimality
theory has begun to use a similar approach, characterizing the distribution over word forms within a
language using a log-linear model applied to features of the words [17, 9]. Using similar features to
deﬁne a distribution over phonological changes thus provides a connection between synchronic and
diachronic linguistics in addition to a linguistically-motivated method for improving reconstruction.

5 Learning and inference

We use a Monte Carlo EM algorithm to ﬁt the parameters of both models. The algorithm iterates
between a stochastic E-step, which computes reconstructions based on the current edit parameters,
and an M-step, which updates the edit parameters based on the reconstructions.

5.1 Monte Carlo E-step: sampling the edits

The E-step computes the expected sufﬁcient statistics required for the M-step, which in our case is
the expected number of times each edit (such as o → O) was used in each context. Note that the
sufﬁcient statistics do not depend on the prior over rule probabilities; in particular, both the model
based on independent Dirichlet priors and the one based on a log-linear prior require the same E-step
computation.
An exact E-step would require summing over all possible edits involving all languages in the phy-
logeny (all unobserved {e},{w} variables in Figure 1(c)), which does not permit a tractable dynamic
program. Therefore, we resort to a Monte Carlo E-step, where many samples of the edit variables
are collected, and counts are computed based on these samples. Samples are drawn using Gibbs
sampling [8]: for each word form of a particular language wil, we ﬁx all other variables in the
model and sample wil along with its corresponding edits.
Consider the simple four-language topology in Figure 1(c). Suppose that the words in languages A,
C and D are ﬁxed, and we wish to sample the word at language B along with the three corresponding
sets of edits (remember that the edits fully determine the words). While there are an exponential
number of possible words/edits, we can exploit the Markov structure in the edit model to consider
all such words/edits using dynamic programming, in a way broadly similar to the forward-backward
algorithm for HMMs. See [3] for details of the dynamic program.

4

Experiment
Latin reconstruction (6.1)

Sound changes (6.2)

Topology

1
1
2

Model
Dirichlet
Log-linear
Log-linear

Heldout
la:293
la:293
None

(a) Topologies

(b) Experimental conditions

Figure 2: Conditions under which each of the experiments presented in this section were performed. The
topology indices correspond to those displayed at the left. The heldout column indicates how many words, if
any, were held out for edit distance evaluation, and from which language. All the experiments were run on a
data set of 582 cognates from [3].

5.2 M-step: updating the parameters
In the M-step, we estimate the distribution over rules for each branch (k → l). In the Dirichlet
model, this can be done in closed form [3]. In the log-linear model, we need to optimize the feature
weights λk→l. Let us ﬁx a single branch and drop the subscript. Let N(C, α) be the expected
number of times the rule (C, α) was used in the E-step. Given these sufﬁcient statistics, the estimate
of λ is given by optimizing the expected complete log-likelihood plus the regularization penalty
from the prior on λ,

(cid:104)

C,α

N(C, α)

O(λ) =(cid:88)
= (cid:88)
= ˆFj −(cid:88)

∂O(λ)
∂λj

C,α

N(C, α)

λT F (C, α) − log(cid:88)
(cid:104)
Fj(C, α) −(cid:88)

α(cid:48)

α(cid:48)

eλT F (C,α(cid:48))(cid:105) − ||λ||2

2σ2 .

θ(C, α(cid:48); λ)Fj(C, α(cid:48))

(cid:105) − λj

σ2

(2)

(3)

(4)

We use L-BFGS to optimize this convex objective. which only requires the partial derivatives:

N(C,·)θ(C, α(cid:48); λ)Fj(C, α(cid:48)) − λj
σ2 ,

def= (cid:80)

C,α N(C, α)Fj(C, α) is the empirical feature vector and N(C,·) def= (cid:80)

where ˆFj
α N(C, α)
is the number of times context C was used. ˆFj and N(C,·) do not depend on λ and thus can be
precomputed at the beginning of the M-step, thereby speeding up each L-BFGS iteration.

C,α(cid:48)

6 Experiments

In this section, we summarize the results of the experiments testing our different probabilistic models
of phonological change. The experimental conditions are summarized in Table 2. Training and test
data sets were taken from [3].

6.1 Reconstruction of ancient word forms

We ran the two models using Topology 1 in Figure 2 to assess the relative performance of Dirichlet-
parametrized versus log-linear-parametrized models. Half of the Latin words at the root of the tree
were held out, and the (uniform cost) Levenshtein edit distance from the predicted reconstruction to
the truth was computed. While the uniform-cost edit distance misses important aspects of phonol-
ogy (all phoneme substitutions are not equal, for instance), it is parameter-free and still seems to
correlate to a large extent with linguistic quality of reconstruction. It is also superior to held-out
log-likelihood, which fails to penalize errors in the modeling assumptions, and to measuring the
percentage of perfect reconstructions, which ignores the degree of correctness of each reconstructed
word.

5

laesitlavlibesptitTopology1Topology2Model
Dirichlet
Log-linear (0)
Log-linear (0,1)
Log-linear (0,1,2)

Baseline Model
3.33
3.21
3.14
3.10

3.59
3.59
3.59
3.59

Improvement

7%
11%
12%
14%

Table 1: Results of the edit distance experiment. The language column corresponds to the language held out for
evaluation. We show the mean edit distance across the evaluation examples. Improvement rate is computed by
comparing the score of the algorithm against the baseline described in Section 6.1. The numbers in parentheses
for the log-linear model indicate which levels of granularity were used to construct the features (see Section 4).

Figure 3: An example of the proper Latin reconstruction given the Spanish and Italian word forms. Our model
produces /dEntes/, which is nearly correct, capturing two out of three of the phenomena.

We ran EM for 10 iterations for each model, and evaluated performance via a Viterbi derivation pro-
duced using these parameters. Our baseline for comparison was picking randomly, for each heldout
node in the tree, an observed neighboring word (i.e., copy one of the modern forms). Both mod-
els outperformed this baseline (see Figure 3), and the log-linear model outperformed the Dirichlet
model, suggesting that the featurized system better captures the phonological changes. Moreover,
adding more features further improved the performance, indicating that being able to express rules
at multiple levels of granularity allows the model to capture the underlying phonological changes
more accurately.
To give a qualitative feel for the operation of the system (good and bad), consider the example
in Figure 3, taken from the Dirichlet-parametrized experiment. The Latin dentis /dEntis/ (teeth) is
nearly correctly reconstructed as /dEntes/, reconciling the appearance of the /j/ in the Spanish and
the disappearance of the ﬁnal /s/ in the Italian. Note that the /is/ vs. /es/ ending is difﬁcult to predict
in this context (indeed, it was one of the early distinctions to be eroded in Vulgar Latin).

6.2

Inference of phonological changes

Another use of this model is to automatically recover the phonological drift processes between
known or partially-known languages. To facilitate evaluation, we continued in the well-studied Ro-
mance evolutionary tree. Again, the root is Latin, but we now add an additional modern language,
Portuguese, and two additional hidden nodes. One of the nodes characterizes the least common an-
cestor of modern Spanish and Portuguese; the other, the least common ancestor of all three modern
languages. In Figure 2, Topology 2, these two nodes are labeled vl (Vulgar Latin) and ib (Proto-
Ibero Romance), respectively. Since we are omitting many other branches, these names should not
be understood as referring to actual historical proto-languages, but, at best, to collapsed points rep-
resenting several centuries of evolution. Nonetheless, the major reconstructed rules still correspond
to well-known phenomena and the learned model generally places them on reasonable branches.
Figure 4 shows the top four general rules for each of the evolutionary branches recovered by the
log-linear model. The rules are ranked by the number of times they were used in the derivations
during the last iteration of EM. The la, es, pt, and it forms are fully observed while the vl and
ib forms are automatically reconstructed. Figure 4 also shows a speciﬁc example of the evolution
of the Latin VERBUM (word), along with the speciﬁc edits employed by the model.
For this particular example, both the Dirichlet and the log-linear models produced the same recon-
struction in the internal nodes. However, the log-linear parametrization makes inspection of sound
laws easier. Indeed, with the Dirichlet model, since the natural classes are of ﬁxed granularity, some

6

/dEntis//djEntes//dEnti/i→EE→jEs→Figure 4: The tree shows the system’s hypothesized transformation of a selected Latin word form, VERBUM
(word) into the modern Spanish, Italian, and Portuguese pronunciations. The Latin root and modern leaves were
observed while the hidden nodes as well as all the derivations were obtained using the parameters computed
by our model after 10 iterations of EM. Nontrivial rules (i.e. rules that are not identities) used at each stage are
shown along the corresponding edge. The boxes display the top four nontrivial rules corresponding to each of
these evolutionary branches, ordered by the number of times they were applied during the last E step. These
are grouped and labeled by their active feature of highest weight. ALV stands for alveolar consonant.

rules must be redundantly discovered, which tends to ﬂood the top of the rule lists with duplicates.
In contrast, the log-linear model groups rules with features of the appropriate degree of generality.
While quantitative evaluation such as measuring edit distance is helpful for comparing results, it is
also illuminating to consider the plausibility of the learned parameters in a historical light, which we
do here brieﬂy. In particular, we consider rules on the branch between la and vl, for which we have
historical evidence. For example, documents such as the Appendix Probi [2] provide indications of
orthographic confusions which resulted from the growing gap between Classical Latin and Vulgar
Latin phonology around the 3rd and 4th centuries AD. The Appendix lists common misspellings of
Latin words, from which phonological changes can be inferred.
On the la to vl branch, rules for word-ﬁnal deletion of classical case markers dominate the list. It
is indeed likely that these were generally eliminated in Vulgar Latin. For the deletion of the /m/, the
Appendix Probi contains pairs such as PASSIM NON PASSI and OLIM NON OLI. For the deletion of
ﬁnal /s/, this was observed in early inscriptions, e.g. CORNELIO for CORNELIOS [1]. The frequent
leveling of the distinction between /o/ and /u/ (which was ranked 5, but was not included for space
reasons) can be also be found in the Appendix Probi: COLUBER NON COLOBER. Note that in the
speciﬁc example shown, the model lowers the original /u/ and then re-raises it in the pt branch due
to a later process along that branch.
Similarly, major canonical rules were discovered in other branches as well, for example, /v/ to /b/
fortition in Spanish, palatalization along several branches, and so on. Of course, the recovered
words and rules are not perfect. For example, reconstructed Ibero /trinta/ to Spanish /treinta/ (thirty)
is generated in an odd fashion using rules /e/ to /i/ and /n/ to /in/. In the Dirichlet model, even when
otherwise reasonable systematic sound changes are captured, the crudeness of the ﬁxed-granularity
contexts can prevent the true context from being captured, resulting in either rules applying with
low probability in overly coarse environments or rules being learned redundantly in overly ﬁne
environments. The featurized model alleviates this problem.

7 Conclusion

Probabilistic models have the potential to replace traditional methods used for comparing languages
in diachronic linguistics with quantitative methods for reconstructing word forms and inferring
phylogenies. In this paper, we presented a novel probabilistic model of phonological change, in
which the rules governing changes in the sound of words are parametrized using the features of the
phonemes involved. This model goes beyond previous work in this area, providing more accurate
reconstructions of ancient word forms and connections to current work on phonology in synchronic
linguistics. Using a log-linear model to deﬁne the probability of a rule being applied results in a

7

r→R/**e→/ALV#t→d/**Ù→s/**u→o/**o→os/C#v→b/**t→te/**/werbum/(la)/verbo/(vl)/veRbo/(ib)/beRbo/(es)/veRbu/(pt)/vErbo/(it)s→/*#m→/*#i→/*Vï→n/*VELARu→o/**e→E/**i→/CVa→ja/**n→m/**a→5/**o→u/**e→1/**m→u→ow→vr→Rv→bo→ue→Estraightforward inference procedure which can be used to both produce accurate reconstructions as
measured by edit distance and identify linguistically plausible rules that account for phonological
changes. We believe that this probabilistic approach has the potential to support quantitative analysis
of the history of languages in a way that can scale to large datasets while remaining sensitive to the
concerns that have traditionally motivated diachronic linguistics.

Acknowledgments We would like to thank Bonnie Chantarotwong for her help with the IPA con-
verter and our reviewers for their comments. This work was supported by a FQRNT fellowship to
the ﬁrst author, a NDSEG fellowship to the second author, NSF grant number BCS-0631518 to the
third author, and a Microsoft Research New Faculty Fellowship to the fourth author.

References
[1] W. Sidney Allen. Vox Latina: The Pronunciation of Classical Latin. Cambridge University

Press, 1989.

[2] W.A. Baehrens. Sprachlicher Kommentar zur vulg¨arlateinischen Appendix Probi. Halle

(Saale) M. Niemeyer, 1922.

[3] A. Bouchard-Cˆot´e, P. Liang, T. Grifﬁths, and D. Klein. A Probabilistic Approach to Diachronic
Phonology. In Empirical Methods in Natural Language Processing and Computational Natu-
ral Language Learning (EMNLP/CoNLL), 2007.

[4] L. Campbell. Historical Linguistics. The MIT Press, 1998.
[5] I. Dyen,

and P. Black.

J.B. Kruskal,

FILE IE-DATA1.

Available

at

http://www.ntu.edu.au/education/langs/ielex/IE-DATA1, 1997.

[6] S. N. Evans, D. Ringe, and T. Warnow. Inference of divergence times as a statistical inverse
problem. In P. Forster and C. Renfrew, editors, Phylogenetic Methods and the Prehistory of
Languages. McDonald Institute Monographs, 2004.

[7] J. Felsenstein. Inferring Phylogenies. Sinauer Associates, 2003.
[8] S. Geman and D. Geman. Stochastic relaxation, Gibbs distributions, and the Bayesian restora-
tion of images. IEEE Transactions on Pattern Analysis and Machine Intelligence, 6:721–741,
1984.

[9] S. Goldwater and M. Johnson. Learning ot constraint rankings using a maximum entropy

model. Proceedings of the Workshop on Variation within Optimality Theory, 2003.

[10] R. D. Gray and Q. Atkinson. Language-tree divergence times support the Anatolian theory of

Indo-European origins. Nature, 2003.

[11] J. P. Huelsenbeck, F. Ronquist, R. Nielsen, and J. P. Bollback. Bayesian inference of phylogeny

and its impact on evolutionary biology. Science, 2001.

[12] G. Kondrak. Algorithms for Language Reconstruction. PhD thesis, University of Toronto,

2002.

[13] L. Nakhleh, D. Ringe, and T. Warnow. Perfect phylogenetic networks: A new methodology
for reconstructing the evolutionary history of natural languages. Language, 81:382–420, 2005.
[14] D. Ringe, T. Warnow, and A. Taylor. Indo-european and computational cladistics. Transactions

of the Philological Society, 100:59–129, 2002.

[15] M. Swadesh. Towards greater accuracy in lexicostatistic dating. Journal of American Linguis-

tics, 21:121–137, 1955.

[16] A. Venkataraman, J. Newman, and J.D. Patrick. A complexity measure for diachronic chinese
phonology. In J. Coleman, editor, Computational Phonology. Association for Computational
Linguistics, 1997.

[17] C. Wilson and B. Hayes. A maximum entropy model of phonotactics and phonotactic learning.

Linguistic Inquiry, 2007.

8

"
925,2007,Learning the 2-D Topology of Images,"We study the following question: is the two-dimensional structure of images a very strong prior or is it something that can be learned with a few examples of natural images? If someone gave us a learning task involving images for which the two-dimensional topology of pixels was not known, could we discover it automatically and exploit it? For example suppose that the pixels had been permuted in a fixed but unknown way, could we recover the relative two-dimensional location of pixels on images? The surprising result presented here is that not only the answer is yes but that about as few as a thousand images are enough to approximately recover the relative locations of about a thousand pixels. This is achieved using a manifold learning algorithm applied to pixels associated with a measure of distributional similarity between pixel intensities. We compare different topology-extraction approaches and show how having the two-dimensional topology can be exploited.","Learning the 2-D Topology of Images

Nicolas Le Roux

University of Montreal

Yoshua Bengio

University of Montreal

nicolas.le.roux@umontreal.ca

yoshua.bengio@umontreal.ca

Pascal Lamblin

University of Montreal

lamblinp@umontreal.ca

Marc Joliveau

´Ecole Centrale Paris

marc.joliveau@ecp.fr

Bal´azs K´egl

LAL/LRI, University of Paris-Sud, CNRS

91898 Orsay, France

kegl@lal.in2p3.fr

Abstract

We study the following question: is the two-dimensional structure of images a
very strong prior or is it something that can be learned with a few examples of
natural images? If someone gave us a learning task involving images for which
the two-dimensional topology of pixels was not known, could we discover it auto-
matically and exploit it? For example suppose that the pixels had been permuted
in a ﬁxed but unknown way, could we recover the relative two-dimensional loca-
tion of pixels on images? The surprising result presented here is that not only the
answer is yes, but that about as few as a thousand images are enough to approxi-
mately recover the relative locations of about a thousand pixels. This is achieved
using a manifold learning algorithm applied to pixels associated with a measure of
distributional similarity between pixel intensities. We compare different topology-
extraction approaches and show how having the two-dimensional topology can be
exploited.

1 Introduction

Machine learning has been applied to a number of tasks involving an input domain with a spe-
cial topology: one-dimensional for sequences, two-dimensional for images, three-dimensional for
videos and for 3-D capture. Some learning algorithms are generic, e.g., working on arbitrary un-
structured vectors in  d, such as ordinary SVMs, decision trees, neural networks, and boosting
applied to generic learning algorithms. On the other hand, other learning algorithms successfully
exploit the speciﬁc topology of their input, e.g., SIFT-based machine vision [10], convolutional
neural networks [6, 7], time-delay neural networks [5, 16].

It has been conjectured [8, 2] that the two-dimensional structure of natural images is a very strong
prior that would require a huge number of bits to specify, if starting from the completely uniform
prior over all possible permutations.

The question studied here is the following: is the two-dimensional structure of natural images a
very strong prior or is it something that can be learned with a few examples? If a small number of
examples is enough to discover that structure, then the conjecture in [8] about the image topology
was probably incorrect. To answer that question we consider a hypothetical learning task involv-
ing images whose pixels have been permuted in a ﬁxed but unknown way. Could we recover the

1

two-dimensional relations between pixels automatically? Could we exploit it to obtain better gener-
alization? A related study performed in the context of ICA can be found in [1].

The basic idea of the paper is that the two-dimensional topology of pixels can be recovered by
looking for a two-dimensional manifold embedding pixels (each pixel is a point in that space), such
that nearby pixels have similar distributions of intensity (and possibly color) values.

We explore a number of manifold techniques with this goal in mind, and explain how we have
adapted these techniques in order to obtain the positive and surprising result: the two-dimensional
structure of pixels can be recovered from a rather small number of training images. On images we
ﬁnd that the ﬁrst 2 dimensions are dominant, meaning that even the knowledge that 2 dimensions
are most appropriate could probably be inferred from the data.

2 Manifold Learning Techniques Used

In this paper we have explored the question raised in the introduction for the particular case of
images, i.e., with 2-dimensional structures, and our experiments have been performed with images
It means that we have to look
of size 27 (cid:2) 27 to 30 (cid:2) 30, i.e., with about a thousand pixels.
for the embedding of about a thousand points (the pixels) on a two-dimensional manifold. Metric
Multi-Dimensional Scaling MDS is a linear embedding technique (analogous to PCA but starting
from distances and yielding coordinates on the principal directions, of maximum variance). Non-
parametric techniques such as Isomap [13], Local Linear Embedding (LLE) [12], or Semideﬁnite
Embedding (SDE, also known as MVU for Maximum Variance Unfolding) [17] have computation
time that scale polynomially in the number of examples n. With n around a thousand, all of these
are feasible, and we experimented with MDS, Isomap, LLE, and MVU.

Since we found Isomap to work best to recover the pixel topology even on small sets of images,
we review the basic elements of Isomap. It applies the metric multidimensional scaling (MDS)
algorithm to geodesic distances in the neighborhood graph. The neighborhood graph is obtained
by connecting the k nearest neighbors of each point. Each arc of the graph is associated with a
distance (the user-provided distance between points), and is used to compute an approximation of
the geodesic distance on the manifold with the length of the shortest path between two points. The
metric MDS algorithm then transforms these distances into d-dimensional coordinates as follows.
It ﬁrst computes the dot-product (or Gram) n (cid:2) n matrix M using the “double-centering” formula,
yielding entries Mij = (cid:0) 1
ij). The d principal eigen-
n Pj D2
vectors vk and eigenvalues (cid:21)k (k = 1; : : : ; d) of M are then computed. This yields the coordinates:
xik = vkip(cid:21)k is the k-th embedding coordinate of point i.

n2 Pi;j D2

n Pi D2

ij (cid:0) 1

2 (D2

ij (cid:0) 1

ij + 1

3 Topology-Discovery Algorithms

In order to apply a manifold learning algorithm, we must generally have a notion of similarity or
distance between the points to embed. Here each point corresponds to a pixel, and the data we have
about the pixels provide an empirical distribution of intensities for all pixels. Therefore we want to
compare two estimate the statistical dependency between two pixels, in order to determine if they
should be “neighbors” on the manifold. A simple and natural dependency statistic is the correlation
between pixel intensities, and it works very well.
The empirical correlation (cid:26)ij between the intensity of pixel i and pixel j is in the interval [(cid:0)1; 1].
However, two pixels highly anti-correlated are much more likely to be close than pixels not corre-
lated (think of edges in an image). We should thus consider the absolute value of the correlations. If
we assume them to be the value of a Gaussian kernel

j(cid:26)ijj = K(xi; xj) = e(cid:0) 1
then by deﬁning Dij = kxi (cid:0) xjk and solving the above for Dij we obtain a “distance” formula
that can be used with the manifold learning algorithms:

2 kxi(cid:0)xj k2

;

Note that scaling the distances in the Gaussian kernel by a variance parameter would only scale the
resulting embedding, so it is unnecessary.

Dij = q(cid:0) logj(cid:26)ijj :

(1)

2

Many other measures of distance would probably work as well. However, we found the absolute
correlation to be simple and easy to understand while yielding nice embeddings.

3.1 Dealing With Low-Variance Pixels

A difﬁculty we observed in experimenting with different manifold learning algorithms on data sets
such as MNIST is the inﬂuence of low-variance pixels. On MNIST digit images the border pixels
may have 0 or very small variance. This makes them all want to be close to each other, which tends
to fold the manifold on itself.

To handle this problem we have simply ignored pixels with very low variance. When these represent
a ﬁxed background (as in MNIST images), this strategy works ﬁne. In the experiments with MNIST
we removed pixels with standard deviation less than 15% of the maximum standard deviation (max-
imum over all pixels). On the NORB dataset, which has varied backgrounds, this step does not
remove any of the pixels (so it is unnecessary).

4 Converting Back to a Grid Image

Once we have obtained an embedding for the pixels, the next thing we would like to do is to trans-
form the data vectors back into images. For this purpose we have performed the following two
steps:

1. Choosing horizontal and vertical axes (since the coordinates on the manifold can be arbi-

trarily rotated), and rotating the embedding coordinates accordingly, and

2. Transforming the input vector of intensity values (along with the pixel coordinates) into an
ordinary discrete image on a grid. This should be done so that the resulting intensity at
position (i; j) is close to the intensity values associated with input pixels whose embedding
coordinates are (i; j).

Such a mapping of pixels to a grid has already been done in [4], where a grid topology is deﬁned
by the connections in a graphical model, which is then trained by maximizing the approximate
likelihood. However, they are not starting from a continuous embedding, but from the original data.
Let pk (k = 1 : : : N) be the embedding coordinates found by the dimensionality reduction algorithm
for the k-th input variable. We select the horizontal axis as the direction of smaller spread, the
vertical axis being in the orthogonal direction, and perform the appropriate rotation.
Once we have a coordinate system that assigns a 2-dimensional position pk to the k-th input pixel,
placed at irregular locations inside a rectangular grid, we can map the input intensities xk into
intensities Mi;j, so as to obtain a regular image that can be processed by standard image-processing
and machine vision learning algorithms. The output image pixel intensity Mi;j at coordinates (i; j)
is obtained through a convex average

where the weights are non-negative and sum to one, and are chosen as follows.

Mi;j = Xk

wi;j;kxk

with an exponential of the L1 distance to give less weight to farther points:

wi;j;k =

vi;j;k

Pk vi;j;k

(2)

(3)

vi;j;k = exp ((cid:13)k(i; j) (cid:0) pkk1)

 N (i;j;k)

where N (i; j; k) is true if k(i; j) (cid:0) pkk1 < 2 (or inferior to a larger radius to make sure that at least
one input pixel k is associated with output grid position (i; j)). We used (cid:13) = 3 in the experiments,
after trying only 1; 3 and 10. Large values of (cid:13) correspond to using only the nearest neighbor of
(i; j) among the pks. Smaller values smooth the intensities and make the output look better if the
embedding is not perfect. Too small values result in a loss of effective resolution.

3

Algorithm 1 Pseudo-code of the topology-learning learning that recovers the 2-D structure of inputs
provided in an arbitrary but ﬁxed order.
Input: X

fRaw input n (cid:2) N data matrix, one row per example, with elements in ﬁxed but
arbitrary orderg
Input: (cid:14) = 0:15 (default value)fMinimum relative standard deviation threshold, to remove too
low-variance pixelsg
Input: k = 4 (default value)fNumber of neighbors used to build Isomap neighborhood graphg
Input: L = pN ; W = pN (default values) fDimensions (length L, width W of output image)g
Input: (cid:13) = 3 (default value) fSmoothing coefﬁcient to recover imagesg
Output: p
fN (cid:2) 2 matrix of embedding coordinates (one per row) for each input variableg
Output: w
fConvolution weights to recover an image from a raw input vectorg

n = number of examples (rows of X)
for all column X:i do

(cid:22)i 1
i 1
(cid:27)2

n Pt Xti fCompute meansg
n Pt(Xti (cid:0) (cid:22)i)2 fCompute variancesg

end for
Remove columns of X for which
for all column X:i do

< (cid:14)

(cid:27)i

maxj (cid:27)j

for all column X:j do

empirical correlation (cid:26)ij = (X:i(cid:0)(cid:22)i)0(X:j (cid:0)(cid:22)j )
tionsg
pseudo-distances Dij = p(cid:0) logj(cid:26)ijj
end for

(cid:27)i(cid:27)j

fCompute all pair-wise empirical correla-

end for
fCompute the 2-D embeddings (pk1; pk2) of each input variable k through Isomapg
p = Isomap(D; k; 2)
fRotate the coordinates p to try to align them to a vertical-horizontal grid (see text)g
fInvert the axes if L < Wg
fCompute the convolution weights that will map raw values to output image pixel intensitiesg
for all grid position (i; j) in output image (i in 1 : : : L, j in 1 : : : W ) do

r = 1
repeat

neighbors fk : jjpk (cid:0) (i; j)jj1 < rg
r r + 1

until neighbors not empty
for all k in neighbors do

vk e(cid:13)jjpk(cid:0)(i;j)jj1

end for
wi;j;: 0
for all k in neighbors do

wi;j;k = vi;j;k

Pk vi;j;k fCompute convolution weightsg

end for

end for

Algorithm 2 Convolve a raw input vector into a regular grid image, using the already discovered
embedding for each input variable.
Input: x
Input: p
Input: w
Output: Y

fRaw input N-vector (in same format as a row of X above)g
fN (cid:2) 2 matrix of embedding coordinates (one per row) for each input variableg
fConvolution weights to recover an image from a raw input vectorg
fL (cid:2) W output imageg

for all grid position (i; j) in output image (i in 1 : : : L, j in 1 : : : W ) do

Yi;j Pk wi;j;kxk fPerform the convolutiong

end for

4

5 Experimental Results

We performed experiments on two sets of images: MNIST digits dataset and NORB object classi-
ﬁcation dataset 1. We used the “jittered objects and cluttered background” image set from NORB.
The MNIST images are particular in that they have a white background, whereas the NORB images
have more varying backgrounds. The NORB images are originally of dimension 108 (cid:2) 108; we
subsampled them by 4 (cid:2) 4 averaging into 27 (cid:2) 27 images. The experiments have been performed
with k = 4 neighbors for the Isomap embedding. Smaller values of k often led to unconnected
neighborhood graphs, which Isomap cannot deal with.

(a) Isomap embedding

(b) LLE embedding

(c) MDS embedding

(d) MVU embedding

Figure 1: Examples of embeddings discovered by Isomap, LLE, MDS and MVU with 250 training
images from NORB. Each of the original pixel is placed at the location discovered by the algorithm.
Size of the circle and gray level indicate the original true location of the pixel. Manifold learning
produces coordinates with an arbitrary rotation. Isomap appears most robust, and MDS the worst
method, for this task.

In Figure 1 we compare four different manifold learning algorithms on the NORB images: Isomap,
LLE, MDS and MVU. Figure 2 explains why Isomap is giving good results, especially in comparison
with MDS. One the one hand, MDS is using the pseudo-distance deﬁned in equation 1, whose
relationship with the real distance between two pixels in the original image is linear only in a small
neighborhood. On the other hand, Isomap uses the geodesic distances in the neighborhood graph,
whose relationship with the real distance is really close to linear.

(a)

(b)

(c)

(d)

Figure 2: (a) and (c): Pseudo-distance Dij (using formula 1) vs. the true distance on the grid.
(b) and (d): Geodesic distance in neighborhood graph vs. the true distance on the grid.
The true distance is on the horizontal axis for all ﬁgures.
(a) and (b) are for a point in the upper-left corner, (c) and (d) for a point in the center.

Figure 3 shows the embeddings obtained on the NORB data using different numbers of examples.
In order to quantitatively evaluate the reconstruction, we applied on each embedding the similarity
transformation that minimizes the Root of the Mean Squared Error (RMSE) between the coordinates
of each pixel on the embedding, and their coordinates on the original grid, before measuring the
residual error. This minimization is justiﬁed because the discovered embedding could be arbitrarily
rotated, isotropically scaled, and mirrored. 100 examples are enough to get a reasonable embedding,
and with 2000 or more a very good embedding is obtained: the RMSE for 2000 examples is 1:13,
meaning that in expectation, each pixel is off by slightly more than one.

1Both can be obtained from Yann Le Cun’s web site: http://yann.lecun.com/.

5

9.25

10 examples

2.43

50 examples

1.68

100 examples

1.21

1000 examples

1.13

2000 examples

Figure 3: Embedding discovered by Isomap on the NORB dataset, with different numbers of training
samples (top row). Second row shows the same embeddings aligned (by a similarity transformation)
on the original grid, third row shows the residual error (RMSE) after the alignment.

Figure 4 shows the whole process of transforming an original image (with pixels possibly permuted)
into an embedded image and ﬁnally into a reconstructed image as per algorithms 1 and 2.

Figure 4: Example of the process of transforming an MNIST image (top) from which pixel order
is unknown (second row) into its embedding (third row) and ﬁnally reconstructed as an image after
rotation and convolution (bottom). In the third row, we show the intensity associated to each original
pixel by the grey level in a circle located at the pixel coordinates discovered by Isomap.

We also performed experiments with acoustic spectral data to see if the time-frequency topology
can be recovered. The acoustic data come from the ﬁrst 100 blues pieces of a publically available
genre classiﬁcation dataset [14]. The FFT is computed for each frame and there are 86 frames per
second. The ﬁrst 30 frequency bands are kept, each covering 21.51 Hz. We used examples formed
by 30-frame spectrograms, i.e., just like images of size 30 (cid:2) 30. Using the ﬁrst 600,000 audio
samples from each recording yielded 2600 30-frames images, on which we applied our technique.
Figure 5 shows the resulting embedding when we removed the 30 coordinates of lowest standard
deviation ((cid:14) = :15).

6

4

3.5

3

2.5

2

1.5

1

0.5

0

 
1

Eigenvalues
Ratio of consecutive eigenvalues

 

2

3

4

5

6

7

8

9

10

(b) Spectrum

(a) Blues embedding

Figure 5: Embedding and spectrum decay for sequences of blues music.

6 Discussion

Although [8] argue that learning the right permutation of pixels with a ﬂat prior might be too difﬁcult
(either in a lifetime or through evolution), our results suggest otherwise.

How do we interpret that apparent contradiction?

The main element of explanation that we see is that the space of permutations of d numbers is not

such a large class of functions. There are approximately N = p2(cid:25)d(cid:0) d

approximation) of d numbers. Since this is a ﬁnite class of functions, its VC-dimension [15] is

e(cid:1)d permutations (Stirling

h = log N (cid:25) d log d (cid:0) d:

1

Hence if we had a bounded criterion (say taking values in [0; 1]) to compare different permutations
and we used n examples (i.e., n images, here), we would expect the difference between generaliza-
with probability 1(cid:0)(cid:17). Hence, with n a
tion error and test error to be bounded [15] by
multiple of d log d, we would expect that one could approximately learn a good permutation. When
d = 400 (the number of pixels with non-negligible variance in MNIST images), d log d(cid:0) d (cid:25) 2000.
This is more than what we have found necessary to recover a “good” representation of the images,
but on the other hand there are equivalent classes within the set of permutations that give as good
results as far as our objective and subjective criteria are concerned: we do not care about image
symmetries, rotations, and small errors in pixel placement.

2r 2 log N=(cid:17)

n

What is the selection criterion that we have used to recover the image structure? Mainly we have
used an additional prior which gives a preference to an order for which nearby pixels have similar
distributions. How speciﬁc to natural images and how strong is that prior? This may be an appli-
cation of a more general principle that could be advantageous to learning algorithms as well as to
brains. When we are trying to compute useful functions from raw data, it is important to discover
dependencies between the input random variables. If we are going to perform computations on sub-
sets of variables at a time (which would seem necessary when the number of inputs is very large,
to reduce the amount of connecting hardware), it would seem wiser that these computations com-
bine variables that have dependencies with each other. That directly gives rise to the notion of local
connectivity between neurons associated to nearby spatial locations, in the case of brains, the same
notion that is exploited in convolutional neural networks.

The fact that nearby pixels are more correlated is true at many scales in natural images. This is well
known and explains why Gabor-like ﬁlters often emerge when trying to learn good ﬁlters for images,
e.g., by ICA [9] or Products of Experts [3, 11].

In addition to the above arguments, there is another important consideration to keep in mind. The
way in which we score permutations is not the way that one would score functions in an ordinary
learning experiment. Indeed, by using the distributional similarity between pairs of pixels, we get
not just a scalar score but d(d(cid:0)1)=2 scores. Since our “scoring function” is much more informative,
it is not surprising that it allows us to generalize from many fewer examples.

7

7 Conclusion and Future Work

We proved here that, even with a small number of examples, we are able to recover almost per-
fectly the 2-D topology of images. This allows us to use image-speciﬁc learning algorithms without
specifying any prior other than the dimensionnality of the coordinates. We also showed that this
algorithm performed well on sound data, even though the topology might be less obvious in that
case.

However, in this paper, we only considered the simple case where we knew in advance the dimen-
sionnality of the coordinates. One could easily apply this algorithm to data whose intrinsic dimen-
sionality of the coordinates is unknown. In that case, one would not convert the embedding to a grid
image but rather keep it and connect only the inputs associated to close coordinates (performing a k
nearest neighbor for instance). It is not known if such an embedding might be useful for other types
of data than the ones discussed above.

Acknowledgements

The authors would like to thank James Bergstra for helping with the audio data. They also want to
acknowledge the support from several funding agencies: NSERC, the Canada Research Chairs, and
the MITACS network.

References
[1] S. Abdallah and M. Plumbley. Geometry dependency analysis. Technical Report C4DM-TR06-05, Center

for Digital Music, Queen Mary, University of London, 2006.

[2] Y. Bengio and Y. Le Cun. Scaling learning algorithms towards AI. In L. Bottou, O. Chapelle, D. DeCoste,

and J. Weston, editors, Large Scale Kernel Machines. MIT Press, 2007.

[3] G. Hinton, M. Welling, Y. Teh, and S. Osindero. A new view of ica. In Proceedings of ICA-2001, San

Diego, CA, 2001.

[4] A. Hyv¨arinen, P. O. Hoyer, and M. Inki. Topographic independent component analysis. Neural Compu-

tation, 13(7):1527–1558, 2001.

[5] K. J. Lang and G. E. Hinton. The development of the time-delay neural network architecture for speech

recognition. Technical Report CMU-CS-88-152, Carnegie-Mellon University, 1988.

[6] Y. LeCun, B. Boser, J. Denker, D. Henderson, R. Howard, W. Hubbard, and L. Jackel. Backpropagation

applied to handwritten zip code recognition. Neural Computation, 1(4):541–551, 1989.

[7] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient based learning applied to document recognition.

Proceedings of the IEEE, 86(11):2278–2324, November 1998.

[8] Y. LeCun and J. S. Denker. Natural versus universal probability complexity, and entropy.

Workshop on the Physics of Computation, pages 122–127. IEEE, 1992.

In IEEE

[9] T.-W. Lee and M. S. Lewicki. Unsupervised classiﬁcation segmentation and enhancement of images using

ica mixture models. IEEE Trans. Image Proc., 11(3):270–279, 2002.

[10] D. Lowe. Distinctive image features from scale-invariant keypoints. International Journal of Computer

Vision, 60(2):91–110, 2004.

[11] S. Osindero, M. Welling, and G. Hinton. Topographic product models applied to natural scene statistics.

Neural Computation, 18:381–344, 2005.

[12] S. Roweis and L. Saul. Nonlinear dimensionality reduction by locally linear embedding. Science,

290(5500):2323–2326, Dec. 2000.

[13] J. Tenenbaum, V. de Silva, and J. Langford. A global geometric framework for nonlinear dimensionality

reduction. Science, 290(5500):2319–2323, Dec. 2000.

[14] G. Tzanetakis and P. Cook. Musical genre classiﬁcation of audio signals. IEEE Transactions on Speech

and Audio Processing, 10(5):293–302, Jul 2002.

[15] V. Vapnik. Estimation of Dependences Based on Empirical Data. Springer-Verlag, Berlin, 1982.
[16] A. Waibel. Modular construction of time-delay neural networks for speech recognition. Neural Compu-

tation, 1:39–46, 1989.

[17] K. Q. Weinberger and L. K. Saul. An introduction to nonlinear dimensionality reduction by maximum
variance unfolding. In Proceedings of the National Conference on Artiﬁcial Intelligence (AAAI), Boston,
MA, 2006.

8

"
1082,2007,Variational Inference for Diffusion Processes,"Diffusion processes are a family of continuous-time continuous-state stochastic processes that are in general only partially observed. The joint estimation of the forcing parameters and the system noise (volatility) in these dynamical systems is a crucial, but non-trivial task, especially when the system is nonlinear and multi-modal. We propose a variational treatment of diffusion processes, which allows us to estimate these parameters by simple gradient techniques and which is computationally less demanding than most MCMC approaches. Furthermore, our parameter inference scheme does not break down when the time step gets smaller, unlike most current approaches. Finally, we show how a cheap estimate of the posterior over the parameters can be constructed based on the variational free energy.","Variational Inference for Diffusion Processes

C´edric Archambeau

University College London

c.archambeau@cs.ucl.ac.uk

Manfred Opper

Technical University Berlin

opperm@cs.tu-berlin.de

Yuan Shen

Aston University

y.shen2@aston.ac.uk

Dan Cornford
Aston University

d.cornford@aston.ac.uk

Abstract

John Shawe-Taylor

University College London

jst@cs.ucl.ac.uk

Diffusion processes are a family of continuous-time continuous-state stochastic
processes that are in general only partially observed. The joint estimation of the
forcing parameters and the system noise (volatility) in these dynamical systems is
a crucial, but non-trivial task, especially when the system is nonlinear and multi-
modal. We propose a variational treatment of diffusion processes, which allows
us to compute type II maximum likelihood estimates of the parameters by sim-
ple gradient techniques and which is computationally less demanding than most
MCMC approaches. We also show how a cheap estimate of the posterior over the
parameters can be constructed based on the variational free energy.

1 Introduction

Continuous-time diffusion processes, described by stochastic differential equations (SDEs), arise
naturally in a range of applications from environmental modelling to mathematical ﬁnance [13]. In
statistics the problem of Bayesian inference for both the state and parameters, within partially ob-
served, non-linear diffusion processes has been tackled using Markov Chain Monte Carlo (MCMC)
approaches based on data augmentation [17, 11], Monte Carlo exact simulation methods [6], or
Langevin / hybrid Monte Carlo methods [1, 3]. Within the signal processing community solutions
to the so called Zakai equation [12] based on particle ﬁlters [8], a variety of extensions to the Kalman
ﬁlter/smoother [2, 5] and mean ﬁeld analysis of the SDE together with moment closure methods [10]
have also been proposed. In this work we develop a novel variational approach to the problem of
approximate inference in continuous-time diffusion processes, including a marginal likelihood (ev-
idence) based inference technique for the forcing parameters. In general, joint parameter and state
inference using naive methods is complicated due to dependencies between state and system noise
parameters.
We work in continuous time, computing distributions over sample paths1, and discretise only in
our posterior approximation, which has advantages over methods based on discretising the SDE
directly [3]. The approximate inference approach we describe is more computationally efﬁcient than
competing Monte Carlo algorithms and could be further improved in speed by deﬁning a variety
of sub-optimal approximations. The approximation is also more accurate than existing Kalman
smoothing methods applied to non-linear systems [4]. Ultimately, we are motivated by the critical
requirement to estimate parameters within large environmental models, where at present only a small
number of Kalman ﬁlter/smoother based estimation algorithms have been attempted [2], and there
have been no likelihood based attempts to estimate the system noise forcing parameters.

1A sample path is a continuous-time realisation of a stochatic process in a certain time interval. Hence, a

sample path is an inﬁnite dimensional object.

1

In Section 2 and 3, we introduce the formalism for a variational treatment of partially observed diffu-
sion processes with measurement noise and we provide the tools to estimate the optimal variational
posterior process [4]. Section 4 deals with the estimation of the drift and the system noise parame-
ter, as well as the estimation of the optimal initial conditions. Finally, the approach is validated on a
bi-stable nonlinear system in Section 5. In this context, we also discuss how to construct an estimate
of the posterior distribution over parameters based on the variational free energy.

2 Diffusion processes with measurement error
Consider the continuous-time continuous-state stochastic process X = {Xt, t0 ≤ t ≤ tf}. We
assume this process is a d-dimensional diffusion process. Its time evolution is described by the
following SDE (to be interpreted as an Ito stochastic integral):

dWt ∼ N (0, dtI).

dXt = fθ(t, Xt) dt + Σ1/2 dWt,

(1)
The nonlinear vector function fθ deﬁnes the deterministic drift and the positive semi-deﬁnite matrix
Σ ∈ Rd×d is the system noise covariance. The diffusion is modelled by a d-dimensional Wiener
process W = {Wt, t0 ≤ t ≤ tf} (see e.g. [13] for a formal deﬁnition). Eq. (1) deﬁnes a process
with additive system noise. This might seem restrictive at ﬁrst sight. However, it can be shown
[13, 17, 6] that a range of state dependent stochastic forcings can be transformed into this form.
It is further assumed that only a small number of discrete-time latent states are observed and that the
observations are subject to measurement error. We denote the set of observations at the discrete times
{tn}N
n=1, with xn = Xt=tn.For
simplicity, the measurement noise is modelled by a zero-mean multivariate Gaussian density,with
covariance matrix R ∈ Rd×d.

n=1 and the corresponding latent states by {xn}N

n=1 by Y = {yn}N

3 Approximate inference for diffusion processes

(cid:28)
ln p(Y, X|θ, Σ)
q(X|Σ)

(cid:29)

Our approximate inference scheme builds on [4] and is based on a variational inference approach
(see for example [7]). The aim is to minimise the variational free energy, which is deﬁned as follows:

FΣ(q, θ) = −

, X = {Xt, t0 ≤ t ≤ tf},

q

(2)
where q(X|Σ) is an approximate posterior process over sample paths in the interval [t0, tf ] and θ
are the parameters, excluding the stochastic forcing covariance matrix Σ. Hence, this quantity is an
upper bound to the negative log-marginal likelihood:

− ln p(Y |θ, Σ) = FΣ(q, θ) − KL [q(X|Σ)(cid:107)p(X|Y, θ, Σ)] ≤ FΣ(q, θ).

(3)
As noted in Appendix A, this bound is ﬁnite if the approximate process is another diffusion process
with a system noise covariance chosen to be identical to that of the prior process induced by (1).
The standard approach for learning the parameters in presence of latent variables is to use an EM
type algorithm [9]. However, since the variational distribution is restricted to have the same system
noise covariance (see Appendix A) as the true posterior, the EM algorithm would leave this covari-
ance completely unchanged in the M step and cannot be used for learning this crucial parameter.
Therefore, we adopt a different approach, which is based on a conjugate gradient method.

3.1 Optimal approximate posterior process

We consider an approximate time-varying linear process with the same diffusion term, that is the
same system noise covariance:

dXt = g(t, Xt) dt + Σ1/2 dWt,

(4)
where g(t, x) = −A(t)x+b(t), with A(t) ∈ Rd×d and b(t) ∈ Rd. In other words, the approximate
posterior process q(X|Σ) is restricted to be a Gaussian process [4]. The Gaussian marginal at time
t is deﬁned as follows:

dWt ∼ N (0, dtI),

q(Xt|Σ) = N (Xt|m(t), S(t)),

t0 ≤ t ≤ tf ,

(5)

2

(cid:90) tf

(cid:90) tf

Eobs(t)(cid:88)

where m(t) ∈ Rd and S(t) ∈ Rd×d are respectively the marginal mean and the marginal covariance
at time t. In the rest of the paper, we denote q(Xt|Σ) by the shorthand notation qt.
For ﬁxed parameters θ and assuming that there is no observation at the initial time t0, the optimal
approximate posterior process q(X|Σ) is the one minimizing the variational free energy, which is
given by (see Appendix A)

FΣ(q, θ) =

Esde(t) dt +

δ(t − tn) dt + KL [q0(cid:107)p0] .

(6)

t0

t0

n

The function δ(t) is Dirac’s delta function. The energy functions Esde(t) and Eobs(t) are deﬁned as
follows:

(cid:10)(fθ(t, Xt) − g(t, Xt))(cid:62)Σ−1(fθ(t, Xt) − g(t, Xt))(cid:11)
(cid:10)(Yt − Xt)(cid:62)R−1(Yt − Xt)(cid:11)

ln|R|.

ln 2π +

,

qt

+ d
2

qt

1
2

Esde(t) =

Eobs(t) =

1
2
1
2

where {Yt, t0 ≤ t ≤ tf} is the underlying continuous-time observable process.

(7)

(8)

3.2 Smoothing algorithm

The variational parameters to optimise in order to ﬁnd the optimal Gaussian process approximation
are A(t), b(t), m(t) and S(t). For a linear SDE with additive system noise, it can be shown that
the time evolution of the means and the covariances are described by a set of ordinary differential
equations [13, 4]:

˙m(t) = −A(t)m(t) + b(t),
˙S(t) = −A(t)S(t) − S(t)A(cid:62)(t) + Σ,

(9)
(10)
where ˙ denotes the time derivtive. These equations provide us with consistency constraints for the
marginal means and the marginal covariances along sample paths. To enforce these constraints we
formulate the Lagrangian

(cid:90) tf

t0

(cid:62)(t)(cid:0) ˙m(t) + A(t)m(t) − b(t)(cid:1) dt
(cid:16) ˙S(t) + 2A(t)S(t) − Σ
(cid:17)(cid:111)

λ

Lθ,Σ = FΣ(q, θ) −

(cid:90) tf

−

(cid:110)

tr

Ψ(t)

(11)
where λ(t) ∈ Rd and Ψ(t) ∈ Rd×d are time dependent Lagrange multipliers, with Ψ(t) symmetric.
First, taking the functional derivatives of Lθ,Σ with respect to A(t) and b(t) results in the following
gradient functions:

dt,

t0

∇ALθ,Σ(t) = ∇AEsde(t) − λ(t)m(cid:62)(t) − 2Ψ(t)S(t),
∇bLθ,Σ(t) = ∇bEsde(t) + λ(t).

(12)
(13)

The gradients ∇AEsde(t) and ∇bEsde(t) are derived in Appendix B.
Secondly, taking the functional derivatives of Lθ,Σ with respect to m(t) and S(t), setting to zero
and rearranging leads to a set of ordinary differential equations, which describe the time evolution
of the Lagrange multipliers, along with jump conditions when there are observations:
n − ∇mEobs(t)|t=tn ,
−
n = λ
n − ∇SEobs(t)|t=tn .
n = Ψ−

˙λ(t) = −∇mEsde(t) + A(cid:62)(t)λ(t), λ+
˙Ψ(t) = −∇SEsde(t) + 2Ψ(t)A(t), Ψ+

(14)
(15)

The optimal variational functions can be computed by means of a gradient descent technique, such
as the conjugate gradient (see e.g., [16]). The explicit gradients with respect to A(t) and b(t)
are given by (12) and (13). Since m(t), S(t), λ(t) and Ψ(t) are dependent on these parameters,
one needs also to take the corresponding implicit derivatives into account. However, these implicit
gradients vanish if the consistency constraints for the means (9) and the covariances (10), as well as
the ones for the Lagrange multipliers (14-15), are satisﬁed. One way to achieve this is to perform a
forward propagation of the means and the covariances, followed by a backward propagation of the
Lagrange multipliers, and then to take a gradient step. The resulting algorithm for computing the
optimal posterior q(X|Σ) over sample paths is detailed in Algorithm 1.

3

for k = 0 to K − 1 do

end for{forward propagation}
for k = K to 1 do

mk+1 ← mk − (Akmk − bk)∆t
Sk+1 ← Sk − (AkSk + SkA(cid:62)

Algorithm 1 Compute the optimal q(X|Σ).
1: input(m0, S0, θ, Σ, t0, tf , ∆t, ω)
2: K ← (tf − t0)/∆t
3: initialise {Ak, bk}k≥0
4: repeat
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18: until minimum of Lθ,Σ is attained {optimisation loop}
19: return {Ak, bk, mk, Sk, λk, Ψk}k≥0

λk−1 ← λk + (∇mEsde|t=tk − A(cid:62)
k λk)∆t
Ψk−1 ← Ψk + (∇SEsde|t=tk − 2ΨkAk)∆t
if observation at tk−1 then

λk−1 ← λk−1 + ∇mEobs|t=tk−1
Ψk−1 ← Ψk−1 + ∇SEobs|t=tk−1

end if{jumps}

k − Σ)∆t

end for{backward sweep (adjoint operation)}
update {Ak, bk}k≥0 using the gradient functions (12) and (13)

4 Parameter estimation

The parameters to optimise include the parameters of the prior over the initial state, the drift func-
tion parameters and the system noise covariance. The estimation of the parameters related to the
observable process are not discussed in this work, although it is a straightforward extension.
The smoothing algorithm described in the previous section computes the optimal posterior process
by providing us with the stationary solution functions A(t) and b(t). Therefore, when subsequently
optimising the parameters we only need to compute their explicit derivatives; the implicit ones
vanish since ∇ALθ,Σ = 0 and ∇bLθ,Σ = 0. Before computing the gradients, we integrate (11) by
parts to make the boundary conditions explicit. This leads to

(cid:110)

(cid:90) tf
(cid:62)(t)(cid:0)A(t)m(t) − b(t)(cid:1) − ˙λ
(cid:110)
(cid:111)
Ψ(t)(cid:0)2A(t)S(t) − Σ(cid:1) − ˙Ψ(t)S(t)

λ

t0

Lθ,Σ = FΣ(q, θ) −

(cid:90) tf

−

tr

(cid:62)

(t)m(t)

dt − λ
(cid:62)
f mf + λ

(cid:62)
0 m0

dt − tr{Ψf Sf} + tr{Ψ0S0} ,

(16)

(cid:111)

t0

At the ﬁnal time tf , there are no consistency constraints, that is λf and Ψf are both equal to zero.

Initial state

4.1
The initial variational posterior q(x0) is chosen equal to N (x0|m0, S0) to ensure that the approxi-
mate process is a Gaussian one. Taking the derivatives of (16) with respect to m0 and S0 results in
the following expressions:

∇m0Lθ,Σ = λ0 + τ−1

0 (m0 − µ0), ∇S0Lθ,Σ = Ψ0 +

1
2

0 I − S−1

0

(17)

where the prior p(x0) is assumed to be an isotropic Gaussian density with mean µ0. Its variance τ0
is taken sufﬁciently large to give a broad prior.

4.2 Drift

(cid:0)τ−1

(cid:1) ,

The gradients for the drift function parameters θf only depend on the total energy associated to the
SDE. Their general expression is given by

∇θfLθ,Σ =

∇θf Esde(t) dt,

(18)

(cid:90) tf

t0

4

(cid:68)

(cid:69)

where ∇θf Esde(t) =
do play a role in this gradient as they enter through g(t, Xt) and the expectation w.r.t. q(Xt|Σ).

(fθ(t, Xt) − g(t, Xt))(cid:62) Σ−1∇θf fθ(t, Xt)

. Note that the observations

qt

4.3 System noise

Estimating the system noise covariance (or volatility) is essential as the system noise, together with
the drift function, determines the dynamics. In general, this parameter is difﬁcult to estimate using
an MCMC approach because the efﬁciency is strongly dependent on the discrete approximation of
the SDE and most methods break down when the time step ∆t gets too small [11, 6]. For example
in a Bayesian MCMC approach, which alternates between sampling paths and parameters, the latent
paths imputed between observations must have a system noise parameter which is arbitrarily close
to its previous value in order to be accepted by a Metropolis sampler. Hence, the algorithm becomes
extremely slow. Note, that for the same reason, a naive EM algorithm within our approach breaks
down. However, in our method, we can simply compute approximations to the marginal likelihood
and its gradient directly. In the next section, we will compare our results to a direct MCMC estimate
of the marginal likelihood which is a time consuming method.
The gradient of (16) with respect to Σ is given by

(cid:90) tf

(cid:90) tf

where ∇ΣEsde(t) = − 1

∇ΣLθ,Σ =

2 Σ−1(cid:68)

∇ΣEsde(t) dt +

(fθ(t, Xt) − g(t, Xt)) (fθ(t, Xt) − g(t, Xt))(cid:62)(cid:69)

Ψ(t) dt,

t0

t0

(19)

Σ−1.

qt

5 Experimental validation on a bi-stable system

fθ(t, x) = 4x(cid:0)θ − x2(cid:1) ,

In order to validate the approach, we consider the 1 dimensional double-well system:

θ > 0,

(20)
where fθ(t, x) is the drift function. This dynamical system is highly nonlinear and its stationary
distribution is multi-modal. It has two stable states, one in x = −θ and one in x = +θ. The system
is driven by the system noise, which makes it occasionally ﬂip from one well to the other.
In the experiments, we set the drift parameter θ to 1, the system noise standard deviation σ to 0.5 and
the measurement error standard deviation r to 0.2. The time step for the variational approximation
is set to ∆t = 0.01, which is identical to the time resolution used to generate the original sample
path. In this setting, the exit time from one of the wells is 4000 time units [15]. In other words, the
transition from one well to the other is highly unlikely in the window of roughly 8 time units that
we consider and where a transition occurs.
Figure 1(a) compares the variational solution to the outcomes of a hybrid MCMC simulation of the
posterior process using the true parameter values. The hybrid MCMC approach was proposed in
[1]. At each step of the sampling process, an entire sample path is generated. In order to keep the
acceptance of new paths sufﬁciently high, the basic MCMC algorithm is combined with ideas from
Molecular Dynamics, such that the MCMC sampler moves towards regions of high probability in the
state space. An important drawback of MCMC approaches is that it might be extremely difﬁcult to
monitor their convergence and that they may require a very large number of samples before actually
converging. In particular, over 100, 000 sample paths were necessary to reach convergence in the
case of the double-well system.
The solution provided by the hybrid MCMC is here considered as the base line solution. One can ob-
serve that the variational solution underestimates the uncertainty (smaller error bars). Nevertheless,
the time of the transition is correctly located. Convergence of the smoothing algorithm was achieved
in approximately 180 conjugate gradient steps, each one involving a forward and backward sweep.
The optimal parameters and the optimal initial conditions for the variational solution are given by

(21)
Convergence of the outer optimization loop is typically reached after less then 10 conjugate gradient
steps. While the estimated value for the drift parameter is within 15% percent from its true value,

ˆm0 = 0.88,

ˆs0 = 0.45.

ˆσ = 0.72,

ˆθf = 0.85,

5

(a)

(b)

Figure 1: (a) Variational solution (solid) compared to the hybrid MCMC solution (dashed), using
the true parameter values. The curves denote the mean paths and the shaded regions are the two-
standard deviation noise tubes. (b) Posterior of the system noise variance (diffusion term). The plain
curve and the dashed curve are respectively the approximations of the posterior shape based on the
variational free energy and MCMC.

the deviation of the system noise is worse. Deviations may be explained by the fact that the number
of observations is relatively small. Furthermore, we have chosen a sample path which contains
a transition between the two wells within a small time interval and is thus highly untypical with
respect to the prior distribution. This fact was experimentally assessed by estimating the parameters
on a sample path without transition, in a time window of the same size. In this case, we obtained
estimate roughly within 5% of the true parameter values: ˆσ = 0.46 and ˆθf = 0.92. Finally, it turns
out that our estimate for ˆσ is close to the one obtained from the MCMC approach as discussed next.

Posterior distribution over the parameters
Interestingly, minimizing the free energy Fσ2 for different values of σ provides us with much more
information than a single point estimate for the parameters [14]. Using a suitable prior over p(σ),
we can approximate the posterior over the system noise variance via

p(σ2|Y ) ∝ e−Fσ2 p(σ2),

(22)
where we take e−Fσ2 (at its minimum) as an approximation to the marginal likelihood of the ob-
servations p(Y |σ2). To illustrate this point, we assume a non-informative Gamma prior p(σ2) =
G(α, β), with α = 10−3 and β = 10−3. A comparison with preliminary MCMC estimates for
p(Y |σ2) for θ = 1 and a set of system noise variances indicates that the shape of our approximation
is a reasonable indicator of the shape of the posterior. Figure 1(b) shows that at least the mean and
the variance of the density come out fairly well.

6 Conclusion

We have presented a variational approach to the approximate inference of stochastic differential
equations from a ﬁnite set of noisy observations. So far, we have tested the method on a one dimen-
sional bi-stable system only. Comparison with a Monte Carlo approach suggests that our method
can reproduce the posterior mean fairly well but underestimates the variance in the region of the
transition. Parameter estimates also agree well with the MC predictions.
In the future, we will extend our method in various directions. Although our approach is based on
a Gaussian approximation of the posterior process, we expect that one can improve on it and obtain
non-Gaussian predictions at least for various marginal posterior distributions, including that of the
latent variable Xt at a ﬁxed time t. This should be possible by generalising our method for the
computation of a non-Gaussian shaped probability density for the system noise parameter using the
free energy. An important extension of our method will be to systems with many degrees of freedom.

6

012345678!2!1.5!1!0.500.511.52tx0.30.40.50.60.70.80.900.20.40.60.811.21.41.6!2We hope that the possibility of using simpler suboptimal parametrisations of the approximating
Gaussian process will allow us to obtain a tractable inference method that scales well to higher
dimensions.

Acknowledgments

This work has been funded by the EPSRC as part of the Variational Inference for Stochastic Dynamic
Environmental Models (VISDEM) project (EP/C005848/1).

A The Kullback-Leibler divergence interpreted as a path integral

In this section, we show that the Kullback-Leibler divergence between the posterior process
p(Xt|Y, θ, Σ) and its approximation q(X|Σ) can be interpreted as a path integral over time.
It
is an average over all possible realisations, called sample paths, of the continuous-time (i.e., inﬁnite
dimensional) random variable described by the SDE in the time interval under consideration.
Consider the Euler-Muryama discrete approximation (see for example [13]) of the SDE (1) and its
linear approximation (4):

(23)
(24)
where ∆xk ≡ xk+1 − xk and wk ∼ N (0, ∆tI). The vectors fk and gk are shorthand notations
for fθ(tk, xk) and g(tk, xk). Hence, the joint distributions of discrete sample paths {xk}k≥0 for the
true process and its approximation follow from the Markov property:

∆xk = fk∆t + Σ1/2∆wk,
∆xk = gk∆t + Σ1/2∆wk,

p(x0, . . . , xK|Σ) = p(x0)(cid:89)
q(x0, . . . , xK|Σ) = q(x0)(cid:89)

k>0

k>0

N (xk+1|xk + fk∆t, Σ∆t),

N (xk+1|xk + gk∆t, Σ∆t),

(25)

(26)

where p(x0) is the prior on the intial state x0 and q(x0) is assumed to be Gaussian. Note thate we
do not restrict the variational posterior to factorise over the latent states.
The Kullback-Leibler divergence between the two discretized prior processes is given by
q(xk)(cid:104)ln p(xk+1|xk)(cid:105)q(xk+1|xk) dxk

(cid:90)
KL [q(cid:107)p] = KL [q(x0)(cid:107)p(x0)] −(cid:88)
(cid:88)

= KL [q(x0)(cid:107)p(x0)] +

k>0
1
2

k>0

(cid:10)(fk − gk)(cid:62)Σ−1(fk − gk)(cid:11)
(cid:90) tf

(cid:10)(ft − gt)(cid:62)Σ−1(ft − gt)(cid:11)

q(xk) ∆t,

where we omitted the conditional dependency on Σ for simplicity. The second term on the right
hand side is a sum in ∆t. As a result, taking limits for ∆t → 0 leads to a proper Riemann integral,
which deﬁnes an integral over the average sample path:

1
2

KL [q(X|Σ)(cid:107)p(X|θ, Σ)] = KL [q0(cid:107)p0] +

(27)
where X = {Xt, t0 ≤ t ≤ tf} denotes the stochastic process in the interval [t0, tf ]. The distribu-
tion qt = q(Xt|Σ) is the marginal at time t for a given system noise covariance Σ.
It is important to realise that the KL between the induced prior process and its approximation is
ﬁnite because the system noise covariances are chosen to be identical.
If this was not the case,
the normalizing constants of p(xk+1|xk) and q(xk+1|xk) would not cancel. This would result in
KL → ∞ when ∆t → 0.
If we assume that the observations are i.i.d., it follows also that

dt,

qt

t0

FΣ(q, θ) = −(cid:88)

(cid:104)ln p(yn|xn)(cid:105)q(xn) + KL [q(X|Σ)(cid:107)p(X|θ, Σ)] .

n

Clearly, minimising this expression with respect to the variational parameters for a given system
noise Σ and for a ﬁxed parameter vector θ is equivalent to minimising the KL between the vari-
ational posterior q(X|Σ) and the true posterior p(X|Y, θ, Σ), since the normalizing constant is
independent of sample paths.

7

B The gradient functions

The general expressions for the gradients of Esde(t) with respect to the variational functions are
given by

∇AEsde(t) = Σ−1(cid:110)(cid:104)∇xfθ(t, Xt)(cid:105)qt
∇bEsde(t) = Σ−1(cid:110)−(cid:104)fθ(t, Xt)(cid:105)qt

(cid:111)
fθ(t, Xt) (Xt − m(t))(cid:62)(cid:69)
(cid:68)

+ A(t)
− A(t)m(t) + b(t)

(cid:111)

,

where (cid:104)∇xfθ(t, Xt)(cid:105)qt

S(t) =

S(t) − ∇bEsde(t)m(cid:62)(t),

(28)

(29)

is invoked in order to obtain (28).

qt

References
[1] F. J. Alexander, G. L. Eyink, and J. M. Restrepo. Accelerated Monte Carlo for optimal estimation of time

series. Journal of Statistical Physics, 119:1331–1345, 2005.

[2] J. D. Annan, J. C. Hargreaves, N. R. Edwards, and R. Marsh. Parameter estimation in an intermediate

complexity earth system model using an ensemble Kalman ﬁlter. Ocean Modelling, 8:135–154, 2005.

[3] A. Apte, M. Hairer, A. Stuart, and J. Voss. Sampling the posterior: An approach to non-Gaussian data

assimilation. Physica D, 230:50–64, 2007.

[4] C. Archambeau, D. Cornford, M. Opper, and J. Shawe-Taylor. Gaussian process approximation of
stochastic differential equations. Journal of Machine Learning Research: Workshop and Conference
Proceedings, 1:1–16, 2007.

[5] D. Barber. Expectation correction for smoothed inference in switching linear dynamical systems. Journal

of Machine Learning Research, 7:2515–2540, 2006.

[6] A. Beskos, O. Papaspiliopoulos, G. Roberts, and P. Fearnhead. Exact and computationally efﬁcient
likelihood-based estimation for discretely observed diffusion processes (with discussion). Journal of
the Royal Statistical Society B, 68(3):333–382, 2006.

[7] Christopher M. Bishop. Pattern Recognition and Machine Learning. Springer, New York, 2006.
[8] D. Crisan and T. Lyons. A particle approximation of the solution of the Kushner-Stratonovitch equation.

Probability Theory and Related Fields, 115(4):549–578, 1999.

[9] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via EM

algorithm. Journal of the Royal Statistical Society B, 39(1):1–38, 1977.

[10] G. L. Eyink, J. L. Restrepo, and F. J. Alexander. A mean ﬁeld approximation in data assimilation for

nonlinear dynamics. Physica D, 194:347–368, 2004.

[11] A. Golightly and D. J. Wilkinson. Bayesian inference for nonlinear multivariate diffusion models observed

with error. Computational Statistics and Data Analysis, 2007. Accepted.

[12] A. H. Jazwinski. Stochastic Processes and Filtering Theory. Academic Press, New York, 1970.
[13] Peter E. Kloeden and Eckhard Platen. Numerical Solution of Stochastic Differential Equations. Springer,

Berlin, 1999.

[14] H. Lappalainen and J. W. Miskin. Ensemble learning. In M. Girolami, editor, Advances in Independent

Component Analysis, pages 76–92. Springer-Verlag, 2000.

[15] R. N. Miller, M. Ghil, and F. Gauthiez. Advanced data assimilation in strongly nonlinear dynamical

systems. Journal of the Atmospheric Sciences, 51:1037–1056, 1994.

[16] Jorge Nocedal and Stephen J. Wright. Numerical Optimization. Springer, 2000.
[17] G. Roberts and O. Stramer. On inference for partially observed non-linear diffusion models using the

Metropolis-Hastings algorithm. Biometirka, 88:603–621, 2001.

8

"
1036,2007,Sparse Overcomplete Latent Variable Decomposition of Counts Data,"An important problem in many fields is the analysis of counts data to extract meaningful latent components. Methods like Probabilistic Latent Semantic Analysis (PLSA) and Latent Dirichlet Allocation (LDA) have been proposed for this purpose. However, they are limited in the number of components they can extract and also do not have a provision to control the expressiveness"" of the extracted components. In this paper, we present a learning formulation to address these limitations by employing the notion of sparsity. We start with the PLSA framework and use an entropic prior in a maximum a posteriori formulation to enforce sparsity. We show that this allows the extraction of overcomplete sets of latent components which better characterize the data. We present experimental evidence of the utility of such representations.""","Sparse Overcomplete Latent Variable Decomposition

of Counts Data

Madhusudana Shashanka

Mars, Incorporated
Hackettstown, NJ

Bhiksha Raj

Mitsubishi Electric Research Labs

Cambridge, MA

Paris Smaragdis
Adobe Systems
Newton, MA

shashanka@cns.bu.edu

bhiksha@merl.com

paris@adobe.com

Abstract

An important problem in many ﬁelds is the analysis of counts data to extract mean-
ingful latent components. Methods like Probabilistic Latent Semantic Analysis
(PLSA) and Latent Dirichlet Allocation (LDA) have been proposed for this pur-
pose. However, they are limited in the number of components they can extract and
lack an explicit provision to control the “expressiveness” of the extracted compo-
nents. In this paper, we present a learning formulation to address these limitations
by employing the notion of sparsity. We start with the PLSA framework and
use an entropic prior in a maximum a posteriori formulation to enforce sparsity.
We show that this allows the extraction of overcomplete sets of latent components
which better characterize the data. We present experimental evidence of the utility
of such representations.

1 Introduction

A frequently encountered problem in many ﬁelds is the analysis of histogram data to extract mean-
ingful latent factors from it. For text analysis where the data represent counts of word occurrences
from a collection of documents, popular techniques available include Probabilistic Latent Semantic
Analysis (PLSA; [6]) and Latent Dirichlet Allocation (LDA; [2]). These methods extract compo-
nents that can be interpreted as topics characterizing the corpus of documents. Although they are
primarily motivated by the analysis of text, these methods can be applied to analyze arbitrary count
data. For example, images can be interpreted as histograms of multiple draws of pixels, where each
draw corresponds to a “quantum of intensity”. PLSA allows us to express distributions that underlie
such count data as mixtures of latent components. Extensions to PLSA include methods that attempt
to model how these components co-occur (eg. LDA, Correlated Topic Model [1]).

One of the main limitations of these models is related to the number of components they can extract.
Realistically, it may be expected that the number of latent components in the process underlying
any dataset is unrestricted. However, the number of components that can be discovered by LDA
or PLSA is restricted by the cardinality of the data, e.g. by the vocabulary of the documents, or
the number of pixels of the image analyzed. Any analysis that attempts to ﬁnd an overcomplete
set of a larger number of components encounters the problem of indeterminacy and is liable to
result in meaningless or trivial solutions. The second limitation of the models is related to the
“expressiveness” of the extracted components i.e. the information content in them. Although the
methods aim to ﬁnd “meaningful” latent components, they do not actually provide any control over
the information content in the components.

In this paper, we present a learning formulation that addresses both these limitations by employing
the notion of sparsity. Sparse coding refers to a representational scheme where, of a set of compo-
nents that may be combined to compose data, only a small number are combined to represent any
particular instance of the data (although the speciﬁc set of components may change from instance to

1

instance). In our problem, this translates to permitting the generating process to have an unrestricted
number of latent components, but requiring that only a small number of them contribute to the com-
position of the histogram represented by any data instance. In other words, the latent components
must be learned such that the mixture weights with which they are combined to generate any data
have low entropy – a set with low entropy implies that only a few mixture weight terms are signiﬁ-
cant. This addresses both the limitations. Firstly, it largely eliminates the problem of indeterminacy
permitting us to learn an unrestricted number of latent components. Secondly, estimation of low
entropy mixture weights forces more information on to the latent components, thereby making them
more expressive.

The basic formulation we use to extract latent components is similar to PLSA. We use an entropic
prior to manipulate the entropy of the mixture weights. We formulate the problem in a maximum a
posteriori framework and derive inference algorithms. We use an artiﬁcial dataset to illustrate the
effects of sparsity on the model. We show through simulations that sparsity can lead to components
that are more representative of the true nature of the data compared to conventional maximum like-
lihood learning. We demonstrate through experiments on images that the latent components learned
in this manner are more informative enabling us to predict unobserved data. We also demonstrate
that they are more discriminative than those learned using regular maximum likelihood methods.
We then present conclusions and avenues for future work.

2 Latent Variable Decomposition

Consider an F × N count matrix V. We will consider each column of V to be the histogram of an
independent set of draws from an underlying multinomial distribution over F discrete values. Each
column of V thus represents counts in a unique data set. Vf n, the f th row entry of Vn, the nth
column of V, represents the count of f (or the f th discrete symbol that may be generated by the
multinomial) in the nth data set. For example, if the columns of V represent word count vectors
for a collection of documents, Vf n would be the count of the f th word of the vocabulary in the nth
document in the collection.

We model all data as having been generated by a process that is characterized by a set of latent
probability distributions that, although not directly observed, combine to compose the distribution
of any data set. We represent the probability of drawing f from the zth latent distribution by P (f |z),
where z is a latent variable. To generate any data set, the latent distributions P (f |z) are combined in
proportions that are speciﬁc to that set. Thus, each histogram (column) in V is the outcome of draws
from a distribution that is a column-speciﬁc composition of P (f |z). We can deﬁne the distribution
underlying the nth column of V as

Pn(f ) = X

P (f |z)Pn(z),

z

(1)

where Pn(f ) represents the probability of drawing f in the nth data set in V, and Pn(z) is the
mixing proportion signifying the contribution of P (f |z) towards Pn(f ).
Equation 1 is functionally identical to that used for Probabilistic Latent Semantic Analysis of text
data [6]1: if the columns Vn of V represent word count vectors for documents, P (f |z) represents
the zth latent topic in the documents. Analogous interpretations may be proposed for other types
of data as well. For example, if each column of V represents one of a collection of images (each
of which has been unraveled into a column vector), the P (f |z)’s would represent the latent “bases”
that compose all images in the collection. In maintaining this latter analogy, we will henceforth refer
to P (f |z) as the basis distributions for the process.
Geometrically, the normalized columns of V (obtained by scaling the entries of Vn to sum to 1.0),
¯Vn, which we refer to as data distributions, may be viewed as F -dimensional vectors that lie in an
(F − 1) simplex. The distributions Pn(f ) and basis distributions P (f |z) are also F -dimensional
vectors in the same simplex. The model expresses Pn(f ) as points within the convex hull formed
by the basis distributions P (f |z). The aim of the model is to determine P (f |z) such that the model

1PLSA actually represents the joint distribution of n and f as P (n, f ) = P (n) Pz P (f |z)P (z|n). How-
ever the maximum likelihood estimate of P (n) is simply the fraction of all observations from all data sets that
occurred in the nth data set and does not affect the estimation of P (f |z) and P (z|n).

2

(100)

(100)

2 Basis Vectors

(010)

 

3 Basis Vectors

(010)

 

 

(001)

Simplex Boundary
Data Points
Basis Vectors
Approximation

 

(001)

Simplex Boundary
Data Points
Basis Vectors
Convex Hull

Figure 1: Illustration of the latent variable model. Panels show 3-dimensional data distributions as
points within the Standard 2-Simplex given by {(001), (010), (100)}. The left panel shows a set of
2 Basis distributions (compact code) derived from the 400 data points. The right panel shows a set
of 3 Basis distributions (complete code). The model approximates data distributions as points lying
within the convex hull formed by the basis distributions. Also shown are two data points (marked
by + and ×) and their approximations by the model (respectively shown by ♦ and (cid:3)).

Pn(f ) for any data distribution ¯Vn approximates it closely. Since Pn(f ) is constrained to lie within
the simplex deﬁned by P (f |z), it can only model ¯Vn accurately if the latter also lies within the
hull. Any ¯Vn that lies outside the hull is modeled with error. Thus, the objective of the model
is to identify P (f |z) such that they form a convex hull surrounding the data distributions. This is
illustrated in Figure 1 for a synthetic data set of 400 3-dimensional data distributions.

2.1 Parameter Estimation

Given count matrix V, we estimate P (f |z) and Pn(z) to maximize the likelihood of V. This can be
done through iterations of equations derived using the Expectation Maximization (EM) algorithm:

Pn(z|f ) =

Pn(z)P (f |z)

Pz Pn(z)P (f |z)

,

and

P (f |z) = Pn Vf nPn(z|f )

Pf Pn Vf nPn(z|f )

,

Pn(z) = Pf Vf nPn(z|f )
Pz Pf Vf nPn(z|f )

(2)

(3)

Detailed derivation is shown in supplemental material. The EM algorithm guarantees that the above
multiplicative updates converge to a local optimum.

2.2 Latent Variable Model as Matrix Factorization

We can write the model given by equation (1) in matrix form as pn = Wgn, where pn is a column
vector indicating Pn(f ), gn is a column vector indicating Pn(z), and W is a matrix with the (f, z)-
th element corresponding to P (f |z). If we characterize V by R basis distributions, W is an F × R
matrix. Concatenating all column vectors pn and gn as matrices P and G respectively, one can
write the model as P = WG, where G is an R × N matrix. It is easy to show (as demonstrated in
the supplementary material) that the maximum likelihood estimator for P (f |z) and Pn(z) attempts
to minimize the Kullback-Leibler (KL) distance between the normalized data distribution Vn and
Pn(f ), weighted by the total count in Vn.
In other words, the model of Equation (1) actually
represents the decomposition

(4)
where D is an N × N diagonal matrix, whose nth diagonal element is the total number of counts
in Vn and H = GD. The astute reader might recognize the decomposition of equation (4) as Non-
negative matrix factorization (NMF; [8]). In fact equations (2) and (3) can be shown to be equivalent
to one of the standard update rules for NMF.

V ≈ WGD = WH

Representing the decomposition in matrix form immediately reveals one of the shortcomings of the
basic model. If R, the number of basis distributions, is equal to F , then a trivial solution exists
that achieves perfect decomposition: W = I; H = V, where I is the identity matrix (although the
algorithm may not always arrive at this solution). However, this solution is no longer of any utility
to us since our aim is to derive basis distributions that are characteristic of the data, whereas the

3

Enclosing triangles for ’+’:
ABG, ABD, ABE, ACG,
ACD, ACE, ACF

(010)

C

(100)

A

B

F

G

DE
(001)

Simplex Boundary
Data Points
Basis Vectors

Figure 2: Illustration of the effect of sparsifying H on the dataset shown in Figure 1. A-G represent
7 basis distributions. The ‘+’ represents a typical data point. It can be accurately represented by
any set of three or more bases that form an enclosing polygon and there are many such polygons.
However, if we restrict the number of bases used to enclose ‘+’ to be minimized, only the 7 enclosing
triangles shown remain as valid solutions. By further imposing the restriction that the entropy of
the mixture weights with which the bases (corners) must be combined to represent ‘+’ must be
minimum, only one triangle is obtained as the unique optimal enclosure.

columns of W in this trivial solution are not speciﬁc to any data, but represent the dimensions of
the space the data lie in. For overcomplete decompositions where R > F , the solution becomes
indeterminate – multiple perfect decompositions are possible.

The indeterminacy of the overcomplete decomposition can, however, be greatly reduced by im-
posing a restriction that the approximation for any ¯Vn must employ minimum number of basis
distributions required. By further imposing the constraint that the entropy of gn must be minimized,
the indeterminacy of the solution can often be eliminated as illustrated by Figure 2. This principle,
which is related to the concept of sparse coding [5], is what we will use to derive overcomplete sets
of basis distributions for the data.

3 Sparsity in the Latent Variable Model

Sparse coding refers to a representational scheme where, of a set of components that may be com-
bined to compose data, only a small number are combined to represent any particular input. In the
context of basis decompositions, the goal of sparse coding is to ﬁnd a set of bases for any data set
such that the mixture weights with which the bases are combined to compose any data are sparse.
Different metrics have been used to quantify the sparsity of the mixture weights in the literature.
Some approaches minimize variants of the Lp norm of the mixture weights (eg. [7]) while other
approaches minimize various approximations of the entropy of the mixture weights.

In our approach, we use entropy as a measure of sparsity. We use the entropic prior, which has
been used in the maximum entropy literature (see [9]) to manipulate entropy. Given a probability
distribution θ, the entropic prior is deﬁned as Pe(θ) ∝ e−αH(θ), where H(θ) = − Pi θi log θi is the
entropy of the distribution and α is a weighting factor. Positive values of α favor distributions with
lower entropies while negative values of α favor distributions with higher entropies. Imposing this
prior during maximum a posteriori estimation is a way to manipulate the entropy of the distribution.
The distribution θ could correspond to the basis distributions P (f |z) or the mixture weights Pn(z)
or both. A sparse code would correspond to having the entropic prior on Pn(z) with a positive
value for α. Below, we consider the case where both the basis vectors and mixture weights have the
entropic prior to keep the exposition general.

3.1 Parameter Estimation

We use the EM algorithm to derive the update equations. Let us examine the case where both
P (f |z) and Pn(z) have the entropic prior. The set of parameters to be estimated is given by Λ =
{P (f |z), Pn(z)}. The a priori distribution over the parameters, P (Λ), corresponds to the entropic
priors. We can write log P (Λ), the log-prior, as

α X

X

P (f |z) log P (f |z) + β X

X

Pn(z) log Pn(z),

(5)

z

f

n

z

4

(100)

(100)

(100)

3 Basis Vectors

(010)

7 Basis Vectors

(010)

10 Basis Vectors

(010)

(001)

(001)

(001)

(100)

(100)

(100)

7 Basis Vectors

(010)

7 Basis Vectors

(010)

7 Basis Vectors

(010)

Sparsity Param = 0.01

Sparsity Param = 0.05

Sparsity Param = 0.3

(001)

(001)

(001)

Figure 3: Illustration of the effect of sparsity on the synthetic data set from Figure 1. For visual
clarity, we do not display the data points.
Top panels: Decomposition without sparsity. Sets of 3 (left), 7 (center), and 10 (right) basis dis-
tributions were obtained from the data without employing sparsity. In each case, 20 runs of the
estimation algorithm were performed from different initial values. The convex hulls formed by the
bases from each of these runs are shown in the panels from left to right. Notice that increasing the
number of bases enlarges the sizes of convex hulls, none of which characterize the distribution of
the data well.
Bottom panels: Decomposition with sparsity. The panels from left to right show the 20 sets of
estimates of 7 basis distributions, for increasing values of the sparsity parameter for the mixture
weights. The convex hulls quickly shrink to compactly enclose the distribution of the data.

where α and β are parameters indicating the degree of sparsity desired in P (f |z) and Pn(z) respec-
tively. As before, we can write the E-step as

The M-step reduces to the equations

Pn(z|f ) =

Pn(z)P (f |z)

Pz Pn(z)P (f |z)

.

ξ

P (f |z)

+ α + α log P (f |z) + ρz = 0,

ω

Pn(z)

+ β + β log Pn(z) + τn = 0

(6)

(7)

where we have let ξ represent Pn Vf nPn(z|f ), ω represent Pf Vf nPn(z|f ), and ρz, τn are La-
grange multipliers. The above M-step equations are systems of simultaneous transcendental equa-
tions for P (f |z) and Pn(z). Brand [3] proposes a method to solve such equations using the Lambert
W function [4]. It can be shown that P (f |z) and Pn(z) can be estimated as

ˆP (f |z) =

−ξ/α

W(−ξe1+ρz /α/α)

,

ˆPn(z) =

−ω/β

W(−ωe1+τn/β/β)

.

(8)

Equations (7), (8) form a set of ﬁxed-point iterations that typically converge in 2-5 iterations [3].

The ﬁnal update equations are given by equation (6), and the ﬁxed-point equation-pairs (7), (8). De-
tails of the derivation are provided in supplemental material. Notice that the above equations reduce
to the maximum likelihood updates of equations (2) and (3) when α and β are set to zero. More
generally, the EM algorithm aims to minimize the KL distance between the true distribution of the
data and that of the model, i.e. it attempts to arrive at a model that conserves the entropy of the data,
subject to the a priori constraints. Consequently, reducing entropy of the mixture weights Pn(z) to
obtain a sparse code results in increased entropy (information) of basis distributions P (f |z).

3.2 Illustration of the Effect of Sparsity

The effect and utility of sparse overcomplete representations is demonstrated by Figure 3. In this
example, the data (from Figure 1) have four distinct quadrilaterally located clusters. This structure
cannot be accurately represented by three or fewer basis distributions, since they can, at best specify

5

A. Occluded Faces

B. Reconstructions

C. Original Test Images

Figure 4: Application of latent variable decomposition for reconstructing faces from occluded im-
ages (CBCL Database). (A). Example of a random subset of 36 occluded test images. Four 6 × 6
patches were removed from the images in several randomly chosen conﬁgurations (corresponding
to the rows). (B). Reconstructed faces from a sparse-overcomplete basis set of 1000 learned compo-
nents (sparsity parameter = 0.1). (C). Original test images shown for comparison.

a triangular simplex, as demonstrated by the top left panel in the ﬁgure. Simply increasing the num-
ber of bases without constraining the sparsity of the mixture weights does not provide meaningful
solutions. However, increasing the sparsity quickly results in solutions that accurately characterize
the distribution of the data.

A clearer intuition is obtained when we consider the matrix form of the decomposition in Equation
4. The goal of the decomposition is often to identify a set of latent distributions that characterize
the underlying process that generated the data V. When no sparsity is enforced on the solution, the
trivial solution W = I, H = V is obtained at R = F . In this solution, the entire information in
V is borne by H and the bases W becomes uninformative, i.e. they no longer contain information
about the underlying process.

However, by enforcing sparsity on H the information V is transferred back to W, and non-trivial
solutions are possible for R > F . As R increases, however, W become more and more data-like.
At R = N another trivial solution is obtained: W = V, and H = D (i.e. G = I). The columns of
W now simply represent (scaled versions) of the speciﬁc data V rather than the underlying process.
For R > N the solutions will now become indeterminate. By enforcing sparsity, we have thus
increased the implicit limit on the number of bases that can be estimated without indeterminacy
from the smaller dimension of V to the larger one.

4 Experimental Evaluation

We hypothesize that if the learned basis distribution are characteristic of the process that generates
the data, they must not only generalize to explain new data from the process, but also enable predic-
tion of components of the data that were not observed. Secondly, the bases for a given process must
be worse at explaining data that have been generated by any other process. We test both these hy-
potheses below. In both experiments we utilize images, which we interpret as histograms of repeated
draws of pixels, where each draw corresponds to a quantum of intensity.

4.1 Face Reconstruction

In this experiment we evaluate the ability of the overcomplete bases to explain new data and predict
the values of unobserved components of the data. Speciﬁcally, we use it to reconstruct occluded
portions of images. We used the CBCL database consisting of 2429 frontal view face images hand-
aligned in a 19 × 19 grid. We preprocessed the images by linearly scaling the grayscale intensities
so that pixel mean and standard deviation was 0.25, and then clipped them to the range [0, 1].
2000 images were randomly chosen as the training set. 100 images from the remaining 429 were
randomly chosen as the test set. To create occluded test images, we removed 6 × 6 grids in ten
random conﬁgurations for 10 test faces each, resulting in 100 occluded images. We created 4 sets of
test images, where each set had one, two, three or four 6 × 6 patches removed. Figure 4A represents
the case where 4 patches were removed from each face.

In a training stage, we learned sets of K ∈ {50, 200, 500, 750, 1000} basis distributions from the
training data. Sparsity was not used in the compact (R < F ) case (50 and 200 bases) and sparsity

6

Basis Vectors

Mixture Weights

Basis Vectors

Mixture Weights

Pixel Image

Pixel Image

Figure 5: 25 Basis distributions (represented as images) extracted for class “2” from training data
without sparsity on mixture weights (Left Panel, sparsity parameter = 0) and with sparsity on mixture
weights (Right Panel, sparsity parameter = 0.2). Basis images combine in proportion to the mixture
weights shown to result in the pixel images shown.

β = 0

β = 0.2

β = 0.5

Figure 6: 25 basis distributions learned from training data for class “3” with increasing sparsity
parameters on the mixture weights. The sparsity parameter was set to 0, 0.2 and 0.5 respectively. In-
creasing the sparsity parameter of mixture weights produces bases which are holistic representations
of the input (histogram) data instead of parts-like features.

was imposed (parameter = 0.1) on the mixture weights in the overcomplete cases (500, 750 and 1000
basis vectors).

The procedure for estimating the occluded regions of the a test image has two steps. In the ﬁrst step,
we estimate the distribution underlying the image as a linear combination of the basis distributions.
This is done by iterations of Equations 2 and 3 to estimate Pn(z) (the bases P (f |z), being already
known, stay ﬁxed) based only on the pixels that are observed (i.e. we marginalize out the occluded
pixels). The combination of the bases P (f |z) and the estimated Pn(z) give us the overall distribution
Pn(f ) for the image. The occluded pixel values at any pixel f is estimated as the expected number
of counts at the pixels, given by Pn(f )(Pf ′∈{Fo} Vf ′ )/(Pf ′∈{Fo} Pn(f ′)) where Vf represents
the value of the image at the f th pixel and {Fo} is the set of observed pixels. Figure 4B shows the
reconstructed faces for the sparse-overcomplete case of 1000 basis vectors. Figure 7A summarizes
the results for all cases. Performance is measured by mean Signal-to-Noise-Ratio (SNR), where
SNR for an image was computed as the ratio of the sum of squared pixel intensities of the original
image to the sum of squared error between the original image pixels and the reconstruction.

4.2 Handwritten Digit Classiﬁcation

In this experiment we evaluate the speciﬁcity of the bases to the process represented by the training
data set, through a simple example of handwritten digit classiﬁcation. We used the USPS Handwrit-
ten Digits database which has 1100 examples for each digit class. We randomly chose 100 examples
from each class and separated them as the test set. The remaining examples were used for training.
During training, separate sets of basis distributions P k(f |z) were learned for each class, where k
represents the index of the class. Figure 5 shows 25 bases images extracted for the digit “2”. To
classify any test image v, we attempted to compute the distribution underlying the image using the
bases for each class (by estimating the mixture weights P k
v (z), keeping the bases ﬁxed, as before).
The “match” of the bases to the test instance was indicated by the likelihood Lk of the image com-
puted using P k(f ) = Pz P k(f |z)P k
v (z) as Lk = Pf vf log P k(f ). Since we expect the bases for
the true class of the image to best compose it, we expect the likelihood for the correct class to be
maximum. Hence, the image v was assigned to the class for which likelihood was the highest.

7

A. Reconstruction Experiment

 

1 patch
2 patches
3 patches
4 patches

24

22

20

18

16

14

12

10

 

R
N
S
n
a
e
M

8

 

50

200
750
Number of Basis Components

500

1000

r
o
r
r

 

E
e
g
a

t

n
e
c
r
e
P

5

4.5

4

3.5

3

2.5

 

2
0

B. Classification Experiment

 

25
50
75
100
200

0.05

0.1

Sparsity Parameter

0.2

0.3

Figure 7: (A). Results of the face Reconstruction experiment. Mean SNR of the reconstructions is
shown as a function of the number of basis vectors and the test case (number of deleted patches,
shown in the legend). Notice that the sparse-overcomplete codes consistently perform better than
the compact codes.
(B). Results of the classiﬁcation experiment. The legend shows number of
basis distributions used. Notice that imposing sparsity almost always leads to better classiﬁcation
performance. In the case of 100 bases, error rate comes down by almost 50% when a sparsity
parameter of 0.3 is imposed.

Results are shown in Figure 7B. As one can see, imposing sparsity improves classiﬁcation perfor-
mance in almost all cases. Figure 6 shows three sets of basis distributions learned for class “3” with
different sparsity values on the mixture weights. As the sparsity parameter is increased, bases tend
to be holistic representations of the input histograms. This is consistent with improved classiﬁcation
performance - as the representation of basis distributions gets more holistic, the more unlike they
become when compared to bases of other classes. Thus, there is a lesser chance that the bases of
one class can compose an image in another class, thereby improving performance.

5 Conclusions

In this paper, we have presented an algorithm for sparse extraction of overcomplete sets of latent
distributions from histogram data. We have used entropy as a measure of sparsity and employed
the entropic prior to manipulate the entropy of the estimated parameters. We showed that sparse-
overcomplete components can lead to an improved characterization of data and can be used in appli-
cations such as classiﬁcation and inference of missing data. We believe further improved characteri-
zation may be achieved by the imposition of additional priors that represent known or hypothesized
structure in the data, and will be the focus of future research.

References

[1] DM Blei and JD Lafferty. Correlated Topic Models. In NIPS, 2006.
[2] DM Blei, AY Ng, and MI Jordan. Latent Dirichlet Allocation. Journal of Machine Learning

Research, 3:993–1022, 2003.

[3] ME Brand. Pattern Discovery via Entropy Minimization. In Uncertainty 99: AISTATS 99, 1999.
[4] RM Corless, GH Gonnet, DEG Hare, DJ Jeffrey, and DE Knuth. On the Lambert W Function.

Advances in Computational mathematics, 1996.

[5] DJ Field. What is the Goal of Sensory Coding? Neural Computation, 1994.
[6] T Hofmann. Unsupervised Learning by Probabilistic Latent Semantic Analysis. Machine Learn-

ing, 42:177–196, 2001.

[7] PO Hoyer. Non-negative Matrix Factorization with Sparseness Constraints. Journal of Machine

Learning Research, 5, 2004.

[8] DD Lee and HS Seung. Algorithms for Non-negative Matrix Factorization. In NIPS, 2001.
[9] J Skilling. Classic Maximum Entropy. In J Skilling, editor, Maximum Entropy and Bayesian

Methods. Kluwer Academic, 1989.

8

"
665,2007,Modelling motion primitives and their timing in biologically executed movements,"Biological movement is built up of sub-blocks or motion primitives. Such primitives provide a compact representation of movement which is also desirable in robotic control applications. We analyse handwriting data to gain a better understanding of use of primitives and their timings in biological movements. Inference of the shape and the timing of primitives can be done using a factorial HMM based model, allowing the handwriting to be represented in primitive timing space. This representation provides a distribution of spikes corresponding to the primitive activations, which can also be modelled using HMM architectures. We show how the coupling of the low level primitive model, and the higher level timing model during inference can produce good reconstructions of handwriting, with shared primitives for all characters modelled. This coupled model also captures the variance profile of the dataset which is accounted for by spike timing jitter. The timing code provides a compact representation of the movement while generating a movement without an explicit timing model produces a scribbling style of output.","Modelling motion primitives and their timing

in biologically executed movements

Ben H Williams
School of Informatics

University of Edinburgh

5 Forrest Hill, EH1 2QL, UK

ben.williams@ed.ac.uk

Marc Toussaint

TU Berlin

Franklinstr. 28/29, FR 6-9

10587 Berlin, Germany

mtoussai@cs.tu-berlin.de

Amos J Storkey

School of Informatics

University of Edinburgh

5 Forrest Hill, EH1 2QL, UK

a.storkey@ed.ac.uk

Abstract

Biological movement is built up of sub-blocks or motion primitives. Such
primitives provide a compact representation of movement which is also
desirable in robotic control applications. We analyse handwriting data to
gain a better understanding of primitives and their timings in biological
movements.
Inference of the shape and the timing of primitives can be
done using a factorial HMM based model, allowing the handwriting to
be represented in primitive timing space. This representation provides a
distribution of spikes corresponding to the primitive activations, which can
also be modelled using HMM architectures. We show how the coupling of
the low level primitive model, and the higher level timing model during
inference can produce good reconstructions of handwriting, with shared
primitives for all characters modelled. This coupled model also captures
the variance proﬁle of the dataset which is accounted for by spike timing
jitter. The timing code provides a compact representation of the movement
while generating a movement without an explicit timing model produces a
scribbling style of output.

1 Introduction

Movement planning and control is a very diﬃcult problem in real-world applications. Cur-
rent robots have very good sensors and actuators, allowing accurate movement execution,
however the ability to organise complex sequences of movement is still far superior in bi-
ological organisms, despite being encumbered with noisy sensory feedback, and requiring
control of many non-linear and variable muscles. The underlying question is that of the
representation used to generate biological movement. There is much evidence to suggest
that biological movement generation is based upon motor primitives, with discrete muscle
synergies found in frog spines, (Bizzi et al., 1995; d’Avella & Bizzi, 2005; d’Avella et al.,
2003; Bizzi et al., 2002), evidence of primitives being locally ﬁxed (Kargo & Giszter, 2000),
and modularity in human motor learning and adaption (Wolpert et al., 2001; Wolpert &
Kawato, 1998). Compact forms of representation for any biologically produced data should
therefore also be based upon primitive sub-blocks.

1

(A)

(B)

Figure 1: (A) A factorial HMM of a handwriting trajectory Yt. The parameters ¯λm
indicate
the probability of triggering a primitive in the mth factor at time t and are learnt for one speciﬁc
character. (B) A hierarchical generative model of handwriting where the random variable c indicates
the currently written character and deﬁnes a distribution over random variables λm
t via a Markov
model over Gm.

t

There are several approaches to use this idea of motion primitives for more eﬃcient robotic
movement control. (Ijspeert et al., 2003; Schaal et al., 2004) use non-linear attractor dy-
namics as a motion primitive and train them to generate motion that solves a speciﬁc task.
(Amit & Matari´c, 2002) use a single attractor system and generate non-linear motion by
modulating the attractor point. These approaches deﬁne a primitive as a segment of move-
ment rather than understanding movement as a superposition of concurrent primitives. The
goal of analysing and better understanding biological data is to extract a generative model of
complex movement based on concurrent primitives which may serve as an eﬃcient represen-
tation for robotic movement control. This is in contrast to previous studies of handwriting
which usually focus on the problem of character classiﬁcation rather than generation (Singer
& Tishby, 1994; Hinton & Nair, 2005).

We investigate handwriting data and analyse whether it can be modelled as a superposition
of sparsely activated motion primitives. The approach we take can intuitively be compared
to a Piano Model (also called Piano roll model (Cemgil et al., 2006)). Just as piano music
can (approximately) be modelled as a superposition of the sounds emitted by each key we
follow the idea that biological movement is a superposition of pre-learnt motion primitives.
This implies that the whole movement can be compactly represented by the timing of each
primitive in analogy to a score of music. We formulate a probabilistic generative model that
reﬂects these assumptions. On the lower level a factorial Hidden Markov Model (fHMM,
Ghahramani & Jordan, 1997) is used to model the output as a combination of signals emitted
from independent primitives (each primitives corresponds to a factor in the fHMM). On the
higher level we formulate a model for the primitive timing dependent upon character class.
The same motion primitives are shared across characters, only their timings diﬀer. We train
this model on handwriting data using an EM-algorithm and thereby infer the primitives and
the primitive timings inherent in this data. We ﬁnd that the inferred timing posterior for a
speciﬁc character is indeed a compact representation for the speciﬁc character which allows
for a good reproduction of this character using the learnt primitives. Further, using the
timing model learnt on the higher level we can generate new movement – new samples of
characters (in the same writing style as the data), and also scribblings that exhibit local
similarity to written characters when the higher level timing control is omitted.

Section 2 will introduce the probabilistic generative model we propose. Section 3 brieﬂy
describes the learning procedures which are variants of the EM-algorithm adapted to our
model. Finally in section 4 we present results on handwriting data recorded with a digi-
tisation tablet, show the primitives and timing code we extract, and demonstrate how the
learnt model can be used to generate new samples of characters.

2

2 Model

Our analysis of primitives and primitive timings in handwriting is based on formulating a
corresponding probabilistic generative model. This model can be described on two levels.
On the lower level (Figure 1(A)) we consider a factorial Hidden Markov Model (fHMM)
where each factor produces the signal of a single primitive and the linear combination of
factors generates the observed movement Yt. This level is introduced in the next section
and was already considered in (Williams et al., 2006; Williams et al., 2007). It allows the
learning and identiﬁcation of primitives in the data but does not include a model of their
timing. In this paper we introduce the full generative model (Figure 1(B)) which includes
a generative model for the primitive timing conditioned on the current character.

2.1 Modelling primitives in data

Let M be the number of primitives we allow for. We describe a primitive as a strongly
constrained Markov process which remains in a zero state most of the time but with some
probability ¯λ ∈ [0, 1] enters the 1 state and then rigorously runs through all states 2, .., K
before it enters the zero state again. While running though its states, this process emits a
ﬁxed temporal signal. More rigorously, we have a fHMM composed of M factors. The state
of the mth factor at time t is Sm

t ∈ {0, .., Km}, and the transition probabilities are

P (Sm

t = b | Sm

t−1 = a, ¯λm

¯λm

t

1 − ¯λm

t

1
0

for a = 0 and b = 1
for a = 0 and b = 0
for a 6= 0 and b = (a + 1) mod Km
otherwise

.

(1)

t ) = 


This process is parameterised by the onset probability ¯λm
t of the mth primitive at time t.
The M factors emit signals which are combined to produce the observed motion trajectory
Yt according to

P (Yt | S 1:M

t

) = N (Yt,

M

Xm=1

W m
Sm

t

, C) ,

(2)

where N (x, a, A) is the Gaussian density function over x with mean a and covariance matrix
A. This emission is parameterised by W m
0 = 0 (the zero state
does not contribute to the observed signal), and C is a stationary output covariance.

s which is constrained to W m

1:Km = (W m

The vector W m
Km ) is what we call a primitive and – to stay in the analogy
– can be compared to the sound of a piano key. The parameters ¯λm
t ∈ [0, 1] could be
compared to the score of the music. We will describe below how we learn the primitives
W m

s and also adapt the primitive lengths Km using an EM-algorithm.

1 , .., W m

2.2 A timing model

Considering the ¯λ’s to be ﬁxed parameters is not a suitable model of biological movement.
The usage and timing of primitives depends on the character that is written and the timing
varies from character to character. Also, the ¯λ’s actually provide a rather high-dimensional
representation for the movement. Our model takes a diﬀerent approach to parameterise
the primitive activations. For instance, if a primitive is activated twice in the course of the
movement we assume that there have been two signals (“spikes”) emitted from a higher
level process which encode the activation times. More formally, let c be a discrete random
variable indicating the character to be written, see Figure 1(B). We assume that for each
primitive we have another Markovian process which generates a length-L sequence of states
Gm

l ∈ {1, .., R, 0},

P (Gm

1:L | c) = P (Gm

1 | c)

L

Yl=2

P (Gm

l

| Gm

l−1, c) .

(3)

The states Gm
Figure 2(b). We now deﬁne λm

encode which primitives are activated and how they are timed, as seen in
t to be a binary random variable that indicate the activation

l

3

r
e
b
m
u
n

 

l

e
p
m
a
s
 

i

g
n
n
a
r
T

i

350

300

250

200

150

100

50

←1

0
0  

0.1

0.2 

(a)

0.6

0.7

←3
0.8

0.3

←2
0.4

0.5
Time /ms
(b)

Figure 2: (a) Illustration of equation (4): The Markov process on the states Gm
l emits Gaussian
components to the onset probabilities P (λm
t = 1). (b) Scatter plot of the MAP onsets of a single
primitive for diﬀerent samples of the same character ‘p’. Gaussian components can be ﬁt to each
cluster.

of a primitive at time t, which we call a “spike”. For a zero-state Gm
emitted and thus the probability of λm = 1 is not increased. A non-zero state Gm
a Gaussian component to the probabilities of λm
µm

l = 0 no spike is
l = r adds
t = 1 centred around a typical spike time

r and with variance σm
r ,

P (λm

t = 1 | Gm

1:Km , c) =

L

Xl=1

δGm

l

>0Z t+0.5

t−0.5

N (t, µm
Gm

l

, σm
Gm

l

) dt .

(4)

l

>0 is zero for Gm

Here, δGm
l = 0 and 1 otherwise, and the integral essentially discretises the
Gaussian density. Additionally, we restrict the Markovian process such that each Gaussian
component can emit at most one spike, i.e., we constrain P (Gm
l−1, c) to be a lower
triangular matrix. Given the λ’s, the state transitions in the fHMM factors are as in equation
(1), replacing ¯λ by λ.
To summarise, the spike probabilities of λm
t = 1 are a sum of at most L Gaussian components
centred around the means µm
l . Whether or not such a Gaussian
component is present is itself randomised and depends on the states Gm
l . We can observe at
most L spikes in one primitive, the spike times between diﬀerent primitives are dependent,
but we have a Markovian dependency between the presence and timing of spikes within a
primitive. The whole process is parameterised by the initial state distribution P (Gm
1 | c),
the transition probabilities P (Gm
r and the variances σm
r . All
these parameters will be learnt using an EM-algorithm.

l−1, c), the spike means µm

l and with variances σm

| Gm

l

| Gm

l

This timing model is motivated from results with the fHMM-only model: When training
the fHMM on data of a single character and then computing the MAP spike times using
a Viterbi alignment for each data sample we ﬁnd that the MAP spike times are roughly
Gaussian distributed around a number of means (see Figure 2(b)). This is why we used a
sum of Gaussian components to deﬁne the onset probabilities P (λ = 1). However, the data
is more complicated than provided for by a simple Mixture of Gaussians. Not every sample
includes an activation for each cluster (which is a source of variation in the handwriting)
and there cannot be more than one spike in each cluster. Therefore we introduced the
constrained Markov process on the states Gm
l which may skip the emission of some spikes.

3 Inference and learning

In the experiments we will compare both the fHMM without the timing model (Figure 1(A))
and the full model including the timing model (Figure 1(B)).

In the fHMM-only model, inference in the fHMM is done using variational inference as
described in (Ghahramani & Jordan, 1997). Using a standard EM-algorithm we can train
the parameters W , C and ¯λ. To prevent overﬁtting we assume the spike probabilities

4

r
e
b
m
u
n
 
e
v
i
t
i

m

i
r

P

10

9

8

7

6

5

4

3

2

1

2

0

−2

−4

−6

−8

m
m

/
 
e
c
n
a
t
s
D

i

−10

−12

−14

←6

←3
←9

←6←7

←8
←1

←2
←4

←10
←9

←2

←8

←5

m
m

/
 

e
c
n
a
t
s
D

i

0.25  0.5 0.75    1

Time /s

(a)

−4 −2

2
Distance /mm

0

4

0
−5
−10
0
−5
−10

−4.5 −1

0.5

0

 −0.5

    0
−0.25
 −0.5
−0.75

0 0.20.4

 −0.5

0

    0
−0.25
 −0.5

  5

2.5

  0

−0.25    0

0 1 2 3

  0.5

 0.25

    0

−0.25

 0.25
    0
−0.25
 −0.5

0 0.2 0.4

−0.25

 −0.5

    0

−0.25

 −0.5

−0.75

0 0.20.4

−0.2 0

−0.2 0

Distance /mm
(b)

   0
−0.1
−0.2
−0.3

−0.2−0.1    0

m
m

/
 

e
c
n
a

t
s
D

i

    0

 −2.5

   −5

 −7.5

  −10

−12.5

  −15

−17.5

←7
←2
←5

←1
←8

←9
←1←2
←8

←7
←5
←4
←5

←3
←4

←2

←7
←1

←7

←3
←8
←1←2
←3

←7
←1
←5

←8

←4
←3

←4
←3

←3

←7

←4

←7

←5

←7
←4
←4

←8
←1

←3
←4
←2
←8←9
←4
←3

←7

←8

←5

←5

←4

←8
←3

←8
←3

  −5

←7
−2.5

   5

   0

 2.5
Distance /mm
(c)

Figure 3: (a) Reconstruction of a character from a training dataset, using a subset of the primitives.
The thickness of the reconstruction represents the pressure of the pen tip, and the diﬀerent colours
represent the activity of the diﬀerent primitives, the onsets of which are labelled with an arrow.
The posterior probability of primitive onset is shown on the left, highlighting why a spike timing
representation is appropriate. (b) Plots of the 10 extracted primitives, as drawn on paper. (c)
Generative samples using a ﬂat primitive onset prior, showing scribbling behaviour of uncoupled
model.

are stationary (λm
primitive.

t constant over t) and learn only a single mean parameter ¯λm for each

In the full model, inference is an iterative process of inference in the timing model and
inference in the fHMM. Note that variational inference in the fHMM is itself an iterative
process which recomputes the posteriors over Sm
t after adapting the variational parameters.
We couple this iteration to inference in the timing model in both directions: In each iteration,
t deﬁnes observation likelihoods for inference in the Markov models Gm
the posterior over Sm
l .
Inversely, the resulting posterior over Gm
l to
λm
t ) which enter the fHMM inference in the next iteration. Standard M-steps are then used
to train all parameters of the fHMM and the timing model. In addition, we use heuristics to
adapt the length Km of each primitive: we increase or decrease Km depending on whether
the learnt primitive is signiﬁcantly diﬀerent to zero in the last time steps. The number of
parameters used in the model therefore varies during learning, as the size of W depends
upon Km, and the size of G depends upon the number of inferred spikes.

l deﬁnes a new prior over λ’s (a message from Gm

In the experiments we will also investigate the reconstruction of data. By this we mean
that we take a trained model, use inference to compute the MAP spikes λ for a speciﬁc
data sample, then we use these λ’s and the deﬁnition of our generative model (including the
learnt primitives W ) to generate a trajectory which can be compared to the original data
sample. Such a reconstruction can be computed using both the fHMM-only model and the
full model.

4 Results

4.1 Primitive and timing analysis using the fHMM-only

We ﬁrst consider a data set of 300 handwritten ‘p’s recorded using an INTUOS 3 WA-
COM digitisation tablet http://www.wacom.com/productinfo/9x12.cfm, providing trajec-
tory data at 200Hz. The trajectory Yt we model is the normalised ﬁrst diﬀerential of the
data, so that the data mean was close to zero, providing the requirements for the zero
state assumption in the model constraints. Three dimensional data was used, x-position,
y-position, and pressure. The data collected were separated into samples, or characters,
allowing each sample to be separately normalised.

Our choice of parameter was M = 10 primitives and we initialised all Km = 20 and con-
strained them to be smaller than 100 throughout learning.

We trained the fHMM-only model on this dataset. Figure 3(a) shows the reconstruction of a
speciﬁc sample of this data set and the corresponding posterior over λ’s. This clean posterior
is the motivation for introducing a model of the spike timings as a compact representation

5

m
m

/
 
e
c
n
a
t
s
D

i

  0

 −5

−10

−15

−20

−25

−30

−35

−40

−45

−50

←5
←3

←4

←4
←9
←3

←4

←10

←7
←10

←3
←1←2
←7
←4
←8
←2

←4
←7
←7
←10←1
←4
←1←2←3
←8
←9 ←9
←3
←1
←10
←7
←4
←1
←3
←7
←3
←4
←4
←6
←8
←1←2
←4
←4
←6
←10
←8
←6
←6
←7←8
←10
←8
←4
←3
←5
←2
←7←7
←9
←3
←5
←6
←10
←10
←9
←10
←9
←2
←10
←4
←7 ←7
←6 ←6
←3
←6
←3
←3
←10
←10
←6
←4
←2
←10
←6
←5
←2
←7
←5
←5
←7
←3
←4
←4
←10
←10
←9
←10
←9
←4
←3
←8
←3
←1←2
←1
←7
←3
←9←10←10
←4
←1←2
←6
←4
←8
←8←8
←4
←7
←7
←8
←3
←10
←2
←6
←5
←9
←4
←7
←10←10
←7
←6
←10
←5
←5
←4
←3
←7
←3
←6
←7
←8
←10
←3
←6
←4

←4
←9
←10
←6
←3
←4
←8
←1←2←3
←10
←8
←1←2
←6
←10
←3
←1←2←2←3
←4
←1←2
←4
←10
←4
←9
←3
←4
←6
←4
←4
←3
←8
←9
←7
←10
←8
←9
←5←7 ←7
←10
←5
←5
←8
←10
←7 ←7
←8
←10
←3
←9
←6 ←6
←5
←10 ←10
←3
←4
←6
←2
←7
←10
←6
←4
←7
←3
←9
←1←2
←4
←9
←1←2
←3
←8
←8 ←9
←4
←10
←10
←1←2←2
←3
←6
←3
←7
←7
←4
←8
←7
←2
←6
←3
←4
←2
←7
←3
←10 ←10
←10
←3
←5
←5
←6
←5
←3
←10
←6
←4
←7

←3
←10
←7
←7
←1←2
←4
←10
←3
←3
←3
←6
←4
←4
←2
←10
←10

←7 ←7
←3
←4
←8
←8
←10
←10
←1
←1←2
←9
←3
←9
←2
←10
←2
←4
←3
←4
←6
←8
←10
←2
←5

←5
←10
←4
←3
←6
←7

←5
←6
←7
←10
←3

←4
←9
←10

←4

←10

←6

←3
←4
←10
←8

←10

←10
←7
←4
←8
←9
−20

←10
←6
←9
←3
←4
←7
−10

  0

 10

Distance /mm
(a)

←10
←8←9
←1←2
←6
←6
←3
←9
←3
←10
←7
←6
←4
←10
←5
←3

←6
←8
←4
←2
←9
←3
←7

←10
←8

x 104

2

l

s
e
p
m
a
s
 
f

o

 
r
e
b
m
u
N

1.8

1.6

1.4

1.2

1

0.8

0.6

0.4

0.2

 

x position
y position
pressure

m
m

/
 

e
c
n
a

t
s
D

i

  0

−10

−20

−30

−40

−50

0

 

−800

−600

−200

−400
400
Velocity error /pixels /sec

200

0

(b)

600

800

−10

 30

 10

  0
 20
Distance /mm
(c)

Figure 4: (a) Reconstructions of ‘p’s using the full model. (b) Histogram of the reconstruction
error, which is 3-dimensional pen movement velocity space. These errors were produced using over
300 samples of a single character. (c) Generative samples using the full generative model (Figure
1(B)).

of the data. Equally the reconstruction (using the Viterbi aligned MAP spikes) shows the
suﬃciency of the spike code to generate the character. Figure 3(b) shows the primitives W m
(translated back into pen-space) that were learnt and implicitly used for the reconstruction
of the ‘p’. These primitives can be seen to represent typical parts of the ‘p’ character; the
arrows in the reconstruction indicate when they are activated.

The fHMM-only model can be used to reconstruct a speciﬁc data sample using the MAP λ’s
of that sample, but it can not ‘autonomously’ produce characters since it lacks a model of
the timing. To show the importance of this spike timing information, we can demonstrate
the eﬀects of removing it. When using the fHMM-only model as a generative model with
the learnt stationary spike probabilities ¯λm the result is a form of primitive babbling, as can
be seen in Figure 3(c). Since these scribblings are generated by random expression of the
learnt primitives they locally resemble parts of the ‘p’ character.

The primitives generalise to other characters if the training dataset contained suﬃcient
variation. Further investigation has shown that 20 primitives learnt from 12 character types
are suﬃciently generalised to represent all remaining novel character types without further
learning, by using a single E-step to ﬁt the pre-learnt parameters to a novel dataset.

4.2 Generating new characters using the full generative model

Next we trained the full model on the same ‘p’-dataset. Figure 4(a) shows the reconstruc-
tions of some samples of the data set. To the right we see the reconstruction errors in
velocity space showing at many time points a perfect reconstruction was attained. Since
the full model includes a timing model it can also be run autonomously as a generative
model for new character samples. Figure 4(c) displays such new samples of the character
‘p’ generated by the learnt model.

As a more challenging problem we collected a data set of over 450 character samples of
the letters a, b and c. The full model includes the written character class as a random
variable and can thus be trained on multi-character data sets. Note that we restrict the
total number of primitives to M = 10 which will require a sharing of primitives across
characters. Figure 5(a) shows samples of the training data set while Figure 5(b) shows
reconstructions of the same samples using the MAP λ’s in the full model. Generally, the
reconstructions using the full model are better than using the fHMM-only model. This can
be understood investigating the distribution of the MAP λ’s across diﬀerent samples under
the fHMM-only and the full model, see Figure 6. Coupling the timing and the primitive
model during learning has the eﬀect of trying to learn primitives from data that are usually in
the same place. Thus, using the full model the inferred spikes are more compactly clustered
at the Gaussian components due to the prior imposed from the timing model (the thick
black lines correspond to Equation (4)).

6

  0

−10

−20

−30

−40

−50

−60

−70

−80

−90

m
m

/
 

e
c
n
a

t
s
D

i

−10

  0  10  20  30  40
Distance /mm
(a)

   0

 −10

 −20

 −30

 −40

 −50

 −60

 −70

 −80

 −90

m
m

/
 

e
c
n
a

t
s
D

i

−100

−10   0  10  20  30  40
Distance /mm

(b)

m
m

/
 

e
c
n
a
t
s
D

i

0

−20

−40

−60

−80

 −100

−10   0  10  20  30  40
Distance /mm
(c)

Figure 5: (a) Training dataset, showing 3 character types, and variation. (b) Reconstruction of
dataset using 10 primitives learnt from the dataset in (a). (c) Generative samples using the full
generative model (Figure 1(B)).

m=5

m=4

m=3

m=2

x
e
d
n

i
 

l

e
p
m
a
S
−
e
v
i
t
i

 

 

m

i
r

P

m=1

  0

0.1

0.2

m=5

m=4

m=3

m=2

m=1

x
e
d
n

i
 

l

e
p
m
a
S
−
 
e
v
i
t
i

 

m

i
r

P

0.6

0.7

0.8

  0

0.1

0.2

0.3

0.4

0.5
Time /ms
(a)

0.3

0.4

0.5
Time /ms
(b)

0.6

0.7

0.8

Figure 6: (a) Scatter plot of primitive onset spikes for a single character type across all samples
and primitives, showing the clustering of certain primitives in particular parts of a character. The
horizontal bars separate the results for diﬀerent primitives. (b) Scatter plot of spikes from same
dataset, with a coupled model, showing suppression of outlying spikes and tightening of clusters.
The thick black lines displays the prior over λ’s imposed from the timing model via Equation (4).

Finally, we run the full model autonomously to generate new character samples, see Figure
5(c). Here the character class, c is ﬁrst sampled uniform randomly and then all learnt
parameters are used to eventually sample a trajectory Yt. The generative samples show
interesting variation while still being readably a character.

5 Conclusions

In this paper we have shown that it is possible to represent handwriting using a primitive
based model. The model consists of a superposition of several arbitrary ﬁxed functions.
These functions are time-extended, of variable length (during learning), and are superim-
posed with learnt oﬀsets. The timing of activations is crucial to the accurate reproduction of
the character. With a small amount of timing variation, a distorted version of the original
character is reproduced, whilst large (and coordinated) diﬀerences in the timing pattern
produce diﬀerent character types.

The spike code provides a compact representation of movement, unlike that which has pre-
viously been explored in the domain of robotic control. We have proposed to use Markov
processes conditioned on the character as a model for these spike emissions. Besides con-
tributing to a better understanding of biological movement, we hope that such models will
inspire applications also in robotic control, e.g., for movement optimisation based on spike
codings.

7

An assumption made in this work is that the primitives are learnt velocity proﬁles. We have
not included any feedback control systems in the primitive production, however the presence
of low-level feedback, such as in a spring system (Hinton & Nair, 2005) or dynamic motor
primitives (Ijspeert et al., 2003; Schaal et al., 2004), would be interesting to incorporate into
the model, and could perhaps be done by changing the outputs of the fHMM to parameterise
the spring systems rather than be Gaussian distributions of velocities.

We make no assumptions about how the primitives are learnt in biology.
It would be
interesting to study the evolution of the primitives during human learning of a new character
set. As humans become more conﬁdent at writing a character, the reproduction becomes
faster, and more repeatable. This could be related to a more accurate and eﬃcient use
of primitives already available. However, it might also be the case that new primitives
are learnt, or old ones adapted. More research needs to be done to examine these various
possibilities of how humans learn new motor skills.

Acknowledgements

Marc Toussaint was supported by the German Research Foundation (DFG), Emmy Noether fel-
lowship TO 409/1-3.

References

Amit, R., & Matari´c, M. (2002). Parametric primitives for motor representation and control. Proc.

of the Int. Conf. on Robotics and Automation (ICRA) (pp. 863–868).

Bizzi, E., d’Avella, A., Saltiel, P., & Trensch, M. (2002). Modular organization of spinal motor

systems. The Neuroscientist, 8, 437–442.

Bizzi, E., Giszter, S., Loeb, E., Mussa-Ivaldi, F., & Saltiel, P. (1995). Modular organization of

motor behavior in the frog’s spinal cord. Trends in Neurosciences, 18, 442–446.

Cemgil, A., Kappen, B., & Barber, D. (2006). A generative model for music transcription. IEEE

Transactions on Speech and Audio Processing, 14, 679–694.

d’Avella, A., & Bizzi, E. (2005). Shared and speciﬁc muscle synergies in natural motor behaviors.

PNAS, 102, 3076–3081.

d’Avella, A., Saltiel, P., & Bizzi, E. (2003). Combinations of muscle synergies in the construction

of a natural motor behavior. Nature Neuroscience, 6, 300–308.

Ghahramani, Z., & Jordan, M. (1997). Factorial hidden Markov models. Machine Learning, 29,

245–275.

Hinton, G. E., & Nair, V. (2005). Inferring motor programs from images of handwritten digits.

Advances in Neural Information Processing Systems 18 (NIPS 2005) (pp. 515–522).

Ijspeert, A. J., Nakanishi, J., & Schaal, S. (2003). Learning attractor landscapes for learning motor
primitives. Advances in Neural Information Processing Systems 15 (NIPS 2003) (pp. 1523–1530).
MIT Press, Cambridge.

Kargo, W., & Giszter, S. (2000). Rapid corrections of aimed movements by combination of force-

ﬁeld primitives. J. Neurosci., 20, 409–426.

Schaal, S., Peters, J., Nakanishi, J., & Ijspeert, A. (2004). Learning movement primitives.

ISRR2003.

Singer, Y., & Tishby, N. (1994). Dynamical encoding of cursive handwriting. Biol.Cybern., 71,

227–237.

Williams, B., M.Toussaint, & Storkey, A. (2006). Extracting motion primitives from natural hand-

writing data. Int. Conf. on Artiﬁcial Neural Networks (ICANN) (pp. 634–643).

Williams, B., M.Toussaint, & Storkey, A. (2007). A primitive based generative model to infer
timing information in unpartitioned handwriting data. Int. Jnt. Conf. on Artiﬁcial Intelligence
(IJCAI) (pp. 1119–1124).

Wolpert, D. M., Ghahramani, Z., & Flanagan, J. R. (2001). Perspectives and problems in motor

learning. TRENDS in Cog. Sci., 5, 487–494.

Wolpert, D. M., & Kawato, M. (1998). Multiple paired forward and inverse models for motor

control. Neural Networks, 11, 1317–1329.

8

"
614,2007,Efficient multiple hyperparameter learning for log-linear models,"Using multiple regularization hyperparameters is an effective method for managing model complexity in problems where input features have varying amounts of noise. While algorithms for choosing multiple hyperparameters are often used in neural networks and support vector machines, they are not common in structured prediction tasks, such as sequence labeling or parsing. In this paper, we consider the problem of learning regularization hyperparameters for log-linear models, a class of probabilistic models for structured prediction tasks which includes conditional random fields (CRFs). Using an implicit differentiation trick, we derive an efficient gradient-based method for learning Gaussian regularization priors with multiple hyperparameters. In both simulations and the real-world task of computational RNA secondary structure prediction, we find that multiple hyperparameter learning provides a significant boost in accuracy compared to models learned using only a single regularization hyperparameter.","Efﬁcient multiple hyperparameter

learning for log-linear models

Chuong B. Do

Chuan-Sheng Foo

Computer Science Department

Stanford University
Stanford, CA 94305

Andrew Y. Ng

{chuongdo,csfoo,ang}@cs.stanford.edu

Abstract

In problems where input features have varying amounts of noise, using distinct
regularization hyperparameters for different features provides an effective means
of managing model complexity. While regularizers for neural networks and sup-
port vector machines often rely on multiple hyperparameters, regularizers for
structured prediction models (used in tasks such as sequence labeling or pars-
ing) typically rely only on a single shared hyperparameter for all features. In this
paper, we consider the problem of choosing regularization hyperparameters for
log-linear models, a class of structured prediction probabilistic models which in-
cludes conditional random ﬁelds (CRFs). Using an implicit differentiation trick,
we derive an efﬁcient gradient-based method for learning Gaussian regularization
priors with multiple hyperparameters. In both simulations and the real-world task
of computational RNA secondary structure prediction, we ﬁnd that multiple hy-
perparameter learning can provide a signiﬁcant boost in accuracy compared to
using only a single regularization hyperparameter.

1 Introduction
In many supervised learning methods, overﬁtting is controlled through the use of regularization
penalties for limiting model complexity. The effectiveness of penalty-based regularization for a
given learning task depends not only on the type of regularization penalty used (e.g., L1 vs L2) [29]
but also (and perhaps even more importantly) on the choice of hyperparameters governing the regu-
larization penalty (e.g., the hyperparameter λ in an isotropic Gaussian parameter prior, λ||w||2).

When only a single hyperparameter must be tuned, cross-validation provides a simple yet reliable
procedure for hyperparameter selection. For example, the regularization hyperparameter C in a
support vector machine (SVM) is usually tuned by training the SVM with several different values
of C, and selecting the one that achieves the best performance on a holdout set. In many situations,
using multiple hyperparameters gives the distinct advantage of allowing models with features of
varying strength; for instance, in a natural language processing (NLP) task, features based on word
bigrams are typically noisier than those based on individual word occurrences, and hence should
be “more regularized” to prevent overﬁtting. Unfortunately, for sophisticated models with multiple
hyperparameters [23], the na¨ıve grid search strategy of directly trying out possible combinations of
hyperparameter settings quickly grows infeasible as the number of hyperparameters becomes large.
Scalable strategies for cross-validation–based hyperparameter learning that rely on computing
the gradient of cross-validation loss with respect to the desired hyperparameters arose ﬁrst in the
neural network modeling community [20, 21, 1, 12]. More recently, similar cross-validation opti-
mization techniques have been proposed for other supervised learning models [3], including sup-
port vector machines [4, 10, 16], Gaussian processes [35, 33], and related kernel learning meth-
ods [18, 17, 39]. Here, we consider the problem of hyperparameter learning for a specialized class
of structured classiﬁcation models known as conditional log-linear models (CLLMs), a generaliza-
tion of conditional random ﬁelds (CRFs) [19].

Whereas standard binary classiﬁcation involves mapping an object x ∈ X to some binary output
y ∈ Y (where Y = {±1}), the input space X and output space Y in a structured classiﬁcation task
generally contain complex combinatorial objects (such as sequences, trees, or matchings). Design-
ing hyperparameter learning algorithms for structured classiﬁcation models thus yields a number of
unique computational challenges not normally encountered in the ﬂat classiﬁcation setting. In this
paper, we derive a gradient-based approach for optimizing the hyperparameters of a CLLM using the
loss incurred on a holdout set. We describe the required algorithms speciﬁc to CLLMs which make
the needed computations tractable. Finally, we demonstrate on both simulations and a real-world
computational biology task that our hyperparameter learning method can give gains over learning
ﬂat unstructured regularization priors.

2 Preliminaries
Conditional log-linear models (CLLMs) are a probabilistic framework for sequence labeling or pars-
ing problems, where X is an exponentially large space of possible input sequences and Y is an
exponentially large space of candidate label sequences or parse trees. Let F : X × Y → Rn be
a ﬁxed vector-valued mapping from input-output pairs to an n-dimensional feature space. CLLMs
model the conditional probability of y given x as P (y | x; w) = exp(wT F(x, y))/Z(x) where
of i.i.d. labeled input-
output pairs drawn from some unknown ﬁxed distribution D over X × Y, the parameter learning
problem is typically posed as maximum a posteriori (MAP) estimation (or equivalently, regularized
logloss minimization):

Z(x) =Py′∈Y exp(wT F(x, y′)). Given a training set T =(cid:8)(x(i), y(i))(cid:9)m
log P (y(i) | x(i); w)!,

w∈Rn   1

w⋆ = arg min

wT Cw −

(OPT1)

i=1

2

m

where 1
overﬁtting. Here, C is the inverse covariance matrix of a Gaussian prior on the parameters w.

2 wT Cw (for some positive deﬁnite matrix C) is a regularization penalty used to prevent

While a number of efﬁcient procedures exist for solving the optimization problem OPT1 [34, 11],
little attention is usually given to choosing an appropriate regularization matrix C. Generally, C is
parameterized using a small number of free variables, d ∈ Rk, known as the hyperparameters of the
of i.i.d. examples drawn from D, hyperparameter

Xi=1

(OPT2)

learning itself can be cast as an optimization problem:

model. Given a holdout set H =(cid:8)(˜x(i), ˜y(i))(cid:9) ˜m
Xi=1

minimize

d∈Rk

i=1

−

˜m

log P(cid:16)˜y(i) | ˜x(i); w⋆(C)(cid:17).

In words, OPT2 ﬁnds the hyperparameters d whose regularization matrix C leads the parameter
vector w⋆(C) learned from the training set to obtain small logloss on holdout data. For many real-
world applications, C is assumed to take a simple form, such as a scaled identity matrix, CI. While
this parameterization may be partially motivated by concerns of hyperparameter overﬁtting [28],
such a choice usually stems from the difﬁculty of hyperparameter inference.

In practice, grid-search procedures provide a reliable method for determining hyperparam-
eters to low-precision: one trains the model using several candidate values of C (e.g., C ∈

(cid:8). . . , 2−2, 2−1, 20, 21, 22, . . .(cid:9)), and chooses the C that minimizes holdout logloss. While this strat-

egy is suitable for tuning a single model hyperparameter, more sophisticated strategies are necessary
when optimizing multiple hyperparameters.

3 Learning multiple hyperparameters
In this section, we lay the framework for multiple hyperparameter learning by describing a simple
yet ﬂexible parameterization of C that arises quite naturally in many practical problems. We then
describe a generic strategy for hyperparameter adaptation via gradient-based optimization.

Consider a setting in which predeﬁned subsets of parameter components (which we call reg-
ularization groups) are constrained to use the same hyperparameters [6]. For instance, in an
NLP task, individual word occurrence features may be placed in a separate regularization group
from word bigram features. Formally, let k be a ﬁxed number of regularization groups, and let
π : {1, . . . , n} → {1, . . . , k} be a prespeciﬁed mapping from parameters to regularization groups.
Furthermore, for a vector x ∈ Rk, deﬁne its expansion x ∈ Rn as x = (xπ(1), xπ(2), . . . , xπ(n)).

In the sequel, we parameterize C ∈ Rn×n in terms of some hyperparameter vector d ∈ Rk
as the diagonal matrix, C(d) = diag(exp(d)). Under this representation, C(d) is necessar-

ily positive deﬁnite, so OPT2 can be written as an unconstrained minimization over the variables

d ∈ Rk. Speciﬁcally, let ℓT (w) = −Pm
ℓH (w) = −P ˜m

i=1 log P(cid:0)y(i) | x(i); w(cid:1) denote the training logloss and
i=1 log P(cid:0)˜y(i) | ˜x(i); w(cid:1) the holdout logloss for a parameter vector w. Omitting the

dependence of C on d for notational convenience, we have the optimization problem

minimize

d∈Rk

ℓH (w⋆)

subject to w⋆ = arg min

w∈Rn (cid:18) 1

2

wT Cw + ℓT (w)(cid:19).

(OPT2’)

For any ﬁxed setting of these hyperparameters, the objective function of OPT2’ can be evaluated by
(1) using the hyperparameters d to determine the regularization matrix C, (2) solving OPT1 using
C to determine w⋆ and (3) computing the holdout logloss using the parameters w⋆. In this next
section, we derive a method for computing the gradient of the objective function of OPT2’ with
respect to the hyperparameters. Given both procedures for function and gradient evaluation, we may
apply standard gradient-based optimization (e.g., conjugate gradient or L-BFGS [30]) in order to
ﬁnd a local optimum of the objective. In general, we observe that only a few iterations (∼ 5) are
usually sufﬁcient to determine reasonable hyperparameters to low accuracy.

4 The hyperparameter gradient
Note that the optimization objective ℓH (w⋆) is a function of w⋆. In turn, w⋆ is a function of the hy-
perparameters d, as implicitly deﬁned by the gradient stationarity condition, Cw⋆ + ∇wℓT (w⋆) =
0. To compute the hyperparameter gradient, we will use both of these facts.

4.1 Deriving the hyperparameter gradient
First, we apply the chain rule to the objective function of OPT2’ to obtain

(1)
where Jd is the n × k Jacobian matrix whose (i, j)th entry is ∂w⋆
i /∂dj. The term ∇wℓH (w⋆) is
simply the gradient of the holdout logloss evaluated at w⋆. For decomposable models, this may
be computed exactly via dynamic programming (e.g., the forward/backward algorithm for chain-
structured models or the inside/outside algorithm for grammar-based models).

∇dℓH (w⋆) = JT

d ∇wℓH (w⋆)

Next, we show how to compute the Jacobian matrix Jd. Recall that at the optimum of the smooth
unconstrained optimization problem OPT1, the partial derivative of the objective with respect to any
parameter must vanish. In particular, the partial derivative of 1
2 wT Cw + ℓT (w) with respect to wi
vanishes when w = w⋆, so

0 = CT

i w⋆ +

∂

∂wi

ℓT (w⋆),

(2)

where CT
i denotes the ith row of the C matrix. Since (2) uniquely deﬁnes w⋆ (as OPT1 is a
strictly convex optimization problem), we can use implicit differentiation to obtain the needed partial
derivatives. Speciﬁcally, we can differentiate both sides of (2) with respect to dj to obtain

(3)

(4)

0 =

n

Xp=1(cid:18)w⋆

p

∂
∂dj

Cip + Cip

= I{π(i)=j}w⋆

i exp(dj) +

n

Xp=1

∂

w⋆

∂
∂dj

p(cid:19) +
Xp=1(cid:18)Cip +

n

∂

∂

∂wp

∂wi

ℓT (w⋆)

∂
∂dj

w⋆
p,

∂wp

∂wi

∂

ℓT (w⋆)(cid:19) ∂

∂dj

w⋆
p.

Stacking (4) for all i ∈ {1, . . . , n} and j ∈ {1, . . . , k}, we obtain the equivalent matrix equation,
(5)
wℓT (w⋆) is the

where B is the n × k matrix whose (i, j)th element is I{π(i)=j}w⋆
Hessian of the training logloss evaluated at w⋆. Finally, solving these equations for Jd, we obtain

i exp(dj), and ∇2

0 = B + (C + ∇2

wℓT (w⋆))Jd

Jd = −(C + ∇2

wℓT (w⋆))−1B.

(6)

4.2 Computing the hyperparameter gradient efﬁciently
In principle, one could simply use (6) to obtain the Jacobian matrix Jd directly. However, computing
the n × n matrix (C + ∇2
wℓT (w⋆) in
a typical CLLM requires approximately n times the cost of a single logloss gradient evaluation.
Once the Hessian has been computed, typical matrix inversion routines take O(n3) time. Even
more problematic, the Ω(n2) memory usage for storing the Hessian is prohibitive as typical log-
linear models (e.g., in NLP) may have thousands or even millions of features. To deal with these

wℓT (w⋆))−1 is difﬁcult. Computing the Hessian matrix ∇2

Algorithm 1: Gradient computation for hyperparameter selection.

Input:

Output:

training set T =(cid:8)(x(i), y(i))(cid:9)m

i=1
current hyperparameters d ∈ Rk
hyperparameter gradient ∇dℓH (w⋆)

, holdout set H =(cid:8)(˜x(i), ˜y(i))(cid:9) ˜m

i=1

1. Compute solution w⋆ to OPT1 using regularization matrix C = diag(exp(d)).
2. Form the matrix B ∈ Rn×k such that (B)ij = I{π(i)=j}w⋆
3. Use conjugate gradient algorithm to solve the linear system,

i exp(dj).

(C + ∇2

wℓT (w⋆))x = ∇wℓH (w⋆).

4. Return −BT x.

Figure 1: Pseudocode for gradient computation

wℓT (w⋆))v for any arbitrary vector v ∈ Rn can be computed
problems, we ﬁrst explain why (C+∇2
in O(n) time, even though forming (C + ∇2
bwℓT (w⋆))−1 is expensive. Using this result, we then
describe an efﬁcient procedure for computing the holdout hyperparameter gradient which avoids the
expensive Hessian computation and inversion steps of the direct method.

First, since C is diagonal, the product of C with any arbitrary vector v is trivially computable in
O(n) time. Second, although direct computation of the Hessian is inefﬁcient in a generic log-linear
model, computing the product of the Hessian with v can be done quickly, using any of the following
techniques, listed in order of increasing implementation effort (and numerical precision):

1. Finite differencing. Use the following numerical approximation:

∇2

wℓT (w⋆) · v = lim
r→0

∇wℓT (w⋆ + rv) − ∇wℓt(w⋆)

r

.

2. Complex step derivative [24]. Use the following identity from complex analysis:

∇2

wℓT (w⋆) · v = lim
r→0

Im {∇wℓT (w⋆ + i · rv)}

.

r

(7)

(8)

where Im {·} denotes the imaginary part of its complex argument (in this case, a vector).
Because there is no subtraction in the numerator of the right-hand expression, the complex-
step derivative does not suffer from the numerical problems of the ﬁnite-differencing
method that result from cancellation. As a consequence, much smaller step sizes can be
used, allowing for greater accuracy.

3. Analytical computation. Given an existing O(n) algorithm for computing gradients ana-

lytically, deﬁne the differential operator

f (w + rv) − f (w)

Rv{f (w)} = lim
r→0

∂
∂r
for which one can verify that Rv{∇wℓT (w⋆)} = ∇2
wℓT (w⋆) · v. By applying stan-
dard rules for differential operators, Rv{∇wℓT (w⋆)} can be computed recursively using
a modiﬁed version of the original gradient computation routine; see [31] for details.

f (w + rv)(cid:12)(cid:12)(cid:12)(cid:12)r=0

(9)

=

r

,

Hessian-vector products for graphical models were previously used in the context of step-size adap-
tation for stochastic gradient descent [36]. In our experiments, we found that the simplest method,
ﬁnite-differencing, provided sufﬁcient accuracy for our application.

Given the above procedure for computing matrix-vector products, we can now use the conjugate
gradient (CG) method to solve the matrix equation (5) to obtain Jd. Unlike direct methods for
solving linear systems Ax = b, CG is an iterative method which relies on the matrix A only
through matrix-vector products Av. In practice, few steps of the CG algorithm are generally needed
to ﬁnd an approximate solution of a linear system with acceptable accuracy. Using CG in this
way amounts to solving k linear systems, one for each column of the Jd matrix. Unlike the direct
method of forming the (C + ∇2
wℓT (w⋆)) matrix and its inverse, solving the linear systems avoids
the expensive Ω(n2) cost of Hessian computation and matrix inversion.

Nevertheless, even this approach for computing the Jacobian matrices still requires the solution
of multiple linear systems, which scales poorly when the number of hyperparameters k is large.

(a)

y1

xj
1

y2

xj
2

· · ·

· · ·

yL

xj
L

“observed features”

j ∈ {1, . . . , R}

xj
1

xj
2

· · ·

xj
Lxj
L

“noise features”

j ∈ {R + 1, . . . , 40}

grid
single
separate
grouped

(b)

0.5

0.45

0.4

0.35

0.3

0.25

0.2

0.15

l

s
e
b
a

l
 
t
c
e
r
r
o
c
n

i
 
f

o

 

n
o

i
t
r
o
p
o
r
P

0.1

10

0
40
Number of relevant features, R

20

30

(c)

0.55

0.5

0.45

0.4

0.35

l

s
e
b
a

l
 
t
c
e
r
r
o
c
n

i
 
f

o

 

n
o

i
t
r
o
p
o
r
P

0.3

0

grid
single
separate
grouped

20

40

60

Training set size, M

80

Figure 2: HMM simulation experiments. (a) State diagram of the HMM used in the simulations. (b)
Testing set performance when varying R, using M = 10. (c) Testing set performance when varying
M, using R = 5. In both (b) and (c), each point represents an average over 100 independent runs of
HMM training/holdout/testing set generation and CRF training and hyperparameter optimization.

However, we can do much better by reorganizing the computations in such a way that the Jacobian
matrix Jd is never explicitly required. In particular, substituting (6) into (1),
wℓT (w⋆))−1∇wℓH (w⋆)

∇dℓH (w⋆) = −BT (C + ∇2

(10)

we observe that it sufﬁces to solve the single linear system,

(C + ∇2

(11)
and then form ∇dℓH (w⋆) = −BT x. By organizing the computations this way, the number of least
squares problems that must be solved is substantially reduced from k to only one. A similar trick
was previously used for hyperparameter adaptation in SVMs [16] and kernel logistic regression [33].
Figure 1 shows a summary of our algorithm for hyperparameter gradient computation.1

wℓT (w⋆))x = ∇wℓH (w⋆)

5 Experiments
To test the effectiveness of our hyperparameter learning algorithm, we applied it to two tasks: a sim-
ulated sequence labeling task involving noisy features, and a real-world application of conditional
log-linear models to the biological problem of RNA secondary structure prediction.

Sequence labeling simulation. For our simulation test, we constructed a simple linear-chain
hidden Markov model (HMM) with binary-valued hidden nodes, yi ∈ {0, 1}.2 We associated 40
binary-valued features xj
i , j ∈ {1, . . . , 40} with each hidden state yi, including R “relevant” ob-
served features whose values were chosen based on yi, and (40 − R) “irrelevant” noise features
whose values were chosen to be either 0 or 1 with equal probability, independent of yi.3 Figure 2a
shows the graphical model representing the HMM. For each run, we used the HMM to simulate
training, holdout, and testing sets of M, 10, and 1000 sequences, respectively, each of length 10.

Next, we constructed a CRF based on an HMM model similar to that shown in Figure 2a in
which potentials were included for the initial node y1, between each yi and yi+1, and between
yi and each xj
i (including both the observed features and the noise features). We then performed
gradient-based hyperparameter learning using three different parameter-tying schemes: (a) all hy-
perparameters constrained to be equal, (b) separate hyperparameter groups for each parameter of the
model, and (c) transitions, observed features, and noise features each grouped together. Figure 2b
shows the performance of the CRF for each of the three parameter-tying gradient-based optimization
schemes, as well as the performance of scheme (a) when using the standard grid-search strategy of

trying regularization matrices CI for C ∈(cid:8). . . , 2−2, 2−1, 20, 21, 22, . . .(cid:9).

As seen in Figures 2b and 2c, the gradient-based procedure performed either as well as or bet-
ter than a grid search for single hyperparameter models. Using either a single hyperparameter or
all separate hyperparameters generally gave similar results, with a slight tendency for the separate

1In practice, roughly 50-100 iterations of CG were sufﬁcient to obtain hyperparameter gradients, meaning
that the cost of running Algorithm 1 was approximately the same as the cost of solving OPT1 for a single ﬁxed
setting of the hyperparameters. Roughly 3-5 line searches were sufﬁcient to identify good hyperparameter
settings; assuming that each line search takes 2-4 times the cost of solving OPT1, the overall hyperparameter
learning procedure takes approximately 20 times the cost of solving OPT1 once.

2For our HMM, we set initial state probabilities to 0.5 each, and used self-transition probabilities of 0.6.
3Speciﬁcally, we drew each xj

i independently according to P (xj

i = v | yi = v) = 0.6, v ∈ {0, 1}.

(a)

(b)

uccguagaaggc

5’

3’

RNA sequence

Regularization group

hairpin loop lengths
helix closing base pairs
symmetric internal loop lengths
external loop lengths
bulge loop lengths
base pairings
internal loop asymmetry
explicit internal loop sizes
terminal mismatch interactions
single base pair stacking interactions
1 × 1 internal loop nucleotides
single base bulge nucleotides
internal loop lengths
multi-branch loop lengths
helix stacking interactions

a .

g

.
a
.
a
.

.
u
.
g
.

|

|

|

.c
.c
.u
5’

g
.
g
.
.a
3’
secondary
structure

exp(di)

fold A
0.0832
0.780
6.32
0.338
0.451
2.01
4.24
12.8
132
71.0
139
136
1990
359
12100

fold B
0.456
0.0947
0.0151
0.401
2.03
7.95
6.90
6.39
50.2
104
120.
130.
35.3
2750
729

(c)

0.8

0.75

0.7

0.65

0.6

y
t
i
v
i
t
i
s
n
e
S

0.55

ILM

CONTRAfold (our algorithm)

Mfold

ViennaRNA

PKNOTS

0.5

0.45

0.4

single (AUC=0.6169, logloss=5916)
separate (AUC=0.6383, logloss=5763)
grouped (AUC=0.6406, logloss=5531)

Pfold

0.45

0.5

0.55

0.6

Specificity

0.65

0.7

0.75

0.8

Figure 3: RNA secondary structure prediction. (a) An illustration of the secondary structure predic-
tion task. (b) Grouped hyperparameters learned using our algorithm for each of the two folds. (c)
Performance comparison with state-of-the-art methods when using either a single hyperparameter
(the “original” CONTRAfold), separate hyperparameters, or grouped hyperparameters.

hyperparameter model to overﬁt. Enforcing regularization groups, however, gave consistently lower
error rates, achieving an absolute reduction in generalization error over the next-best model of 6.7%,
corresponding to a relative reduction of 16.2%.

RNA secondary structure prediction. We also applied our framework to the problem of RNA
secondary structure prediction. Ribonucleic acid (RNA) molecules are long nucleic acid polymers
present in the cells of all living organisms. For many types of RNA, three-dimensional (or tertiary)
structure plays an important role in determining the RNA’s function. Here, we focus on the task
of predicting RNA secondary structure, i.e., the pattern of nucleotide base pairings which form the
two-dimensional scaffold upon which RNA tertiary structures assemble (see Figure 3a).

As a starting point, we used CONTRAfold [7], a current state-of-the-art secondary structure
prediction program based on CLLMs. In brief, the CONTRAfold program models RNA secondary
structures using a variant of stochastic context-free grammars (SCFGs) which incorporates features
chosen to closely match the energetic terms found in standard physics-based models of RNA struc-
ture. These features model the various types of loops that occur in RNAs (e.g., hairpin loops, bulge
loops, interior loops, etc.). To control overﬁtting, CONTRAfold uses ﬂat L2 regularization. Here,
we modiﬁed the existing implementation to perform an “outer” optimization loop based on our al-
gorithm, and chose regularization groups either by (a) enforcing a single hyperparameter group, (b)
using separate groups for each parameter, or (c) grouping according to the type of each feature (e.g.,
all features for describing hairpin loop lengths were placed in a single regularization group).

For testing, we collected 151 RNA sequences from the Rfam database [13] for which
experimentally-determined secondary structures were already known. We divided this dataset into
two folds (denoted A and B) and performed two-fold cross-validation. Despite the small size of
the training set, the hyperparameters learned on each fold were nonetheless qualitatively similar,
indicating the robustness of the procedure (see Figure 3b). As expected, features with small regular-
ization hyperparameters correspond to properties of RNAs which are known to contribute strongly
to the energetics of RNA secondary structure, whereas many of the features with larger regulariza-
tion hyperparameters indicate structural properties whose presence/absence are either less correlated
with RNA secondary structure or sufﬁciently noisy that their parameters are difﬁcult to determine
reliably from the training data.

We then compared the cross-validated performance of algorithm with state-of-the-art methods
(see Figure 3c).4 Using separate or grouped hyperparameters both gave increased sensitivity and
increased speciﬁcity compared to the original model, which was learned using a single regulariza-
tion hyperparameter. Overall, the testing logloss (summed over the two folds) decreased by roughly
6.5% when using grouped hyperparameters and 2.6% when using multiple separate hyperparame-
ters, while the estimated testing ROC area increased by roughly 3.8% and 3.4%, respectively.

6 Discussion and related work
In this work, we presented a gradient-based approach for hyperparameter learning based on mini-
mizing logloss on a holdout set. While the use of cross-validation loss as a proxy for generalization
error is fairly natural, in many other supervised learning methods besides log-linear models, other
objective functions have been proposed for hyperparameter optimization.
In SVMs, approaches
based on optimizing generalization bounds [4], such as the radius/margin-bound [15] or maximal
discrepancy criterion [2] have been proposed. Comparable generalization bounds are not generally
known for CRFs; even in SVMs, however, generalization bound-based methods empirically do not
outperform simpler methods based on optimizing ﬁve-fold cross-validation error [8].

A different method for dealing with hyperparameters, common in neural network modeling, is
the Bayesian approach of treating hyperparameters themselves as parameters in the model to be es-
timated. In an ideal Bayesian scheme, one does not perform hyperparameter or parameter inference,
but rather integrates over all possible hyperparameters and parameters in order to obtain a posterior
distribution over predicted outputs given the training data. This integration can be performed using
a hybrid Monte Carlo strategy [27, 38]. For the types of large-scale log-linear models we consider in
this paper, however, the computational expense of sampling-based strategies can be extremely high
due to slow convergence of MCMC techniques [26].

Empirical Bayesian (i.e., ML-II) strategies, such as Automatic Relevance Determination
(ARD) [22], take the intermediate approach of integrating over parameters to obtain the marginal
likelihood (known as the log evidence), which is then optimized with respect to the hyperparame-
ters. Computing marginal likelihoods, however, can be quite costly, especially for log-linear models.
One method for doing this involves approximating the parameter posterior distribution as a Gaussian
centered at the posterior mode [22, 37]. In this strategy, however, the “Occam factor” used for hyper-
parameter optimization still requires a Hessian computation, which does not scale well for log-linear
models. An alternate approach based on using a modiﬁcation of expectation propagation (EP) [25]
was applied in the context of Bayesian CRFs [32] and later extended to graph-based semi-supervised
learning [14]. As described, however, inference in these models relies on non-traditional “probit-
style” potentials for efﬁciency reasons, and known algorithms for inference in Bayesian CRFs are
limited to graphical models with ﬁxed structure.

In contrast, our approach works broadly for a variety of log-linear models, including the
grammar-based models common in computational biology and natural language processing. Fur-
thermore, our algorithm is simple and efﬁcient, both conceptually and in practice: one iteratively
optimizes the parameters of a log-linear model using a ﬁxed setting of the hyperparameters, and then
one changes the hyperparameters based on the holdout logloss gradient. The gradient computation
relies primarily on a simple conjugate gradient solver for linear systems, coupled with the ability
to compute Hessian-vector products (straightforward in any modern programming language that al-
lows for operation overloading). As we demonstrated in the context of RNA secondary structure
prediction, gradient-based hyperparameter learning is a practical and effective method for tuning
hyperparameters when applied to large-scale log-linear models.

Finally we note that for neural networks, [9] and [5] proposed techniques for simultaneous opti-
mization of hyperparameters and parameters; these results suggest that similar procedures for faster
hyperparameter learning that do not require a doubly-nested optimization may be possible.

References
[1] L. Andersen, J. Larsen, L. Hansen, and M. Hintz-Madsen. Adaptive regularization of neural classiﬁers.

In NNSP, 1997.

[2] D. Anguita, S. Ridella, F. Rivieccio, and R. Zunino. Hyperparameter design criteria for support vector

classiﬁers. Neurocomputing, 55:109–134, 2003.

4Following [7], we used the maximum expected accuracy algorithm for decoding, which returns a set of
candidates parses reﬂecting different trade-offs between sensitivity (proportion of true base-pairs called) and
speciﬁcity (proportion of called base-pairs which are correct).

[3] Y. Bengio. Gradient-based optimization of hyperparameters. Neural Computation, 12:1889–1900, 2000.
[4] O. Chapelle, V. Vapnik, O. Bousquet, and S. Mukherjee. Choosing multiple parameters for support vector

machines. Machine Learning, 46(1–3):131–159, 2002.

[5] D. Chen and M. Hagan. Optimal use of regularization and cross-validation in neural network modeling.

In IJCNN, 1999.

[6] C. B. Do, S. S. Gross, and S. Batzoglou. CONTRAlign: discriminative training for protein sequence

alignment. In RECOMB, pages 160–174, 2006.

[7] C. B. Do, D. A. Woods, and S. Batzoglou. CONTRAfold: RNA secondary structure prediction without

physics-based models. Bioinformatics, 22(14):e90–e98, 2006.

[8] K. Duan, S. S. Keerthi, and A.N. Poo. Evaluation of simple performance measures for tuning SVM

hyperparameters. Neurocomputing, 51(4):41–59, 2003.

[9] R. Eigenmann and J. A. Nossek. Gradient based adaptive regularization. In NNSP, pages 87–94, 1999.
[10] T. Glasmachers and C. Igel. Gradient-based adaptation of general Gaussian kernels. Neural Comp.,

17(10):2099–2105, 2005.

[11] A. Globerson, T. Y. Koo, X. Carreras, and M. Collins. Exponentiated gradient algorithms for log-linear

structured prediction. In ICML, pages 305–312, 2007.

[12] C. Goutte and J. Larsen. Adaptive regularization of neural networks using conjugate gradient. In ICASSP,

1998.

[13] S. Grifﬁths-Jones, S. Moxon, M. Marshall, A. Khanna, S. R. Eddy, and A. Bateman. Rfam: annotating

non-coding RNAs in complete genomes. Nucleic Acids Res, 33:D121–D124, 2005.

[14] A. Kapoor, Y. Qi, H. Ahn, and R. W. Picard. Hyperparameter and kernel learning for graph based semi-

supervised classiﬁcation. In NIPS, pages 627–634, 2006.

[15] S. S. Keerthi. Efﬁcient tuning of SVM hyperparameters using radius/margin bound and iterative algo-

rithms. IEEE Transaction on Neural Networks, 13(5):1225–1229, 2002.

[16] S. S. Keerthi, V. Sindhwani, and O. Chapelle. An efﬁcient method for gradient-based adaptation of

hyperparameters in SVM models. In NIPS, 2007.

[17] K. Kobayashi, D. Kitakoshi, and R. Nakano. Yet faster method to optimize SVR hyperparameters based

on minimizing cross-validation error. In IJCNN, volume 2, pages 871–876, 2005.

[18] K. Kobayashi and R. Nakano. Faster optimization of SVR hyperparameters based on minimizing cross-

validation error. In IEEE Conference on Cybernetics and Intelligent Systems, 2004.

[19] J. Lafferty, A. McCallum, and F. Pereira. Conditional random ﬁelds: probabilistic models for segmenting

and labeling sequence data. In ICML 18, pages 282–289, 2001.

[20] J. Larsen, L. K. Hansen, C. Svarer, and M. Ohlsson. Design and regularization of neural networks: the

optimal use of a validation set. In NNSP, 1996.

[21] J. Larsen, C. Svarer, L. N. Andersen, and L. K. Hansen. Adaptive regularization in neural network

modeling. In Neural Networks: Tricks of the Trade, pages 113–132, 1996.

[22] D. J. C. MacKay. Bayesian interpolation. Neural Computation, 4(3):415–447, 1992.
[23] D. J. C. MacKay and R. Takeuchi. Interpolation models with multiple hyperparameters. Statistics and

Computing, 8:15–23, 1998.

Math. Softw., 29(3):245–262, 2003.

362–369, 2001.

[24] J. R. R. A. Martins, P. Sturdza, and J. J. Alonso. The complex-step derivative approximation. ACM Trans.

[25] T. P. Minka. Expectation propagation for approximate Bayesian inference. In UAI, volume 17, pages

[26] I. Murray and Z. Ghahramani. Bayesian learning in undirected graphical models: approximate MCMC

algorithms. In UAI, pages 392–399, 2004.

[27] R. M. Neal. Bayesian Learning for Neural Networks. Springer, 1996.
[28] A. Y. Ng. Preventing overﬁtting of cross-validation data. In ICML, pages 245–253, 1997.
[29] A. Y. Ng. Feature selection, L1 vs. L2 regularization, and rotational invariance. In ICML, 2004.
[30] J. Nocedal and S. J. Wright. Numerical Optimization. Springer, 1999.
[31] B. A. Pearlmutter. Fast exact multiplication by the Hessian. Neural Comp, 6(1):147–160, 1994.
[32] Y. Qi, M. Szummer, and T. P. Minka. Bayesian conditional random ﬁelds. In AISTATS, 2005.
[33] M. Seeger. Cross-validation optimization for large scale hierarchical classiﬁcation kernel methods. In

NIPS, 2007.

[34] F. Sha and F. Pereira. Shallow parsing with conditional random ﬁelds. In NAACL, pages 134–141, 2003.
[35] S. Sundararajan and S. S. Keerthi. Predictive approaches for choosing hyperparameters in Gaussian

processes. Neural Comp., 13(5):1103–1118, 2001.

[36] S. V. N. Vishwanathan, N. N. Schraudolph, M. W. Schmidt, and K. P. Murphy. Accelerated training of

conditional random ﬁelds with stochastic gradient methods. In ICML, pages 969–976, 2006.

[37] M. Wellings and S. Parise. Bayesian random ﬁelds: the Bethe-Laplace approximation. In ICML, 2006.
[38] C. K. I. Williams and D. Barber. Bayesian classiﬁcation with Gaussian processes. IEEE Transactions on

Pattern Analysis and Machine Intelligence, 20(12):1342–1351, 1998.

[39] X. Zhang and W. S. Lee. Hyperparameter learning for graph based semi-supervised learning algorithms.

In NIPS, 2007.

"
197,2007,Stability Bounds for Non-i.i.d. Processes,"The notion of algorithmic stability has been used effectively in the past to derive tight generalization bounds. A key advantage of these bounds is that they are de- signed for specific learning algorithms, exploiting their particular properties. But, as in much of learning theory, existing stability analyses and bounds apply only in the scenario where the samples are independently and identically distributed (i.i.d.). In many machine learning applications, however, this assumption does not hold. The observations received by the learning algorithm often have some inherent temporal dependence, which is clear in system diagnosis or time series prediction problems. This paper studies the scenario where the observations are drawn from a station- ary beta-mixing sequence, which implies a dependence between observations that weaken over time. It proves novel stability-based generalization bounds that hold even with this more general setting. These bounds strictly generalize the bounds given in the i.i.d. case. We also illustrate their application in the case of several general classes of learning algorithms, including Support Vector Regression and Kernel Ridge Regression.","Stability Bounds for Non-i.i.d. Processes

Mehryar Mohri

Courant Institute of Mathematical Sciences

and Google Research

251 Mercer Street

New York, NY 10012
mohri@cims.nyu.edu

Afshin Rostamizadeh

Department of Computer Science

Courant Institute of Mathematical Sciences

251 Mercer Street

New York, NY 10012
rostami@cs.nyu.edu

Abstract

The notion of algorithmic stability has been used effectively in the past to derive
tight generalization bounds. A key advantage of these bounds is that they are de-
signed for speciﬁc learning algorithms, exploiting their particular properties. But,
as in much of learning theory, existing stability analyses and bounds apply only
in the scenario where the samples are independently and identically distributed
(i.i.d.). In many machine learning applications, however, this assumption does
not hold. The observations received by the learning algorithm often have some
inherent temporal dependence, which is clear in system diagnosis or time series
prediction problems. This paper studies the scenario where the observations are
drawn from a stationary mixing sequence, which implies a dependence between
observations that weaken over time. It proves novel stability-based generalization
bounds that hold even with this more general setting. These bounds strictly gen-
eralize the bounds given in the i.i.d. case. It also illustrates their application in the
case of several general classes of learning algorithms, including Support Vector
Regression and Kernel Ridge Regression.

1 Introduction

The notion of algorithmic stability has been used effectively in the past to derive tight generalization
bounds [2–4,6]. A learning algorithm is stable when the hypotheses it outputs differ in a limited way
when small changes are made to the training set. A key advantage of stability bounds is that they are
tailored to speciﬁc learning algorithms, exploiting their particular properties. They do not depend
on complexity measures such as the VC-dimension, covering numbers, or Rademacher complexity,
which characterize a class of hypotheses, independently of any algorithm.

But, as in much of learning theory, existing stability analyses and bounds apply only in the scenario
where the samples are independently and identically distributed (i.i.d.). Note that the i.i.d. assump-
tion is typically not tested or derived from a data analysis. In many machine learning applications
this assumption does not hold. The observations received by the learning algorithm often have some
inherent temporal dependence, which is clear in system diagnosis or time series prediction prob-
lems. A typical example of time series data is stock pricing, where clearly prices of different stocks
on the same day or of the same stock on different days may be dependent.

This paper studies the scenario where the observations are drawn from a stationary mixing sequence,
a widely adopted assumption in the study of non-i.i.d. processes that implies a dependence between
observations that weakens over time [8, 10, 16, 17]. Our proofs are also based on the independent
block technique commonly used in such contexts [17] and a generalized version of McDiarmid’s
inequality [7]. We prove novel stability-based generalization bounds that hold even with this more
general setting. These bounds strictly generalize the bounds given in the i.i.d. case and apply to all
stable learning algorithms thereby extending the usefulness of stability-bounds to non-i.i.d. scenar-

1

ios. It also illustrates their application to general classes of learning algorithms, including Support
Vector Regression (SVR) [15] and Kernel Ridge Regression [13].

Algorithms such as support vector regression (SVR) [14, 15] have been used in the context of time
series prediction in which the i.i.d. assumption does not hold, some with good experimental re-
sults [9, 12]. To our knowledge, the use of these algorithms in non-i.i.d. scenarios has not been
supported by any theoretical analysis. The stability bounds we give for SVR and many other kernel
regularization-based algorithms can thus be viewed as the ﬁrst theoretical basis for their use in such
scenarios.

In Section 2, we will introduce the deﬁnitions for the non-i.i.d. problems we are considering and
discuss the learning scenarios. Section 3 gives our main generalization bounds based on stabil-
ity, including the full proof and analysis. In Section 4, we apply these bounds to general kernel
regularization-based algorithms, including Support Vector Regression and Kernel Ridge Regression.

2 Preliminaries

We ﬁrst introduce some standard deﬁnitions for dependent observations in mixing theory [5] and
then brieﬂy discuss the learning scenarios in the non-i.i.d. case.

2.1 Non-i.i.d. Deﬁnitions
Deﬁnition 1. A sequence of random variables Z = {Zt}∞
t=−∞ is said to be stationary if for any t
and non-negative integers m and k, the random vectors (Zt, . . . , Zt+m) and (Zt+k, . . . , Zt+m+k)
have the same distribution.

Thus, the index t or time, does not affect the distribution of a variable Zt in a stationary sequence.
This does not imply independence however. In particular, for i < j < k, Pr[Zj | Zi] may not
equal Pr[Zk | Zi]. The following is a standard deﬁnition giving a measure of the dependence of the
random variables Zt within a stationary sequence. There are several equivalent deﬁnitions of this
quantity, we are adopting here that of [17].

Deﬁnition 2. Let Z = {Zt}∞
t=−∞ be a stationary sequence of random variables. For any i, j ∈
Z ∪ {−∞, +∞}, let σj
i denote the σ-algebra generated by the random variables Zk, i ≤ k ≤ j.
Then, for any positive integer k, the β-mixing and ϕ-mixing coefﬁcients of the stochastic process Z
are deﬁned as
−∞h sup

n+k(cid:12)(cid:12)(cid:12)Pr[A | B] − Pr[A](cid:12)(cid:12)(cid:12)i ϕ(k) = sup

(cid:12)(cid:12)(cid:12)Pr[A | B] − Pr[A](cid:12)(cid:12)(cid:12).

β(k) = sup

n
n+k

A∈σ∞
B∈σn

−∞

(1)

E
B∈σn

n

A∈σ∞

Z is said to be β-mixing (ϕ-mixing) if β(k) → 0 (resp. ϕ(k) → 0) as k → ∞. It is said to be
algebraically β-mixing (algebraically ϕ-mixing) if there exist real numbers β0 > 0 (resp. ϕ0 > 0)
and r > 0 such that β(k) ≤ β0/kr (resp. ϕ(k) ≤ ϕ0/kr) for all k, exponentially mixing if there
exist real numbers β0 (resp. ϕ0 > 0) and β1 (resp. ϕ1 > 0) such that β(k) ≤ β0 exp(−β1kr) (resp.
ϕ(k) ≤ ϕ0 exp(−ϕ1kr)) for all k.
Both β(k) and ϕ(k) measure the dependence of the events on those that occurred more than k
units of time in the past. β-mixing is a weaker assumption than φ-mixing. We will be using a
concentration inequality that leads to simple bounds but that applies to φ-mixing processes only.
However, the main proofs presented in this paper are given in the more general case of β-mixing
sequences. This is a standard assumption adopted in previous studies of learning in the presence
of dependent observations [8, 10, 16, 17]. As pointed out in [16], β-mixing seems to be “just the
right” assumption for carrying over several PAC-learning results to the case of weakly-dependent
sample points. Several results have also been obtained in the more general context of α-mixing but
they seem to require the stronger condition of exponential mixing [11]. Mixing assumptions can be
checked in some cases such as with Gaussian or Markov processes [10]. The mixing parameters can
also be estimated in such cases.

2

Most previous studies use a technique originally introduced by [1] based on independent blocks of
equal size [8,10,17]. This technique is particularly relevant when dealing with stationary β-mixing.
We will need a related but somewhat different technique since the blocks we consider may not have
the same size. The following lemma is a special case of Corollary 2.7 from [17].

Lemma 1 (Yu [17], Corollary 2.7). Let µ ≥ 1 and suppose that h is measurable function, with
ri(cid:17) where ri ≤
absolute value bounded by M , on a product probability space(cid:16)Qµ
si ≤ ri+1 for all i. Let Q be a probability measure on the product space with marginal measures Qi
rj(cid:17), i = 1, . . . , µ−1.
), and let Qi+1 be the marginal measure of Q on(cid:16)Qi+1
on (Ωi, σsi
ri
Let β(Q) = sup1≤i≤µ−1 β(ki), where ki = ri+1 − si, and P =Qµ

j=1 Ωj,Qµ
j=1 Ωj,Qi+1

j=1 σsj
i=1 Qi. Then,

i=1 σsi

(2)

[h]| ≤ (µ − 1)M β(Q).

| E

Q

[h] − E

P

The lemma gives a measure of the difference between the distribution of µ blocks where the blocks
are independent in one case and dependent in the other case. The distribution within each block
is assumed to be the same in both cases. For a monotonically decreasing function β, we have
β(Q) = β(k∗), where k∗ = mini(ki) is the smallest gap between blocks.

2.2 Learning Scenarios

We consider the familiar supervised learning setting where the learning algorithm receives a sample
of m labeled points S = (z1, . . . , zm) = ((x1, y1), . . . , (xm, ym)) ∈ (X × Y )m, where X is the
input space and Y the set of labels (Y = R in the regression case), both assumed to be measurable.
For a ﬁxed learning algorithm, we denote by hS the hypothesis it returns when trained on the sample
S. The error of a hypothesis on a pair z ∈ X×Y is measured in terms of a cost function c : Y ×Y →
R+. Thus, c(h(x), y) measures the error of a hypothesis h on a pair (x, y), c(h(x), y) = (h(x)−y)2
in the standard regression cases. We will use the shorthand c(h, z) := c(h(x), y) for a hypothesis h
and z = (x, y) ∈ X × Y and will assume that c is upper bounded by a constant M > 0. We denote
by bR(h) the empirical error of a hypothesis h for a training sample S = (z1, . . . , zm):

c(h, zi).

(3)

In the standard machine learning scenario, the sample pairs z1, . . . , zm are assumed to be i.i.d., a
restrictive assumption that does not always hold in practice. We will consider here the more general
case of dependent samples drawn from a stationary mixing sequence Z over X × Y . As in the i.i.d.
case, the objective of the learning algorithm is to select a hypothesis with small error over future
samples. But, here, we must distinguish two versions of this problem.

In the most general version, future samples depend on the training sample S and thus the general-
ization error or true error of the hypothesis hS trained on S must be measured by its expected error
conditioned on the sample S:

1
m

mXi=1

bR(h) =

(4)

(5)

R(hS) = E
z

[c(hS, z) | S].

This is the most realistic setting in this context, which matches time series prediction problems.
A somewhat less realistic version is one where the samples are dependent, but the test points are
assumed to be independent of the training sample S. The generalization error of the hypothesis hS
trained on S is then:

R(hS) = E
z

[c(hS, z) | S] = E

z

[c(hS, z)].

This setting seems less natural since if samples are dependent, then future test points must also
depend on the training points, even if that dependence is relatively weak due to the time interval
after which test points are drawn. Nevertheless, it is this somewhat less realistic setting that has
been studied by all previous machine learning studies that we are aware of [8,10,16,17], even when
examining speciﬁcally a time series prediction problem [10]. Thus, the bounds derived in these
studies cannot be applied to the more general setting.

We will consider instead the most general setting with the deﬁnition of the generalization error based
on Eq. 4. Clearly, our analysis applies to the less general setting just discussed as well.

3

3 Non-i.i.d. Stability Bounds

This section gives generalization bounds for ˆβ-stable algorithms over a mixing stationary distribu-
tion.1 The ﬁrst two sections present our main proofs which hold for β-mixing stationary distri-
butions. In the third section, we will be using a concentration inequality that applies to φ-mixing
processes only.

The condition of ˆβ-stability is an algorithm-dependent property ﬁrst introduced in [4] and [6]. It has
been later used successfully by [2, 3] to show algorithm-speciﬁc stability bounds for i.i.d. samples.
Roughly speaking, a learning algorithm is said to be stable if small changes to the training set do
not produce large deviations in its output. The following gives the precise technical deﬁnition.
Deﬁnition 3. A learning algorithm is said to be (uniformly) ˆβ-stable if the hypotheses it returns for
any two training samples S and S′ that differ by a single point satisfy
|c(hS, z) − c(hS ′, z)| ≤ ˆβ.

∀z ∈ X × Y,

(6)

Many generalization error bounds rely on McDiarmid’s inequality. But this inequality requires the
random variables to be i.i.d. and thus is not directly applicable in our scenario. Instead, we will
use a theorem that extends McDiarmid’s inequality to general mixing distributions (Theorem 1,
Section 3.3).
To obtain a stability-based generalization bound, we will apply this theorem to Φ(S) = R(hS) −
bR(hS). To do so, we need to show, as with the standard McDiarmid’s inequality, that Φ is a Lipschitz
function and, to make it useful, bound E[Φ]. The next two sections describe how we achieve both of
these in this non-i.i.d. scenario.

3.1 Lipschitz Condition

As discussed in Section 2.2, in the most general scenario, test points depend on the training sample.
We ﬁrst present a lemma that relates the expected value of the generalization error in that scenario
and the same expectation in the scenario where the test point is independent of the training sample.

We denote by R(hS) = Ez[c(hS, z)|S] the expectation in the dependent case and by eR(hSb) =
Eez[c(hSb,ez)] that expectation when the test points are assumed independent of the training, with

Sb denoting a sequence similar to S but with the last b points removed. Figure 1(a) illustrates that
sequence. The block Sb is assumed to have exactly the same distribution as the corresponding block
of the same size in S.
Lemma 2. Assume that the learning algorithm is ˆβ-stable and that the cost function c is bounded
by M . Then, for any sample S of size m drawn from a β-mixing stationary distribution and for any
b ∈ {0, . . . , m}, the following holds:

| E

S

[R(hS)] − E

S

[eR(hSb)]| ≤ b ˆβ + β(b)M.

Proof. The ˆβ-stability of the learning algorithm implies that

E
S

[R(hS)] = E
S,z

[c(hS, z)] ≤ E

S,z

[c(hSb, z)] + b ˆβ.

The application of Lemma 1 yields

E
[R(hS)] ≤ E
S

S,ez

The other side of the inequality of the lemma can be shown following the same steps.

[c(hSb,ez)] + b ˆβ + β(b)M =eES[R(hSb)] + b ˆβ + β(b)M.

(7)

(8)

(9)

We can now prove a Lipschitz bound for the function Φ.

1The standard variable used for the stability coefﬁcient is β. To avoid the confusion with the β-mixing

coefﬁcient, we will use ˆβ instead.

4

Sb

(a)

z

b

b

Si

zi

b
(b)

Si,b

zi

b

b
(c)

Si

i,b

z

z

z

b

b

b

b
(d)

Figure 1: Illustration of the sequences derived from S that are considered in the proofs.

Lemma 3. Let S = (z1, z2, . . . , zm) and Si = (z′
m) be two sequences drawn from a
β-mixing stationary process that differ only in point i ∈ [1, m], and let hS and hSi be the hypotheses
returned by a ˆβ-stable algorithm when trained on each of these samples. Then, for any i ∈ [1, m],
the following inequality holds:

2, . . . , z′

1, z′

|Φ(S) − Φ(Si)| ≤ (b + 1)2 ˆβ + 2β(b)M +

M
m

.

(10)

Proof. To prove this inequality, we ﬁrst bound the difference of the empirical errors as in [3], then
the difference of the true errors. Bounding the difference of costs on agreeing points with ˆβ and the
one that disagrees with M yields

|bR(hS) − bR(hSi )| =

=

1
m

mXj=1
mXj6=i

1

|c(hS, zj) − c(hSi, z′
j)|

(11)

|c(hS, zj) − c(hSi, z′

j)| +

1
m|c(hS, zi) − c(hSi , z′

i)| ≤ ˆβ +

M
m

.

Now, applying Lemma 2 to both generalization error terms and using ˆβ-stability result in

|R(hS) − R(hSi)| ≤ |eR(hSb) − eR(hSi
[c(hSb,ez) − c(hSi

= E
ez

b

b

)| + 2b ˆβ + 2β(b)

The lemma’s statement is obtained by combining inequalities 11 and 12.

,ez)] + 2b ˆβ + 2β(b)M ≤ ˆβ + 2b ˆβ + 2β(b)M.

(12)

3.2 Bound on E[Φ]

As mentioned earlier, to make the bound useful, we also need to bound ES[Φ(S)]. This is done by
analyzing independent blocks using Lemma 1.
Lemma 4. Let hS be the hypothesis returned by a ˆβ-stable algorithm trained on a sample S drawn
from a stationary β-mixing distribution. Then, for all b ∈ [1, m], the following inequality holds:

[|Φ(S)|] ≤ (6b + 1) ˆβ + 3β(b)M.
E
S

(13)

denote a similar set of three blocks each with the same distribution as the corresponding block in Si,
but such that the three blocks are independent. In particular, the middle block reduced to one point

Proof. We ﬁrst analyze the term ES[bR(hS)]. Let Si be the sequence S with the b points before and
after point zi removed. Figure 1(b) illustrates this deﬁnition. Si is thus made of three blocks. Let eSi
ezi is independent of the two others. By the ˆβ-stability of the algorithm,

(14)

(15)

Applying Lemma 1 to the ﬁrst term of the right-hand side yields

E
S

m

S"" 1
[bR(hS)] = E

c(hS, zi)# ≤ E
Si"" 1
c(hSi , zi)# + 2b ˆβ.
mXi=1
eSi"" 1
,ezi)# + 2b ˆβ + 2β(b)M.
[bR(hS)] ≤ E

mXi=1

mXi=1

c(h eSi

E
S

m

m

5

Combining the independent block sequences associated to bR(hS) and R(hS) will help us prove the
lemma in a way similar to the i.i.d. case treated in [3]. Let Sb be deﬁned as in the proof of Lemma 2.
To deal with independent block sequences deﬁned with respect to the same hypothesis, we will
consider the sequence Si,b = Si ∩ Sb, which is illustrated by Figure 1(c). This can result in as many
as four blocks. As before, we will consider a sequence eSi,b with a similar set of blocks each with
the same distribution as the corresponding blocks in Si,b, but such that the blocks are independent.
Since three blocks of at most b points are removed from each hypothesis, by the ˆβ-stability of the
learning algorithm, the following holds:

E
S

[Φ(S)] = E
S

S,z"" 1
[bR(hS) − R(hS)] = E
Si,b,z"" 1

m

E

m

c(hSi,b , zi) − c(hSi,b, z)# + 6b ˆβ.

c(hS, zi) − c(hS, z)#

mXi=1

≤

mXi=1

(16)

(17)

Now, the application of Lemma 1 to the difference of two cost functions also bounded by M as in
the right-hand side leads to

eSi,b,ez"" 1

mXi=1

,ez)# + 6b ˆβ + 3β(b)M.

m

c(h eSi,b

E
[Φ(S)] ≤ E
S

,ezi) − c(h eSi,b
Sinceez andezi are independent and the distribution is stationary, they have the same distribution and
we can replaceezi withez in the empirical cost and write
,ez)# + 6b ˆβ + 3β(b)M ≤ ˆβ + 6b ˆβ + 3β(b)M, (19)
eSi,b,ez"" 1
i,b is the sequence derived from eSi,b by replacingezi withez. The last inequality holds by
where eSi

ˆβ-stability of the learning algorithm. The other side of the inequality in the statement of the lemma
can be shown following the same steps.

,ez) − c(h eSi,b

E
[Φ(S)] ≤ E
S

mXi=1

c(h eSi

m

i,b

(18)

3.3 Main Results

This section presents several theorems that constitute the main results of this paper. We will use the
following theorem which extends McDiarmid’s inequality to ϕ-mixing distributions.
Theorem 1 (Kontorovich and Ramanan [7], Thm. 1.1). Let Φ : Z m → R be a function deﬁned over
a countable space Z. If Φ is l-Lipschitz with respect to the Hamming metric for some l > 0, then
the following holds for all ǫ > 0:

Pr
Z

[|Φ(Z) − E[Φ(Z)]| > ǫ] ≤ 2 exp(cid:18)
mXk=1

ϕ(k).

−ǫ2

2ml2||∆m||2

∞(cid:19) ,

where ||∆m||∞ ≤ 1 + 2
Theorem 2 (General Non-i.i.d. Stability Bound). Let hS denote the hypothesis returned by a ˆβ-
stable algorithm trained on a sample S drawn from a ϕ-mixing stationary distribution and let c be
a measurable non-negative cost function upper bounded by M > 0, then for any b ∈ [0, m] and any
ǫ > 0, the following generalization bound holds
2m((b + 1)2 ˆβ + 2M ϕ(b) + M/m)2! .

S h˛˛˛R(hS) − bR(hS)˛˛˛ > ǫ + (6b + 1) ˆβ + 6M ϕ(b)i ≤ 2 exp 

−ǫ2(1 + 2Pm

Proof. The theorem follows directly the application of Lemma 3 and Lemma 4 to Theorem 1.

i=1 ϕ(i))−2

Pr

(20)

The theorem gives a general stability bound for ϕ-mixing stationary sequences.
If we further
assume that the sequence is algebraically ϕ-mixing, that is for all k, ϕ(k) = ϕ0k−r for some r > 1,
then we can solve for the value of b to optimize the bound.

6

Theorem 3 (Non-i.i.d. Stability Bound for Algebraically Mixing Sequences). Let hS denote the
hypothesis returned by a ˆβ-stable algorithm trained on a sample S drawn from an algebraically
ϕ-mixing stationary distribution, ϕ(k) = ϕ0k−r with r > 1 and let c be a measurable non-negative
cost function upper bounded by M > 0, then for any ǫ > 0, the following generalization bound
holds

2m(2 ˆβ + (r + 1)2M ϕ(b) + M/m)2! ,

−ǫ2(4 + 2/(r − 1))−2

Pr

S h˛˛˛R(hS) − bR(hS)˛˛˛ > ǫ + ˆβ + (r + 1)6M ϕ(b)i ≤ 2 exp 
where ϕ(b) = ϕ0(cid:16) ˆβ
satisﬁes ˆβb = rM ϕ(b), which gives b = (cid:16) ˆβ

rϕ0M(cid:17)r/(r+1)

rϕ0M(cid:17)−1/(r+1)

following term can be bounded as

.

Proof. For an algebraically mixing sequence, the value of b minimizing the bound of Theorem 2

1 + 2

ϕ(i) = 1 + 2

mXi=1

i−r ≤ 1 + 2(cid:18)1 +Z m

1

i−rdi(cid:19) = 1 + 2(cid:18)1 +

mXi=1

For r > 1, the exponent of m is negative, and so we can bound this last term by 3 + 2/(r − 1).
Plugging in this value and the minimizing value of b in the bound of Theorem 2 yields the statement
of the theorem.

and ϕ(b) = ϕ0(cid:16) ˆβ

rϕ0M(cid:17)r/(r+1)
1 − r (cid:19) .
m1−r − 1

. The

(21)

In the case of a zero mixing coefﬁcient (ϕ = 0 and b = 0), the bounds of Theorem 2 and Theorem 3
coincide with the i.i.d. stability bound of [3]. In order for the right-hand side of these bounds to
converge, we must have ˆβ = o(1/√m) and ϕ(b) = o(1/√m). For several general classes of
algorithms, ˆβ ≤ O(1/m) [3]. In the case of algebraically mixing sequences with r > 1 assumed in
Theorem 3, ˆβ ≤ O(1/m) implies ϕ(b) = ϕ0( ˆβ/(rϕ0M ))(r/(r+1)) < O(1/√m). The next section
illustrates the application of Theorem 3 to several general classes of algorithms.

4 Application

We now present the application of our stability bounds to several algorithms in the case of an al-
gebraically mixing sequence. Our bound applies to all algorithms based on the minimization of a
regularized objective function based on the norm k·kK in a reproducing kernel Hilbert space, where
K is a positive deﬁnite symmetric kernel:

argmin

h∈H

1
m

mXi=1

c(h, zi) + λkhk2
K,

(22)

under some general conditions, since these algorithms are stable with ˆβ ≤ O(1/m) [3]. Two speciﬁc
instances of these algorithms are SVR, for which the cost function is based on the ǫ-insensitive cost:

c(h, z) = |h(x) − y|ǫ =(cid:26)0

|h(x) − y| − ǫ

if |h(x) − y| ≤ ǫ,
otherwise,

(23)

a. Support vector regression (SVR):

and Kernel Ridge Regression [13], for which c(h, z) = (h(z) − y)2.
Corollary 1. Assume a bounded output Y = [0, B], for some B > 0, and assume that K(x, x) ≤ κ
for all x for some κ > 0. Let hS denote the hypothesis returned by the algorithm when trained on
a sample S drawn from an algebraically ϕ-mixing stationary distribution. Then, with probability at
least 1 − δ, the following generalization bounds hold for
+ 5  3κ2
+ 5  12κ2B 2

λ!r 2 ln(1/δ)
+ κr B
λ!r 2 ln(1/δ)
+ κr B

R(hS) ≤ bR(hS) +
R(hS) ≤ bR(hS) +

b. Kernel Ridge Regression (KRR):

13κ2
2λm

26κ2B 2

(25)

(24)

λm

m

λ

;

m

λ

.

7

Proof. It has been shown in [3] that for SVR ˆβ ≤ κ2/(2λm) and that M < κpB/λ and for KRR,
ˆβ ≤ 2κ2B2/(λm) and M < κpB/λ. Plugging in these values in the bound of Theorem 3 and

using the lower bound on r, r > 1, yield the statement of the corollary.

These bounds give, to the best of our knowledge, the ﬁrst stability-based generalization bounds for
SVR and KRR in a non-i.i.d. scenario. Similar bounds can be obtained for other families of algo-
rithms such as maximum entropy discrimination, which can be shown to have comparable stability
properties [3]. Our bounds have the same convergence behavior as those derived by [3] in the i.i.d.
case. In fact, they differ only by some constants. As in the i.i.d. case, they are non-trivial when the

condition λ ≫ 1/√m on the regularization parameter holds for all large values of m. It would be

interesting to give a quantitative comparison of our bounds and the generalization bounds of [10]
based on covering numbers for mixing stationary distributions, in the scenario where test points
are independent of the training sample. In general, because the bounds of [10] are not algorithm-
dependent, one can expect tighter bounds using stability, provided that a tight bound is given on
the stability coefﬁcient. The comparison also depends on how fast the covering number grows with
the sample size and trade-off parameters such as λ. For a ﬁxed λ, the asymptotic behavior of our
stability bounds for SVR and KRR is tight.

5 Conclusion

Our stability bounds for mixing stationary sequences apply to large classes of algorithms, including
SVR and KRR, extending to weakly dependent observations existing bounds in the i.i.d. case. Since
they are algorithm-speciﬁc, these bounds can often be tighter than other generalization bounds.
Weaker notions of stability might help further improve or reﬁne them.

References
[1] S. N. Bernstein. Sur l’extension du th´eor`eme limite du calcul des probabilit´es aux sommes de quantit´es

d´ependantes. Math. Ann., 97:1–59, 1927.

[2] O. Bousquet and A. Elisseeff. Algorithmic stability and generalization performance. In NIPS 2000, 2001.
[3] O. Bousquet and A. Elisseeff. Stability and generalization. JMLR, 2:499–526, 2002.
[4] L. Devroye and T. Wagner. Distribution-free performance bounds for potential function rules. In Infor-

mation Theory, IEEE Transactions on, volume 25, pages 601–604, 1979.

[5] P. Doukhan. Mixing: Properties and Examples. Springer-Verlag, 1994.
[6] M. Kearns and D. Ron. Algorithmic stability and sanity-check bounds for leave-one-out cross-validation.

In Computational Learing Theory, pages 152–162, 1997.

[7] L. Kontorovich and K. Ramanan. Concentration inequalities for dependent random variables via the

martingale method, 2006.

[8] A. Lozano, S. Kulkarni, and R. Schapire. Convergence and consistency of regularized boosting algorithms

with stationary β-mixing observations. In NIPS, 2006.

[9] D. Mattera and S. Haykin. Support vector machines for dynamic reconstruction of a chaotic system. In
Advances in kernel methods: support vector learning, pages 211–241. MIT Press, Cambridge, MA, 1999.
[10] R. Meir. Nonparametric time series prediction through adaptive model selection. Machine Learning,

39(1):5–34, 2000.

[11] D. Modha and E. Masry. On the consistency in nonparametric estimation under mixing assumptions.

IEEE Transactions of Information Theory, 44:117–133, 1998.

[12] K.-R. M¨uller, A. Smola, G. R¨atsch, B. Sch¨olkopf, J. K., and V. Vapnik. Predicting time series with support

vector machines. In Proceedings of ICANN’97, LNCS, pages 999–1004. Springer, 1997.

[13] C. Saunders, A. Gammerman, and V. Vovk. Ridge Regression Learning Algorithm in Dual Variables. In

Proceedings of the ICML ’98, pages 515–521. Morgan Kaufmann Publishers Inc., 1998.
[14] B. Sch¨olkopf and A. Smola. Learning with Kernels. MIT Press: Cambridge, MA, 2002.
[15] V. N. Vapnik. Statistical Learning Theory. Wiley-Interscience, New York, 1998.
[16] M. Vidyasagar. Learning and Generalization: With Applications to Neural Networks. Springer, 2003.
[17] B. Yu. Rates of convergence for empirical processes of stationary mixing sequences. The Annals of

Probability, 22(1):94–116, Jan. 1994.

8

"
754,2007,Evaluating Search Engines by Modeling the Relationship Between Relevance and Clicks,"We propose a model that leverages the millions of clicks received by web search engines, to predict document relevance. This allows the comparison of ranking functions when clicks are available but complete relevance judgments are not. After an initial training phase using a set of relevance judgments paired with click data, we show that our model can predict the relevance score of documents that have not been judged. These predictions can be used to evaluate the performance of a search engine, using our novel formalization of the confidence of the standard evaluation metric discounted cumulative gain (DCG), so comparisons can be made across time and datasets. This contrasts with previous methods which can provide only pair-wise relevance judgements between results shown for the same query. When no relevance judgments are available, we can identify the better of two ranked lists up to 82% of the time, and with only two relevance judgments for each query, we can identify the better ranking up to 94% of the time. While our experiments are on sponsored search results, which is the financial backbone of web search, our method is general enough to be applicable to algorithmic web search results as well. Furthermore, we give an algorithm to guide the selection of additional documents to judge to improve confidence.","Evaluating Search Engines by Modeling the
Relationship Between Relevance and Clicks

Ben Carterette∗

Center for Intelligent Information Retrieval

University of Massachusetts Amherst

Amherst, MA 01003

carteret@cs.umass.edu

Rosie Jones

Yahoo! Research
3333 Empire Ave
Burbank, CA 91504

jonesr@yahoo-inc.com

Abstract

We propose a model that leverages the millions of clicks received by web search
engines to predict document relevance. This allows the comparison of ranking
functions when clicks are available but complete relevance judgments are not.
After an initial training phase using a set of relevance judgments paired with click
data, we show that our model can predict the relevance score of documents that
have not been judged. These predictions can be used to evaluate the performance
of a search engine, using our novel formalization of the conﬁdence of the standard
evaluation metric discounted cumulative gain (DCG), so comparisons can be made
across time and datasets. This contrasts with previous methods which can provide
only pair-wise relevance judgments between results shown for the same query.
When no relevance judgments are available, we can identify the better of two
ranked lists up to 82% of the time, and with only two relevance judgments for
each query, we can identify the better ranking up to 94% of the time. While our
experiments are on sponsored search results, which is the ﬁnancial backbone of
web search, our method is general enough to be applicable to algorithmic web
search results as well. Furthermore, we give an algorithm to guide the selection of
additional documents to judge to improve conﬁdence.

1 Introduction

Web search engine evaluation is an expensive process: it requires relevance judgments that indicate
the degree of relevance of each document retrieved for each query in a testing set.
In addition,
reusing old relevance judgements to evaluate an updated ranking function can be problematic, since
documents disappear or become obsolete, and the distribution of queries entered changes [15]. Click
data from web searchers, used in aggregate, can provide valuable evidence about the relevance of
each document. The general problem with using clicks as relevance judgments is that clicks are
biased. They are biased to the top of the ranking [12], to trusted sites, to attractive abstracts; they
are also biased by the type of query and by other things shown on the results page. To cope with
this, we introduce a family of models relating clicks to relevance. By conditioning on clicks, we can
predict the relevance of a document or a set of documents.
Joachims et al. [12] used eye-tracking devices to track what documents users looked at before click-
ing. They found that users tend to look at results ranked higher than the one they click on more
often than they look at results ranked lower, and this information can in principle be used to train
a search engine using these “preference judgments”[10]. The problem with using preference judg-
ments inferred from clicks for learning is that they will tend to learn to reverse the list. A click at the
lowest rank is preferred to everything else, while a click at the highest rank is preferred to nothing

∗Work done while author was at Yahoo!

1

else. Radlinski and Joachims [13] suggest an antidote to this: randomly swapping adjacent pairs of
documents. This ensures that users will not prefer document i to document i + 1 solely because of
rank. However, we may not wish to show a suboptimal document ordering in order acquire data.
Our approach instead will be to use discounted cumulative gain (DCG [9]), an evaluation metric
commonly used in search engine evaluation. Using click data, we can estimate the conﬁdence that
a difference in DCG exists between two rankings without having any relevance judgments for the
documents ranked. We will show how a comparison of ranking functions can be performed when
clicks are available but complete relevance judgments are not. After an initial training phase with a
few relevance judgments, the relevance of unjudged documents can be predicted from clickthrough
rates. The conﬁdence in the evaluation can be estimated with the knowledge of which documents are
most frequently clicked. Conﬁdence can be dramatically increased with only a few more judiciously
chosen relevance judgments.
Our contributions are (1) a formalization of the information retrieval metric DCG as a random vari-
able (2) analysis of the sign of the difference between two DCGs as an indication that one ranking is
better than another (3) empirical demonstration that combining click-through rates over all results on
the page is better at predicting the relevance of the document at position i than just the click-through
rate at position i (4) empirically modeling relevance of documents using clicks, and using this model
to estimate DCG (5) empirical evaluation of comparison of different rankings using DCG derived
from clicks (6) an algorithm for selection of minimal numbers of documents for manual relevance
judgement to improve the conﬁdence in DCG over the estimate derived from clicks alone.
Section 2 covers previous work on using clickthrough rates and on estimating evaluation metrics.
Section 3 describes the evaluation of web retrieval systems using the metric discounted cumulative
gain (DCG) and shows how to estimate the conﬁdence that a difference exists when relevance judg-
ments are missing. Our model for predicting relevance from clicks is described in Section 4. We
discuss our data in Section 5 and in Section 6 we return to the task of estimating relevance for the
evaluation of search engines. Our experiments are conducted in the context of sponsored search, but
the methods we use are general enough to translate to general web search engines.

2 Previous Work

There has been a great deal of work on low-cost evaluation in TREC-type settings ([20, 6, 16, 5] are a
few), but we are aware of little for the web. As discussed above, Joachims [10, 12] and Radlinski and
Joachims [13] conducted seminal work on using clicks to infer user preferences between documents.
Agichtein et al.[2, 1] used and applied models of user interaction to predict preference relationships
and to improve ranking functions. They use many features beyond clickthrough rate, and show that
they can learn preference relationships using these features. Our work is superﬁcially similar, but
we explicitly model dependencies among clicks for results at different ranks with the purpose of
learning probabilistic relevance judgments. These relevance judgments are a stronger result than
preference ordering, since preference ordering can be derived from them. In addition, given a strong
probabilistic model of relevance from clicks, better combined models can be built.
Dupret et al. [7] give a theoretical model for the rank-position effects of click-through rate, and
build theoretical models for search engine quality using them. They do not evaluate estimates of
document quality, while we empirically compare relevance estimated from clicks to manual rele-
vance judgments. Joachims [11] investigated the use of clickthrough rates for evaluation, showing
that relative differences in performance could be measured by interleaving results from two ranking
functions, then observing which function produced results that are more frequently clicked. As we
will show, interleaving results can change user behavior, and not necessarily in a way that will lead
to the user clicking more relevant documents.
Soboroff [15] proposed methods for maintaining the relevance judgments in a corpus that is con-
stantly changing. Aslam et al. [3] investigated minimum variance unbiased estimators of system
performance, and Carterette et al. [5] introduced the idea of treating an evaluation measure as a ran-
dom variable with a distribution over all possible relevance judgments. This can be used to create
an optimal sampling strategy to obtain judgments, and to estimate the conﬁdence in an evaluation
measure. We extend their methods to DCG.

2

3 Evaluating Search Engines

at a particular rank, up to some maximum rank ‘: DCG‘ = P‘

Search results are typically evaluated using Discounted Cumulative Gain (DCG) [9]. DCG is deﬁned
as the sum of the “gain” of presenting a particular document times a “discount” of presenting it
i=1 gainidiscounti. For web
search, “gain” is typically a relevance score determined from a human labeling, and “discount” is
the reciprocal of the log of the rank, so that putting a document with a high relevance score at a low
rank results in a much lower discounted gain than putting the same document at a high rank.

DCG‘ = rel1 +

‘X

i=2

reli
log2 i

The constants reli are the relevance scores. Human assessors typically judge documents on an
ordinal scale, with labels such as “Perfect”, “Excellent”, “Good”, “Fair”, and “Bad”. These are then
mapped to a numeric scale for use in DCG computation. We will denote ﬁve levels of relevance aj,
with a1 > a2 > a3 > a4 > a5. In this section we will show that we can compare ranking functions
without having labeled all the documents.

3.1 Estimating DCG from Incomplete Information

DCG requires that the ranked documents have been judged with respect to a query. If the index has
recently been updated, or a new algorithm is retrieving new results, we have documents that have not
been judged. Rather than ask a human assessor for a judgment, we may be able to infer something
about DCG based on the judgments we already have.
Let Xi be a random variable representing the relevance of document i. Since relevance is ordinal,
the distribution of Xi is multinomial. We will deﬁne pij = p(Xi = aj) for 1 ≤ j ≤ 5 with
j=1 pijaj, and its variance is V ar[Xi] =

P5
j=1 pij = 1. The expectation of Xi is E[Xi] = P5
P5
‘X

We can then express DCG as a random variable:

j − E[Xi]2.

j=1 pija2

DCG‘ = X1 +

Xi
log2 i

i=2

Its expectation and variance are:

E[DCG‘] = E[X1] +

V ar[DCG‘] = V ar[X1] +

E[Xi]
log2 i

‘X
‘X

i=2

i=2

V ar[Xi]
(log2 i)2 + 2

‘X

i=1

Cov(X1, Xi)

log2 i

+ 2 X

1<i<j

(1)

Cov(Xi, Xj)
log2 i · log2 j

− E[DCG‘]2

(2)

If the relevance of documents i and j are independent, the covariance Cov(Xi, Xj) is zero.
When some relevance judgments are not available, Eq. (1) and (2) can be used to estimate conﬁdence
intervals for DCG. Thus we can compare ranking functions without having judged all the documents.

3.2 Comparative Evaluation

If we only care about whether one index or ranking function outperforms another, the actual values
of DCG matter less than the sign of their difference. We now turn our attention to estimating the
sign of the difference with high conﬁdence. We redeﬁne DCG in terms of an arbitrary indexing of
documents, instead of the indexing by rank we used in the previous section. Let rj(i) be the rank
at which document i was retrieved by system j. We deﬁne the discounted gain gij of document i to
log2 rj (i) if 1 < rj(i) ≤ ‘, and gij = 0 if
the DCG of system j as gij = reli if rj(i) = 1, gij = reli

3

document i was not ranked by system j. Then we can write the difference in DCG for systems 1
and 2 as

∆DCG‘ = DCG‘1 − DCG‘2 =

gi1 − gi2

(3)

NX

i=1

where N is the number of documents in the entire collection. In practice we need only consider
those documents returned in the top ‘ by either of the two systems. We can deﬁne a random variable
Gij by replacing reli with Xi in gij; we can then compute the expectation of ∆DCG:

NX

E[∆DCG‘] =

E[Gi1] − E[Gi2]

We can compute its variance as well, which is omitted here due to space constraints.

i=1

3.3 Conﬁdence in a Difference in DCG

Following Carterette et al. [5], we deﬁne the conﬁdence in a difference in DCG as the probability
that ∆DCG = DCG1 − DCG2 is less than zero. If P (∆DCG < 0) ≥ 0.95, we say that we
have 95% conﬁdence that system 1 is worse than system 2: over all possible judgments that could
be made to the unjudged documents, 95% of them will result in ∆DCG < 0.
To compute this probability, we must consider the distribution of ∆DCG. For web search, we are
typically most interested in performance in the top 10 retrieved. Ten documents is too few for any
convergence results, so instead we will estimate the conﬁdence using Monte Carlo simulation. We
simply draw relevance scores for the unjudged documents according to the multinomial distribution
p(Xi) and calculate ∆DCG using those scores. After T trials, the probability that ∆DCG is less
than 0 is simply the number of times ∆DCG was computed to be less than 0 divided by T .
How can we estimate the distribution p(Xi)? In the absence of any other information, we may
assume it to be uniform over all ﬁve relevance labels. Relevance labels that have been made in
the past provide a useful prior distribution. As we shall see below, clicks are a useful source of
information that we can leverage to estimate this distribution.

3.4 Selecting Documents to Judge

If conﬁdence estimates are low, we may want to obtain more relevance judgments to improve it. In
order to do as little work as necessary, we should select the documents that are likely to tell us a
lot about ∆DCG and therefore tell us a lot about conﬁdence. The most informative document is
the one that would have the greatest effect on ∆DCG. Since ∆DCG is linear, it is quite easy to
determine which document should be judged next. Eq. (3) tells us to simply choose the document i
that is unjudged and has maximum |E[Gi1]− E[Gi2]|. Algorithm 1 shows how relevance judgments
would be acquired iteratively until conﬁdence is sufﬁciently high. This algorithm is provably optimal
in the sense that after k judgments, we know more about the difference in DCG than we would with
any other k judgments.

Algorithm 1 Iteratively select documents to judge until we have high conﬁdence in ∆DCG.
1: while 1 − α ≤ P (∆DCG < 0) ≤ α do
2:
3:

i∗ ← maxi |E[Gi1] − E[Gi2]| for all unjudged documents i
judge document i∗
(human annotator provides reli∗)
P (Xi∗ = reli∗) ← 1
P (Xi∗ 6= reli∗) ← 0
estimate P (∆DCG) using Monte Carlo simulation

4:
5:
6:
7: end while

4 Modeling Clicks and Relevance

Our goal is to model the relationship between clicks and relevance in a way that will allow us
to estimate a distribution of relevance p(Xi) from the clicks on document i and on surrounding

4

documents. We ﬁrst introduce a joint probability distribution including the query q, the relevance
Xi of each document retrieved (where i indicates the rank), and their respective clickthrough rates
ci:

p(q, X1, X2, ..., X‘, c1, c2, ..., c‘) = P (q, X, c)

(4)

Boldface X and c indicate vectors of length ‘.
Suppose we have a query for which we have few or no relevance judgments (perhaps because it has
only recently begun to appear in the logs, or because it reﬂects a trend for which new documents are
rapidly being indexed). We can nevertheless obtain click-through data. We are therefore interested
in the conditional probability p(X|q, c).
Note that X = {X1, X2,···} is a vector of discrete ordinal variables; doing inference in this model
is not easy. To simplify, we make the assumption that the relevance of document i and document j
are conditionally independent given the query and the clickthrough rates:

‘Y

i=1

p(X|q, c) =

p(Xi|q, c)

(5)

This gives us a separate model for each rank, while still conditioning the relevance at rank i on the
clickthrough rates at all of the ranks. We do not lose the dependence between relevance at each rank
and clickthrough rates on other ranks. We will see the importance of this empirically in section 6.
The independence assumption allows us to model p(Xi) using ordinal regression. Ordinal regression
is a generalization of logistic regression to a variable with two or more outcomes that are ranked by
preference.
The proportional odds model for our ordinal response variable is

log p(X > aj|q, c)
p(X ≤ aj|q, c)

= αj + βq +

‘X

‘X

βici +

βikcick

i=1

i<k

where aj is one of the ﬁve relevance levels. The sums are over all ranks in the list; this models the
dependence of the relevance of the document to the clickthrough rates of everything else that was
retrieved, as well as any multiplicative dependence between the clickthrough rates at any two ranks.
After the model is trained, we can obtain p(X ≤ aj|q, c) using the inverse logit function. Then
p(X = aj|q, c) = p(X ≤ aj|q, c) − p(X ≤ aj−1|q, c).
A generalization to the proportional odds model is the vector generalized additive model (VGAM)
described by Yee and Wild [19]. VGAM has the same relationship to ordinal regression that
GAM [8] has to logistic regression. It is useful in our case because clicks do not necessarily have
linear relationships to relevance. VGAM is implemented in the R library VGAM. Once the model is
trained, we have p(X = aj) using the same arithmetic as for the proportional odds model.

5 Data

We obtained data from Yahoo! sponsored search logs for April 2006. Although we limited our data
to advertisements, there is no reason in principle our method should not be applicable to general web
search, since we see the same effects of bias towards the top of search results, to trusted sites and
so on. We have a total of 28,961 relevance judgments for 2,021 queries. The queries are a random
sample of all queries entered in late 2005 and early 2006. Relevance judgments are based on details
of the advertisement, such as title, summary, and URL.
We ﬁltered out queries for which we had no relevance judgments. We then aggregated records
into distinct lists of advertisements for a query as follows: Each record L consisted of a query, a
search identiﬁcation string, a set of advertisement ids, and for each advertisement id, the rank the
advertisement appeared at and the number of times it was clicked. Different sets of results for a
query, or results shown in a different order, were treated as distinct lists. We aggregated distinct lists
of results to obtain a clickthrough rate at each rank for a given list of results for a given query. The
clickthrough rate on each ad is simply the number of times it was clicked when served as part of list
L divided by the impressions, the number of times L was shown to any user. We did not adjust for
impression bias.

5

5.1 Dependence of Clicks on Entire Result List

Our model takes into account the clicks at all ranks to estimate the rel-
evance of the document at position i. As the ﬁgure to the right shows,
when there is an “Excellent” document at rank 1, its clickthrough rate
varies depending on the relevance of the document at rank 2. For ex-
ample, a “Perfect” document at rank 2 may decrease the likelihood of a
click on the “Excellent” document at rank 1, while a “Fair” document
at rank 2 may increase the clickthrough rate for rank 1. Clickthrough
rate at rank 1 more than doubles as the relevance of the document at
rank 2 drops from “Perfect” to “Fair”.

6 Experiments
6.1 Fit of Document Relevance Model

We ﬁrst want to test our proposed model (Eq. (5)) for predicting relevance from clicks. If the model
ﬁts well, the distributions of relevance it produces should compare favorably to the actual relevance
of the documents. We will compare it to a simpler model that does not take into account the click
dependence. The two models are contrasted below:

dependence model: p(X|q, c) =Y
independence model: p(X|q, c) =Y

p(Xi|q, c)
p(Xi|q, ci)

The latter models the relevance being conditional only on the query and its own clickthrough rate,
ignoring the clickthrough rates of the other items on the page. Essentially, it discretizes clicks into
relevance label bins at each rank using the query as an aid.
We removed all instances for which we had fewer than 500 impressions, then performed 10-fold
cross-validation. For simplicity, the query q is modeled as the aggregate clickthrough rate over
all results ever returned for that query. Both models produce a multinomial distribution for the
probability of relevance of a document p(Xi). Predicted relevance is the expected value of this

distribution: E[Xi] =P5

j=1 p(Xi = aj)aj.

The correlation between predicted relevance and actual relevance starts from 0.754 at rank 1 and
trends downward as we move down the list; by rank 5 it has fallen to 0.527. Lower ranks are
clicked less often; there are fewer clicks to provide evidence for relevance. Correlations for the
independence model are signiﬁcantly lower at each point.
Figure 1 depicts boxplots for each value of relevance for both models. Each box represents the
distribution of predictions for the true value on the x axis. The center line is the median prediction;
the edges are the 25% and 75% quantiles. The whiskers are roughly a 95% conﬁdence interval,
with the points outside being outliers. When dependence is modeled (Figure 1(a)), the distributions
are much more clearly separated from each other, as shown by the fact that there is little overlap
in the boxes. The correlation between predicted and acutal relevance is 18% higher, a statistically
signiﬁcant difference.

6.2 Estimating DCG

Since our model works fairly well, we now turn our attention to using relevance predictions to
estimate DCG for the evaluation of search engines. Recall that we are interested in comparative
evaluation—determining the sign of the difference in DCG rather than its magnitude. Our conﬁdence
in the sign is P (∆DCG < 0), which is estimated using the simulation procedure described in
Section 3.3. The simulation samples from the multinomial distributions p(Xi).
Methodology: To be able to calculate the exact DCG to evaluate our models, we need all ads
in a list to have a relevance judgment. Therefore our test set will consist of all of the lists for
which we have complete relevance judgments and at least 500 impressions. The remainder will
be used for training. The size of the test set is 1720 distinct lists. The training sets will include
all lists for which we have at least 200 impressions, over 5000 lists. After training the model, we

6

BadFairGoodExcellentPerfectrelevance at rank 2relative clickthrough rate at rank 10.00.20.40.60.81.0(a) Dependence model; ρ = 0.754

(b) No dependence modeled; ρ = 0.638

Figure 1: Predicted vs. actual relevance for rank 1. Correlation increases 18% when dependence of
relevance of the document at rank 1 on clickthrough at all ranks is modeled.

Conﬁdence
Accuracy clicks-only
Accuracy 2 judgments

0.5 − 0.6
0.522
0.572

0.6 − 0.7
0.617
0.678

0.7 − 0.8
0.734
0.697

0.8 − 0.9
0.818
0.890

0.9 − 0.95

–

0.918

0.95 − 1.0

–

0.940

Table 1: Conﬁdence vs. accuracy of predicting the better ranking for pairs of ranked lists using the
relevance predictions of our model based on clicks alone, and with two additional judgments for
each pair of lists. Conﬁdence estimates are good predictions of accuracy.

P ci, the naive approach described in our introduction.

predict relevance for the ads in the test set. We then use these expected relevances to calculate the
expectation E[DCG]. We will compare these expectations to the true DCG calculated using the
actual relevance judgments. As a baseline for automatic evaluation, we will compare to the average
clickthrough rate on the list E[CT R] = 1
k
We then estimate the conﬁdence P (∆DCG < 0) for pairs of ranked lists for the same query and
compare it to the actual percentage of pairs that had ∆DCG < 0. Conﬁdence should be less than
or equal to this percentage; if it is, we can “trust” it in some sense.
Results: We ﬁrst looked at the ability of E[DCG] to predict DCG, as well as the ability of
the average clickthrough rate E[CT R] to predict DCG. The correlation between the latter two
is 0.622, while the correlation between the former two is 0.876. This means we can approxi-
mate DCG better using our model than just using the mean clickthrough rate as a predictor.

The ﬁgure to the right shows actual vs. predicted relevance for ads in
the test set. (This is slightly different from Figure 1: the earlier ﬁgure
shows predicted results for all data from cross-validation while this
one only shows predicted results on our test data.) The separation of
the boxes shows that our model is doing quite well on the testing data,
at least for rank 1. Performance degrades quite a bit as rank increases
(not shown), but it is important to note that the upper ranks have the
greatest effect on DCG—so getting those right is most important.
In Table 1, we have binned pairs of ranked lists by their estimated conﬁdence. We computed the
accuracy of our predictions (the percent of pairs for which the difference in DCG was correctly
identiﬁed) for each bin. The ﬁrst line shows results when evaluating with no additional relevance
judgments beyond those used for training the model: although conﬁdence estimates tend to be low,
they are accurate in the sense that a conﬁdence estimate predicts how well we were able to distin-
guish between the two lists. This means that the conﬁdence estimates provide a guide for identifying
which evaluations require “hole-ﬁlling” (additional judgments).
The second line shows how results improve when only two judgments are made. Conﬁdence es-
timates increase a great deal (to a mean of over 0.8 from a mean of 0.6), and the accuracy of the
conﬁdence estimates is not affected.

7

BadFairGoodExcellentPerfect0.00.51.01.52.02.53.0expected relevanceBadFairGoodExcellentPerfect0.00.51.01.52.02.5expected relevanceBadFairGoodExcellentPerfect0.00.51.01.52.02.53.0predicted relevanceIn general, performance is very good: using only the predictions of our model based on clicks, we
have a very good sense of the conﬁdence we should have in our evaluation. Judging only two more
documents dramatically improves our conﬁdence: there are many more pairs in high-conﬁdence
bins after two judgments.

7 Conclusion

We have shown how to compare ranking functions using expected DCG. After a single initial train-
ing phase, ranking functions can be compared by predicting relevance from clickthrough rates. Es-
timates of conﬁdence can be computed; the conﬁdence gives a lower bound on how accurately
we have predicted that a difference exists. With just a few additional relevance judgments cho-
sen cleverly, we signiﬁcantly increase our success at predicting whether a difference exists. Using
our method, the cost of acquiring relevance judgments for web search evaluation is dramatically
reduced, when we have access to click data.

References
[1] E. Agichtein, E. Brill, and S. T. Dumais. Improving web search ranking by incorporating user behavior

information. In Proceedings SIGIR, pages 19–26, 2006.

[2] E. Agichtein, E. Brill, S. T. Dumais, and R. Ragno. Learning user interaction models for predicting web

search result preferences. In Proceedings SIGIR, pages 3–10, 2006.

[3] J. A. Aslam, V. Pavlu, and E. Yilmaz. A sampling technique for efﬁciently estimating measures of
query retrieval performance using incomplete judgments. In Proceedings of the 22nd ICML Workshop on
Learning with Partially Classiﬁed Training Data, pages 57–66, 2005.

[4] A. Broder. A taxonomy of web search. SIGIR Forum, 36(2):3–10, 2002.
[5] B. Carterette, J. Allan, and R. K. Sitaraman. Minimal test collections for retrieval evaluation. In Proceed-

ings of SIGIR, pages 268–275, 2006.

[6] G. V. Cormack, C. R. Palmer, and C. L. Clarke. Efﬁcient Construction of Large Test Collections. In

Proceedings of SIGIR, pages 282–289, 1998.

[7] G. Dupret, B. Piwowarski, C. Hurtado, and M. Mendoza. A statistical model of query log generation. In

SPIRE, LNCS 4209, pages 217–228. Springer, 2006.

[8] T. Hastie and R. Tibshirani. Generalized additive models. Statistical Science, 1:297–318, 1986.
[9] K. Jarvelin and J. Kekalainen. Cumulated gain-based evaluation of ir techniques. ACM Trans. Inf. Syst.,

20(4):422–446, 2002.

[10] T. Joachims. Optimizing search engines using clickthrough data. In Proceedings of KDD, pages 133–142,

2002.

[11] T. Joachims. Evaluating retrieval performance using clickthrough data. In Text Mining, pages 79–96.

2003.

[12] T. Joachims, L. A. Granka, B. Pan, H. Hembrooke, and G. Gay. Accurately interpreting clickthrough data

as implicit feedback. In Proceedings of SIGIR, pages 154–161, 2005.

[13] F. Radlinski and T. Joachims. Minimally invasive randomization fro collecting unbiased preferences from

clickthrough logs. In Proceedings of AAAI, 2006.

[14] M. Richardson, E. Dominowska, and R. Ragno. Predicting clicks: Estimating the click-through rate for

new ads. In Proceedings of WWW 2007, 2007.

[15] I. Soboroff. Dynamic test collections: measuring search effectiveness on the live web. In Proceedings of

SIGIR, pages 276–283, 2006.

[16] I. Soboroff, C. Nicholas, and P. Cahan. Ranking Retrieval Systems without Relevance Judgments. In

Proceedings of SIGIR, pages 66–73, 2001.

[17] L. Wasserman. All of Nonparametric Statistics. Springer, 2006.
[18] S. N. Wood. Thin plate regression splines. Journal of the Royal Statistical Society: Series B (Statistical

Methodology), 65(1):95–114, 2003.

[19] T. W. Yee and C. J. Wild. Vector generalized additive models. Journal of the Royal Statistical Society,

Series B (Methodological), 58(3):481–493, 1996.

[20] J. Zobel. How Reliable are the Results of Large-Scale Information Retrieval Experiments? In Proceedings

of SIGIR, pages 307–314, 1998.

8

"
1076,2007,Estimating disparity with confidence from energy neurons,"Binocular fusion takes place over a limited region smaller than one degree of visual angle (Panum's fusional area), which is on the order of the range of preferred disparities measured in populations of disparity-tuned neurons in the visual cortex. However, the actual range of binocular disparities encountered in natural scenes ranges over tens of degrees. This discrepancy suggests that there must be a mechanism for detecting whether the stimulus disparity is either inside or outside of the range of the preferred disparities in the population. Here, we present a statistical framework to derive feature in a population of V1 disparity neuron to determine the stimulus disparity within the preferred disparity range of the neural population. When optimized for natural images, it yields a feature that can be explained by the normalization which is a common model in V1 neurons. We further makes use of the feature to estimate the disparity in natural images. Our proposed model generates more correct estimates than coarse-to-fine multiple scales approaches and it can also identify regions with occlusion. The approach suggests another critical role for normalization in robust disparity estimation.","Estimating disparity with confidence from energy 

neurons

Eric K. C. Tsang

Dept. of Electronic and Computer Engr.

Hong Kong Univ. of Sci. and Tech.

Kowloon, HONG KONG SAR

eeeric@ee.ust.hk

Bertram E. Shi

Dept. of Electronic and Computer Engr.

Hong Kong Univ. of Sci. and Tech.

Kowloon, HONG KONG SAR

eebert@ee.ust.hk

Abstract

The peak location in a population of phase-tuned neurons has been shown to be a
more reliable estimator for disparity than the peak location in a population of
position-tuned neurons. Unfortunately, the disparity range covered by a phase-
tuned population is limited by phase wraparound. Thus, a single population can-
not cover the large range of disparities encountered in natural scenes unless the
scale of the receptive fields is chosen to be very large, which results in very low
resolution depth estimates. Here we describe a biologically plausible measure of
the confidence that the stimulus disparity is inside the range covered by a popula-
tion of phase-tuned neurons. Based upon this confidence measure, we propose an
algorithm for disparity estimation that uses many populations of high-resolution
phase-tuned neurons that are biased to different disparity ranges via position
shifts between the left and right eye receptive fields. The population with the
highest confidence is used to estimate the stimulus disparity. We show that this
algorithm outperforms a previously proposed coarse-to-fine algorithm for dispar-
ity estimation, which uses disparity estimates from coarse scales to select the
populations used at finer scales and can effectively detect occlusions.

1 Introduction 
Binocular disparity, the displacement between the image locations of an object between two eyes or
cameras, is an important depth cue. Mammalian brains appear to represent the stimulus disparity
using populations of disparity-tuned neurons in the visual cortex [1][2]. The binocular energy
model is a first order model that explains the responses of individual disparity-tuned neurons [3]. In
this model, the preferred disparity tuning of the neurons is determined by the phase and position
shifts between the left and right monocular receptive fields (RFs). 
Peak picking is a common disparity estimation strategy for these neurons([4]-[6]). In this strategy,
the disparity estimates are computed by the preferred disparity of the neuron with the largest
response among the neural population. Chen and Qian [4] have suggested that the peak location in
a population of phase-tuned disparity energy neurons is a more reliable estimate than the peak loca-
tion in a population of position-tuned neurons. 
It is difficult to estimate disparity from a single phase-tuned neuron population because its range of
preferred disparities is limited. Figure 1 shows the population response of phase-tuned neurons
(vertical cross section) for different stimulus disparities. If the stimulus disparity is confined to the
range of preferred disparities of this population, the peak location changes linearly with the stimu-
lus disparity. Thus, we can estimate the disparity from the peak. However, in natural viewing condi-
tion, the stimulus disparity ranges over ten times larger than the range of the preferred disparities of
the population [7]. The peak location no longer indicates the stimulus disparity, since the peaks still
occur even when the stimulus disparity is outside the range of neurons’ preferred disparities. The
false peaks arise from two sources: the phase wrap-around due to the sinusoidal modulation in the

Dpref

5
0
-5

-40

-20

-30
30
stimulus disparity (pixels)

-10

10

20

0

40

Fig. 1: Sample population responses of the phase-tuned disparity neurons for different disparities.
This was generated by presenting the left image of the “Cones” stereogram shown in Figure 5a to
both eyes but varying the disparity by keeping the left image fixed and shifting the right image. At
each point, the image intensity represents the response of a disparity neuron tuned to a fixed
preferred disparity (vertical axis) in response to a fixed stimulus disparity (horizontal axis). The
dashed vertical lines indicate the stimulus disparities that fall within the range of preferred
disparities of the population (

 pixels).

8±

Gabor function modelling neuron’s receptive field (RF) profile, or unmatching edges entering the
neuron's RF [5].
Although a single population can cover a large disparity range, the large size of the required recep-
tive fields results in very low resolution depth estimates. To address this problem, Chen and Qian
[4] proposed a coarse-to-fine algorithm which refines the estimates computed from coarse scales
using populations tuned to finer scales.
Here we present an alternative way to estimate the stimulus disparity using a biologically plausible
confidence measure that indicates whether the stimulus disparity lies inside or outside the range of
preferred disparities in a population of phase tuned neurons. We motivate this measure by examin-
ing the empirical statistics of the model neuron responses on natural images. Finally, we demon-
strate the efficacy of using this measure to estimate the stimulus disparity. Our model generates
better estimates than the coarse-to-fine approach [4], and can detect occlusions. 

2 Features of the phase-tuned disparity population
In this section, we define different features of a population of phase-tuned neurons. These features
will be used to define the confidence measure. Figure 2a illustrates the binocular disparity energy
model of a phase-tuned neuron [3]. For simplicity, we assume 1D processing, which is equivalent
to considering one orientation in the 2D case. The response of a binocular simple cell is modelled
by summing of the outputs of linear monocular Gabor filters applied to both left and right images,
followed by a positive or negative half squaring nonlinearity. The response of a binocular complex
cell is the sum of the four simple cell responses. 
 denotes the dis-
Formally, we define the left and right retinal images by 
 is the difference between the locations of corresponding
tance from the RF center. The disparity 
 in the left image
points in the left and right images, i.e., an object that appears at point 
appears at point 
 in the right image. Pairs of monocular responses are generated by integrating
image intensities weighted by pairs of phase quadrature RF profiles, which are the real and imagi-
nary parts of a complex-valued Gabor function (

, where 

Ur x( )

Ul x( )

 and 

d+

1–

):

=

d

x

x

x

j

(

(

σ

)

=

)

=

Ω

cos

g x( )

 and 

g x( )ej Ωx ψ+

Ωx ψ+

h x ψ,(
ψ

 is a zero mean Gaussian with standard deviation 

(1)
where 
 are the spatial frequency and the phase of the left and right monocular RFs, and
g x( )
, which is inversely proportional to the spa-
tial frequency bandwidth. The spatial frequency and the standard deviation of the left and right RFs
). We can compactly express the pairs of left
are identical, but the phases may differ (
and right monocular responses as the real and imaginary parts of 
 and
Vr ψr
)

, where with a slight abuse of notation, we define

Vrejψr

Vlejψl

Ωx ψ+

Vl ψl

 and 

jg x( )

ψr

sin

ψl

=

)

+

(

)

(

=

(

)

Vl

∫=

g x( )ejΩxUl x( ) xd

 and 

Vr

∫=

g x( )ejΩxUr x( ) xd

(2)

(a)

Ul x( )

h x ψl
,(
)
Re.

Im.

h x ψr
,(
)

Re.

Σ

Σ

Half Squaring

(b)

P

Σ

Ed Δψ(
)

Ur x( )

ψl

0=

Im.
ψr

π
---=
2

Binocular 
Simple Cells

Binocular 
Complex Cell

Ed Δψ(

)

S

Δψ

π–

ΔΦ

π

Fig. 2: (a) Binocular disparity energy model of a disparity neuron in the phase-shift mechanism.
 between the left and right monocular RFs determines the preferred
The phase-shift 
disparity of the neuron. The neuron shown is tuned to a negative disparity of 
. (b) The
 centered at a retinal location with the
population response of the phase-tuned neurons 
phase-shifts 

 can be characterized by three features 

ψr ψl–

π 2Ω(
⁄–

Ed Δψ(

 and 

S P,

ΔΦ

Δψ

π–

π,

∈

)

)

[

]

.

The response of the binocular complex cell (the disparity energy) is the squared modulus of the
sum of the monocular responses:

Ed Δψ(

)

=

Vlejψl Vrejψr

+

2

=

Vl

2 VlVr
+

*e j– Δψ Vl

+

*VrejΔψ

+

2

Vr

(3)

=

ψr ψl–

 controls the preferred disparity 

where the * superscript indicates the complex conjugation. The phase-shift between the right and
left neurons 
 of the binocular
Δψ
complex cell [6].
If we fix the stimulus and allow 
population response of phase-tuned neurons whose preferred disparities range between 
 and 
π Ω⁄

Ed Δψ(
. The population response can be completely specified by three features 

 to vary between 

, the function 

Dpref Δψ(

Δψ Ω⁄

S P

–≈

Δψ

π±

, 

)

)

 in (3) describes the
 and
 [4][5].
(4)

π Ω⁄–
ΔΦ

where

Ed Δψ(

)

=

S P

+

cos

(

ΔΦ Δψ–

)

S
P

ΔΦ

=
=

=

2

Vl
+
2 Vl Vr
Φl Φr

–

=

2

Vr
=

*
2 VlVr
*
VlVr
arg
(

)

(5)

 is the average
Figure 2b shows the graphical interpretation of these features. The feature 
P
 is the difference between the peak and average
response across the population. The feature 
Vl
 is the peak location
responses. Note that 
–
of the population response. Peak picking algorithms compute the estimates from the peak location,
i.e. 

. The feature 

, since 

ΔΦ Ω⁄

S P≥

S P–

 [6]. 

–=

ΔΦ

0>

Vr

)2

=

(

S

dest

3 Feature Analysis 
In this section, we suggest a simple confidence measure that can be used to differentiate between
two classes of stimulus disparities: DIN and DOUT corresponding to stimulus disparities inside
(
We find this confidence measure by analyzing the empirical joint densities of 
R
=
ering 

S
 conditioned on the two disparity classes. Considering 
ΔΦ

 and the ratio
 and 
 is equivalent to consid-
 will be less effective in distin-

) the range of preferred disparities in the population. 

. Intuitively, the peak location 

) and outside (

. We ignore 

P S⁄
S

π Ω⁄>

π Ω⁄≤

 and 

ΔΦ

R

P

S

d

d

(a)

R

1

0.8

0.6

0.4

0.2

0

(b)

R

1

0.8

0.6

0.4

0.2

0

5

10

S

15

20

5

10

S

15

20

(c)

R

0.9

0.8

0.7

0.6

0.5

x 10-3

(d)

PeΔ

8

6

4

2

15

20

0
0.1

5

10

S

0.2

0.3

P DIN[

0.4

0.5

]

Fig. 3: The empirical joint density of 
 given (a) DIN and (b) DOUT. Red indicates large
values. Blue indicates small values. (c) The optimal decision boundaries derived from the Bayes
factor. (d) The change in total probability of error 
 between using a flat boundary (thresholding
R

) versus the optimal boundary. 

PeΔ

 and 

R

S

R

guishing between DIN and DOUT, since Figure 1 shows that the phase ranges between 
for both disparity classes. The ratio 
Because of the uncertainties in the natural scenes, the features 
 are random variables. In
making a decision based on random features, Bayesian classifiers minimize the classification error.
Bayesian classifiers compare the conditional probabilities of the two disparity classes (DIN and
DOUT) given the observed feature values. The decision can be specified by thresholding the Bayes
factor.

 is bounded between 

1
 and 

 and 
S

, since 

S P≥

 and 

π–

R

π

0

.

BS R,

=

(

fS R, C s r, DIN
)
----------------------------------------------
fS R, C s r, DOUT
)

(

DIN
<>

DOUT

TS R,

(6)

c

)

σ

]

]

S

R

c

)

(

}
(

}

. 

[
{

8±

2σ

TS R,

 and 

P DIN[

 and 
c

DIN DOUT

fS R, C s r,

P DOUT
∈
,

 and depends upon the prior class probabilities 

 is the conditional density of the features given the class 

 controls the location of the decision boundary in the feature space
. The function

where the threshold 
S R,{
fS R, C s r,
To find the optimal decision boundary for the features 
, we estimated the joint class likeli-
hood 
 from data obtained using the “Cones” and the “Teddy” stereograms from Mid-
dlebury College [8][9], shown in Figure 5a. The stereograms are rectified, so that the
correspondences are located in the same horizontal scan-lines. Each image has 1500 x 1800 pixels.
We constructed a population of phase-tuned neurons at each pixel. The disparity neurons had the
same spatial frequency and standard deviation, and were selective to vertical orientations. The spa-
tial frequency was 
 radians per pixel and the standard deviation in the horizontal
2π 16⁄
=
Ω
 pixels, corresponding to a spatial bandwidth of 1.8 octaves. The standard
direction was 
=
6.78
deviation in the vertical direction was 
. The range of the preferred disparities (DIN) of the pop-
ulation is between 
 pixels. To reduce the variability in the classification, we also applied Gauss-
R
ian spatial pooling with the standard deviation 
computed from population were separated into two classes (DIN and DOUT) according to the
ground truth in Figure 5b. 
Figure 3a-b show the empirically estimated joint conditional densities for the two disparity classes.
They were computed by binning the features 
 and 0.01 for
R
. Given the disparity within the range of preferred disparities (DIN), the joint density concen-
trates at small 
. For the out-of-range disparities (DOUT), the joint density shifts to
both large 
. Intuitively, a horizontal hyperplane, illustrated by the red dotted line in
Figure 3a-b, is an appropriate decision boundary to separate the DIN and DOUT data. This indi-
cates that the feature 
 can be an indicator to distinguish between the in-range and out-of-range
disparities. Mathematically, we can compute the optimal decision boundaries by applying different
thresholds to the Bayes factor in (6). Figure 3c shows the boundaries. They are basically flat except
at small 
We also demonstrate the efficacy of thresholding 
 instead of using the optimal decision boundar-
ies to distinguish between in-range and out-of-range disparities. Given the prior class probability

 to the population [4][5]. The features 

 with the bin sizes of 0.25 for 

 and small 

 and large 

 and 

 and 

0.5σ

S

. 

S

R

R

R

S

S

S

S

R

R

phase tuned population

Ul x( )

Ed Δψ(

)

R128 ΔΦ128

,

Δc

=

128

Ed Δψ(

)

Ur x( )

Δc

0=

Ed Δψ(
)

Δc

–=

128

RΔc*
ΔΦΔc*

l
l
a
 
e
k
a
t
 
r
e
n
n
i
W

R TR>

DIN
/DOUT

dest

Fig. 4: Proposed disparity estimator with the validation of disparity estimates. 

]

, we compute a threshold 

c

∈

0 1,[

]

 that minimizes the total probability of classification

P DIN[
error: 

Pe

=

P DIN[

]

∫
R c<

fS R, C s r, DIN

(

)

+

1 P DIN[
–(

]

)

fS R, C s r, DOUT

(

)

(7)

∫
R c>

We then compare this total probability of error with the one computed using the optimal decision
boundaries derived in (6). Figure 3d shows the deviation in the total probability of error between
the two approaches versus 
) suggesting that
R
thresholding 
can be used as a confidence measure for distinguishing DIN and DOUT. Moreover, this measure
can be computed by normalization, which is a common component in models for V1 neurons [11]. 

 results in similar performance as using the optimal decision boundaries. Thus, 

. The deviation is small (on the order of 

P DIN[

10 2–

R

]

R

Δψ

Δc

4 Hybrid position-phase model for disparity estimation with validation 
Our analysis above shows that 
 is a simple indicator to distinguish between in-range and out-of-
range disparities. In this section, we describe a model that uses this feature to estimate the stimulus
disparity with validation. 
Figure 4 shows the proposed model, which consists of populations of hybrid tuned disparity neu-
rons tuned to different phase-shifts 
. For each population tuned to the
same position-shift but different phase-shifts (phase-tuned population), we compute the ratio
RΔc
 can be computed by pooling the responses of the
entire phase-tuned neurons. The feature 
 can be computed by subtracting the peak response
SΔc PΔc
 at dif-
ferent position-shifts are compared through a winner-take-all network to select the position-shift
Δc*
ΔΦΔc*
by 

RΔc
. The disparity estimate is further refined by the peak location 

 of the phase tuned population with the average activation 

. The average activation 

 and position-shifts 

 with the maximum 

. The features 

PΔc SΔc

SΔc
PΔc

RΔc

⁄

=

+

SΔc

dest

=

Δc*

–

ΔΦΔc*
-----------------

Ω

(8)

In additional to estimate the stimulus disparity, we also validate the estimates by comparing 
with a threshold 
feature 

RΔc*
. Instead of choosing a fixed threshold, we vary the threshold to show that the

 can be an occlusion detector. 

TR

RΔc

4.1 Disparity estimation with confidence
We applied the proposed model to estimate the disparity of the “Cones” and the “Teddy” stereo-
grams, shown in Figure 5a. The spatial frequency and the spatial standard deviation of the neurons

(a)

left

right

(b)

(c)

s
e
n
o
C

y
d
d
e
T

(d)

estimate

error

(e)

-100

0

100

estimate

error

Fig. 5: (a) The two natural stereograms used to evaluate the model performance. (b) The ground
truth disparity maps with respect to the left images, obtained by the structured light method. (c) The
ground truth occlusion maps. (d) The disparity maps and the error maps computed by the coarse-to-
fine approach. (e) The disparity maps and the error maps computed by the proposed model. The
detected invalid estimates are labelled in black in the disparity maps. 

σ

128±

 pixels, according to the ground truth.

were kept the same as the previous analysis. We also performed spatial pooling and orientation
pooling to improve the estimation. For spatial pooling, we applied a circularly symmetric Gaussian
function with standard deviation 
. For orientation pooling, we pooled the responses over five ori-
entations ranging from 30 to 150 degrees. The range of the position-shifts for the populations was
set to the largest disparity range, 
We also implemented the coarse-to-fine model as described in [4] for comparison. In this model, an
initial disparity estimate computed from a population of phase-tuned neurons at the coarsest scale is
successively refined by the populations of phase-tuned neurons at the finer scales. By choosing the
coarsest scale large enough, the disparity range covered by this method can be arbitrarily large. The
coarsest and the finest scales had the Gabor periods of 512 and 16 pixels. The Gabor periods of the
successive scales differed by a factor of 
. Neurons at the finest scale had the same RF parame-
ters as our model. Same spatial pooling and orientation pooling were applied on each scale. 
Figure 5d-e show the estimated disparity maps and the error maps of the two approaches. The error
maps show the regions where the disparity estimates exceed 1 pixel of error in the disparity. Both
models correctly recover the stimulus disparity at most locations with gradual disparity changes,
but tend to make errors at the depth boundaries. However, the proposed model generates more
accurate estimates. In the coarse-to-fine model, the percentage of pixels being incorrectly estimated
is 36.3%, while our proposed model is only 27.8%. 

2

The coarse-to-fine model tends to make errors around the depth boundaries. This arises because the
assumption that the stimulus disparity is constant over the RF of the neuron is unlikely at very large
scales. At boundaries, the coarse-to-fine model generates poor initial estimates, which cannot be
corrected at the finer scales, because the actual stimulus disparities are outside the range considered
at the finer scales.
On the other hand, the proposed model can not only estimate the stimulus disparity, but also can
validate the estimates. In general, the responses of neurons selective to different position disparities
are not comparable, since they depend upon image contrast which varies at different spatial loca-
tions. However, the feature 
, which is computed by normalizing the response peak by the average
response, eliminates such dependency. Moreover, the invalid regions detected (the black regions on
the disparity maps) are in excellent agreement with the error labels.

R

4.2 Occlusion detection 
In addition to validating the disparity estimates, the feature 
 can also be used to detect occlusion.
Occlusion is one of the challenging problems in stereo vision. Occlusion occurs near the depth dis-
continuities where there is no correspondence between the left and right images. The disparity in
the occlusion regions is undefined. The occlusion regions for these stereograms are shown in
Figure 5c.
There are three possibilities for image pixels that are labelled as out of range (DOUT). They are
occluded pixels, pixels with valid disparities that are incorrectly estimated, and pixels with valid
disparity that are correctly estimated. Figure 6a shows the percentages of DOUT pixels that fall
into each possibility as the threshold 

 applied to 

R

P1 occluded

(

)

=

×

100%

(9)

R

TR
 varies, e.g.,
# of occluded pixels in DOUT
------------------------------------------------------------------------

total # of pixels in DOUT

. For small thresholds, the detector mainly
These percentages sum to unity for any thresholds 
identifies the occlusion regions. As the threshold increases, the detector also begins to detect incor-
rect disparity estimates. Figure 6b shows the percentages of pixels in each possibility that are clas-
sified as DOUT as a function of 

TR

P2 occluded

(

)

=

TR

, e.g.,
# of occluded pixels in DOUT
------------------------------------------------------------------------
# of occluded pixels in image

×

100%

(10)

For a large threshold (
 close to unity), all estimates are labelled as DOUT, so the three percent-
ages approach 100%. The proposed detector is effective in identifying occlusion. At the threshold
TR
, it identifies ~70% of the occluded pixels, ~20% of the pixels with incorrect estimates
with only ~10% misclassification.

0.3

TR

=

(a)

)

%
0
0
1
x
(
 
1
P

1

0.8

0.6

0.4

0.2

(b)

)

%
0
0
1
x
(
 
2
P

1

0.8

0.6

0.4

0.2

0

0

0.2

0.4

0.6

0.8

1

TR

0

0

0.2

0.4

0.6

0.8

1

TR

Fig. 6: The percentages of occluded pixels (thick), pixels with incorrect disparity estimates (thin)
and pixels with correct estimates (dotted) identified as DOUT. (a) Percentages as a fraction of total
number of DOUT pixels. (b) Percentages as a fraction of number of pixels of each type. 

5 Discussion
In this paper, we have proposed an algorithm to estimate stimulus disparities based on a confidence
measure computed from population of hybrid tuned disparity neurons. Although there have been
previously proposed models that estimate the stimulus disparity from populations of hybrid tuned
neurons [4][10], our model is the first that also provides a confidence measure for these estimates.
Our analysis suggests that pixels with low confidence are likely to be in occluded regions. The
detection of occlusion, an important problem in stereo vision, was not addressed in these previous
approaches.
The confidence measure used in the proposed algorithm can be computed using normalization,
which has been used to model the responses of V1 neurons [11]. Previous work has emphasized the
role of normalization in reducing the effect of image contrast or in ensuring that the neural
responses tuned to different stimulus dimensions are comparable [12]. Our results show that, in
addition to these roles, normalization also serves to make the magnitude of the neural responses
more representative of the confidence in validating the hypothesis that the input disparity is close to
the neurons preferred disparity. The classification performance using this normalized feature is
close to that using the statistical optimal boundaries. 
Aggregating the neural responses over locations, orientations and scales is a common technique to
improve the estimation performance. For the consistency with the coarse-to-fine approach, our
algorithm also applies spatial and orientation pooling before computing the confidence. An inter-
esting question, which we are now investigating, is whether individual confidence measures com-
puted from different locations or orientations can be combined systematically. 

Acknowledgements
This work was supported in part by the Hong Kong Research Grants Council under Grant 619205. 

References
[1]

H. B. Barlow, C. Blakemore, and J. D. Pettigrew. The neural mechanism of binocular depth discrimi-
nation. Journal of Neurophysiology, vol. 193(2), 327-342, 1967.
G. F. Poggio, B. C. Motter, S. Squatrito, and Y. Trotter. Responses of neurons in visual cortex (V1 and
V2) of the alert macaque to dynamic random-dot stereograms. Vision Research, vol. 25, 397-406,
1985.
I. Ohzawa, G. C. Deangelis, and R. D. Freeman. Stereoscopic depth discrimination in the visual cortex:
neurons ideally suited as disparity detectors. Science, vol. 249, 1037-1041, 1990. 
Y. Chen and N. Qian. A Coarse-to-Fine Disparity Energy Model with Both Phase-Shift and Position-
Shift Receptive Field Mechanisms. Neural Computation, vol. 16, 1545-1577, 2004.
D. J. Fleet, H. Wagner and D. J. Heeger. Neural encoding of binocular disparity: energy models, posi-
tion shifts and phase shifts. Vision Research, 1996, vol. 36, 1839-1857. 
N. Qian, and Y. Zhu. Physiological computation of binocular disparity. Vision Research, vol. 37, 1811-
1827, 1997. 
S. J. D. Prince, B. G. Cumming, and A. J. Parker. Range and Mechanism of Encoding of Horizontal
Disparity in Macaque V1. Journal of Neurophysiology, vol. 87, 209-221, 2002.
D. Scharstein and R. Szeliski. A Taxonomy and Evaluation of Dense Two-Frame Stereo Correspon-
dence Algorithms. International Journal of Computer Vision, vol. 47(1/2/3), 7-42, 2002. 
D. Scharstein and R. Szeliski. High-accuracy stereo depth maps using structured light. IEEE Confer-
ence on Computer Vision and Pattern Recognition, vol. 1, 195-202, 2003.
J. C. A. Read and B. G. Cumming. Sensors for impossible stimuli may solve the stereo correspondence
problem. Nature Neuroscience, vol. 10, 1322-1328, 2007.
D. J. Heeger. Normalization of cell responses in cat striate cortex. Visual Neuroscience, vol. 9, 181-
198, 1992.
S. R. Lehky and T. J. Sejnowski. Neural model of stereoacuity and depth interpolation based on a dis-
tributed representation of stereo disparity. Journal of Neuroscience, vol. 10, 2281-2299, 1990.

[2]

[3]

[4]

[5]

[6]

[7]

[8]

[9]

[10]

[11]

[12]

"
908,2007,Locality and low-dimensions in the prediction of natural experience from fMRI,"Functional Magnetic Resonance Imaging (fMRI) provides an unprecedented window into the complex functioning of the human brain, typically detailing the activity of thousands of voxels during hundreds of sequential time points. Unfortunately, the interpretation of fMRI is complicated due both to the relatively unknown connection between the hemodynamic response and neural activity and the unknown spatiotemporal characteristics of the cognitive patterns themselves. Here, we use data from the Experience Based Cognition competition to compare global and local methods of prediction applying both linear and nonlinear techniques of dimensionality reduction. We build global low dimensional representations of an fMRI dataset, using linear and nonlinear methods. We learn a set of time series that are implicit functions of the fMRI data, and predict the values of these times series in the future from the knowledge of the fMRI data only. We find effective, low-dimensional models based on the principal components of cognitive activity in classically-defined anatomical regions, the Brodmann Areas. Furthermore for some of the stimuli, the top predictive regions were stable across subjects and episodes, including WernickeÕs area for verbal instructions, visual cortex for facial and body features, and visual-temporal regions (Brodmann Area 7) for velocity. These interpretations and the relative simplicity of our approach provide a transparent and conceptual basis upon which to build more sophisticated techniques for fMRI decoding. To our knowledge, this is the first time that classical areas have been used in fMRI for an effective prediction of complex natural experience.","Locality and low-dimensions in the prediction of

natural experience from fMRI

Franc¸ois G. Meyer

Center for the Study of Brain, Mind and Behavior,
Program in Applied and Computational Mathematics

Princeton University

fmeyer@colorado.edu

Greg J. Stephens

Center for the Study of Brain, Mind and Behavior,

Department of Physics
Princeton University

gstephen@princeton.edu

Both authors contributed equally to this work.

Abstract

Functional Magnetic Resonance Imaging (fMRI) provides dynamical access into
the complex functioning of the human brain, detailing the hemodynamic activ-
ity of thousands of voxels during hundreds of sequential time points. One ap-
proach towards illuminating the connection between fMRI and cognitive function
is through decoding; how do the time series of voxel activities combine to provide
information about internal and external experience? Here we seek models of fMRI
decoding which are balanced between the simplicity of their interpretation and the
effectiveness of their prediction. We use signals from a subject immersed in vir-
tual reality to compare global and local methods of prediction applying both linear
and nonlinear techniques of dimensionality reduction. We ﬁnd that the prediction
of complex stimuli is remarkably low-dimensional, saturating with less than 100
features. In particular, we build effective models based on the decorrelated com-
ponents of cognitive activity in the classically-deﬁned Brodmann areas. For some
of the stimuli, the top predictive areas were surprisingly transparent, including
Wernicke’s area for verbal instructions, visual cortex for facial and body features,
and visual-temporal regions for velocity. Direct sensory experience resulted in
the most robust predictions, with the highest correlation (c ∼ 0.8) between the
predicted and experienced time series of verbal instructions. Techniques based on
non-linear dimensionality reduction (Laplacian eigenmaps) performed similarly.
The interpretability and relative simplicity of our approach provides a conceptual
basis upon which to build more sophisticated techniques for fMRI decoding and
offers a window into cognitive function during dynamic, natural experience.

1 Introduction

Functional Magnetic Resonance Imaging (fMRI) is a non-invasive imaging technique that can quan-
tify changes in cerebral venous oxygen concentration. Changes in the fMRI signal that occur during
brain activation are very small (1-5%) and are often contaminated by noise (created by the imaging

system hardware or physiological processes). Statistical techniques that handle the stochastic nature
of the data are commonly used for the detection of activated voxels. Traditional methods of analy-
sis – which are designed to test the hypothesis that a simple cognitive or sensory stimulus creates
changes in a speciﬁc brain area – are unable to analyze fMRI datasets collected in “natural stimuli”
where the subjects are bombarded with a multitude of uncontrolled stimuli that cannot always be
quantiﬁed [1, 2].
The Experience Based Cognition competition (EBC) [3] offers an opportunity to study complex re-
sponses to natural environments, and to test new ideas and new methods for the analysis of fMRI
collected in natural environments. The EBC competition provides fMRI data of three human sub-
jects in three 20-minute segments (704 scanned samples in each segment) in an urban virtual reality
environment along with quantitative time series of natural stimuli or features (25 in total) ranging
from objective features such as the presence of faces to self-reported, subjective cognitive states
such as the experience of fear. During each session, subjects were audibly instructed to complete
three search tasks in the environment: looking for weapons (but not tools) taking pictures of people
with piercings (but not others), or picking up fruits (but not vegetables). The data was collected with
a 3T EPI scanner and typically consists of the activity of 35000 volume elements (voxels) within
the head. The feature time series was provided for only the ﬁrst two segments (1408 time samples)
and competitive entries are judged on their ability to predict the feature on the third segment (704
time samples, see Fig. 1). At a microscopic level, a large number of internal variables associated

Figure 1: We study the variation of the set of features fk(t), k = 1,··· , K as a function of the
dynamical changes in the fMRI signal X(t) = [x1(t),··· , xN (t)] during natural experience. The
features represent both external stimuli such as the presence of faces and internal emotional states
encountered during the exploration of a virtual urban environment (left and right images). We predict
the feature functions fk for t = Tl+1,··· T , from the knowledge of the entire fMRI dataset X , and
the partial knowledge of fk(t) for t = 1,··· , Tl. The “toy” activation patterns (middle diagram)
illustrate the changes in “brain states” occurring as a function of time.

with various physical and physiological phenomena contribute to the dynamic changes in the fMRI
signal. Because the fMRI signal is a large scale (as compared to the scale of neurons) measurement
of neuronal activity, we expect that many of these variables will be coupled resulting in a low di-
mensional set for all possible conﬁgurations of the activated fMRI signal. In this work we seek a
low dimensional representation of the entire fMRI dataset that provides a new set of ‘voxel-free”
coordinates to study cognitive and sensory features.
We denote a three-dimensional volumes of fMRI composed of a total of N voxels by X(t) =
[x1(t),··· , xN (t)]. We have access to T such volumes. We can stack the spatio-temporal fMRI
dataset into a N × T matrix,

(1)

 x1(1)

...

xN (1)

X =

 ,

x1(T )

···
...
··· xN (T )

...

where each row n represents a time series xn generated from voxel n and each column j represents
a scan acquired at time tj. We call the set of features to be predicted fk, k = 1, ,··· , K. We are
interested in studying the variation of the set of features fk(t), k = 1,··· , K describing the subject

?tjTttjti0tTlkf(t)tit0experience as a function of the dynamical changes of the brain, as measured by X(t). Formally, we
need to build predictions of fk(t) for t = Tl+1,··· T , from the knowledge of the entire fMRI dataset
X , and the partial knowledge of fk(t) for the training time samples t = 1,··· , Tl (see Fig. 1).

Figure 2: Low-dimensional parametrization of the set of “brain states”. The parametrization is
constructed from the samples provided by the fMRI data at different times, and in different states.

2 A voxel-free parametrization of brain states

We use here the global information provided by the dynamical evolution of X(t) over time, both
during the training times and the test times. We would like to effectively replace each fMRI dataset
X(t) by a small set of features that facilitates the identiﬁcation of the brain states, and make the
prediction of the features easier. Formally, our goal is to construct a map φ from the voxel space to
low dimensional space.

φ : RN (cid:55)→ D ⊂ RL

X(t) = [x1(t),··· , xN (t)]T (cid:55)→ (y1(t),··· , yL(t)),

(2)
(3)
where L (cid:28) N. As t varies over the training and the test sets, we hope that we explore most of
the possible brain conﬁgurations that are useful for predicting the features. The map φ provides a
parametrization of the brain states. Figure 2 provides a pictorial rendition of the map φ. The range
D, represented in Fig. 2 as a smooth surface, is the set of parameters y1,··· , yL that characterize
the brain dynamics. Different values of the parameters produce different “brain states”, associated
with different patterns of activation. Note that time does not play any role on D, and neighboring
points on D correspond to similar brain states. Equipped with this re-parametrization of the dataset
X , the goal is to learn the evolution of the feature time series as a function of the new coordinates
[y1(t),··· , yL(t)]T . Each feature function is an implicit function of the brain state measured by
[y1(t),··· , yL(t)]. For a given feature fk, the training data provide us with samples of fk at cer-
tain locations in D. The map φ is build by globally computing a new parametrization of the set
{X(1),··· , X(T )}. This parametrization is built into two stages. First, we construct a graph that is
a proxy for the entire set of fMRI data {X(1),··· , X(T )}. Second, we compute some eigenfunc-
tions φk deﬁned on the graph. Each eigenfunctions provides one speciﬁc coordinate for each node
of the graph.

2.1 The graph of brain states

We represent the fMRI dataset for the training times and test times by a graph. Each vertex i
corresponds to a time sample ti, and we compute the distance between two vertices i and j by
measuring a distance between X(ti) and X(tj). Global changes in the signal due to residual head
motion, or global blood ﬂow changes were removed by computing a a principal components analysis
(PCA) of the dataset X and removing a small number components. We then used the l2 distance
between the fMRI volumes (unrolled as N ×1 vectors). This distance compares all the voxels (white
and gray matter, as well as CSF) inside the brain.

Dtttφj0tφi2.2 Embedding of the dataset

Once the network of connected brain states is created, we need a distance to distinguish between
strongly connected states (the two fMRI data are in the same cognitive state) and weakly connected
states (the fMRI data are similar, but do not correspond to the same brain states). The Euclidean
distance used to construct the graph is only useful locally: we can use it to compare brain states
that are very similar, but is unfortunately very sensitive to short-circuits created by the noise in the
data. A standard alternative to the geodesic (shortest distance) is provided by the average commute
time, κ(i, j), that quantiﬁes the expected path length between i and j for a random walk started at i.
Formally, κ(i, j) = H(j, i) + H(i, j), where H(i, j) is the hitting time,

H(i, j) = Ei[Tj] with Tj = min{n ≥ 0; Zn = j},

di = Di,i =(cid:80)

for a random walk Zn on the graph with transition probability P, deﬁned by Pi,j = wi,j/di, and
j wi,j is the degree of the vertex i. The commute time can be conveniently computed
from the eigenfunctions φ1,··· , φN of N = D 1
2 , with the eigenvalues −1 ≤ λN ··· ≤ λ2 <
λ1 = 1. Indeed, we have

2 PD− 1

N(cid:88)

1

(cid:32)

(cid:33)2

.

− φk(j)(cid:112)dj

φk(i)√
di

1 − λk
As proposed in [4, 5, 6], we deﬁne an embedding

κ(i, j) =

k=2

i (cid:55)→ Ik(i) =

1

1 − λk
Because −1 ≤ λN ··· ≤ λ2 < λ1 = 1, we have
. We can therefore
neglect φk(j)√
for large k, and reduce the dimensionality of the embedding by using only the ﬁrst
1−λk
K coordinates in (4). The spectral gap measures the difference between the ﬁrst two eigenvalues,
λ1 − λ2 = 1 − λ2. A large spectral gap indicates that the low dimensional will provide a good
approximation. The algorithm for the construction of the embedding is summarized in Fig. 3.

> 1√

1−λN

1−λ3

1−λ2

1√

k = 2,··· , N
> ···

,

φk(i)√
di
1√

(4)

Algorithm 1: Construction of the embedding

Input:

– X(t), t = 1,··· , T , K: number of eigenfunctions.

Algorithm:

1. construct the graph deﬁned by the nn nearest neighbors
2. ﬁnd the ﬁrst K eigenfunctions, φk, of N

• Output: For ti = 1 : T

– new co-ordinates of X(ti): yk(ti) = 1√
πi

φk(i)√
1−λk

k = 2,··· , K + 1

Figure 3: Construction of the embedding

A parameter of the embedding (Fig. 3) is K, the number of coordinates. K can be optimized
by minimizing the prediction error. We expect that for small values of K the embedding will not
describe the data with enough precision, and the prediction will be inaccurate. If K is too large, some
of the new coordinates will be describing the noise in the dataset, and the algorithm will overﬁt the
training data. Fig. 4-(a) illustrates the effect of K on the performance of the nonlinear dimension
reduction. The quality of the prediction for the features: faces, instruction and velocity is plotted
against K. Instructions elicits a strong response in the auditory cortex that can be decoded with as
few as 20 coordinates. Faces requires more (about 50) dimensions to become optimal. As expected
the performance eventually drops when additional coordinates are used to describe variability that
is not related to the features to be decoded. This conﬁrms our hypothesis that we can replace about
15,000 voxels with 50 appropriately chosen coordinates.

2.3 Semi-supervised learning of the features

The problem of predicting a feature fk at an unknown time tu is formulated as kernel ridge regres-
sion problem. The training set {fk(t) for t = 1,··· , Tl} is used to estimate the optimal choice of
weights in the following model,

Tl(cid:88)

ˆf(tu) =

ˆα(t)K(y(tu), y(t)),

where K is a kernel and tu is a time point where we need to predict.

t=1

2.4 Results

We compared the nonlinear embedding approach (referred to as global Laplacian) to dimension
reduction obtained with a PCA of X . Here the principal components are principal volumes, and for
each time t we can expand X(t) onto the principal components.
The 1408 training data were divided into two subsets of 704 time samples. We use fk(t) in a subset
to predict fk(t) in the other subset. In order to quantify the stability of the prediction we randomly
selected 85 % of the training set (ﬁrst subset), and predicted 85 % of the testing set (other subset).
The role, training or testing, of each subset of 704 time samples was also chosen randomly. We
generated 20 experiments for each value of K, the number of predictors. The performance was
quantiﬁed with the normalized correlation between the model prediction and the real value of fk,

r = (cid:104)δf est

k (t), δfk(t)(cid:105)/

(5)
where δfk = fk(t)−(cid:104)fk(cid:105). Finally, r was averaged over the 20 experiments. Fig. 4-(a) and (b) show
the performance of the nonlinear method and linear method as a function of K. The approach based
on the nonlinear embedding yields very stable results, with low variance. For both global methods
the optimal performance is reached with less than 50 coordinates. Fig. 5 shows the correlation
coefﬁcients for 11 features, using K = 33 coordinates. For most features, the nonlinear embedding
performed better than global PCA.

k )2(cid:105)(cid:104)δf 2
k(cid:105),

(cid:113)(cid:104)δ(f est

3 From global to local

While models based on global features leverage predictive components from across the brain, cog-
nitive function is often localized within speciﬁc regions. Here we explore whether simple models
based on classical Brodmann regions provide an effective decoder of natural experience. The Brod-
mann areas were deﬁned almost a century ago (see e.g [7]) and divide the cortex into approximately
50 regions, based on the structure and arrangement of neurons within each region. While the ar-
eas are characterized structurally many also have distinct functional roles and we use these roles to
provide useful interpretations of our predictive models. Though the partitioning of cortical regions
remains an open and challenging problem, the Brodmann areas represent a transparent compromise
between dimensionality, function and structure.
Using data supplied by the competition, we warp each brain into standard Talairach coordinates and
locate the Brodmann area corresponding to each voxel. Within each Brodmann region, differing in
size from tens to thousands of elements, we build the covariance matrix of voxel time series using
all three virtual reality episodes. We then project the voxel time series onto the eigenvectors of the
covariance matrix (principal components) and build a simple, linear stimulus decoding model using
the top n modes ranked by their eigenvalues,

n(cid:88)

k (t) =
f est

wk

i mk

i (t).

(6)

i (t)} are the
where k indexes the different Brodmann areas, {wk
mode time series in each region. The weights are chosen to minimize the RMS error on the training
i (t)(cid:105).
set and have a particularly simple form here as the modes are decorrelated, wk
Performance is measured as the normalized correlation r (Eq. 5) between the model prediction and

i } are the linear weights and {mk

i = (cid:104)S(t)mk

i=1

Figure 4: Performance of the prediction of natural experience for three features, faces, instructions
and velocity as a function of the model dimension. (a) nonlinear embedding, (b) global principal
components, (c) local (Brodmann area) principal components. In all cases we ﬁnd that the predic-
tion is remarkably low-dimensional, saturating with less than 100 features. (d) stability and inter-
pretability of the optimal Brodmann areas used for decoding the presence of faces. All three areas
are functionally associated with visual processing. Brodmann area 22 (Wernicke’s area) is the best
predictor of instructions (not shown). The connections between anatomy, function and prediction
add an important measure of interpretability to our decoding models.

the real stimulus averaged over the two virtual reality episodes and we use the region with the lowest
training error to make the prediction. In principle, we could use a large number of modes to make a
prediction with n limited only by the number of training samples. In practice the predictive power
of our linear model saturates for a remarkably low number of modes in each region. In Fig 4(c) we
demonstrate the performance of the model on the number of local modes for three stimuli that are
predicted rather well (faces, instructions and velocity).
For many of the well-predicted stimuli, the best Brodmann areas were also stable across subjects and
episodes offering important interpretability. For example, in the prediction of instructions (which
the subjects received through headphones), the top region was Brodmann Area 22, Wernicke’s area,
which has long been associated with the processing of human language. For the prediction of the
face stimulus the best region was usually visual cortex (Brodmann Areas 17 and 19) and for the
prediction of velocity it was Brodmann Area 7, known to be important for the coordination of visual
and motor activity. Using modes derived from Laplacian eigenmaps we were also able to predict an
emotional state, the self-reporting of fear and anxiety. Interestingly, in this case the best predictions
came from higher cognitive areas in frontal cortex, Brodmann Area 11.
While the above discussion highlights the usefulness of classical anatomical location, many aspects
of cognitive experience are not likely to be so simple. Given the reasonable results above it’s natural

1601200local eigenmodesglobal eigenmodesீrு0.90Best Area (faces)(c)(b)00.930100 facesinstructionsvelocityBrodmann37Brodmann19Brodmann21(d)ீrுlocal eigenmodes16030148facesinstructionsvelocity 12001000ீrு0.9(a)global LaplacianfacesinstructionsvelocityFigure 5: Performance of the prediction of natural experience for eleven features, using three differ-
ent methods. Local decoders do well on stimuli related to objects while nonlinear global methods
better capture stimuli related to emotion.

to look for ways of combining the intuition derived from single classical location with more global
methods that are likely to do better in prediction. As a step in this direction, we modify our model
to include multiple Brodmann areas

k (t) =(cid:88)

f est

n(cid:88)

wl

iml

i(t),

(7)

l∈A

i=1

where A represents a collection of areas. To make a prediction using the modiﬁed model we ﬁnd the
top three Brodmann areas as before (ranked by their training correlation with the stimulus) and then
incorporate all of the modes in these areas (nA in total) in the linear model of Eq 7. The weights
{wl
i} are chosen to minimize RMS error on the training data. The combined model leverages both
the interpretive power of single areas and also some of the interactions between them. The results
of this combined predictor are shown in Fig. 5 (black) and are generally signiﬁcantly better than
the single region predictions. For ease of comparison, we also show the best global results (both
nonlinear Laplacian and global principal components). For many (but not all) of the stimuli, the
local, low-dimensional linear model is signiﬁcantly better than both linear and nonlinear global
methods.

4 Discussion

Incorporating the knowledge of functional, cortical regions, we used fMRI to build low-dimensional
models of natural experience that performed surprisingly well at predicting many of the complex
stimuli in the EBC competition. In addition, the regional basis of our models allows for transparent
cognitive interpretation, such as the emergence of Wernicke’s area for the prediction of auditory
instructions in the virtual environment. Other well-predicted experiences include the presence of
body parts and faces, both of which were decoded by areas in visual cortex. In future work, it will
be interesting to examine whether there is a well-deﬁned cognitive difference between stimuli that
can be decoded with local brain function and those that appear to require more global techniques.

arousalbodydoginterior/exteriorfacesfearful/anxiousfruits/veggiehitsinstructionsweapons/toolsvelocity0.90ீrு global eigenbrainglobal laplacianlocal eigenbrainWe also learned in this work that nonlinear methods for embedding datasets, inspired by manifold
learning methods [4, 5, 6], outperform linear techniques in their ability to capture the complex
dynamics of fMRI. Finally, our particular use of Brodmann areas and linear methods represent only
a ﬁrst step towards combining prior knowledge of broad regional brain function with the construction
of models for the decoding of natural experience. Despite the relative simplicity, an entry based on
this approach scored within the top 5 of the EBC2007 competition [3].

Acknowledgments

GJS was supported in part by National Institutes of Health Grant T32 MH065214 and by the Swartz
Foundation. FGM was partially supported by the Center for the Study of Brain, Mind and Behavior,
Princeton University. The authors are very grateful to all the members of the center for their support
and insightful discussions.

References
[1] Y. Golland, S. Bentin, H. Gelbard, Y. Benjamini, R. Heller, and Y. Nir et al. Extrinsic and
intrinsic systems in the posterior cortex of the human brain revealed during natural sensory
stimulation. Cerebral Cortex, 17:766–777, 2007.

[2] S. Malinen, Y. Hlushchuk, and R. Hari. Towards natural stimulation in fMRI–issues of data

analysis. NeuroImage, 35:131–139, 2007.

[3] http://www.ebc.pitt.edu.
[4] M. Belkin and P. Niyogi. Laplacian eigenmaps for dimensionality reduction and data represen-

tation. Neural Computations, 15:1373–1396, 2003.

[5] P. B´erard, G. Besson, and S. Gallot. Embeddings Riemannian manifolds by their heat kernel.

Geometric and Functional Analysis, 4(4):373–398, 1994.

[6] R.R. Coifman and S. Lafon. Diffusion maps. Applied and Computational Harmonic Analysis,

21:5–30, 2006.

[7] E.R. Kandel, J.H. Schwartz, and T.M. Jessell. Principles of Neural Science. McGraw-Hill, New

York, 2000.

"
644,2007,Configuration Estimates Improve Pedestrian Finding,"Fair discriminative pedestrian finders are now available. In fact, these pedestrian finders make most errors on pedestrians in configurations that are uncommon in the training data, for example, mounting a bicycle. This is undesirable. However, the human configuration can itself be estimated discriminatively using structure learning. We demonstrate a pedestrian finder which first finds the most likely human pose in the window using a discriminative procedure trained with structure learning on a small dataset. We then present features (local histogram of oriented gradient and local PCA of gradient) based on that configuration to an SVM classifier. We show, using the INRIA Person dataset, that estimates of configuration significantly improve the accuracy of a discriminative pedestrian finder.","Conﬁguration Estimates Improve Pedestrian Finding

Duan Tran∗

U.Illinois at Urbana-Champaign

Urbana, IL 61801 USA
ddtran2@uiuc.edu

D.A. Forsyth

U.Illinois at Urbana-Champaign

Urbana, IL 61801 USA

daf@uiuc.edu

Abstract

Fair discriminative pedestrian ﬁnders are now available. In fact, these pedestrian
ﬁnders make most errors on pedestrians in conﬁgurations that are uncommon in
the training data, for example, mounting a bicycle. This is undesirable. However,
the human conﬁguration can itself be estimated discriminatively using structure
learning. We demonstrate a pedestrian ﬁnder which ﬁrst ﬁnds the most likely hu-
man pose in the window using a discriminative procedure trained with structure
learning on a small dataset. We then present features (local histogram of oriented
gradient and local PCA of gradient) based on that conﬁguration to an SVM clas-
siﬁer. We show, using the INRIA Person dataset, that estimates of conﬁguration
signiﬁcantly improve the accuracy of a discriminative pedestrian ﬁnder.

1 Introduction

Very accurate pedestrian detectors are an important technical goal; approximately half-a-million
pedestrians are killed by cars each year (1997 ﬁgures, in [1]). At relatively low resolution, pedestri-
ans tend to have a characteristic appearance. Generally, one must cope with lateral or frontal views
of a walk. In these cases, one will see either a “lollipop” shape — the torso is wider than the legs,
which are together in the stance phase of the walk — or a “scissor” shape — where the legs are
swinging in the walk. This encourages the use of template matching. Early template matchers in-
clude: support vector machines applied to a wavelet expansion ([2], and variants described in [3]); a
neural network applied to stereoscopic reconstructions [4]; chamfer matching to a hierachy of con-
tour templates [5]; a likelihood threshold applied to a random ﬁeld model [6]; an SVM applied to
spatial wavelets stacked over four frames to give dynamical cues [3]; a cascade architecture applied
to spatial averages of temporal differences [7]; and a temporal version of chamfer matching to a
hierachy of contour templates [8].

By far one of the most successful static template matcher is due to Dalal and Triggs [9]. Their
method is based on a comprehensive study of features and their effects on performance for the
pedestrian detection problem. The method that performs best involves a histogram of oriented gra-
dient responses (a HOG descriptor). This is a variant of Lowe’s SIFT feature [10]. Each window
is decomposed into overlapping blocks (large spatial domains) of cells (smaller spatial domains).In
each block, a histogram of gradient directions (or edge orientations) is computed for each cell with
a measure of histogram “energy”. These cell histograms are concatenated into block histograms
followed by normalization which obtains a modicum of illumination invariance. The detection win-
dow is tiled with an overlapping grid. Within each block HOG descriptors are computed, and the
∗We would like to thank Alexander Sorokin for his providing the annotation software and Pietro Perona
for insightful comments. This work was supported by Vietname Education Foundation as well as in part
by the National Science Foundation under IIS - 0534837 and in part by the Ofﬁce of Naval Research under
N00014-01-1-0890 as part of the MURI program. Any opinions, ﬁndings and conclusions or recommendations
expressed in this material are those of the author(s) and do not necessarily reﬂect those of the National Science
Foundation or the Ofﬁce of Naval Research.

resulting feature vector is presented to an SVM. Dalal and Triggs show this method produces no
errors on the 709 image MIT dataset of [2]; they describe an expanded dataset of 1805 images. Fur-
thermore, they compare HOG descriptors with the original method of Papageorgiou and Poggio [2];
with an extended version of the Haar wavelets of Mohan et al. [11]; with the PCA-Sift of Ke and
Sukthankar ([12]; see also [13]); and with the shape contexts of Belongie et al. [14]. The HOG
descriptors outperform all other methods. Recently, Sabzmeydani and Mori [15] reported improved
results by using AdaBoost to select shapelet features (triplets of location, direction and strength of
local average gradient responses in different directions).

A key difﬁculty with pedestrian detection is that detectors must work on human conﬁgurations not
often seen in datasets. For systems to be useful, they cannot fail even on conﬁgurations that are very
uncommon — it is not acceptable to run people over when they stand on their hands. There is some
evidence (ﬁgure 1) that less common conﬁgurations present real difﬁculties for very good current
pedestrian detectors (our reimplementation of Dalal and Triggs’ work [9]).

Figure 1. Conﬁguration estimates result in our method producing fewer false negatives than our
implementation of Dalal and Triggs does. The ﬁgure shows typical images which are incorrectly
classiﬁed by our implementation of Dalal and Triggs, but correctly classiﬁed when a conﬁguration
estimate is attached. We conjecture that a conﬁguration estimate can avoid problems with occlusion
or contrast failure because the conﬁguration estimate reduces noise and the detector can use lower
detection thresholds.

1.1 Conﬁguration and Parts

Detecting pedestrians with templates most likely works because pedestrians appear in a relatively
limited range of conﬁgurations and views (e.g. “Our HOG detectors cue mainly on silhouette con-
tours (especially the head, shoulders and feet)” [9], p.893). It appears certain that using the architec-
ture of constructing features for whole image windows and then throwing the result into a classiﬁer
could be used to build a person-ﬁnder for arbitrary conﬁgurations and arbitrary views only with a
major engineering effort. The set of examples required would be spectacularly large, for example.
This is unattractive, because this set of examples implicitly encodes a set of facts that are relatively
easy to make explicit. In particular, people are made of body segments which individually have a
quite simple structure, and these segments are connected into a kinematic structure which is quite
well understood.

All this suggests ﬁnding people by ﬁnding the parts and then reasoning about their layout — essen-
tially, building templates with complex internal kinematics. The core idea is very old (see the review
in [16]) but the details are hard to get right and important novel formulations are a regular feature of
the current research literature.
Simply identifying the body parts can be hard. Discriminative approaches use classiﬁers to detect
parts, then reason about conﬁguration [11]. Generative approaches compare predictions of part
appearance with the image; one can use a tree structured conﬁguration model [17], or an arbitrary
graph [18]. If one has a video sequence, part appearance can itself be learned [19, 20]; more recently,

Ramanan has shown knowledge of articulation properties gives an appearance model in a single
image [21]. Mixed approaches use a discriminative model to identify parts, then a generative
model to construct and evaluate assemblies [22, 23, 24]. Codebook approaches avoid explicitly
modelling body segments, and instead use unsupervised methods to ﬁnd part decompositions that
are good for recognition (rather than disarticulation) [25].

Our pedestrian detection strategy consists of two steps: ﬁrst, for each window, we estimate the
conﬁguration of the best person available in that window; second, we extract features for that win-
dow conditioned on the conﬁguration estimate, and pass these features to a support vector machine
classiﬁer, which makes the ﬁnal decision on the window.

Figure 2. This ﬁgure is best viewed in color. Our model of human layout is parametrized by seven
vertices, shown on an example on the far left. The root is at the hip; the arrows give the direc-
tion of conditional dependence. Given a set of features, the extremal model can be identiﬁed by
dynamic programming on point locations. We compute segment features by placing a box around
some vertices (as in the head), or pairs of vertices (as in the torso and leg). Histogram features are
then computed for base points referred to the box coordinate frame; the histogram is shifted by the
orientation of the box axis (section 3) within the rectiﬁed box. On the far right, a window showing
the color key for our structure learning points; dark green is a foot, green a knee, dark purple the
other foot, purple the other knee, etc. Note that structure learning is capable of ﬁnding distinction of
left legs (green points) and right legs (pink points). On the center right, examples of conﬁgurations
estimated by our conﬁguration estimator after 20 rounds of structure learning to estimate W.

2 Conﬁguration Estimation and Structure Learning

We are presented with a window within which may lie a pedestrian. We would like to be able
to estimate the most likely conﬁguration for any pedestrian present. Our research hypothesis is
that this estimate will improve pedestrian detector perfomance by reducing the amount of noise
the ﬁnal detector must cope with — essentially, the segmentation of the pedestrian is improved
from a window to a (rectiﬁed) ﬁgure. We follow convention (established by [26]) and model the
conﬁguration of a person as a tree model of segments (ﬁgure 2), with a score of segment quality and
a score of segment-segment conﬁguration. We ignore arms because they are small and difﬁcult to
localize. Our conﬁguration estimation procedure will use dynamic programming to extract the best
conﬁguration estimate from a set of scores depending on the location of vertices on the body model.

However, we do not know which features are most effective at estimating segment location; this is a
well established difﬁculty in the literature [16]. Structure learning is a method that uses a series of
correct examples to estimate appropriate weightings of features relative to one another to produce a
score that is effective at estimating conﬁguration [27, 28]. We will write the image as I; coordinates
in the image as x; the coordinates of an estimated conﬁguration as y (which is a stack of 7 point
coordinates); the score for this conﬁguration as WT f(I, x; y) (which is a linear combination of a
collection of scores, each of which depends on the conﬁguration and the image).
For a given image I0 and known W and f, the best conﬁguration estimate is

WT f(I0, x; y)

arg max
y∈y(I0)

and this can be found with dynamic programming for appropriate choice of f and y(I0). There is a
variety of sensible choices of features for identifying body segments, but there is little evidence that
a particular choice of features is best; different choices of W may lead to quite different behaviours.
In particular, we will collect a wide range of features likely to identify segments well in f, and wish
to learn a choice of W that will give good conﬁguration estimates.
We choose a loss function L(yt, yp) that gives the cost of predicting yp when the correct answer
is yt. Write the set of n examples as E, and yp,i as the prediction for the i’th example. Structure
learning must now estimate a W to minimize the hinge loss as in [29]

X

1
2

||W || 2 +

1
n

βiξi

i∈examples

subject to the constraints

∀i ∈ E, WT f(Ii, x; yt,i) + ξi ≥ max

(WT (Ii, x; yp,i) + L(yt,i, yp,i))

yp,i∈y(Ii)

At the minimum, the slack variables ξi happen at the equality of the constraints. Therefore, we can
move the constraints to the objective function, which is:

1
2

||W || 2 +

1
n

i∈examples

βi( max

yp,i∈y(Ii)

(WT (Ii, x; yp,i) + L(yt,i, yp,i)) − WT f(Ii, x; yt,i))

X

Notice that this function is convex, but not differentiable. We follow Ratliff et al. [29], and use
the subgradient method (see [30]) to minimize. In this case, the derivative of the cost function at
an extremal yp,i is a subgradient (but not a gradient, because the cost function is not differentiable
everywhere).

3 Features

There are two sets of features: ﬁrst, those used for estimating conﬁguration of a person from a
window; and second, those used to determine whether a person is present conditioned on the best
estimate of conﬁguration.

3.1 Features for Estimating Conﬁguration

We use a tree structured model, given in ﬁgure 2. The tree is given by the position of seven points,
and encodes the head, torso and legs; arms are excluded because they are small and difﬁcult to
identify, and pedestrians can be identiﬁed without localizing arms. The tree is rooted at hips, and
the arrows give the direction of conditional dependence. We assume that torso, lef tleg, rightleg
are conditionally independent given the root (at the hip).
The feature vector f(I, x; y) contains two types of feature: appearance features encode the appear-
ance of putative segments; and geometric features encode relative and absolute conﬁguration of the
body segments.
Each geometric feature depends on at most three point positions. We use three types of feature.
First, the length of a segment, represented as a 15-dimensional binary vector whose elements encode
whether the segment is longer than each of a set of test segments. Second, the cosine of the angle
between a segment and the vertical axis. Third, the cosine of the angle between pairs of adjoin-
ing segments (except at the lower torso, for complexity reasons); this allows the structure learning
method to prefer straight backs, and reasonable knees.
Appearance features are computed for rectangles constructed from pairs of points adjacent in the
tree. For each rectangle, we compute Histogram of Oriented Gradient (HOG) features, after [9].
These features have a strong record in pedestrian detection, because they can detect the patterns
of orientation associated with characteristic segment outlines (typically, strong vertical orientations
in the frame of the segment for torso and legs; strong horizontal orientations at the shoulders and
head). However, histograms involve spatial pooling; this means that one can have many strong
vertical orientations that do not join up to form a segment boundary. This effect means that HOG
features alone are not particularly effective at estimating conﬁguration.

To counter this effect, we use the local gradient features described by Ke and Sukthankar [12].
To form these features, we concatenate the horizontal and vertical gradients of the patches in the
segment coordinate frame, then normalize and apply PCA to reduce the number of dimensions.
Since we want to model the appearance, we do not align the orientation to a canonical orientation as
in PCA-SIFT. This feature reveals whether the pattern of a body part appears at that location. The
PCA space for each body part is constructed from 500 annotated positive examples.

3.2 Features for Detection

Once the best conﬁguration has been obtained for a window, we must determine whether a person
is present or not. We do this with a support vector machine. Generally, the features that determine
conﬁguration should also be good for determining whether a person is present or not. However, a set
of HOG features for the whole image window has been shown to be good at pedestrian detection [9].
The support vector machine should be able to distinguish between good and bad features, so it is
natural to concatenate the conﬁguration features described above with a set of HOG features. We
ﬁnd it helpful to reduce the dimension of the set of HOG features to 500, using principal components.
We ﬁnd that these whole window features help recover from incorrect structure predictions. These
combined features are used in training the SVM classiﬁer and in detection as well.

4 Results

Dataset: We use INRIA Person, consisting of 2416 pedestrian images (1208 images with their left-
right reﬂections) and 1218 background images for training. For testing, there are 1126 pedestrian
images (563 images with their left-right reﬂections) and 453 background images.
Training structure learning: we manually annotate 500 selected pedestrian images in the training
set examples. We use all 500 annotated examples to build the PCA spaces for each body segment.
In training, each example is learned to update the weight vector. The order of selecting examples in
each round is randomly drawn based on the differences of their scores on the predictions and their
scores on the true targets. For each round, we choose 300 examples drawn (since structure learning
is expensive). We have trained the structure learning on 10 rounds and 20 rounds for comparisons.
Quality of conﬁguration estimates: Conﬁguration estimates look good (ﬁgure 2). A persistent
nuisance associated with pictorial structure models of people is the tendency of such models to
place legs on top of one another. This occurs if one uses only appearance and relative geometric
features. However, our results suggest that if one uses absolute conﬁguration features as well as
different appearance features for left and right legs (implicit in the structure learning procedure), the
left and right legs are identiﬁed correctly. The conditional independence assumption (which means
we cannot use the angle between the legs as a feature) does not appear to cause problems, perhaps
because absolute conﬁguration features are sufﬁcient.
Bootstrapping the SVM: The ﬁnal SVM is bootstrapped, as in [9]. We use 2146 pedestrian images
with 2756 window images extracted from 1218 background images. We apply the learned structure
model to generate on these 2416 positive examples and 2756 negative examples to train the initial
SVM classiﬁer. We then use this classiﬁer to scan over 1218 background images with step side of
32 pixels and ﬁnd hard examples (including false positives and true negatives of low conﬁdence by
using LibSVM [31] with probability option). These negatives yield a bootstrap training set for the
ﬁnal SVM classiﬁer. This bootstrap learning helps to reduce the false alarm signiﬁcantly.
Testing: We test on 1126 positive images and scan 64x128 image windows over 453 negative test
images, stepping by 16 pixels, a total of 182, 934 negative windows.
Scanning rate and comparison: Pedestrian detection systems work by scanning image windows,
and presenting each window to a detector. Dalal and Triggs established a methodology for evaluating
pedestrian detectors, which is now quite widely used. Their dataset offers a set of positive windows
(where pedestrians are centered), and a set of negative images. The negative images produce a
pool of negative windows, and the detector is evaluated on detect rate on the positive windows
and the false positive per window (FPPW) rate on the negative windows. This strategy — which
evaluates the detector, rather than the combination of detection and scanning — is appropriate for
comparing systems that scan image windows at approximately the same high rate. Current systems

do so, because the detectors require nearly centered pedestrians. However, the important practical
parameter for evaluating a system is the false positive per image (FPPI) rate. If one has a detector
that does not require a pedestrian to be centered in the image window, then one can obtain the same
detect rate while scanning fewer image windows. In turn, the FPPI rate will go down even if the
FPPW rate is ﬁxed. To date, this issue has not arisen, because pedestrian detectors have required
pedestrians to be centered.

Figure 3. Left: a comparison of our method with the best detector of Dalal and Triggs, and the
detector of Sabzmaydani and Mori, on the basis of FPPW rate. This comparison ignores the fact
that we can look at fewer image windows without loss of system sensitivity. We show ROC’s for
a conﬁguration estimator trained on 10 (blue) and 20 (red) rounds of structure learning. With 20
rounds of structure learning, our detector easily outperforms that of Dalal and Triggs. Note that
at high speciﬁcity, our detector is slightly more sensitive than that of Sabzmaydani and Mori, too.
Right: a comparison of our method with the best detector of Dalal and Triggs, and the detector of
Sabzmaydani and Mori, on the basis of FPPI rate. This comparison takes into account the fact that
we can look at fewer image windows (by a factor of four). However, scanning by larger steps might
cause a loss of sensitivity. We test this with a procedure of replicating positive examples, described
in the text, and show the results of four runs. The low variance in the detect rate under this procedure
shows that our detector is highly insensitive to the conﬁguration of the pedestrian within a window. If
one evaluates on the basis of false positives per image — which is likely the most important practical
parameter — our system easily outperforms the state of the art.

4.1 The Effect of Conﬁguration Estimates

Figure 3 compares our detector with that of Dalal and Triggs, and of Sabzmeydani and Mori on the
basis of detect and FPPW rates. We plot detect rate against FPPW rate for the three detectors. For
this plot, note that at low FPPW rate our method is somewhat more sensitive than that of Sabzmey-
dani and Mori, but has no advantage at higher FPPW rates.

However, this does not tell the whole story. We scan images at steps of 16 pixels (rather than 8
pixels for Dalal and Triggs and Sabzmeydani and Mori). This means that we scan four times fewer
windows than they do. If we can establish that the detect rate is not signiﬁcantly affected by big
offsets in pedestrian position, then we expect a large advantage in FPPI rate.

We evaluate the effect on the detect rate of scanning by large steps by a process of sampling. Each
positive example is replaced by a total of 256 replicates, obtained by offsetting the image window by
steps in the range -7 to 8 in x and y (ﬁgure 4). We now conduct multiple evaluation runs. For each,
we select one replicate of each positive example uniformly at random. For each run, we evaluate
the detect rate. A tendency of the detector to require centered pedestrians would appear as variance
in the reported detect rate. The FPPI rate of the detector is not affected by this procedure, which
evaluates only the spatial tuning of the detector.

Figure 4. In color, original positive examples from the INRIA test set; next to each, are three of the
replicates we use to determine the effect on our detection system of scanning relatively few windows,
or, equivalently, the effect on our detector of not having a pedestrian centered in the window. See
section 4.1, and ﬁgure 3.

Figure 3 compares system performance, combining detect and scanning rates, by plotting detect rate
against FPPI rate. We show four evaluation runs for our system; there is no evidence of substantial
variance in detect rate. Our system shows a very substantial increase in detect rate at ﬁxed FPPI rate.

5 Discussion

There is a difﬁculty with the evaluation methodology for pedestrian detection established by Dalal
and Triggs (and widely followed). A pedestrian detector that tests windows cannot ﬁnd more pedes-
trians than there are windows. This does not usually affect the interpretation of precision and recall
statistics because the windows are closely packed. However, in our method, because a pedestrian
need not be centered in the window to be detected, the windows need not be closely packed, and
there is a possibility of undercounting pedestrians who stand too close together. We believe that this
does not occur in our current method, because our window spacing is narrow relative to the width of
a pedestrian.

Part representations appear to be a natural approach to identifying people. However, to our knowl-
edge, there is no clear evidence to date that shows compelling advantages to using such an approach
(e.g. the review in [16]). We believe our method does so. Conﬁguration estimates appear to have two
important advantages. First, they result in a detector that is relatively insensitive to the placement of
a pedestrian in an image window, meaning one can look at fewer image windows to obtain the same
detect rate, with consequent advantages to the rate at which the system produces false positives. This
is probably the dominant advantage. Second, conﬁguration estimates appear to be a signiﬁcant help
at high speciﬁcity settings (notice that our method beats all others on the FPPW criterion at very
low FPPW rates). This is most likely because the process of estimating conﬁgurations focuses the
detector on important image features (rather than pooling information over space). The result would
be that, when there is low contrast or a strange body conﬁguration, the detector can use a somewhat
lower detection threshold for the same FPPW rate. Figure 1 shows human conﬁgurations detected
by our method but not by our implementation of Dalal and Triggs; notice the predominance of either
strange body conﬁgurations or low contrast. Structure learning is an attractive method to determine
which features are discriminative in conﬁguration estimation, and it produces good conﬁguration
estimates in complex images. Future work will include: tying W components for legs; evaluating
arm detection; and formulating strategies to employ structure learning for detecting other objects.

References
[1] D.M. Gavrila. Sensor-based pedestrian protection. Intelligent Transportation Systems, pages 77–81, 2001.
[2] C. Papageorgiou and T. Poggio. A trainable system for object detection. Int. J. Computer Vision, 38(1):15–

33, June 2000.

[3] C.P. Papageorgiou and T. Poggio. A pattern classiﬁcation approach to dynamical object detection. In Int.

Conf. on Computer Vision, pages 1223–1228, 1999.

[4] L. Zhao and C.E. Thorpe. Stereo- and neural network-based pedestrian detection. Intelligent Transporta-

tion Systems, 1(3):148–154, September 2000.

[5] D. Gavrila. Pedestrian detection from a moving vehicle. In European Conference on Computer Vision,

pages II: 37–49, 2000.

[6] Y. Wu, T. Yu, and G. Hua. A statistical ﬁeld model for pedestrian detection. In IEEE Conf. on Computer

Vision and Pattern Recognition, pages I: 1023–1030, 2005.

[7] P. Viola, M.J. Jones, and D. Snow. Detecting pedestrians using patterns of motion and appearance. Int. J.

Computer Vision, 63(2):153–161, July 2005.

[8] M. Dimitrijevic, V. Lepetit, and P. Fua. Human body pose recognition using spatio-temporal templates.

In ICCV workshop on Modeling People and Human Interaction, 2005.

[9] N. Dalal and B. Triggs. Histograms of oriented gradients for human detection. In IEEE Conf. on Computer

Vision and Pattern Recognition, pages I: 886–893, 2005.

[10] D.G. Lowe. Distinctive image features from scale-invariant keypoints. Int. J. Computer Vision, 60(2):91–

110, November 2004.

[11] A. Mohan, C.P. Papageorgiou, and T. Poggio. Example-based object detection in images by components.

IEEE T. Pattern Analysis and Machine Intelligence, 23(4):349–361, April 2001.

[12] Y. Ke and R. Sukthankar. Pca-sift: a more distinctive representation for local image descriptors. In IEEE

Conf. on Computer Vision and Pattern Recognition, pages II: 506–513, 2004.

[13] K. Mikolajczyk and C. Schmid. A performance evaluation of local descriptors. IEEE T. Pattern Analysis

and Machine Intelligence, 2004. accepted.

[14] Serge Belongie, Jitendra Malik, and Jan Puzicha. Shape matching and object recognition using shape

contexts. IEEE T. Pattern Analysis and Machine Intelligence, 24(4):509–522, 2002.

[15] P. Sabzmeydani and G. Mori. Detecting pedestrians by learning shapelet features. In CVPR, 2007.
[16] D.A. Forsyth, O.Arikan, L. Ikemoto, J. O’Brien, and D. Ramanan. Computational studies in human

motion 1: Tracking and animation. Foundations and Trends in Computer Vision, 2006. In press.

[17] P.F. Felzenszwalb and D.P. Huttenlocher. Pictorial structures for object recognition.

Vision, 61(1):55–79, January 2005.

Int. J. Computer

[18] M. P. Kumar, P. H. S. Torr, and A. Zisserman. Extending pictorial structures for object recognition. In

Proceedings of the British Machine Vision Conference, 2004.

[19] Deva Ramanan, D.A. Forsyth, and A. Zisserman. Strike a pose: Tracking people by ﬁnding stylized

poses. In IEEE Conf. on Computer Vision and Pattern Recognition, 2005.

[20] D. Ramanan and D.A. Forsyth. Using temporal coherence to build models of animals. In Proc. ICCV,

2003.

[21] D. Ramanan. Learning to parse images of articulated objects. In Proc. NIPS, 2006.
[22] R. Ronfard, C. Schmid, and B. Triggs. Learning to parse pictures of people. In European Conference on

Computer Vision, page IV: 700 ff., 2002.

[23] K. Mikolajczyk, C. Schmid, and A. Zisserman. Human detection based on a probabilistic assembly of

robust part detectors. In European Conference on Computer Vision, pages Vol I: 69–82, 2004.

[24] A. Micilotta, E. Ong, and R. Bowden. Detection and tracking of humans by probabilistic body part

assembly. In British Machine Vision Conference, volume 1, pages 429–438, 2005.

[25] B. Leibe, E. Seemann, and B. Schiele. Pedestrian detection in crowded scenes. In IEEE Conf. on Com-

puter Vision and Pattern Recognition, pages I: 878–885, 2005.

[26] Pedro F. Felzenszwalb and Daniel P. Huttenlocher. Efﬁcient matching of pictorial structures. In IEEE

Conf. on Computer Vision and Pattern Recognition, 2000.

[27] B. Taskar. Learning Structured Prediction Models: A Large Margin Approach. PhD thesis, Stanford

University, 2004.

[28] B. Taskar, S. Lacoste-Julien, and M. Jordan. Structured prediction via the extragradient method. In Neural

Information Processing Systems Conference, 2005.

[29] N. Ratliff, J. A. Bagnell, and M. Zinkevich. Subgradient methods for maximum margin structured learn-

ing. In ICML 2006 Workshop on Learning in Structured Output Spaces, 2006.

[30] N.Z. Shor. Minimization Methods for Non-Differentiable Functions and Applications. 1985.
[31] Chih-Chung Chang and Chih-Jen Lin. LIBSVM: a library for support vector machines, 2001.

"
747,2007,A General Boosting Method and its Application to Learning Ranking Functions for Web Search,"We present a general boosting method extending functional gradient boosting to optimize complex loss functions that are encountered in many machine learning problems. Our approach is based on optimization of quadratic upper bounds of the loss functions which allows us to present a rigorous convergence analysis of the algorithm. More importantly, this general framework enables us to use a standard regression base learner such as decision trees for fitting any loss function. We illustrate an application of the proposed method in learning ranking functions for Web search by combining both preference data and labeled data for training. We present experimental results for Web search using data from a commercial search engine that show significant improvements of our proposed methods over some existing methods.","A General Boosting Method and its Application to

Learning Ranking Functions for Web Search

Zhaohui Zhengy Hongyuan Zha? Tong Zhangy Olivier Chapelley Keke Cheny Gordon Suny

yYahoo! Inc.

701 First Avene

Sunnyvale, CA 94089

fzhaohui,tzhang,chap,kchen,gzsung@yahoo-inc.com

?College of Computing

Georgia Institute of Technology

Atlanta, GA 30032

zha@cc.gatech.edu

Abstract

We present a general boosting method extending functional gradient boosting to
optimize complex loss functions that are encountered in many machine learning
problems. Our approach is based on optimization of quadratic upper bounds of the
loss functions which allows us to present a rigorous convergence analysis of the
algorithm. More importantly, this general framework enables us to use a standard
regression base learner such as single regression tree for £tting any loss function.
We illustrate an application of the proposed method in learning ranking functions
for Web search by combining both preference data and labeled data for training.
We present experimental results for Web search using data from a commercial
search engine that show signi£cant improvements of our proposed methods over
some existing methods.

1 Introduction

There has been much interest in developing machine learning methods involving complex loss func-
tions beyond those used in regression and classi£cation problems [13]. Many methods have been
proposed dealing with a wide range of problems including ranking problems, learning conditional
random £elds and other structured learning problems [1, 3, 4, 5, 6, 7, 11, 13]. In this paper we
propose a boosting framework that can handle a wide variety of complex loss functions. The pro-
posed method uses a regression black box to optimize a general loss function based on quadratic
upper bounds, and it also allows us to present a rigorous convergence analysis of the method. Our
approach extends the gradient boosting approach proposed in [8] but can handle substantially more
complex loss functions arising from a variety of machine learning problems.

As an interesting and important application of the general boosting framework we apply it to the
problem of learning ranking functions for Web search. Speci£cally, we want to rank a set of docu-
ments according to their relevance to a given query. We adopt the following framework: we extract
a set of features x for each query-document pair, and learn a function h(x) so that we can rank the
documents using the values h(x), say x with larger h(x) values are ranked higher. We call such
a function h(x) a ranking function. In Web search, we can identify two types of training data for
learning a ranking function: 1) preference data indicating a document is more relevant than another
with respect to a query [11, 12]; and 2) labeled data where documents are assigned ordinal labels
representing degree of relevancy. In general, we will have both preference data and labeled data for

1

training a ranking function for Web search, leading to a complex loss function that can be handled
by our proposed general boosting method which we now describe.

2 A General Boosting Method

We consider the following general optimization problem:

(1)
where h denotes a prediction function which we are interested in learning from the data, H is a pre-
chosen function class, and R(h) is a risk functional with respect to h. We consider the following
form of the risk functional R:

h2HR(h);

^h = arg min

n

R(h) =

1
n

Xi=1

`i(h(xi;1);¢¢¢ ; h(xi;mi ); yi);

(2)

where `i(h1; : : : ; hmi ; y) is a loss function with respect to the £rst m i arguments h1; : : : ; hmi.
For example, each function `i can be a single variable function (mi = 1) such as in regression:
`i(h; y) = (h ¡ y)2; or a two-variable function (mi = 2), such as those in ranking based on pair-
wise comparisons: `i(h1; h2; y) = max(0; 1¡ y(h1 ¡ h2))2, where y 2 f§1g indicates whether h1
is preferred to h2 or not; or it can be a multi-variable function as used in some structured prediction
problems: `i(h1; : : : ; hmi ; y) = supz –(y; z) + ˆ(h; z) ¡ ˆ(h; y), where – is a loss function [13].
Assume we do not have a general solver for the optimization problem (1), but we have a learning
algorithm A which we refer to as regression weak learner. Given any set of data points X =
[x1; : : : ; xk], with corresponding target values R = [r1; : : : ; rk], weights W = [w1; : : : ; wk], and
tolerance † > 0, the regression weak learner A produces a function ^g = A(W; X; R; †) 2 C such
that

k

k

Xj=1

wj(^g(xj) ¡ rj)2 • min

g2C

Xj=1

wj(g(xj) ¡ rj)2 + †:

(3)

Our goal is to use this weak learner A to solve the original optimization problem (1). Here H =
span(C), i.e., h 2 H can be expressed as h(x) = Pj ajhj(x) with hj 2 C.

Friedman [8] proposed a solution when the loss function in (2) can be expressed as

R(h) =

n

Xi=1

`i(h(xi));

(4)

which he named as gradient boosting. The idea is to estimate the gradient r`i(h(xi)) using regres-
sion at each step with uniform weighting, and update. However, there is no convergence proof.

Following his work, we consider an extension that is more principly motivated, for which a conver-
gence analysis can be obtained. We £rst rewrite (2) in the more general form:

R(h) = R(h(x1); : : : ; h(xN ));

(5)

where N • P mi.1 Note that R depends on h only through the function values h(xi) and from
now on we identify the function h with the vector [h(xi)]. Also the function R is considered to be a
function of N variables.
Our main observation is that for twice differentiable risk functional R, at each tentative solution hk,
we can expand R(h) around hk using Taylor expansion as
R(hk + g) = R(hk) + rR(hk)T g +

gTr2R(h0)g;

1
2

where h0 lies between hk and hk + g. The right hand side is almost quadratic, and we can then
replace it by a quadratic upper-bound

R(hk + g) • Rk(g) = R(hk) + rR(hk)T g +

1
2

gT W g;

(6)

1We consider that all xi are different, but some of the xi;mi in (2) might have been identical, hence the

inequality.

2

where W is a diagonal matrix upper bounding the Hessian between hk and hk + g. If we de£ne

rj = ¡[rR(hk)]j=wj, then 8g 2 C;Pj wj(g(xj) ¡ rj)2 is equal to the above quadratic form
(up to a constant). So g can be found by calling the regression weak learner A. Since at each
step we try to minimize an upper bound Rk of R, if we let the minimum be gk, it is clear that
R(hk + gk) • Rk(gk) • R(hk). This means that by optimizing with respect to the problem Rk
that can be handled by A, we also make progress with respect to optimizing R. The algorithm based
on this idea is listed in Algorithm 1 for the loss function in (5).

Convergence analysis of this algorithm can be established using the idea summarized above; see
details in appendix. However, in partice, instead of the quadratic upper bound (which has a theo-
retical garantee easier to derive), one may also consider minimizing an approximation to the Taylor
expansion, which would be closer to a Newton type method.

Algorithm 1 Greedy Algorithm with Quadratic Approximation

Input: X = [x‘]‘=1;:::;N
let h0 = 0
for k = 0; 1; 2; : : :

let W = [w‘]‘=1;:::;N , with either
w‘ = @2R=@hk(x‘)2 or
W global diagonal upper bound on the Hessian
let R = [r‘]‘=1;:::;N , where r‘ = w¡1
‘ @R=@hk(x‘)
pick †k ‚ 0
let gk = A(W; X; R; †k)
pick step-size sk ‚ 0, typically by line search on R
let hk+1 = hk + skgk
end

% Newton-type method with diagonal Hessian
% Upper-bound minimization

The main conceptual difference between our view and that of Friedman is that he views regression
as a “reasonable” approximation to the £rst order gradient rR, while our work views it as a natural
consequence of second order approximation of the objective function (in which the quadratic term
serve as an upper bound of the Hessian either locally or globally). This leads to algorithmic differ-
ence. In our approach, a good choice of the second order upper bound (leading to tighter bound)
may require non-uniform weights W . This is inline with earlier boosting work in which sample-
reweighting was a central idea. In our framework, the reweighting naturally occurs when we choose
a tight second order approximation. Different reweighting can affect the rate of convergence in our
analysis. The other main difference with Friedman is that he only considered objective functions of
the form (4); we propose a natural extension to the ones of the form (5).

3 Learning Ranking Functions

We now apply Algorithm 1 to the problem of learning ranking functions. We use preference data as
well as labeled data for training the ranking function. For preference data, we use x ´ y to mean
that x is preferred over y or x should be ranked higher than y, where x and y are the feature vectors
for corresponding items to be ranked. We denote the set of available preferences as S = fxi ´
yi; i = 1; : : : ; Ng: In addition to the preference data, there are also labeled data, L = f(zi; li); i =
1; : : : ; ng; where zi is the feature of an item and li is the corresponding numerically coded label.2
We formulate the ranking problem as computing a ranking function h 2 H, such that h satis£es as
much as possible the set of preferences, i.e., h(xi) ‚ h(yi), if xi ´ yi; i = 1; : : : ; N; while at the
same time h(zi) matches the label li in a sense to be detailed below.

2Some may argue that, absolute relevance judgments can also be converted to relative relevance judgments.
For example, for a query, suppose we have three documents d1; d2 and d3 labeled as perfect, good, and bad,
respectively. We can obtain the following relative relevance judgments: d1 is preferred over d2, d1 is preferred
over d3 and d2 is preferred over d3. However, it is often the case in Web search that for many queries there
only exist documents with a single label and for such kind of queries, no preference data can be constructed.

3

THE OBJECTIVE FUNCTION. We use the following objective function to measure the empirical risk
of a ranking function h,

R(h) =

w
2

N

Xi=1

(maxf0; h(yi) ¡ h(xi) + ¿g)2 +

1 ¡ w

2

n

Xi=1

(li ¡ h(zi))2:

The objective function consists of two parts: 1) for the preference data part, we introduce a margin
parameter ¿ and would like to enforce that h(xi) ‚ h(yi) + ¿ ; if not, the difference is quadratically
penalized; and 2) for the labeled data part, we simply minimize the squared errors. The parameter
w is the relative weight for the preference data and could typically be found by cross-validation.
The optimization problem we seek to solve is h⁄ = argmin h2H R(h); where H is some given
function class. Note that R depends only on the values h(xi); h(yi); h(zi) and we can optimize it
using the general boosting framework discussed in section 2.
QUADRATIC APPROXIMATION. To this end consider the quadratic approximation (6) for R(h).
For simplicity let us assume that each feature vector xi, yi and zi only appears in S and L once,
otherwise we need to compute appropriately formed averages. We consider

h(xi); h(yi);

i = 1; : : : ; N;

h(zi);

i = 1; : : : ; n

as the unknowns, and compute the gradient of R(h) with respect to those unknowns. The com-
ponents of the negative gradient corresponding to h(zi) is just li ¡ h(zi): The components of the
negative gradient corresponding to h(xi) and h(yi), respectively, are

maxf0; h(yi) ¡ h(xi) + ¿g; ¡ maxf0; h(yi) ¡ h(xi) + ¿g:

Both of the above equal to zero when h(xi)¡h(yi) ‚ ¿ . For the second-order term, it can be readily
veri£ed that the Hessian of R(h) is block-diagonal with 2-by-2 blocks corresponding to h(x i) and
h(yi) and 1-by-1 blocks for h(zi). In particular, if we evaluate the Hessian at h, the 2-by-2 block
equals to

• 1 ¡1
¡1

1 ‚ ;

• 0

0

0

0 ‚ ;

for xi ´ yi with h(xi) ¡ h(yi) < ¿ and h(xi) ¡ h(yi) ‚ ¿ , respectively. We can upper bound the
£rst matrix by the diagonal matrix diag(2; 2) leading to a quadratic upper bound. We summarize
the above derivations in the following algorithm.

Algorithm 2 Boosted Ranking using Successive Quadratic Approximation (QBRank)

Start with an initial guess h0, for m = 1; 2; : : : ,
1) we construct a training set for £tting gm(x) by adding the following for each hxi; yii 2 S,
(xi; maxf0; hm¡1(yi) ¡ hm¡1(xi) + ¿g); (yi;¡ maxf0; hm¡1(yi) ¡ hm¡1(xi) + ¿g);
and
f(zi; li ¡ hm¡1(zi));
The £tting of gm(x) is done by using a base regressor with the above training set; We weigh
the above preference data by w and the labeled data by 1 ¡ w respectively.
2) forming hm = hm¡1 + ·smgm(x);
where sm is found by line search to minimize the objective function. · is a shrinkage factor.

i = 1; : : : ; ng:

The shrinkage factor · by default is 1, but Friedman [8] reported better results (coming from better
regularization) by taking · < 1. In general, we choose · and w by cross-validation. ¿ could be the
degree of preference if that information is available, e.g., the absolute grade difference between each
prefernce if it is converted from labeled data. Otherwise, we simply set it to be 1.0. When there is
no preference data and the weak regression learner produces a regression tree, QBrank is identical
to Gradient Boosting Trees (GBT) as proposed in [8].
REMARK. An xi can appear multiple times in Step 1), in this case we use the average gradient
values as the target value for each distinct xi.

4

4 Experiment Results

We carried out several experiments illustrating the properties and effectiveness of QBrank using
combined preference data and labeled data in the context of learning ranking functions for Web
search [3]. We also compared its performance with QBrank using preference data only and several
existing algorithms such as Gradient Boosting Trees [8] and RankSVM [11, 12]. RankSVM is a
preference learning method which learns pair-wise preferences based on SVM approach.

DATA COLLECTION. We £rst describe how the data used in the experiments are collected. For
each query-document pair we extracted a set of features to form a feature vector. which consists of
three parts, x = [xQ; xD; xQD]; where 1) the query-feature vector xQ comprises features dependent
on the query q only and have constant values across all the documents d in the document set, for
example, the number of terms in the query, whether or not the query is a person name, etc.; 2)
the document-feature vector xD comprises features dependent on the document d only and have
constant values across all the queries q in the query set, for example, the number of inbound links
pointing to the document, the amount of anchor-texts in bytes for the document, and the language
identity of the document, etc.; and 3) the query-document feature vector xQD which comprises
features dependent on the relation of the query q with respect to the document d, for example, the
number of times each term in the query q appears in the document d, the number of times each term
in the query q appears in the anchor-texts of the document d, etc.
We sampled a set of queries from the query logs of a commercial search engine and generated
a certain number of query-document pairs for each of the queries. A £ve-level numerical grade
(0; 1; 2; 3; 4) is assigned to each query-document pair based on the degree of relevance. In total
we have 4,898 queries and 105,243 query-document pairs. We split the data into three subsets as
follows: 1) we extract all the queries which have documents with a single label. The set of feature
vectors and the corresponding labels form training set L1, which contains around 2000 queries
giving rise to 20,000 query-document pairs. (Some single-labeled data are from editorial database,
where each query has a few ideal results with the same label. Other are bad ranking cases submitted
internally and all the documents for a query are labeled as bad. As we will see those type of single-
labeled data are very useful for learning ranking functions); and 2) we then randomly split the
remaining data by queries, and construct a training set L2 containing about 1300 queries and 40,000
query-document pairs and a test set L3 with about 1400 queries and 44,000 query-document pairs.
We use L2 or L3 to generate a set of preference data as follows: given a query q and two documents
dx and dy. Let the feature vectors for (q; dx) and (q; dy) be x and y, respectively. If dx has a higher
grade than dy, we include the preference x ´ y while if dy has a higher grade than dx, we include
the preference y ´ x. For each query, we consider all pairs of documents within the search results
for that query except those with equal grades. This way, we generate around 500,000 preference
pairs in total. We denote the preference data as P2 and P3 corresponding to L2 and L3, respectively.
EVALUATION METRICS. The output of QBrank is a ranking function h which is used to rank the
documents x according to h(x). Therefore, document x is ranked higher than y by the ranking
function h if h(x) > h(y), and we call this the predicted preference. We propose the following two
metrics to evaluate the performance of a ranking function with respect to a given set of preferences
which we considered as the true preferences.

1) Precision at K%: for two documents x and y (with respect to the same query), it is reasonable to
assume that it is easy to compare x and y if jh(x) ¡ h(y)j is large, and x and y should have about
the same rank if h(x) is close to h(y). Base on this, we sort all the document pairs hx; yi according
to jh(x) ¡ h(y)j. We call precision at K%, the fraction of non-contradicting pairs in the top K% of
the sorted list. Precision at 100% can be considered as an overall performance measure of a ranking
function.

2) Discounted Cumulative Gain (DCG): DCG has been widely used to assess relevance in the context
of search engines [10]. For a ranked list of N documents (N is set to be 5 in our experiments), we
i=1 Gi= log2 (i + 1), where Gi represents the
weights assigned to the label of the document at position i. Higher degree of relevance corresponds
to higher value of the weight.

use the following variation of DCG, DCGN = PN

PARAMETERS. There are three parameters in QBrank: ¿ , ·, and w. In our experiments, ¿ is the
absolute grade difference between each pair hxi; yii. We set · to be 0.05, and w to be 0.5 in our

5

Table 1: Precision at K% for QBrank, GBT, and RankSVM

QBrank
%K
0.9446
10%
0.903
20%
0.8611
30%
0.8246
40%
0.7938
50%
0.7673
60%
0.7435
70%
0.7218
80%
90%
0.7015
100% 0.6834

GBT RankSVM
0.8524
0.8152
0.7839
0.7578
0.7357
0.7151
0.6957
0.6779
0.6615
0.6465

0.9328
0.8939
0.8557
0.8199
0.7899
0.7637
0.7399
0.7176
0.6977
0.6803

experiments. For a fair comparsion, we used single regression tree with 20 leaf nodes as the base
regressor of both GBT and QBrank in our experiments. · and number of leaf nodes were tuned for
GBT through cross validation. We did not retune them for QBrank.

EXPERIMENTS AND RESULTS. We are interested in the following questions: 1) How does GBT
using labeled data L2 compare with QBrank or RankSVM using the preference data extracted from
the same labeled data: P2? and 2) Is it useful to include single-labeled data L1 in GBT and QBrank?
To this end, we considered the following six experiments for comparison: 1) GBT using L1, 2) GBT
using L2, 3) GBT using L1 [ L2, 4) RankSVM using P2, 5) QBrank using P2, and 6) QBrank using
P2 [ L1.
Table 1 presents the precision at K% on data P3 for the ranking function learned from GBT with
labeled training data L2, and QBrank and RankSVM with the corresponding preference data P2.
This shows that QBrank outperforms both GBT and RankSVM with respect to the precision at K%
metric.
The DCG-5 for RankSVM using P2 is 6.181 while that for the other £ve methods are shown in
Figure 1, from which we can see it is useful to include single-labeled data in GBT training. In case
of preference learning, no preference pairs could be extracted from single labeled data. Therefore,
existing methods such as RankSVM, RankNet and RankBoost that are formulated for preference
data only can not take advantage of such data. The QBrank framework can combine preference data
and labeled data in a natural way. From Figure 1, we can see QBrank using combined preference
data and labeled data outperforms both QBrank and RankSVM using preference data only, which
indicates that singled labeled data are also useful to QBrank training. Another observation is that
GBT using labeled data is signi£cantly worse than QBrank using preference data extracted from the
same labeled data3. The clear convergence trend of QBrank is also demonstrated in Figure 1. Notice
that, we excluded all tied data (pairs of documents with the same grades) when converting preference
data from the absolute relevance judgments, which can be signi£cant information loss, for example
of x1 > x2, and x3 > x4.
If we know x2 ties with x3, then we can have the whole ranking
x1 > fx2; x3g > x4. Including tied data could further improve performance of both GBrank and
QBrank.

5 Conclusions and Future Work

We proposed a general boosting method for optimizing complex loss functions. We also applied
the general framework to the problem of learning ranking functions. Experimental results using a
commercial search engine data show that our approach leads to signi£cant improvements. In future
work, 1) we will add regularization to the preference part in the objective function; 2) we plan
to apply our general boosting method to other structured learning problems; and 3) we will also
explore other applications where both preference and labeled data are available for training ranking
functions.

3a 1% dcg gain is considered sign£cant on this data set for commercial search engines.

6

6.9

6.85

6.8

6.75

6.7

6.65

5
-
G
C
D

6.6

50

DCG-5 v. Iterations

100

150

200

250

300

350

400

Iterations (Number of trees)

GBT using L2

GBT using L1

GBT using L1+L2

QBrank using P2

QBrank using P2+L1

Figure 1: DCG v. Iterations. Notice that DCG for RankSVM using P2 is 6.181.

Appendix: Convergence results

n P‘ w‘g(x‘)2.

We introduce a few de£nitions.
De£nition 1 C is scale-invariant if 8g 2 C and ﬁ 2 R, ﬁg 2 C.
De£nition 2 kgkW;X = q 1
De£nition 3 Let h 2 span(C), then khkW;X = inf nPj jﬁjj : h = Pj ﬁjgj=kgjkW;X ; gj 2 Co.
De£nition 4 Let R(h) be a function of h, an global upper bound M of its Hessian with respect to
[W; X] satisfy: 8h; ﬂ and g: R(h + ﬂg) • R(h) + ﬂrR(h)T g + ﬂ 2
Although we only consider global upper bounds, it is easy to see that results with respect to local
upper bounds can also be established.

2 Mkgk2

W;X.

R(hk+1) • R(„h)+

max(0; R(0)¡R(„h))+inf

i =2), then

k„hkW;X

k„hkW;X + ak

Theorem 1 Consider Algorithm 1, where R is a convex function of h. Let M be an upper bound of
the Hessian of R. Assume that C is scale-invariant. Let „h 2 span(C). Let „sk = skkgkkW;X be the
normalized step-size, aj = Pj

i=0 „si, and bj = Pi‚j(„sip2†i + M „s2
j •(b0 ¡ bj+1)
+ (bj+1 ¡ bk+1)‚ :
k + „skp†k) < 1, then limk!1 R(hk) =
If we choose „sk ‚ 0 such that Pk „sk = 1 and Pk(„s2
inf „h2span(C) R(„h), and the rate of convergence compared to any target „h 2 span(C) only depends
on k„hkW;X, and the sequences fajg and fbjg.
The proof is a systematic application of the idea outlined earlier and will be detailed in a sep-
arate publication.
In particu-

In practice, one often set the step size to be a small constant.

lar, for for some £xed s > 0, we can choose p2†i • M s2=2, and skkgkkW;X = s2 when
R(hk + „sk ~gk) • R(hk) („sk = 0 otherwise). Theorem 1 gives the following bound when
k ‚ qk„hkW;X max(0; R(0) ¡ R(„h))=M s¡3,

k„hkW;X + aj
k„hkW;X + ak

R(hk+1) • R(„h) + 2sqmax(0; R(0) ¡ R(„h))k„hkW;X M + M s4:

7

The convergence results show that in order to have a risk not much worse than any target function
„h 2 span(C), the approximation function hk does not need to be very complex when the complexity
is measured by its 1-norm. It is also important to see that quantities appearing in the generalization
analysis do not depend on the number of samples. These results imply that statistically, Algorithm 1
(with small step-size) has an implicit regularization effect that prevents the procedure from over£ting
the data. Standard empirical process techniques can then be applied to obtain generalization bounds
for Algorithm 1.

References

[1] BALCAN N., BEYGELZIMER A., LANGFORD J., AND SORKIN G. Robust Reductions from Ranking to

Classi£cation, manuscript, 2007.

[2] BERTSEKAS D. Nonlinear programming. Athena Scienti£c, second edition, 1999.
[3] BURGES, C., SHAKED, T., RENSHAW, E., LAZIER, A., DEEDS, M., HAMILTON, N., AND HULLEN-
DER, G. Learning to rank using gradient descent. Proc. of Intl. Conf. on Machine Learning (ICML)
(2005).

[4] DIETTERICH, T. G., ASHENFELTER, A., BULATOV, Y. Training Conditional Random Fields via Gradi-

ent Tree Boosting Proc. of Intl. Conf. on Machine Learning (ICML) (2004).

[5] CLEMENCON S., LUGOSI G., AND VAYATIS N. Ranking and scoring using empirical risk minimization.

Proc. of COLT (2005).

[6] COHEN, W. W., SCHAPIRE, R. E., AND SINGER, Y. Learning to order things. Journal of Arti£cial

Intelligence Research, Neural Computation, 13, 14431472 (1999).

[7] FREUND, Y., IYER, R., SCHAPIRE, R. E., AND SINGER, Y. An ef£cient boosting algorithm for com-

bining preferences. Journal of Machine Learning Research 4 (2003), 933–969.

[8] FRIEDMAN, J. H. Greedy function approximation: A gradient boosting machine. Annals of Statistics 29,

5 (2001), 1189–1232.

[9] HERBRICH, R., GRAEPEL, T., AND OBERMAYER, K. Large margin rank boundaries for ordinal regres-

sion. 115–132.

[10] JARVELIN, K., AND KEKALAINEN, J. Ir evaluation methods for retrieving highly relevant documents.

Proc. of ACM SIGIR Conference (2000).

[11] JOACHIMS, T. Optimizing search engines using clickthrough data. Proc. of ACM SIGKDD Conference

(2002).

[12] JOACHIMS, T., GRANKA, L., PAN, B., AND GAY, G. Accurately interpreting clickthough data as

implicit feedback. Proc. of ACM SIGIR Conference (2005).

[13] TSOCHANTARIDIS, I., JOACHIMS, T., HOFMANN, T., AND ALTUN, Y. Large margin methods for
structured and interdependent output variables. Journal of Machine Learning Research, 6:1453–1484,
2005.

8

"
940,2007,Fixing Max-Product: Convergent Message Passing Algorithms for MAP LP-Relaxations,"We present a novel message passing algorithm for approximating the MAP problem in graphical models. The algorithm is similar in structure to max-product but unlike max-product it always converges, and can be proven to find the exact MAP solution in various settings. The algorithm is derived via block coordinate descent in a dual of the LP relaxation of MAP, but does not require any tunable parameters such as step size or tree weights. We also describe a generalization of the method to cluster based potentials. The new method is tested on synthetic and real-world problems, and compares favorably with previous approaches.","Fixing Max-Product: Convergent Message Passing

Algorithms for MAP LP-Relaxations

Amir Globerson Tommi Jaakkola

Computer Science and Artiﬁcial Intelligence Laboratory

Massachusetts Institute of Technology

Cambridge, MA 02139

gamir,tommi@csail.mit.edu

Abstract

We present a novel message passing algorithm for approximating the MAP prob-
lem in graphical models. The algorithm is similar in structure to max-product but
unlike max-product it always converges, and can be proven to ﬁnd the exact MAP
solution in various settings. The algorithm is derived via block coordinate descent
in a dual of the LP relaxation of MAP, but does not require any tunable parameters
such as step size or tree weights. We also describe a generalization of the method
to cluster based potentials. The new method is tested on synthetic and real-world
problems, and compares favorably with previous approaches.

Graphical models are an effective approach for modeling complex objects via local interactions. In
such models, a distribution over a set of variables is assumed to factor according to cliques of a graph
with potentials assigned to each clique. Finding the assignment with highest probability in these
models is key to using them in practice, and is often referred to as the MAP (maximum aposteriori)
assignment problem. In the general case the problem is NP hard, with complexity exponential in the
tree-width of the underlying graph.

Linear programming (LP) relaxations have proven very useful in approximating the MAP problem,
and often yield satisfactory empirical results. These approaches relax the constraint that the solution
is integral, and generally yield non-integral solutions. However, when the LP solution is integral,
it is guaranteed to be the exact MAP. For some classes of problems the LP relaxation is provably
correct. These include the minimum cut problem and maximum weight matching in bi-partite graphs
[8]. Although LP relaxations can be solved using standard LP solvers, this may be computationally
intensive for large problems [13]. The key problem with generic LP solvers is that they do not use
the graph structure explicitly and thus may be sub-optimal in terms of computational efﬁciency.

The max-product method [7] is a message passing algorithm that is often used to approximate the
MAP problem.
In contrast to generic LP solvers, it makes direct use of the graph structure in
constructing and passing messages, and is also very simple to implement. The relation between
max-product and the LP relaxation has remained largely elusive, although there are some notable
exceptions: For tree-structured graphs, max-product and LP both yield the exact MAP. A recent
result [1] showed that for maximum weight matching on bi-partite graphs max-product and LP also
yield the exact MAP [1]. Finally, Tree-Reweighted max-product (TRMP) algorithms [5, 10] were
shown to converge to the LP solution for binary xi variables, as shown in [6].
In this work, we propose the Max Product Linear Programming algorithm (MPLP) - a very simple
variation on max-product that is guaranteed to converge, and has several advantageous properties.
MPLP is derived from the dual of the LP relaxation, and is equivalent to block coordinate descent in
the dual. Although this results in monotone improvement of the dual objective, global convergence
is not always guaranteed since coordinate descent may get stuck in suboptimal points. This can
be remedied using various approaches, but in practice we have found MPLP to converge to the LP

1

solution in a majority of the cases we studied. To derive MPLP we use a special form of the dual
LP, which involves the introduction of redundant primal variables and constraints. We show how
the dual variables corresponding to these constraints turn out to be the messages in the algorithm.
We evaluate the method on Potts models and protein design problems, and show that it compares
favorably with max-product (which often does not converge for these problems) and TRMP.

1 The Max-Product and MPLP Algorithms

The max-product algorithm [7] is one of the most often used methods for solving MAP problems.
Although it is neither guaranteed to converge to the correct solution, or in fact converge at all, it
provides satisfactory results in some cases. Here we present two algorithms: EMPLP (edge based
MPLP) and NMPLP (node based MPLP), which are structurally very similar to max-product, but
have several key advantages:

• After each iteration, the messages yield an upper bound on the MAP value, and the se-
quence of bounds is monotone decreasing and convergent. The messages also have a limit
point that is a ﬁxed point of the update rule.

• No additional parameters (e.g., tree weights as in [6]) are required.
• If the ﬁxed point beliefs have a unique maximizer then they correspond to the exact MAP.
• For binary variables, MPLP can be used to obtain the solution to an LP relaxation of the
MAP problem. Thus, when this LP relaxation is exact and variables are binary, MPLP will
ﬁnd the MAP solution. Moreover, for any variable whose beliefs are not tied, the MAP
assignment can be found (i.e., the solution is partially decodable).

Pseudo code for the algorithms (and for max-product) is given in Fig. 1. As we show in the next
sections, MPLP is essentially a block coordinate descent algorithm in the dual of a MAP LP re-
laxation. Every update of the MPLP messages corresponds to exact minimization of a set of dual
variables. For EMPLP minimization is over the set of variables corresponding to an edge, and for
NMPLP it is over the set of variables corresponding to all the edges a given node appears in (i.e., a
star). The properties of MPLP result from its relation to the LP dual. In what follows we describe
the derivation of the MPLP algorithms and prove their properties.

2 The MAP Problem and its LP Relaxation

We consider functions over n variables x = {x1, . . . , xn} deﬁned as follows. Given a graph G =
(V, E) with n vertices, and potentials θij(xi, xj) for all edges ij ∈ E, deﬁne the function1

f (x; θ) = Xij∈E

θij (xi, xj) .

(1)

The MAP problem is deﬁned as ﬁnding an assignment xM that maximizes the function f (x; θ).
Below we describe the standard LP relaxation for this problem. Denote by {µij(xi, xj)}ij∈E distri-
butions over variables corresponding to edges ij ∈ E and {µi(xi)}i∈V distributions corresponding
to nodes i ∈ V . We will use µ to denote a given set of distributions over all edges and nodes. The
set ML(G) is deﬁned as the set of µ where pairwise and singleton distributions are consistent

µij(ˆxi, xj) = µj(xj ) , Pˆxj

µi(xi) = 1

µij (xi, ˆxj) = µi(xi) ∀ij ∈ E, xi, xj

∀i ∈ V

(cid:27)

ML(G) = (cid:26)µ ≥ 0(cid:12)(cid:12)(cid:12)(cid:12)

Pˆxi
Pxi

Now consider the following linear program:

MAPLPR :

µL∗ = arg max

µ∈ML(G)

µ · θ .

(2)

where µ·θ is shorthand for µ·θ = Pij∈E Pxi,xj

θij(xi, xj )µij(xi, xj). It is easy to show (see e.g.,
[10]) that the optimum of MAPLPR yields an upper bound on the MAP value, i.e. µL∗ ·θ ≥ f (xM ).
Furthermore, when the optimal µi(xi) have only integral values, the assignment that maximizes
µi(xi) yields the correct MAP assignment. In what follows we show how the MPLP algorithms can
be derived from the dual of MAPLPR.

1We note that some authors also add a term Pi∈V θi(xi) to f (x; θ). However, these terms can be included

in the pairwise functions θij (xi, xj), so we ignore them for simplicity.

2

3 The LP Relaxation Dual

Since MAPLPR is an LP, it has an equivalent convex dual. In App. A we derive a special dual of
MAPLPR using a different representation of ML(G) with redundant variables. The advantage of
this dual is that it allows the derivation of simple message passing algorithms. The dual is described
in the following proposition.

Proposition 1 The following optimization problem is a convex dual of MAPLPR

DMAPLPR :
min

s.t.

Pi

max

xi Pk∈N (i)

max
xk

βki(xk, xi)

(3)

βji(xj , xi) + βij(xi, xj) = θij(xi, xj) ,

where the dual variables are βij (xi, xj) for all ij, ji ∈ E and values of xi and xj.

xi Pk∈N (i)

max
xk

The dual has an intuitive interpretation in terms of re-parameterizations. Consider the star
shaped graph Gi consisting of node i and all its neighbors N (i). Assume the potential on
edge ki (for k ∈ N (i)) is βki(xk, xi). The value of the MAP assignment for this model is
βki(xk, xi). This is exactly the term in the objective of DMAPLPR. Thus the dual
max

corresponds to individually decoding star graphs around all nodes i ∈ V where the potentials on the
graph edges should sum to the original potential. It is easy to see that this will always result in an
upper bound on the MAP value. The somewhat surprising result of the duality is that there exists a
β assignment such that star decoding yields the optimal value of MAPLPR.

4 Block Coordinate Descent in the Dual

To obtain a convergent algorithm we use a simple block coordinate descent strategy. At every
iteration, ﬁx all variables except a subset, and optimize over this subset. It turns out that this can
be done in closed form for the cases we consider. We begin by deriving the EMPLP algorithm.
Consider ﬁxing all the β variables except those corresponding to some edge ij ∈ E (i.e., βij and
βji), and minimizing DMAPLPR over the non-ﬁxed variables. Only two terms in the DMAPLPR
objective depend on βij and βji. We can write those as

i (xi) + max

xj

f (βij, βji) = max

xi (cid:20)λ−i

βji(xj, xi)(cid:21) + max

xi (cid:20)λ−j
where we deﬁned λ−j
i (xi) = Pk∈N (i)\j λki(xi) and λki(xi) = maxxk βki(xk, xi) as in App. A.
Note that the function f (βij , βji) depends on the other β values only through λ−i
i (xi).
This implies that the optimization can be done solely in terms of λij (xj) and there is no need to
store the β values explicitly. The optimal βij , βji are obtained by minimizing f (βij, βji) subject
to the re-parameterization constraint βji(xj , xi) + βij(xi, xj) = θij (xi, xj ). The following propo-
sition characterizes the minimum of f (βij, βji). In fact, as mentioned above, we do not need to
characterize the optimal βij (xi, xj ) itself, but only the new λ values.

βij(xi, xj)(cid:21)

j (xj) and λ−j

j (xj ) + max

xi

(4)

Proposition 2 Maximizing the function f (βij, βji) yields the following λji(xi) (and the equivalent
expression for λij (xj))

λji(xi) = −

1
2

λ−j
i (xi) +

1
2

max

xj (cid:2)λ−i

j (xj ) + θij(xi, xj )(cid:3)

The proposition is proved in App. B. The λ updates above result in the EMPLP algorithm, described
in Fig. 1. Note that since the β optimization affects both λji(xi) and λij (xj ), both these messages
need to be updated simultaneously.

We proceed to derive the NMPLP algorithm. For a given node i ∈ V , we consider all its neighbors
j ∈ N (i), and wish to optimize over the variables βji(xj, xi) for ji, ij ∈ E (i.e., all the edges in a
star centered on i), while the other variables are ﬁxed. One way of doing so is to use the EMPLP
algorithm for the edges in the star, and iterate it until convergence. We now show that the result of

3

Inputs: A graph G = (V, E), potential functions θij(xi, xj) for each edge ij ∈ E.

Initialization: Initialize messages to any value.

Algorithm:

• Iterate until a stopping criterion is satisﬁed:

– Max-product: Iterate over messages and update (cji shifts the max to zero)

mji(xi)← max

xj hm−i

j (xj) + θij(xi, xj)i − cji

– EMPLP: For each ij ∈ E, update λji(xi) and λij(xj) simultaneously (the update

for λij(xj) is the same with i and j exchanged)

λji(xi)← −

1
2

λ−j

i (xi) +

1
2

max

xj hλ−i

j (xj) + θij (xi, xj)i

– NMPLP: Iterate over nodes i ∈ V and update all γij(xj) where j ∈ N (i)

γij(xj)← max

xi

2
4

θij(xi, xj) − γji(xi) +

2

|N (i)| + 1 X

k∈N(i)

γki(xi)

3
5

• Calculate node “beliefs”: Set bi(xi) to be the sum of incoming messages into node i ∈ V

(e.g., for NMPLP set bi(xi) = Pk∈N(i) γki(xi)).

Output: Return assignment x deﬁned as xi = arg maxˆxi b(ˆxi).

Figure 1: The max-product, EMPLP and NMPLP algorithms. Max-product, EMPLP and NMPLP use mes-
sages mij, λij and γij respectively. We use the notation m−i

j (xj) = Pk∈N(j)\i mkj(xj).

this optimization can be found in closed form. The assumption about β being ﬁxed outside the star
implies that λ−i
yields the following relation between λ−j

j (xj ) is ﬁxed. Deﬁne: γji(xi) = maxxj (cid:2)θij (xi, xj) + λ−i

j (xj )(cid:3). Simple algebra

i (xi) and γki(xi) for k ∈ N (i)

λ−j
i (xi) = −γji(xi) +

γki(xi)

(5)

2

|N (i)| + 1 Xk∈N (i)

Plugging this into the deﬁnition of γji(xi) we obtain the NMPLP update in Fig. 1. The messages
for both algorithms can be initialized to any value since it can be shown that after one iteration they
will correspond to valid β values.

5 Convergence Properties

The MPLP algorithm decreases the dual objective (i.e., an upper bound on the MAP value) at every
iteration, and thus its dual objective values form a convergent sequence. Using arguments similar to
[5] it can be shown that MPLP has a limit point that is a ﬁxed point of its updates. This in itself does
not guarantee convergence to the dual optimum since coordinate descent algorithms may get stuck
at a point that is not a global optimum. There are ways of overcoming this difﬁculty, for example by
smoothing the objective [4] or using techniques as in [2] (see p. 636). We leave such extensions for
further work. In this section we provide several results about the properties of the MPLP ﬁxed points
and their relation to the corresponding LP. First, we claim that if all beliefs have unique maxima then
the exact MAP assignment is obtained.

Proposition 3 If the ﬁxed point of MPLP has bi(xi) such that for all i the function bi(xi) has a
unique maximizer x∗

i , then x∗ is the solution to the MAP problem and the LP relaxation is exact.

Since the dual objective is always greater than or equal to the MAP value, it sufﬁces to show that
there exists a dual feasible point whose objective value is f (x∗). Denote by β∗, λ∗ the value of the
corresponding dual parameters at the ﬁxed point of MPLP. Then the dual objective satisﬁes

Xi

max

xi Xk∈N (i)

λ∗

ki(xi) = Xi Xk∈N (i)

max
xk

ki(xk, x∗
β∗

i ) = Xi Xk∈N (i)

4

ki(x∗
β∗

k, x∗

i ) = f (x∗)

i ) = maxxi,xj λ−j

j ) = maxxi,xj λ−i

i (xi) + βji(xj, xi) and
j (xj ) + βij(xi, xj). By the equalization property in Eq. 9 the arguments of
j are

To see why the second equality holds, note that bi(x∗
bj(x∗
the two max operations are equal. From the unique maximum assumption it follows that x∗
the unique maximizers of the above. It follows that βji, βij are also maximized by x∗
In the general case, the MPLP ﬁxed point may not correspond to a primal optimum because of the
local optima problem with coordinate descent. However, when the variables are binary, ﬁxed points
do correspond to primal solutions, as the following proposition states.

j .
i , x∗

i , x∗

Proposition 4 When xi are binary, the MPLP ﬁxed point can be used to obtain the primal optimum.

i (x∗

ij (x∗

i ) to 1. If bi, bj are not tied we set µ∗

i (xi) to 0.5
The claim can be shown by constructing a primal optimal solution µ∗. For tied bi, set µ∗
j ) = 1. If bi is not tied but bj
and for untied bi, set µ∗
i , xj) = 0.5. If bi, bj are tied then βji, βij can be shown to be maximized at either
is, we set µ∗
ij to be 0.5 at one of these assignment
x∗
i , x∗
pairs. The resulting µ∗ is clearly primal feasible. Setting δ∗
i we obtain that the dual variables
(δ∗, λ∗, β∗) and primal µ∗ satisfy complementary slackness for the LP in Eq. 7 and therefore µ∗ is
primal optimal. The binary optimality result implies partial decodability, since [6] shows that the
LP is partially decodable for binary variables.

j = (0, 1), (1, 0). We then set µ∗

j = (0, 0), (1, 1) or x∗

i = b∗

ij (x∗

i , x∗

i , x∗

6 Beyond pairwise potentials: Generalized MPLP

In the previous sections we considered maximizing functions which factor according to the edges of
the graph. A more general setting considers clusters c1, . . . , ck ⊂ {1, . . . , n} (the set of clusters is

denoted by C), and a function f (x; θ) = Pc θc(xc) deﬁned via potentials over clusters θc(xc). The

MAP problem in this case also has an LP relaxation (see e.g. [11]). To deﬁne the LP we introduce
the following deﬁnitions: S = {c ∩ ˆc : c, ˆc ∈ C, c ∩ ˆc 6= ∅} is the set of intersection between clusters
and S(c) = {s ∈ S : s ⊆ c} is the set of overlap sets for cluster c.We now consider marginals over
the variables in c ∈ C and s ∈ S and require that cluster marginals agree on their overlap. Denote
this set by ML(C). The LP relaxation is then to maximize µ · θ subject to µ ∈ ML(C).
As in Sec. 4, we can derive message passing updates that result in monotone decrease of the dual
LP of the above relaxation. The derivation is similar and we omit the details. The key observation
is that one needs to introduce |S(c)| copies of each marginal µc(xc) (instead of the two copies
in the pairwise case). Next, as in the EMPLP derivation we assume all β are ﬁxed except those
corresponding to some cluster c. The resulting messages are λc→s(xs) from a cluster c to all of its
intersection sets s ∈ S(c). The update on these messages turns out to be:

1

|S(c)|(cid:19) λ−c

λc→s(xs) = −(cid:18)1 −

ˆs (xˆs) + θc(xc)

where for a given c ∈ C all λc→s should be updated simultaneously for s ∈ S(c), and λ−c
s (xs) is
deﬁned as the sum of messages into s that are not from c. We refer to this algorithm as Generalized
EMPLP (GEMPLP). It is possible to derive an algorithm similar to NMPLP that updates several
clusters simultaneously, but its structure is more involved and we do not address it here.


 Xˆs∈S(c)\s

s (xs) +

max
xc\s

|S(c)|

λ−c

1

7 Related Work

Weiss et al. [11] recently studied the ﬁxed points of a class of max-product like algorithms. Their
analysis focused on properties of ﬁxed points rather than convergence guarantees. Speciﬁcally, they
showed that if the counting numbers used in a generalized max-product algorithm satisfy certain
properties, then its ﬁxed points will be the exact MAP if the beliefs have unique maxima, and for
binary variables the solution can be partially decodable. Both these properties are obtained for the
MPLP ﬁxed points, and in fact we can show that MPLP satisﬁes the conditions in [11], so that
we obtain these properties as corollaries of [11]. We stress however, that [11] does not address
convergence of algorithms, but rather properties of their ﬁxed points, if they converge.

MPLP is similar in some aspects to Kolmogorov’s TRW-S algorithm [5]. TRW-S is also a monotone
coordinate descent method in a dual of the LP relaxation and its ﬁxed points also have similar

5

guarantees to those of MPLP [6]. Furthermore, convergence to a local optimum may occur, as it
does for MPLP. One advantage of MPLP lies in the simplicity of its updates and the fact that it is
parameter free. The other is its simple generalization to potentials over clusters of nodes (Sec. 6).
Recently, several new dual LP algorithms have been introduced, which are more closely related to
our formalism. Werner [12] presented a class of algorithms which also improve the dual LP at every
iteration. The simplest of those is the max-sum-diffusion algorithm, which is similar to our EMPLP
algorithm, although the updates are different from ours. Independently, Johnson et al. [4] presented
a class of algorithms that improve duals of the MAP-LP using coordinate descent. They decompose
the model into tractable parts by replicating variables and enforce replication constraints within the
Lagrangian dual. Our basic formulation in Eq. 3 could be derived from their perspective. However,
the updates in the algorithm and the analysis differ. Johnson et al. also presented a method for
overcoming the local optimum problem, by smoothing the objective so that it is strictly convex.
Such an approach could also be used within our algorithms. Vontobel and Koetter [9] recently
introduced a coordinate descent algorithm for decoding LDPC codes. Their method is speciﬁcally
tailored for this case, and uses updates that are similar to our edge based updates.

Finally, the concept of dual coordinate descent may be used in approximating marginals as well. In
[3] we use such an approach to optimize a variational bound on the partition function. The derivation
uses some of the ideas used in the MPLP dual, but importantly does not ﬁnd the minimum for each
coordinate. Instead, a gradient like step is taken at every iteration to decrease the dual objective.

8 Experiments

We compared NMPLP to three other message passing algorithms:2 Tree-Reweighted max-product
(TRMP) [10],3 standard max-product (MP), and GEMPLP. For MP and TRMP we used the standard
approach of damping messages using a factor of α = 0.5. We ran all algorithms for a maximum of
2000 iterations, and used the hit-time measure to compare their speed of convergence. This measure
is deﬁned as follows: At every iteration the beliefs can be used to obtain an assignment x with value
f (x). We deﬁne the hit-time as the ﬁrst iteration at which the maximum value of f (x) is achieved.4
We ﬁrst experimented with a 10 × 10 grid graph, with 5 values per state. The function f (x) was

a Potts model: f (x) = Pij∈E θijI(xi = xj) + Pi∈V θi(xi).5 The values for θij and θi(xi)

were randomly drawn from [−cI, cI ] and [−cF , cF ] respectively, and we used values of cI and
cF in the range range [0.1, 2.35] (with intervals of 0.25), resulting in 100 different models. The
clusters for GEMPLP were the faces of the graph [14]. To see if NMPLP converges to the LP
solution we also used an LP solver to solve the LP relaxation. We found that the the normalized
difference between NMPLP and LP objective was at most 10−3 (median 10−7), suggesting that
NMPLP typically converged to the LP solution. Fig. 2 (top row) shows the results for the three
algorithms. It can be seen that while all non-cluster based algorithms obtain similar f (x) values,
NMPLP has better hit-time (in the median) than TRMP and MP, and MP does not converge in many
cases (see caption). GEMPLP converges more slowly than NMPLP, but obtains much better f (x)
values. In fact, in 99% of the cases the normalized difference between the GEMPLP objective and
the f (x) value was less than 10−5, suggesting that the exact MAP solution was found.
We next applied the algorithms to the real world problems of protein design.
In [13], Yanover
et al. show how these problems can be formalized in terms of ﬁnding a MAP in an appropriately
constructed graphical model.6 We used all algorithms except GNMPLP (since there is no natural
choice for clusters in this case) to approximate the MAP solution on the 97 models used in [13].
In these models the number of states per variable is 2 − 158, and there are up to 180 variables per
model. Fig. 2 (bottom) shows results for all the design problems. In this case only 11% of the MP
runs converged, and NMPLP was better than TRMP in terms of hit-time and comparable in f (x)
value. The performance of MP was good on the runs where it converged.

2As expected, NMPLP was faster than EMPLP so only NMPLP results are given.
3The edge weights for TRMP corresponded to a uniform distribution over all spanning trees.
4This is clearly a post-hoc measure since it can only be obtained after the algorithm has exceeded its maxi-

mum number of iterations. However, it is a reasonable algorithm-independent measure of convergence.

5The potential θi(xi) may be folded into the pairwise potential to yield a model as in Eq. 1.
6Data available from http://jmlr.csail.mit.edu/papers/volume7/yanover06a/Rosetta Design Dataset.tgz

6

(a)

100

50

0

−50

−100

(b)

l

)
e
u
a
V
(
∆

0.04

0.02

0

−0.02

−0.04

−0.06

(c)

2000

1000

0

−1000

(d)

0.6

0.4

0.2

0

−0.2

−0.4

l

)
e
u
a
V
(
∆

)
e
m
T

i

 
t
i

H
(
∆

MP

TRMP

GMPLP

MP

TRMP

GMPLP

MP

TRMP

MP

TRMP

)
e
m
T

i

 
t
i

H
(
∆

Figure 2: Evaluation of message passing algorithms on Potts models and protein design problems.
(a,c):
Convergence time results for the Potts models (a) and protein design problems (c). The box-plots (horiz. red
line indicates median) show the difference between the hit-time for the other algorithms and NMPLP. (b,d):
Value of integer solutions for the Potts models (b) and protein design problems (d). The box-plots show the
normalized difference between the value of f (x) for NMPLP and the other algorithms. All ﬁgures are such
that better MPLP performance yields positive Y axis values. Max-product converged on 58% of the cases for
the Potts models, and on 11% of the protein problems. Only convergent max-product runs are shown.

9 Conclusion

We have presented a convergent algorithm for MAP approximation that is based on block coordi-
nate descent of the MAP-LP relaxation dual. The algorithm can also be extended to cluster based
functions, which result empirically in improved MAP estimates. This is in line with the observa-
tions in [14] that generalized belief propagation algorithms can result in signiﬁcant performance
improvements. However generalized max-product algorithms [14] are not guaranteed to converge
whereas GMPLP is. Furthermore, the GMPLP algorithm does not require a region graph and only
involves intersection between pairs of clusters. In conclusion, MPLP has the advantage of resolving
the convergence problems of max-product while retaining its simplicity, and offering the theoretical
guarantees of LP relaxations. We thus believe it should be useful in a wide array of applications.

A Derivation of the dual

Before deriving the dual, we ﬁrst express the constraint set ML(G) in a slightly different way. The
deﬁnition of ML(G) in Sec. 2 uses a single distribution µij(xi, xj) for every ij ∈ E. In what
follows, we use two copies of this pairwise distribution for every edge, which we denote ¯µij(xi, xj )
and ¯µji(xj , xi), and we add the constraint that these two copies both equal the original µij(xi, xj).
For this extended set of pairwise marginals, we consider the following set of constraints which
is clearly equivalent to ML(G). On the rightmost column we give the dual variables that will
correspond to each constraint (we omit non-negativity constraints).

¯µij(xi, xj ) = µij(xi, xj)
¯µji(xj , xi) = µij(xi, xj)

∀ij ∈ E, xi, xj βij(xi, xj)
∀ij ∈ E, xi, xj βji(xj , xi)

¯µij (ˆxi, xj) = µj(xj ) ∀ij ∈ E, xj
∀ji ∈ E, xi
¯µji(ˆxj , xi) = µi(xi)
∀i ∈ V
µi(xi) = 1

λij (xj )
λji(xi)
δi

(6)

Pˆxi
Pˆxj
Pxi

We denote the set of (µ, ¯µ) satisfying these constraints by ¯ML(G). We can now state an LP that
is equivalent to MAPLPR, only with an extended set of variables and constraints. The equivalent
problem is to maximize µ · θ subject to (µ, ¯µ) ∈ ¯ML(G) (note that the objective uses the original
µ copy). LP duality transformation of the extended problem yields the following LP

min Pi δi

s.t.

λij (xj ) − βij(xi, xj ) ≥ 0
βij (xi, xj) + βji(xj, xi) = θij (xi, xj) ∀ij ∈ E, xi, xj

∀ij, ji ∈ E, xi, xj

(7)

−Pk∈N (i) λki(xi) + δi ≥ 0

∀i ∈ V, xi

We next simplify the above LP by eliminating some of its constraints and variables. Since each
variable δi appears in only one constraint, and the objective minimizes δi it follows that δi =

maxxi Pk∈N (i) λki(xi) and the constraints with δi can be discarded. Similarly, since λij (xj ) ap-

pears in a single constraint, we have that for all ij ∈ E, ji ∈ E, xi, xj λij(xj ) = maxxi βij(xi, xj )
and the constraints with λij (xj ), λji(xi) can also be discarded. Using the eliminated δi and λji(xi)

7

variables, we obtain that the LP in Eq. 7 is equivalent to that in Eq. 3. Note that the objective in
Eq. 3 is convex since it is a sum of point-wise maxima of convex functions.

B Proof of Proposition 2

We wish to minimize f in Eq. 4 subject to the constraint that βij + βji = θij. Rewrite f as

f (βij, βji) = max

xi,xj hλ−j

i (xi) + βji(xj , xi)i + max
in the max is λ−j

xi,xj (cid:2)λ−i

j (xj) + βij(xi, xj)(cid:3)

i (xi) + λ−i

Thus

the minimum must be greater

j (xj) + θij(xi, xj )
than

(8)

sum of

The
(because of
1

two arguments
constraints on β).

the
the
i (xi) + λ−i

2 maxxi,xj hλ−j
mum is obtained by requiring an equalization condition:7

j (xj ) + θij(xi, xj)i. One assignment to β that achieves this mini-

j (xj ) + βij(xi, xj ) = λ−j
λ−i

i (xi) + βji(xj, xi) =

1

2 (cid:16)θij (xi, xj) + λ−j

i (xi) + λ−i

j (xj )(cid:17)

(9)

which implies βij(xi, xj) = 1
The resulting λij (xj ) = maxxi βij (xi, xj) are then the ones in Prop. 2.

2 (cid:16)θij (xi, xj ) + λ−j

i (xi) − λ−i

j (xj)(cid:17) and a similar expression for βji.

Acknowledgments
The authors acknowledge support from the Defense Advanced Research Projects Agency (Transfer
Learning program). Amir Globerson was also supported by the Rothschild Yad-Hanadiv fellowship.

References
[1] M. Bayati, D. Shah, and M. Sharma. Maximum weight matching via max-product belief propagation.

IEEE Trans. on Information Theory (to appear), 2007.

[2] D. P. Bertsekas, editor. Nonlinear Programming. Athena Scientiﬁc, Belmont, MA, 1995.
[3] A. Globerson and T. Jaakkola. Convergent propagation algorithms via oriented trees. In UAI. 2007.
[4] J.K. Johnson, D.M. Malioutov, and A.S. Willsky. Lagrangian relaxation for map estimation in graphical

models. In Allerton Conf. Communication, Control and Computing, 2007.

[5] V. Kolmogorov. Convergent tree-reweighted message passing for energy minimization. IEEE Transac-

tions on Pattern Analysis and Machine Intelligence, 28(10):1568–1583, 2006.

[6] V. Kolmogorov and M. Wainwright. On the optimality of tree-reweighted max-product message passing.

In 21st Conference on Uncertainty in Artiﬁcial Intelligence (UAI). 2005.

[7] J. Pearl. Probabilistic Reasoning in Intelligent Systems. Morgan Kaufmann, 1988.
[8] B. Taskar, S. Lacoste-Julien, and M. Jordan. Structured prediction, dual extragradient and bregman pro-

jections. Journal of Machine Learning Research, pages 1627–1653, 2006.

[9] P.O. Vontobel and R. Koetter. Towards low-complexity linear-programming decoding. In Proc. 4th Int.

Symposium on Turbo Codes and Related Topics, 2006.

[10] M. J. Wainwright, T. Jaakkola, and A. S. Willsky. Map estimation via agreement on trees: message-

passing and linear programming. IEEE Trans. on Information Theory, 51(11):1120–1146, 2005.

[11] Y. Weiss, C. Yanover, and T. Meltzer. Map estimation, linear programming and belief propagation with

convex free energies. In UAI. 2007.

[12] T. Werner. A linear programming approach to max-sum, a review. IEEE Trans. on PAMI, 2007.
[13] C. Yanover, T. Meltzer, and Y. Weiss. Linear programming relaxations and belief propagation – an

empirical study. Jourmal of Machine Learning Research, 7:1887–1907, 2006.

[14] J.S. Yedidia, W.T. W.T. Freeman, and Y. Weiss. Constructing free-energy approximations and generalized

belief propagation algorithms. IEEE Trans. on Information Theory, 51(7):2282–2312, 2005.

7Other solutions are possible but may not yield some of the properties of MPLP.

8

"
471,2007,GRIFT: A graphical model for inferring visual classification features from human data,"This paper describes a new model for human visual classification that enables the recovery of image features that explain human subjects' performance on different visual classification tasks. Unlike previous methods, this algorithm does not model their performance with a single linear classifier operating on raw image pixels. Instead, it models classification as the combination of multiple feature detectors. This approach extracts more information about human visual classification than has been previously possible with other methods and provides a foundation for further exploration.","GRIFT: A graphical model for inferring visual

classiﬁcation features from human data

Michael G. Ross

Department of Brain and Cognitive Sciences

Massachusetts Institute of Technology

Cambridge, MA 02139
mgross@mit.edu

Andrew L. Cohen

Psychology Department

University of Massachusetts Amherst

Amherst, MA 01003

acohen@psych.umass.edu

Abstract

This paper describes a new model for human visual classiﬁcation that enables the
recovery of image features that explain human subjects’ performance on differ-
ent visual classiﬁcation tasks. Unlike previous methods, this algorithm does not
model their performance with a single linear classiﬁer operating on raw image
pixels. Instead, it represents classiﬁcation as the combination of multiple feature
detectors. This approach extracts more information about human visual classiﬁ-
cation than previous methods and provides a foundation for further exploration.

1 Introduction

Although a great deal is known about the low-level features computed by the human visual system,
determining the information used to make high-level visual classiﬁcations is an active area of re-
search. When a person distinguishes between two faces, for example, what image regions are most
salient? Since the early 1970s, one of the most important research tools for answering such questions
has been the classiﬁcation image (or reverse correlation) algorithm, which assumes a linear classi-
ﬁcation model [1]. This paper describes a new approach, GRIFT (GRaphical models for Inferring
Feature Templates). Instead of representing human visual discrimination as a single linear classiﬁer,
GRIFT models it as the non-linear combination of multiple independently detected features. This
allows GRIFT to extract more detailed information about human classiﬁcation.
This paper describes GRIFT and the algorithms for ﬁtting it to data, demonstrates the model’s efﬁ-
cacy on simulated and human data, and concludes with a discussion of future research directions.

2 Related work

Ahumada’s classiﬁcation image algorithm [1] models an observer’s classiﬁcations of visual stimuli
with a noisy linear classiﬁer — a ﬁxed set of weights and a normally distributed threshold. The
random threshold accounts for the fact that multiple presentations of the same stimulus are often
classiﬁed inconsistently.
In a typical classiﬁcation image experiment, participants are presented
with hundreds or thousands of noise-corrupted examples from two categories and asked to classify
each one. The noise ensures that the samples cover a large volume of the sample space in order to
allow recovery of a unique linear classiﬁer that best explains the data.
Although classiﬁcation images are useful in many cases, it is well established that there are domains
in which recognition and classiﬁcation are the result of combining the detection of parts or fea-
tures, rather than applying a single linear template. For example, Pelli et al. [10], have convincingly
demonstrated that humans recognize noisy word images by parts, even when whole-word templates
would perform better. Similarly, Gold et al. [7] veriﬁed that subjects employed feature-based clas-

1

Figure 1: Left: The GRIFT model is a Bayes net that describes classiﬁcation as the result of com-
bining N feature detectors. Right: Targets and sample stimuli from the three experiments.

siﬁcation strategies for some simple artiﬁcial image classes. GRIFT takes the next step and infers
features which predict human performance directly from classiﬁcation data.
Most work on modeling non-linear, feature-based classiﬁcation in humans has focused on verifying
the use of a predeﬁned set of features. Recent work by Cohen et al. [4] demonstrates that Gaussian
mixture models can be used to recover features from human classiﬁcation data without specifying
a ﬁxed set of possible features. The GRIFT model, described in the remainder of this paper, has
the same goals as the previous work, but removes several limitations of the Gaussian mixture model
approach, including the need to only use stimuli the subjects classiﬁed with high conﬁdence and
the bias that the signals can exert on the recovered features. GRIFT achieves these and other im-
provements by generatively modeling the entire classiﬁcation process with a graphical model. Fur-
thermore, the similarity between single-feature GRIFT models and the classiﬁcation image process,
described in more detail below, make GRIFT a natural successor to the traditional approach.

3 GRIFT model

GRIFT models classiﬁcation as the result of combining N conditionally independent feature detec-
tors, F = {F1, F2, . . . , FN}. Each feature detector is binary valued (1 indicates detection), as is
the classiﬁcation, C (1 indicates one class and 2 the other). The stimulus, S, is an array of con-
tinuously valued pixels representing the input image. The stimulus only inﬂuences C through the
feature detectors, therefore the joint probability of a stimulus and classiﬁcation pair is

P (C, S) =X

 

P (C|F )P (S)

P (Fi|S)

.

NY

!

F

i

Figure 1 represents the causal relationship between these variables (C, F , and S) with a Bayesian
network. The network also includes nodes representing model parameters (ω, β, and λ), whose role
will be described below. The boxed region in the ﬁgure indicates the parts of the model that are
replicated when N > 1 — each feature detector is represented by an independent copy of those
variables and parameters.
The distribution of the stimulus, P (S), is under the control of the experimenter. The algorithm for
ﬁtting the model to data only assumes that the stimuli are independent and identically distributed
across trials. The conditional distribution of each feature detector’s value, P (Fi|S), is modeled with
a logistic regression function on the pixel values of S. Logistic regression is desirable because it is a
probabilistic linear classiﬁer. Humans can successfully classify images in the presence of extremely
high additive noise, which suggests the use of averaging and contrast, linear computations which

2

NSFiCωiλiβiλ0four squarefaceslight-darktargetssamplestargetstargetssamplessamplesclass 1class 2class 1class 2class 1class 2are known to play important roles in human visual perception [9]. Just as the classiﬁcation image
used a random threshold to represent uncertainty in the output of its single linear classiﬁer, logistic
regression also allows GRIFT to represent uncertainty in the output of each of its feature detectors.
The conditional distribution of C is represented by logistic regression on the feature outputs.
Each Fi’s distribution has two parameters, a weight vector ωi and a threshold βi, such that

P (Fi = 1|S, ωi, βi) = (1 + exp(βi +

ωijSj))−1,

where |S| is the number of pixels in a stimulus. Similarly, the conditional distribution of C is
determined by λ = {λ0, λ1, . . . , λN} where

|S|X

j=1

NX

i=1

P (C = 1|F, λ) = (1 + exp(λ0 +

λiFi))−1.

Detecting a feature with negative λi increases the probability that the subject will respond “class 1,”
those with positive λi are associated with “class 2” responses.
A GRIFT model with N features applied to the classiﬁcation of images each containing |S| pixels
has N(|S| + 2) + 1 parameters. This large number of parameters, coupled with the fact that the
F variables are unobservable, makes ﬁtting the model to data very challenging. Therefore, GRIFT
deﬁnes prior distributions on its parameters. These priors reﬂect reasonable assumptions about the
parameter values and, if they are wrong, can be overturned if enough contrary data is available. The
prior on each of the λi parameters for which i > 0 is a mixture of two normal distributions,

P (λi) =

1
√
2
2π

(exp(−(λi + 2)2

2

) + exp(−(λi − 2)2

)).

2

This prior reﬂects the assumption that each feature detector should have a signiﬁcant impact on the
classiﬁcation, but no single detector should make it deterministic — a single-feature model with
λ0 = 0 and λ1 = −2 has an 88% chance of choosing class 1 if the feature is active. The λ0
parameter has an improper non-informative prior, P (λ0) = 1, indicating no preference for any
particular value [5] because the best λ0 is largely determined by the other λis and the distributions
of F and S. For analogous reasons, P (βi) = 1.
The ωi parameters, which each have dimensionality equal to the stimulus, present the biggest infer-
ential challenge. As mentioned previously, human visual processing is sensitive to contrasts between
image regions. If one image region is assigned positive ωijs and another is assigned negative ωijs,
the feature detector will be sensitive to the contrast between them. This contrast between regions re-
quires all the pixels within each region to share similar ωij values. To encourage this local structure,
the ωi parameters have Markov random ﬁeld prior distributions:

P (ωi) ∝

(exp(−(ωij + 1)2

2

) + exp(−(ωij − 1)2

2

))

exp(−(ωij − ωik)2

2

)

 Y

(j,k)∈A

Y

j

 ,

NY

where A is the set of neighboring pixel locations. The ﬁrst factor encourages weight values to be
near the -1 to 1 range, while the second encourages the assignment of similar weights to neighboring
pixels. Fitting the model to data does not require the normalization of this distribution.
The Bayesian joint probability distribution of all the parameters and variables is

P (C, F, S, ω, β, λ) = P (C|F, λ)P (S)P (λ0)

P (Fi|S, ωi, βi)P (ωi)P (βi)P (λi).

(1)

4 GRIFT algorithm

i=1

The goal of the algorithm is to ﬁnd the parameters that satisfy the prior distributions and best ac-
count for the (S, C) samples gathered from a human subject. Mathematically, this goal corresponds
to ﬁnding the mode of P (ω, β, λ|S, C), where S and C refer to all of the observed samples. The

3

algorithm is derived using the expectation-maximization (EM) method [3], a widely used optimiza-
tion technique for dealing with unobserved variables, in this case F, the feature detector outputs for
all the trials. In order to determine the most probable parameter assignments, the algorithm chooses
random initial parameters θ∗ = (ω∗, β∗, λ∗) and then ﬁnds the θ that maximizes
P (F|S, C, θ∗) log P (C, F, S|θ) + log P (θ).

Q(θ|θ∗) =X

F

Q(θ|θ∗) is the expected log posterior probability of the parameters computed by using the current θ∗
to estimate the distribution of F, the unobserved feature detector activations. The θ that maximizes
Q then becomes θ∗ for the next iteration, and the process is repeated until convergence.
The presence of both the P (C, F, S|θ) and P (θ) terms encourages the algorithm to ﬁnd parameters
that explain the data and match the assumptions encoded in the parameter prior distributions. As the
amount of available data increases, the inﬂuence of the priors decreases, so it is possible to discover
features that are contrary to prior belief given enough evidence.
Using the conditional independences from the Bayes net:

!

Q(θ|θ∗) ∝ X
NX

F

+

 

NX

P (F|S, C, θ∗)

log P (C|F, λ) +

log P (Fi|S, ωi, βi)

i=1

(log P (ωi) + log P (λi)) ,

i=1

dropping the log P (S) term, which is independent of the parameters, and the log P (λ0) and
log P (βi) terms, which are 0. As mentioned before, the normalization terms for the log P (ωi)
elements can be ignored during optimization — the log makes them additive constants to Q. The
functional form of every additive term is described in Section 3, and P (F|S, C, θ∗) can be calculated
using the model’s joint probability function (Equation 1).
Each iteration of EM requires maximizing Q, but it is not possible to compute the maximizing θ in
closed form. Fortunately, it is relatively easy to search for the best θ. Because Q is separable into
many additive components, it is possible to efﬁciently compute its gradient with respect to each of
the elements of θ and use this information to ﬁnd a locally maximum θ assignment using the scaled
conjugate gradient algorithm [2]. Even a locally maximum value of θ usually provides good EM
results — P (ω, β, λ|S, C) is still guaranteed to improve after every iteration.
The result of any EM procedure is only guaranteed to be a locally optimal answer, and ﬁnding the
globally optimal θ is made more challenging by the large number of parameters. GRIFT adopts
the standard solution of running EM many times, each instance starting with a random θ∗, and then
accepting the θ from the run which produced the most probable parameters. For this model and the
data presented in the following sections, 20-30 random restarts were sufﬁcient.

5 Experiments

The GRIFT model was ﬁt to data from 3 experiments.
In each experiment, human participants
classiﬁed stimuli into two classes. Each class contained one or more target stimuli. In each trial,
the participant saw a stimulus (a sample from S) that consisted of a randomly chosen target with
high levels of independent identically distributed noise added to each pixel. The noise samples were
drawn from a truncated normal distribution to ensure that the stimulus pixel values remained within
the display’s output range. Figure 1 shows the classes and targets from each experiment and a sample
stimulus from each class. In the four-square experiment four participants were asked to distinguish
between two artiﬁcial stimulus classes, one in which there were bright squares in the upper-left
or upper-right corners and one in which there were bright squares in the lower-left or lower-right
corners. In the light-dark experiment three participants were asked to distinguish between three
strips that each had two light blobs and three strips that each had only one light blob. Finally, in the
faces experiment three participants were asked to distinguish between two faces. The four-square
data were collected by [7] and were also analyzed in [4]. The other data are newly gathered. Each
data set consists of approximately 4000 trials from each subject. To maintain their interest in the
task, participants were given auditory feedback after each trial that indicated success or failure.

4

Figure 2: The most probable ω parameters found for the four-square experiments for different values
of N and the mutual information between these feature detectors and the observed classiﬁcations.

Fitting GRIFT models is not especially sensitive to the random initialization procedure used to start
each EM instance. The λ∗ parameters were initialized by normal random samples and then half
were negated so the features would tend to start evenly assigned to the two classes, except for λ∗
0,
which was initialized to 0. In the four-square experiments, the ω∗ parameters were initialized by
a mixture of normal distributions and in the light-dark experiments they were initialized from a
uniform distribution. In the faces experiments the ω∗ were initialized by adding normal noise to the
optimal linear classiﬁer separating the two targets. Because of the large number of pixels in the faces
stimuli, the other initialization procedures frequently produced initial assignments with extremely
low probabilities, which led to numerical precision problems. In the four-square experiments, the
β∗ were initialized randomly. In the other experiments, the intent was to set them to the optimal
threshold for distinguishing the classes using the initial ω∗ as a linear classiﬁer, but a programming
error set them to the negation of that value. In most cases, the results were insensitive to the choice
of initialization method.
In the four-square experiment, the noise levels were continually adjusted to keep the participants’
performance at approximately 71% using the stair-casing algorithm [8]. This performance level is
high enough to keep the participants engaged in the task, but allows for sufﬁcient noise to explore
their responses in a large volume of the stimulus space. After an initial adaptation period, the
noise level remains relatively constant across trials, so the inter-trial dependence introduced by the
stair-casing can be safely ignored. Two simulated observers were created to validate GRIFT on
the four-square task. Each used a GRIFT model with pre-speciﬁed parameters to probabilistically
classify four-square data at a ﬁxed noise level, which was chosen to produce approximately 70%
correct performance. The corners observer used four feature detectors, one for each bright corner,
whereas the top-v.-bottom observer contrasted the brightness of the top and bottom pixels.
The result of using GRIFT to recover the feature detectors are displayed in Figure 2. Only the
ω parameters are displayed because they are the most informative. Dark pixels indicate negative
weights and bright pixels correspond to positive weights. The presence of dark and light regions in a
feature detector indicates the computation of contrasts between those areas. The sign of the weights
is not signiﬁcant — given a ﬁxed number of features, there are typically several equivalent sets of
feature detectors that only differ from each other in the signs of their ω terms and in the associated
λ and β values.
Because the optimal number of features for human subjects is unknown, GRIFT models with 1–4
features were ﬁt to the data from each subject. The correct number of features could be determined
by holding out a test set or by performing cross-validation. Simulation demonstrated that a reliable
test set would need to contain nearly all of the gathered samples, and computational expense made
cross-validation impractical with our current MATLAB implementation. Instead, after recovering
the parameters, we estimated the mutual information between the unobserved F variables and the
observed classiﬁcations C. Mutual information measures how well the feature detector outputs can

5

123450.050.10.150.2simulations12340.050.10.150.20.25humansN=1N=2N=3N=4four square: most probable ωi valueshumanssimulations-+Nmutual informationJG12340.050.10.150.20.25  abcdRS12340.050.10.150.20.25  abcdcorners1234567891000.10.20.30.40.50.60.70.80.91  ztop v. bottom12340.050.10.150.20.25  abcdEA1234567891000.10.20.30.40.50.60.70.80.91  z12340.050.10.150.20.25  abcdACpredict the subject’s classiﬁcation decision. Unlike the log likelihood of the observations, which is
dependent on the choice to model C with a logistic regression function, mutual information does
not assume a particular relationship between F and C and does not necessarily increase with N.
Plotting the mutual information as N increases can indicate if new detectors are making a substantial
contribution or are overﬁtting the data. On the simulated observers’ data, for which the true values of
N were known, mutual information was a more accurate model selection indicator than traditional
statistics such as the Bayesian or Akaike information criteria [3].
Fitting GRIFT to the simulated observers demonstrated that if the model is accurate, the correct
features can be recovered reliably. The top-v.-bottom observer showed no substantial increase in
mutual information as the number of features increased from 1 to 4. Each set of recovered feature
detectors included a top-bottom contrast detector and other detectors with noisy ωis that did not
contribute much to predicting C. Although the observer truly used two detectors, one top-brighter
detector and one bottom-brighter detector, the recovery of only one top-bottom contrast detector is
a success because one contrast detector plus a suitable λ0 term is logically equivalent to the original
two-feature model. The corners observer showed a substantial increase in mutual information as N
increased from 1 to 4 and the ω values clearly indicate four corner-sensitive feature detectors. The
corners data was also tested with a ﬁve-feature GRIFT model (ω not shown) which produced four
corner detectors and one feature with noisy ωi. Its gain in mutual information was smaller than that
observed on any of the previous steps. Note the corner areas in the ωis recovered from the corners
data are sometimes black and sometimes white. Recall that these are not image pixel values that the
detectors are attempting to match, but positive and negative weights indicating that the brightness in
the corner region is being contrasted to the brightness of the rest of the image.
Even though targets consisted of four bright-corner stimuli, recovering the parameters from the top-
v.-bottom observer never produced ω values indicating corner-speciﬁc feature detectors. An impor-
tant advantages of GRIFT over previous methods such as [4] is that targets will not “contaminate”
the recovered detectors. The simulations demonstrate that the recovered detectors are determined by
the classiﬁcation strategy, not by the structure of the targets and classes.
The data of the four human participants revealed some interesting differences. Participants EA and
RS were naive, while AC and JG were not. The largest disparity was between EA and JG. EA’s
data indicated no consistent pattern of mutual information increase after two features, and the two-
feature model appears to contain two top-bottom contrast detectors. Therefore, it is reasonable to
conclude that EA was not explicitly detecting the corners. At the other extreme is participant JG,
whose data shows four very clear corner detectors and a steady increase in mutual information up to
four features. Therefore, it seems very likely that this participant was matching corners and probably
should be tested with a ﬁve-feature model to gain additional insight. AC and RS’s data suggest three
corner detectors and a top-bottom contrast detector. GRIFT’s output indicates qualitative differences
in the classiﬁcation strategies used by the four human participants.
Across all participants, the best one-feature model was based on the contrast between the top of the
image and the bottom. This is extremely similar to the result produced by a classiﬁcation image of
the data, reinforcing the strong similarity between one-feature GRIFT and that approach.
In the light-dark and faces experiments, stair-casing was used the adjust the noise level to the 71%
performance level at the beginning of each session and then the noise level was ﬁxed for the remain-
ing trials to improve the independence of the samples. Participants were paid and promised a $10
reward for achieving the highest score on the task.
Participants P1, P2, and P3 classiﬁed the light-dark stimuli. P1 and P2 achieved at or above the ex-
pected performance level (82% and 73% accuracy), while P3’s performance was near chance (55%).
Because the noise levels were ﬁxed after the ﬁrst 101 trials, a participant with good luck at the end
of that period could experience very high noise levels for the remainder of the experiment, leading
to poor performance. All three participants appear to have used different classiﬁcation methods,
providing a very informative contrast. The results of ﬁtting the GRIFT model are in Figure 3.
The ﬂat mutual information graph and the presence of a feature detector thresholding the overall
brightness for each value of N indicate that P1 pursued a one-feature, linear-classiﬁer strategy. P2,
on the other hand, clearly employed a multi-feature, non-linear strategy. For N = 1 and N = 2, the
most interpretable feature detector is an overall brightness detector, which disappears when N = 3
and the best ﬁt model consists of three detectors looking for speciﬁc patterns, one for each position a

6

Figure 3: The most probable ω parameters found for the light-dark and faces experiments for differ-
ent N and the mutual information between these feature detectors and the observed classiﬁcations.

bright or dark spot can appear. Then when N = 4 the overall brightness detector reappears, added to
the three spot detectors. Apparently the spot detectors are only effective if they are all present. With
only three available detectors, the overall brightness detector is excluded, but the optimal assignment
includes all four detectors. This is the best-ﬁt model because increasing to N = 5 keeps the mutual
information constant and adds a detector that is active for every stimulus. Always active detectors
function as constant additions to λ0, therefore this is equivalent to the N = 4 solution.
The GRIFT models of participant P3 do not show a substantial increase in mutual information as the
number of features rises. This lack of increase leads to the conclusion that the one-feature model is
probably the best ﬁt, and since performance was extremely low, it can be assumed that the subject
was reduced to near random guessing much of the time.
The clear distinction between the results for all three subjects demonstrates the effectiveness of
GRIFT and the mutual information measure in distinguishing between classiﬁcation strategies.
The faces presented the largest computational challenges. The targets were two unﬁltered faces
from Gold et al.’s data set [6], down-sampled to 128x128. After the experiment, the stimuli were
down-sampled further to 32x32 and the background surrounding the faces was removed by cropping,
reducing the stimuli to 26x17. These steps made the algorithm computationally feasible, and reduced
the number of parameters so they would be sufﬁciently constrained by the samples.
The results for three participants (P4, P5, and P6) are in Figure 3. Participants P4 and P5’s data were
clearly best ﬁt by one-feature GRIFT models. Increasing the number of features simply caused the
algorithm to add features that were never or always active. Never active features cannot affect the
classiﬁcation, and, as explained previously, always active features are also superﬂuous. P4’s one-
feature model clearly places signiﬁcant weight near the eyebrows, nose, and other facial features.
P5’s one-feature weights are much noisier and harder to interpret. This might be related to P5’s poor
performance on the task — only 53% accuracy compared to P4’s 72% accuracy. Perhaps the noise
level was too high and P5 was guessing rather than using image information much of the time.
Participant P6’s data did produce a two-feature GRIFT model, albeit one that is difﬁcult to interpret
and which only caused a small rise in mutual information. Instead of recovering independent part
detectors, such as a nose detector and an eye detector, GRIFT extracted two subtly different holistic
feature detectors. Given P6’s poor performance (58% accuracy) these features may, like P5’s results,
be indicative of a guessing strategy that was not strongly inﬂuenced by the image information.
The results on faces support the hypothesis that face classiﬁcation is holistic and conﬁgural, rather
than the result of part classiﬁcations, especially when individual feature detection is difﬁcult [11].

7

light-dark: most probable ωi valuesN=1N=2N=3N=4N=5-+P212340.050.10.150.20.25  abcdP31234567891000.10.20.30.40.50.60.70.80.91  zP112340.050.10.150.20.25  abcdN=1N=2N=3faces: most probable ωi valuesP512340.050.10.150.20.25  abcdP61234567891000.10.20.30.40.50.60.70.80.91  zP412340.050.10.150.20.25  abcd-+1234500.050.10.150.2mutual informationNlight-dark12300.010.020.030.04mutual informationNfacesAcross these experiments, the data collected were compatible with the original classiﬁcation image
method. In fact, the four-square human data were originally analyzed using that algorithm. One of
the advantages of GRIFT is that it can reanalyze old data to reveal new information. In the one-
feature case, GRIFT enables the use of prior probabilities on the parameters, which may improve
performance when data is too scarce for the classiﬁcation image approach. Most importantly, ﬁtting
multi-feature GRIFT models can reveal previously hidden non-linear classiﬁcation strategies.

6 Conclusion

This paper has described the GRIFT model for determining the features used in human image classi-
ﬁcation. GRIFT is an advance over previous methods that assume a single linear classiﬁer on pixels
because it describes classiﬁcation as the combination of multiple independently detected features. It
provides a probabilistic model of human visual classiﬁcation that accounts for data and incorporates
prior beliefs about the features. The feature detectors it ﬁnds are associated with the classiﬁcation
strategy employed by the observer and are not the result of structure in the classes’ target images.
GRIFT’s value has been demonstrated by modeling the performance of humans on the four-square,
light-dark, and faces classiﬁcation tasks and by successfully recovering the parameters of computer
simulated observers in the four-square task. Its inability to ﬁnd multiple local features when analyz-
ing human performance on the faces task agrees with previous results.
One of the strengths of the graphical model approach is that it allows easy replacement of model
components. An expert can easily change the prior distributions on the parameters to reﬂect knowl-
edge gained in previous experiments. For example, it might be desirable to encourage the formation
of edge detectors. New resolution-independent feature parameterizations can be introduced, as can
transformation parameters to make the features translationally and rotationally invariant. If the fea-
tures have explicitly parameterized locations and orientations, the model could be extended to model
their joint relative positions, which might provide more information about domains such as face clas-
siﬁcation. The success of this version of GRIFT provides a ﬁrm foundation for these improvements.

Acknowledgments

This research was supported by NSF Grant SES-0631602 and NIMH grant MH16745. The authors
thank the reviewers, Tom Grifﬁths, Erik Learned-Miller, and Adam Sanborn for their suggestions.

References
[1] A.J. Ahumada, Jr. Classiﬁcation image weights and internal noise level estimation. Journal of Vision,

2(1), 2002.

[2] C.M. Bishop. Neural Networks for Pattern Recognition. Oxford University Press, 1995.
[3] C.M. Bishop. Pattern Recognition and Machine Learning. Springer, 2006.
[4] A.L. Cohen, R.M. Shiffrin, J.M. Gold, D.A. Ross, and M.G. Ross. Inducing features from visual noise.

Journal of Vision, 7(8), 2007.

[5] A. Gelman, J.B. Carlin, H.S. Stern, and D.B. Rubin. Bayesian Data Analysis. Chapman & Hall/CRC,

2003.

[6] J.M. Gold, P.J. Bennett, and A.B. Sekuler. Identiﬁcation of band-pass ﬁltered letters and faces by human

and ideal observers. Vision Research, 39, 1999.

[7] J.M. Gold, A.L. Cohen, and R. Shiffrin. Visual noise reveals category representations. Psychonomics

Bulletin & Review, 15(4), 2006.

[8] N.A. Macmillan and C.D. Creelman. Detection Theory: A User’s Guide. Lawrence Erlbaum Associates,

2005.

[9] S.E. Palmer. Vision Science: Photons to Phenomenology. The MIT Press, 1999.
[10] D.G. Pelli, B. Farell, and D.C. Moore. The remarkable inefﬁciency of word recognition. Nature, 425,

2003.

[11] J. Sergent. An investigation into component and conﬁgural processes underlying face perception. British

Journal of Psychology, 75, 1984.

8

"
416,2007,Simplified Rules and Theoretical Analysis for Information Bottleneck Optimization and PCA with Spiking Neurons,"We show that under suitable assumptions (primarily linearization) a simple and perspicuous online learning rule for Information Bottleneck optimization with spiking neurons can be derived. This rule performs on common benchmark tasks as well as a rather complex rule that has previously been proposed \cite{KlampflETAL:07b}. Furthermore, the transparency of this new learning rule makes a theoretical analysis of its convergence properties feasible. A variation of this learning rule (with sign changes) provides a theoretically founded method for performing Principal Component Analysis {(PCA)} with spiking neurons. By applying this rule to an ensemble of neurons, different principal components of the input can be extracted. In addition, it is possible to preferentially extract those principal components from incoming signals $X$ that are related or are not related to some additional target signal $Y_T$. In a biological interpretation, this target signal $Y_T$ (also called relevance variable) could represent proprioceptive feedback, input from other sensory modalities, or top-down signals.","Simpliﬁed Rules and Theoretical Analysis for

Information Bottleneck Optimization and PCA with

Spiking Neurons

Lars Buesing, Wolfgang Maass

Institute for Theoretical Computer Science

Graz University of Technology

A-8010 Graz, Austria

{lars,maass}@igi.tu-graz.at

Abstract

We show that under suitable assumptions (primarily linearization) a simple and
perspicuous online learning rule for Information Bottleneck optimization with
spiking neurons can be derived. This rule performs on common benchmark tasks
as well as a rather complex rule that has previously been proposed [1]. Further-
more, the transparency of this new learning rule makes a theoretical analysis of
its convergence properties feasible. A variation of this learning rule (with sign
changes) provides a theoretically founded method for performing Principal Com-
ponent Analysis (PCA) with spiking neurons. By applying this rule to an ensem-
ble of neurons, different principal components of the input can be extracted. In
addition, it is possible to preferentially extract those principal components from
incoming signals X that are related or are not related to some additional target
signal YT . In a biological interpretation, this target signal YT (also called rele-
vance variable) could represent proprioceptive feedback, input from other sensory
modalities, or top-down signals.

1 Introduction

The Information Bottleneck (IB) approach [2] allows the investigation of learning algorithms for
unsupervised and semi-supervised learning on the basis of clear optimality principles from infor-
mation theory. Two types of time-varying inputs X and YT are considered. The learning goal is
to learn a transformation from X into another signal Y that extracts only those components from
X that are related to the relevance signal YT . In a more global biological interpretation X might
represent for example some sensory input, and Y the output of the ﬁrst processing stage for X in
the cortex. In this article Y will simply be the spike output of a neuron that receives the spike trains
X as inputs. The starting point for our analysis is the ﬁrst learning rule for IB optimization in for
this setup, which has recently been proposed in [1], [3]. Unfortunately, this learning rule is compli-
cated, restricted to discrete time and no theoretical analysis of its behavior is feasible. Any online
learning rule for IB optimization has to make a number of simplifying assumptions, since true IB
optimization can only be carried out in an ofﬂine setting. We show here, that with a slightly different
set of assumptions than those made in [1] and [3], one arrives at a drastically simpler and intuitively
perspicuous online learning rule for IB optimization with spiking neurons. The learning rule in [1]
was derived by maximizing the objective function1 L0:

L0 = −I(X, Y ) + βI(Y, YT ) − γDKL(P (Y )kP ( ˜Y )),

(1)

1The term DKL(P (Y )kP ( ˜Y )) denotes the Kullback-Leibler divergence between the distribution P (Y )
and a target distribution P ( ˜Y ). This term ensures that the weights remain bounded, it is shortly discussed in
[4].

1

where I(., .) denotes the mutual information between its arguments and β is a positive trade-off fac-
tor. The target signal YT was assumed to be given by a spike train. The learning rule from [1] (see
[3] for a detailed interpretation) is quite involved and requires numerous auxiliary deﬁnitions (hence
we cannot repeat it in this abstract). Furthermore, it can only be formulated in discrete time (steps
size ∆t) for reasons we want to outline brieﬂy: In the limit ∆t → 0 the essential contribution to the
learning rule, which stems from maximizing the mutual information I(Y, YT ) between output and
target signal, vanishes. This difﬁculty is rooted in a rather technical assumption, made in appendix
k at time step k of the neural ﬁring probability ρ ,
A.4 in [3], concerning the expectation value ρ
given the information about the postsynaptic spikes and the target signal spikes up to the preceding
time step k − 1 (see our detailed discussion in [4])2. The restriction to discrete time prevents the
application of powerful analytical methods like the Fokker-Planck equation, which requires contin-
uous time, for analyzing the dynamics of the learning rule.
In section 2 of this paper, we propose a much simpler learning rule for IB optimization with spiking
neurons, which can also be formulated in continuous time. In contrast to [3], we approximate the
k with a linear estimator, under the assumption that X and YT are positively corre-
critical term ρ
lated. Further simpliﬁcations in comparison to [3] are achieved by considering a simpler neuron
model (the linear Poisson neuron, see [5]). However we show through computer simulation in [4]
that the resulting simple learning rule performs equally well for the more complex neuron model
with refractoriness from [1] - [5]. The learning rule presented here can be analyzed by the means of
the drift function of the corresponding Fokker-Planck equation. The theoretical results are outlined
in section 3, followed by the consideration of a concrete IB optimization task in section 4. A link
between the presented learning rule and Principal Component Analysis (PCA) is established in sec-
tion 5. A more detailed comparison of the learning rule presented here and the one of [3] as well as
results of extensive computer tests on common benchmark tasks can be found in [4].

2 Neuron model and learning rule for IB optimization

We consider a linear Poisson neuron with N synapses of weights w = (w1, . . . , wN ) . It is driven by
j denotes
the time of the i’th spike at synapse j. The membrane potential u(t) of the neuron at time t is given
by the weighted sum of the presynaptic activities ν(t) = (ν1(t), . . . , νN (t)):

the input X, consisting of N spike trains Xj(t) = Pi δ(t − ti

j), j ∈ {1, . . . , N }, where ti

N

u(t) =

Xj=1
νj(t) = Z t

wjνj(t)

(2)

ǫ(t − s)Xj(s)ds.

−∞

The kernel ǫ(.) models the EPSP of a single spike (in simulations ǫ(t) was chosen to be a decaying
exponential with a time constant of τm = 10 ms). The postsynaptic neuron spikes at time t with the
probability density g(t):

g(t) =

u(t)
u0

,

with u0 being a normalization constant. The postsynaptic spike train is denoted as Y (t) = Pi δ(t −
f ), with the ﬁring times ti
f .
ti
We now consider the IB task described in general in [2], which consists of maximizing the objective
function LIB, in the context of spiking neurons. As in [6], we introduce a further term L3 into
the the objective function that reﬂects the higher metabolic costs for the neuron to maintain strong
j . Thus the complete objective function L

synapses, a natural, simple choice being L3 = −λP w2

to maximize is:

L = LIB + L3 = −I(X, Y ) + βI(YT , Y ) − λ

N

Xj=1

w2
j .

(3)

2The remedy, proposed in section 3.1 in [3], of replacing the mutual information I(Y, YT ) in L0 by an
information rate I(Y, YT )/∆t does not solve this problem, as the term I(Y, YT )/∆t diverges in the continuous
time limit.

2

The objective function L differs slightly from L0 given in (1), which was optimized in [3]; this
change turned out to be advantageous for the PCA learning rule given in section 5, without signiﬁ-
cantly changing the characteristics of the IB learning rule.
The online learning rule governing the change of the weights wj(t) at time t is obtained by a gradient
ascent of the objective function L:

d
dt

wj(t) = α

∂L
∂wj

.

For small learning rates α and under the assumption that the presynaptic input X and the target
signal YT are stationary processes, the following learning rule can be derived:

d
dt

wj(t) = α

Y (t)νj(t)

u(t)u(t) (cid:16)− (u(t) − u(t)) + β(cid:16)F [YT ](t) − F [YT ](t)(cid:17)(cid:17) − αλwj(t),

(4)

where the operator (.) denotes the low-pass ﬁlter with a time constant τC (in simulations τC = 3s),
i. e. for a function f:

f (t) =

1

τC Z t

−∞

exp(cid:18)−

t − s

τC (cid:19) f (s)ds.

(5)

The operator F [YT ](t) appearing in (4) is equal to the expectation value of the membrane potential
hu(t)iX|YT = E[u(t)|YT ], given the observations (YT (τ )|τ ∈ R) of the relevance signal; F is thus
closely linked to estimation and ﬁltering theory. For a known joint distribution of the processes X
and YT , the operator F could in principal be calculated exactly, but it is not clear how this quantity
can be estimated in an online process; thus we look for a simple approximation to F . Under the
above assumptions, F is time invariant and can be approximated by a Volterra series (for details see
[4]):

hu(t)iX|YT = F [YT ](t) =

∞

Xn=0ZR

· · ·ZR

κn(t − t1, . . . , t − tn)

n

Yi=1

YT (ti)dti.

(6)

In this article, we concentrate on the situation, where F can be well approximated by its linearization
F1[YT ](t), corresponding to a linear estimator of hu(t)iX|YT
. For F1[YT ](t) we make the following
ansatz:

F [YT ](t) ≈ F1[YT ](t) = c · uT (t) = cZR

κ1(t − t1)YT (t1)dt1.

(7)

According to (7), F is approximated by a convolution uT (t) of the relevance signal YT and a suit-
able prefactor c. Assuming positively correlated X and YT , κ1(t) is chosen to be a non-anticipating
decaying exponential exp(−t/τ0)Θ(t) with a time constant τ0 (in simulations τ0 = 100 ms), where
Θ(t) is the Heaviside step function. This choice is motivated by the standard models for the impact
of neuromodulators (see [7]), thus such a kernel may be implemented in a realistic biological mech-
anism. It turned out that the choice of τ0 was not critical, it could be varied over a decade ranging
from 10 ms to 100 ms. The prefactor c appearing in (7) can be determined from the fact that F1 is
the optimal linear estimator of the form given in (7), leading to:

c =

huT (t), u(t)i
huT (t), uT (t)i

.

The quantity c can be estimated online in the following way:

d
dt

c(t) = (uT (t) − uT (t)) [(u(t) − u(t)) − c(t)(uT (t) − uT (t))] .

Using the above deﬁnitions, the resulting learning rule is given by (in vector notation):

d
dt

w(t) = α

Y (t)ν(t)
u(t)u(t)

[− (u(t) − u(t)) + c(t)β(uT (t) − uT (t))] − αλw(t).

(8)

Equation (8) will be called the spike-based learning rule, as the postsynaptic spike train Y (t) explic-
itly appears. An accompanying rate-base learning rule can also be derived:

d
dt

w(t) = α

ν(t)

u0u(t)

[− (u(t) − u(t)) + c(t)β(uT (t) − uT (t))] − αλw(t).

(9)

3

3 Analytical results

The learning rules (8) and (9) are stochastic differential equations for the weights wj driven by the
processes Y (.), νj(.) and uT (.), of which the last two are assumed to be stationary with the means
hνj(t)i = ν0 and huT (t)i = uT ,0 respectively. The evolution of the solutions w(t) to (8) and (9)
may be studied via a Master equation for the probability distribution of the weights p(w, t) (see [8]).
For small learning rates α, the stationary distribution p(w) sharply peaks3 at the roots of the drift
function A(w) of the corresponding Fokker-Planck equation (the detailed derivation is given in [4]).
Thus, for α ≪ 1, the temporal evolution of the learning rules (8) and (9) may be studied via the
deterministic differential equation:

d
dt

ˆw = A( ˆw) = α

z =

N

Xj=1

ˆwj,

1

ν0u0z (cid:0)−C 0 + βC 1(cid:1) ˆw − αλ ˆw

(10)

(11)

where z is the total weight. The matrix C = −C 0 + βC 1 (with the elements Cij) has two contribu-
tions. C 0 is the covariance matrix of the input and the matrix C 1 quantiﬁes the covariance between
the activities νj and the trace uT :

C 0

ij = hνi(t), νj(t)i

C 1

ij =

hνi(t), uT (t)ihuT (t), νj(t)i

huT (t), uT (t)i

.

Now the critical points w∗ of dynamics of (10) are investigated. These critical points, if asymptoti-
cally stable, determine the peaks of the stationary distribution p(w) of the weights w; we therefore
expect the solutions of the stochastic equations to ﬂuctuate around these ﬁxed points w∗. If β and λ
are much larger than one, the term containing the matrix C 0 can be neglected and equation (10) has
a unique stable ﬁxed point w∗:

w∗ ∝ C T
C T
i = hνi(t), uT (t)i .

Under this assumption the maximal mutual information between the target signal YT (t) and the
output of the neuron Y (t) is obtained by a weight vector w = w∗ that is parallel to the covariance
vector C T .
In general, the critical points of equation (10) depend on the eigenvalue spectrum of the symmetric
matrix C: If all eigenvalues are negative, the weight vector ˆw decays to the lower hard bound 0. In
case of at least one positive eigenvalue (which exists if β is chosen large enough), there is a unique
stable ﬁxed point w∗:

w∗ =

b

:=

b

µ

λu0ν0b
N

bi.

Xi=1

(12)

The vector b appearing in (12) is the eigenvector of C corresponding to the largest eigenvalue µ.
Thus, a stationary unimodal4 distribution p(w) of the weights w is predicted, which is centered
around the value w∗.

4 A concrete example for IB optimization

A special scenario of interest, that often appears in the literature (see for example [1], [9] and [10]),
is the following: The synapses, and subsequently the input spike trains, form M different subgroups

3It can be shown that the diffusion term in the FP equation scales like O(α), i. e. for small learning rates α,

ﬂuctuations tend to zero and the dynamics can be approximated by the differential equation (10) .

4Note that p(w) denotes the distribution of the weight vector, not the distribution of a single weight p(wj).

4

A

X  (t)

1

X  (t)

2

X  (t)

N

C

Output Y(t)

Relevance
Signal Y (t)T

B

D

Figure 1: A The basic setup for the Information Bottleneck optimization. B-D Numerical and
analytical results for the IB optimization task described in section 4. The temporal evolution of

the average weights ˜wl = 1/M Pj∈Gl wj of the four different synaptic subgroups Gl are shown.

B The performance of the spike-based rule (8). The highest trajectory corresponds to ˜w1; it stays
close to its analytical predicted ﬁxed point value obtained from (12), which is visualized by the
upper dashed line. The trajectory just below belongs to ˜w3, for which the ﬁxed point value is also
plotted as dashed line. The other two trajectories ˜w2 and ˜w4 decay and eventually ﬂuctuate above
the predicted value of zero. C The performance of the rate-based rule (9); results are analogous to
the ones of the spike-based rule. D Simulation of the deterministic equation (10).

Gl, l ∈ {1, . . . , N/M } of the same size N/M ∈ N. The spike trains Xj and Xk, j 6= k, are statis-
tically independent if they belong to different subgroups; within a subgroup there is a homogeneous
covariance term C 0
jk = cl, j 6= k for j, k ∈ Gl, which can be due either to spike-spike correlations
or correlations in rate modulations. The covariance between the target signal YT and the spike trains
Xj is homogeneous among a subgroup.
As a numerical example, we consider in ﬁgure 1 a modiﬁcation of the IB task presented in ﬁgure 2 of
[1]. The N = 100 synapses form M = 4 subgroups Gl = {25(l − 1) + 1, . . . , 25l}, l ∈ {1, . . . , 4}.
Synapses in G1 receive Poisson spike trains of constant rate ν0 = 20 Hz, which are mutually spike-
spike correlated with a correlation-coefﬁcient5 of 0.5. The same holds for the spike trains of G2.
Spike trains for G3 and G4 are uncorrelated Poisson trains with a common rate modulation, which is
equal to low pass ﬁltered white noise (cut-off frequency 5 Hz) with mean ν0 and standard deviation
(SD) σ = ν0/2. The rate modulations for G3 and G4 are however independent (though identically
distributed). Two spike trains for different synapse subgroups are statistically independent. The
target signal YT was chosen to be the sum of two Poisson trains. The ﬁrst is of constant rate ν0 and
has spike-spike correlations with G1 of coefﬁcient 0.5; the second is a Poisson spike train with the
same rate modulation as the spike trains of G3 superimposed by additional white noise of SD 2 Hz.
Furthermore, the target signal was turned off during random intervals6. The resulting evolution of
the weights is shown in ﬁgure 1, illustrating the performance of the spike-based rule (8) as well as
of the rate-based rule (9). As expected, the weights of G1 and G3 are potentiated as YT has mutual
information with the corresponding part of the input. The synapses of G2 and G4 are depressed.
The analytical result for the stable ﬁxed point w∗ obtained from (12) is shown as dashed lines and
is in good agreement with the numerical results. Furthermore the trajectory of the solution ˆw(t) to

5Spike-spike correlated Poisson spike trains were generated according to the method outlined in [9].
6These intervals of silence were modeled as random telegraph noise with a time constant of 200 ms and a

overall probability of silence of 0.5.

5

the deterministic equation (10) is plotted.
The presented concrete IB task was slightly changed from the one presented in [1], because for the
setting used here, the largest eigenvalue µ of C and its corresponding eigenvector b can be calculated
analytically. The simulation results for the original setting in [1] can also be reproduced with the
simpler rules (8) and (9) (not shown).

5 Relevance-modulated PCA with spiking neurons

The presented learning rules (8) and (9) exhibit a close relation to Principal Component Analysis
(PCA). A learning rule which enables the linear Poisson neuron to extract principal components
from the input X(.) can be derived by maximizing the following objective function:

LPCA = −LIB − λ

N

Xj=1

w2

j = +I(X, Y ) − βI(YT , Y ) − λ

N

Xj=1

w2
j ,

(13)

which just differs from (3) by a change of sign in front of LIB. The resulting learning rule is in close
analogy to (8):

d
dt

w(t) = α

Y (t)ν(t)
u(t)u(t)

[(u(t) − u(t)) − c(t)β(uT (t) − uT (t))] − αλw(t).

(14)

The corresponding rate-based version can also be derived. Without the trace uT (.) of the target sig-
nal, it can be seen that the solution ˆw(t) of deterministic equation corresponding to (14) (which is of
the same form as (10) with the obvious sign changes) converges to an eigenvector of the covariance
matrix C 0. Thus, for β = 0 we expect the learning rule (14) to perform PCA for small learning rates
α. The rule (14) without the relevance signal is comparable to other PCA rules, e. g. the covariance
rule (see [11]) for non-spiking neurons.
The side information given by the relevance signal YT (.) can be used to extract speciﬁc principal
components from the input, thus we call this paradigm relevance-modulated PCA. Before we con-
sider a concrete example for relevance-modulated PCA, we want to point out a further application
of the learning rule (14).
The target signal YT can also be used to extract different components from the input with different
neurons (see ﬁgure 2). Consider m neurons receiving the same input X. These neurons have the
outputs Y1(.), . . . , Ym(t), target signals Y 1
T (t) and weight vectors w1(t), . . . , wm(t),
the latter evolving according to (14). In order to prevent all weight vectors from converging towards
the same eigenvector of C 0 (the principal component), the target signal Y i
T for neuron i is chosen to
be the sum of all output spike trains except Yi:

T (.), . . . , Y m

Y i

T (t) =

N

Xj=1, j6=i

Yj(t).

(15)

If one weight vector wi(t) is already close to the eigenvector ek of C 0, than by means of (15), the
basins of attraction of ek for the other weight vectors wj(t), j 6= i are reduced (or even vanish,
depending on the value of β). It is therefore less likely (or impossible) that they also converge to ek.
In practice, this setup is sufﬁciently robust, if only a small number (≤ 4) of different components is
to be extracted and if the differences between the eigenvalues λi of these principal components are
not too big7. For the PCA learning rule, the time constant τ0 of the kernel κ1 (see (7)) had to be
chosen smaller than for the IB tasks in order to obtain good performance; we used τ0 = 10 ms in
simulations. This is in the range of time constants for IPSPs. Hence, the signals Y i
T could probably
be implemented via lateral inhibition.
The learning rule considered in [3] displayed a close relation to Independent Component Analysis
(ICA). Because of the linear neuron model used here and the linearization of further terms in the
derivation, the resulting learning rule (14) performs PCA instead of ICA.
The results of a numerical example are shown in ﬁgure 2. The m = 3 for the regular PCA experi-
ment neurons receive the same input X and their weights change according to (14). The weights and
input spike trains are grouped into four subgroups G1, . . . , G4, as for the IB optimization discussed

7Note that the input X may well exhibit a much larger number of principal components. However it is only

possible to extract a limited number of them by different neurons at the same time.

6

A

X  (t)

1

X  (t)

2

NX  (t)

D

B

neuron 1

C

neuron 2

Output Y (t)1

Output Y (t)

m

neuron 1

E

neuron 2

F

neuron 3

Figure 2: A The basic setup for the PCA task: The m different neurons receive the same input X
and are expected to extract different principal components of it. B-F The temporal evolution of the

average subgroup weights ˜wl = 1/25Pj∈Gl wj for the groups G1 (black solid line), G2 (light gray

solid line) and G3 (dotted line). B-C Results for the relevance-modulated PCA task: neuron 1 (ﬁg.
B) specializes on G2 and neuron 2 (ﬁg. C) on subgroup G3. D-F Results for the regular PCA task:
neuron 1 (ﬁg. D) specialize on G1, neuron 2 (ﬁg. E) on G2 and neuron 3 (ﬁg. F) on G3 .

in section 4. The only difference is that all groups (except for G4) receive spike-spike correlated
Poisson spike trains with a correlation coefﬁcient for the groups G1, G2, G3 of 0.5, 0.45, 0.4
respectively. Group G4 receives uncorrelated Poisson spike trains. As can be seen in ﬁgure 2 D to
F, the different neurons specialize on different principal components corresponding to potentiated
synaptic subgroups G1, G2 and G3 respectively. Without the relevance signals Y i
T (.), all neurons
tend to specialize on the principal component corresponding to G1 (not shown).
As a concrete example for relevance-modulated PCA, we consider the above setup with slight mod-
iﬁcations: Now we want m = 2 neurons to extract the components G2 and G3 from the input X,
and not the principal component G1. This is achieved with an additional relevance signal Y 0
T , which
is the same for both neurons and has spike-spike correlations with G2 and G3 of 0.45 and 0.4. We
T ) to the objective function (13), where γ is a positive trade-off factor. The
add the term γI(Y, Y 0
resulting learning rule has exactly the same structure as (14), with an additional term due to Y 0
T .
The numerical results are presented in ﬁgure 2 B and C, showing that it is possible in this setup to
explicitly select the principle components that are extracted (or not extracted) by the neurons.

6 Discussion

We have introduced and analyzed a simple and perspicuous rule that enables spiking neurons to
perform IB optimization in an online manner. Our simulations show that this rule works as well
as the substantially more complex learning rule that had previously been proposed in [3]. It also
performs well for more realistic neuron models as indicated in [4]. We have shown that the con-
vergence properties of our simpliﬁed IB rule can be analyzed with the help of the Fokker-Planck
equation (alternatively one may also use the theoretical framework described in A.2 in [12] for its
analysis). The investigation of the weight vectors to which this rule converges reveals interesting
relationships to PCA. Apparently, very little is known about learning rules that enable spiking neu-
rons to extract multiple principal components from an input stream (a discussion of a basic learning
rule performing PCA is given in chapter 11.2.4 of [5]). We have demonstrated both analytically and
through simulations that a slight variation of our new learning rule performs PCA. Our derivation
of this rule within the IB framework opens the door to new variations of PCA where preferentially
those components are extracted from a high dimensional input stream that are –or are not– related to
some external relevance variable. We expect that a further investigation of such methods will shed
light on the unknown principles of unsupervised and semi-supervised learning that might shape and
constantly retune the output of lower cortical areas to intermediate and higher cortical areas. The
learning rule that we have proposed might in principle be able to extract from high-dimensional

7

sensory input streams X those components that are related to other sensory modalities or to internal
expectations and goals.
Quantitative biological data on the precise way in which relevance signals YT (such as for example
dopamin) might reach neurons in the cortex and modulate their synaptic plasticity are still missing.
But it is fair to assume that these signals reach the synapse in a low-pass ﬁltered form of the type uT
that we have assumed for our learning rules. From that perspective one can view the learning rules
that we have derived (in contrast to the rules proposed in [3]) as local learning rules.

Acknowledgments

Written under partial support by the Austrian Science Fund FWF, project # P17229, project # S9102
and project # FP6-015879 (FACETS) of the European Union.

References

[1] S. Klampﬂ, R. A. Legenstein, and W. Maass. Information bottleneck optimization and independent com-
ponent extraction with spiking neurons. In Proc. of NIPS 2006, Advances in Neural Information Process-
ing Systems, volume 19. MIT Press, 2007.

[2] N. Tishby, F. C. Pereira, and W. Bialek. The information bottleneck method. In Proceedings of the 37-th

Annual Allerton Conference on Communication, Control and Computing, pages 368–377, 1999.

[3] S. Klampﬂ, R. Legenstein, and W. Maass. Spiking neurons can learn to solve information bottleneck

problems and to extract independent components. Neural Computation, 2007. in press.

[4] L. Buesing and W. Maass. journal version. 2007. in preparation.
[5] W. Gerstner and W. M. Kistler. Spiking Neuron Models. Cambridge University Press, Cambridge, 2002.
[6] Taro Toyoizumi, Jean-Pascal Pﬁster, Kazuyuki Aihara, and Wulfram Gerstner. Optimality Model of
Unsupervised Spike-Timing Dependent Plasticity: Synaptic Memory and Weight Distribution. Neural
Computation, 19(3):639–671, 2007.

[7] Eugene M. Izhikevich. Solving the Distal Reward Problem through Linkage of STDP and Dopamine

Signaling. Cereb. Cortex, page bhl152, 2007.

[8] H. Risken. The Fokker-Planck Equation. Springer, 3rd edition, 1996.
[9] R. G¨utig, R. Aharonov, S. Rotter, and H. Sompolinsky. Learning input correlations through non-linear

temporally asymmetric hebbian plasticity. Journal of Neurosci., 23:3697–3714, 2003.

[10] H. Mefﬁn, J. Besson, A. N. Burkitt, and D. B. Grayden. Learning the structure of correlated synaptic
subgroups using stable and competitive spike-timing-dependent plasticity. Physical Review E, 73, 2006.
[11] T. J. Sejnowski and G. Tesauro. The hebb rule for synaptic plasticity: algorithms and implementations. In
J. H. Byrne and W. O. Berry, editors, Neural Models of Plasticity, pages 94–103. Academic Press, 1989.
[12] N. Intrator and L. N. Cooper. Objective function formulation of the BCM theory of visual cortical plas-

ticity: statistical connections, stability conditions. Neural Networks, 5:3–17, 1992.

8

"
953,2007,Modeling image patches with a directed hierarchy of Markov random fields,"We describe an efficient learning procedure for multilayer generative models that combine the best aspects of Markov random fields and deep, directed belief nets. The generative models can be learned one layer at a time and when learning is complete they have a very fast inference procedure for computing a good approximation to the posterior distribution in all of the hidden layers. Each hidden layer has its own MRF whose energy function is modulated by the top-down directed connections from the layer above. To generate from the model, each layer in turn must settle to equilibrium given its top-down input. We show that this type of model is good at capturing the statistics of patches of natural images.","Modeling image patches with a directed hierarchy of

Markov random ﬁelds

Simon Osindero and Geoffrey Hinton

Department of Computer Science, University of Toronto

6, King’s College Road, M5S 3G4, Canada
osindero,hinton@cs.toronto.edu

Abstract

We describe an efﬁcient learning procedure for multilayer generative models that
combine the best aspects of Markov random ﬁelds and deep, directed belief nets.
The generative models can be learned one layer at a time and when learning is
complete they have a very fast inference procedure for computing a good approx-
imation to the posterior distribution in all of the hidden layers. Each hidden layer
has its own MRF whose energy function is modulated by the top-down directed
connections from the layer above. To generate from the model, each layer in turn
must settle to equilibrium given its top-down input. We show that this type of
model is good at capturing the statistics of patches of natural images.

1 Introduction

The soldiers on a parade ground form a neat rectangle by interacting with their neighbors. An ofﬁcer
decides where the rectangle should be, but he would be ill-advised to try to tell each individual sol-
dier exactly where to stand. By allowing constraints to be enforced by local interactions, the ofﬁcer
enormously reduces the bandwidth of top-down communication required to generate a familiar pat-
tern. Instead of micro-managing the soldiers, the ofﬁcer speciﬁes an objective function and leaves
it to the soldiers to optimise that function. This example of pattern generation suggests that a multi-
layer, directed belief net may not be the most effective way to generate patterns. Instead of using
shared ancestors to create correlations between the variables within a layer, it may be more efﬁcient
for each layer to have its own energy function that is modulated by directed, top-down input from
the layer above. Given the top-down input, each layer can then use lateral interactions to settle on
a good conﬁguration and this conﬁguration can then provide the top-down input for the next layer
down. When generating an image of a face, for example, the approximate locations of the mouth
and nose might be speciﬁed by a higher level and the local interactions would then ensure that the
accuracy of their vertical alignment was far greater than the accuracy with which their locations
were speciﬁed top-down.

In this paper, we show that recently developed techniques for learning deep belief nets (DBN’s) can
be generalized to solve the apparently more difﬁcult problem of learning a directed hierarchy of
Markov Random Fields (MRF’s). The method we describe can learn models that have many hidden
layers, each with its own MRF whose energy function is conditional on the values of the variables in
the layer above. It does not require detailed prior knowledge about the data to be modeled, though
it obviously works better if the architecture and the types of latent variable are well matched to the
task.

1

2 Learning deep belief nets: An overview

The learning procedure for deep belief nets has now been described in several places (Hinton et al.,
2006; Hinton and Salakhutdinov, 2006; Bengio et al., 2007) and will only be sketched here. It relies
on a basic module, called a restricted Boltzmann machine (RBM) that can be trained efﬁciently
using a method called “contrastive divergence” (Hinton, 2002).

2.1 Restricted Boltzmann Machines

An RBM consists of a layer of binary stochastic “visible” units connected to a layer of binary,
stochastic “hidden” units via symmetrically weighted connections. A joint conﬁguration, (v, h) of
the visible and hidden units has an energy given by:

E(v, h) = − X

bivi − X

bjhj − X

vihjwij

(1)

i∈visibles

j∈hiddens

i,j

where vi, hj are the binary states of visible unit i and hidden unit j, bi, bj are their biases and wij is
the symmetric weight between them. The network assigns a probability to every possible image via
this energy function and the probability of a training image can be raised by adjusting the weights
and biases to lower the energy of that image and to raise the energy of similar, reconstructed images
that the network would prefer to the real data.
Given a training vector, v, the binary state, hj, of each feature detector, j, is set to 1 with probability
σ(bj + Pi viwij), where σ(x) is the logistic function 1/(1 + exp(−x)), bj is the bias of j, vi is
the state of visible unit i, and wij is the weight between i and j. Once binary states have been
chosen for the hidden units, a reconstruction is produced by setting each vi to 1 with probability
σ(bi + Pj hjwij). The states of the hidden units are then updated once more so that they represent
features of the reconstruction. The change in a weight is given by

∆wij = ǫ(hvihjidata − hvihjirecon)

(2)
where ǫ is a learning rate, hvihjidata is the fraction of times that visible unit i and hidden units j
are on together when the hidden units are being driven by data and hvihjirecon is the corresponding
fraction for reconstructions. A simpliﬁed version of the same learning rule is used for the biases.
The learning works well even though it is not exactly following the gradient of the log probability
of the training data (Hinton, 2002).

2.2 Compositions of experts

A single layer of binary features is usually not the best way to capture the structure in the data. We
now show how RBM’S can be composed to create much more powerful, multilayer models.

After using an RBM to learn the ﬁrst layer of hidden features we have an undirected model that
deﬁnes p(v, h) via the energy function in Eq. 1. We can also think of the model as deﬁning p(v, h)
by deﬁning a consistent pair of conditional probabilities, p(h|v) and p(v|h) which can be used to
sample from the model distribution. A different way to express what has been learned is p(v|h)
and p(h). Unlike a standard directed model, this p(h) does not have its own separate parameters.
It is a complicated, non-factorial prior on h that is deﬁned implicitly by the weights. This peculiar
decomposition into p(h) and p(v|h) suggests a recursive algorithm: keep the learned p(v|h) but
replace p(h) by a better prior over h, i.e. a prior that is closer to the average, over all the data
vectors, of the conditional posterior over h.

We can sample from this average conditional posterior by simply applying p(h|v) to the training
data. The sampled h vectors are then the “data” that is used for training a higher-level RBM that
learns the next layer of features. We could initialize the higher-level RBM model by using the same
parameters as the lower-level RBM but with the roles of the hidden and visible units reversed. This
ensures that p(v) for the higher-level RBM starts out being exactly the same as p(h) for the lower-
level one. Provided the number of features per layer does not decrease, Hinton et al. (2006) show
that each extra layer increases a variational lower bound on the log probability of the data.

The directed connections from the ﬁrst hidden layer to the visible units in the ﬁnal, composite
graphical model are a consequence of the the fact that we keep the p(v|h) but throw away the p(h)
deﬁned by the ﬁrst level RBM. In the ﬁnal composite model, the only undirected connections are

2

between the top two layers, because we do not throw away the p(h) for the highest-level RBM. To
suppress noise in the learning signal, we use the real-valued activation probabilities for the visible
units of all the higher-level RBM’s, but to prevent hidden units from transmitting more than one bit
of information from the data to its reconstruction, we always use stochastic binary values for the
hidden units.

3 Semi-restricted Boltzmann machines

For contrastive divergence learning to work well, it is important for the hidden units to be sampled
from their conditional distribution given the data or the reconstructions. It not necessary, however,
for the reconstructions to be sampled from their conditional distribution given the hidden states. All
that is required is that the reconstructions have lower free energy than the data. So it is possible to
include lateral connections between the visible units and to create reconstructions by taking a small
step towards the conditional equilibrium distribution given the hidden states. If we are using mean-
ﬁeld activities for the reconstructions, we can move towards the equilibrium distribution by using a
few damped mean-ﬁeld updates (Welling and Hinton, 2002). We call this a semi-restricted Boltz-
mann machine (SRBM). The visible units form a conditional MRF with the biases of the visible
units being determined by the hidden states. The learning procedure for the visible to hidden con-
nections is unaffected and the same learning procedure applies to the lateral connections. Explicitly,
the energy function for a SRBM is given by

E(v, h) = − X

bivi − X

bjhj − X

vihjwij − X

vivi′ Lii′

i∈visibles

j∈hiddens

i,j

i<i′

and the update rule for the lateral connections is

∆Lii′ = ǫ(hvivi′ idata − hvivi′irecon)

(3)

(4)

Semi-restricted Boltzmann machines can be learned greedily and composed to form a directed hi-
erarchy of conditional MRF’s. To generate from the composite model we ﬁrst get an equilbrium
sample from the top level SRBM and then we get an equilibrium sample from each lower level MRF
in turn, given the top-down input from the sample in the layer above. The settling at each interme-
diate level does not need to explore a highly multi-modal energy landscape because the top-down
input has already selected a good region of the space. The role of the settling is simply to sharpen
the somewhat vague top-down speciﬁcation and to ensure that the resulting conﬁguration repects
learned constraints. Each intermediate level ﬁlls in the details given the larger picture deﬁned by the
level above.

4 Inference in a directed hierarchy of MRF’s

In a deep belief network, inference is very simple and very fast because of the way in which the
network is learned. Rather than ﬁrst deciding how to represent the data and then worrying about in-
ference afterwards, deep belief nets restrict themselves to learning representations for which accurate
variational inference can be done in a single bottom-up pass. Each layer computes an approximate
sample from its posterior distribution given the activities in the layer below. This can be done with
a single matrix multiply using the bottom-up “recognition” connections that were originally learned
by an RBM but are no longer part of the generative model. The recognition connections compute
an approximation to the product of a data-dependent likelihood term coming from the layer below
and a data-independent prior term that depends on the learned parameters of all the higher layers.
Each of these two terms can contain strong correlations, but the way in which the model is learned
ensures that these correlations cancel each other out so that the true posterior distribution in each
layer is very close to factorial and very simple to compute from the activities in the layer below.

The inference process is unaltered by adding an MRF at each hidden layer. The role of the MRF’s
is to allow the generative process to mimic the constraints that are obeyed by the variables within a
layer when the network is being driven bottom-up by data. During inference, these constraints are
enforced by the data. From a biological perspective, it is very important for perceptual inference to
be fast and accurate, so it is very good that it does not involve any kind of iterative settling or belief
propagation. The MRF’s are vital for imposing constraints during generation and for whitening the

3

learning signal so that weak higher-order structure is not masked by strong pairwise correlations.
During perceptual inference, however, the MRF’s are mere spectators.

5 Whitening without waiting

Data is often whitened to prevent strong pairwise correlations from masking weaker but more in-
teresting structure. An alternative to whitening the data is to modify the learning procedure so that
it acts as if the data were whitened and ignores strong pairwise correlations when learning the next
level of features. This has the advantage that perceptual inference is not slowed down by an explicit
whitening stage. If the lateral connections ensure that a pairwise correlation in the distribution of the
reconstructions is the same as in the data distribution, that correlation will be ignored by contrastive
divergence since the learning is driven by the differences between the two distributions. This also
explains why different hidden units learn different features even when they have the same connec-
tivity: once one feature has made one aspect of the reconstructions match the data, there is no longer
any learning signal for another hidden unit to learn that same aspect.

Figure 1 shows how the features learned by the hidden units are affected by the presence of lateral
connections between the visible units. Hidden units are no longer required for modeling the strong
pairwise correlations between nearby pixels so they are free to discover more interesting features
than the simple on-center off-surround fetaures that are prevalent when there are no connections
between visible units.

(A)

(B)

Figure 1: (A) A random sample of the ﬁlters learned by an RBM trained on 60,000 images of hand-
written digits from the MNIST database (see Hinton et al. (2006) for details). (B) A random sample
of the ﬁlters learned by an SRBM trained on the same data. To produce each reconstruction, the
SRBM used 5 damped mean-ﬁeld iterations with the top-down input from the hidden states ﬁxed.
Adding lateral connections between the visible units changes the types of hidden features that are
learned. For simplicity each visible unit in the SRBM was connected to all 783 other visible units,
but only the local connections developed large weights and the lateral connections to each pixel
formed a small on-center off-surround ﬁeld centered on the pixel. Pixels close to the edge of the
image that were only active one or two times in the whole training set behaved very differently:
They learned to predict the whole of the particular digit that caused them to be active.

6 Modeling patches of natural images

To illustrate the advantages of adding lateral connections to the hidden layers of a DBN we use
the well-studied task of modelling the statistical structure of images of natural scenes (Bell and
Sejnowski, 1997; Olshausen and Field, 1996; Karklin and Lewicki, 2005; Osindero et al., 2006; Lyu
and Simoncelli, 2006). Using DBN’s, it is easy to build overcomplete and hierchical generative
models of image patches. These are able to capture much richer types of statistical dependency than
traditional generative models such as ICA. They also have the potential to go well beyond the types
of dependencies that can be captured by other, more sophisticated, multi-stage approaches such as
(Karklin and Lewicki, 2005; Osindero et al., 2006; Lyu and Simoncelli, 2006).

6.1 Adapting Restricted Boltzmann machines to real-valued data

Hinton and Salakhutdinov (2006) show how the visible units of an RBM can be modiﬁed to allow it
to model real-valued data using linear visible variables with Gaussian noise, but retaining the binary
stochastic hidden units. The learning procedure is essentially unchanged especially if we use the
mean-ﬁeld approximation for the visible units which is what we do.

4

Two generative DBN models, one with and one without lateral connectivity, were trained using
the updates from equations 2 and 4. The training data used consisted of 150,000 20 × 20 patches
extracted from images of natural scenes taken from the collection of Van Hateren1. The raw im-
age intensities were pre-processed using a standard set of operations — namely an initial log-
transformation, and then a normalisation step such that each pixel had zero-mean across the training
set. The patches were then whitened using a Zero-Phase Components analysis (ZCA) ﬁlter-bank.
The set of whitening ﬁlters is obtained by rotating the data into a co-ordinate system aligned with
the eigenvectors of the covariance matrix, then rescaling each component by the inverse square-root
of the correspoding eigenvalue, then rotating back into the original pixel co-ordinate system.

Using ZCA has a similar effect to learning lateral connections between pixels (Welling and Hinton,
2002). We used ZCA whitened data for both models to make it clear that the advantage of lateral
connections is not just caused by their ability to whiten the input data. Because the data was whitened
we did not include lateral connections in the bottom layer of the lateral DBN. The results presented
in the ﬁgures that follow are all shown in “unwhitened pixel-space”, i.e. the effects of the whitening
ﬁlter are undone for display purposes.

The models each had 2000 units in the ﬁrst hidden layer, 500 in the second hidden layer and 1000
units in the third hidden layer. The generative abilities of both models are very robust against
variations in the number of hidden units in each layer, though it seems to be important for the
top layer to be quite large. In the case where lateral connections were used, the ﬁrst and second
hidden layers of the ﬁnal, composite model were fully laterally connected.

Data was taken in mini-batches of size 100, and training was performed for 50 epochs for the ﬁrst
layer and 30 epochs for the remaining layers. A learning rate of 10−3 was used for the interlayer
connections, and half that rate for the lateral connections. Multiplicative weight decay of 10−2 mul-
tiplied by the learning rate was used, and a momentum factor of 0.9 was employed. When training
the higher-level SRBM’s in the model with lateral connectivity, 30 parallel mean ﬁeld updates were
used to produce the reconstructions with the top-down input from the hidden states held constant.
Each mean ﬁeld update set the new activity of every “visible” unit to be 0.2 times the previous ac-
tivity plus 0.8 times the value computed by applying the logistic function to the total input received
from the hidden units and the previous states of the visible units.

Learned ﬁlters

Figure 2 shows a random sample of the ﬁlters learned using an RBM with Gaussian visible units.
These ﬁlters are the same for both models. This representation is 5× overcomplete.

Figure 2: Filters from the ﬁrst hidden layer. The results are generally similar to previous work on
learning representations of natural image patches. The majority of the ﬁlters are tuned in location,
orientation, and spatial frequency. The joint space of location and orientation is approximately
evenly tiled and the spatial frequency responses span a range of about four octaves.

6.1.1 Generating samples from the model

The same issue that necessitates the use of approximations when learning deep-networks – namely
the unknown value of the partition function – also makes it difﬁcult to objectively assess how well
they ﬁt the data in the absence of predictive tasks such as classiﬁcation. Since our main aim is to
demonstrate the improvement in data modelling ability that lateral connections bring to DBN’s, we
simply present samples from similarly structured models, with and without lateral connections, and
compare these samples with real data.

1http://hlab.phys.rug.nl/imlib/index.html

5

Ten-thousand data samples were generated by randomly initialising the top-level (S)RBM states and
then running 300 iterations of a Gibbs sampling scheme between the top two layers. For models
without lateral connections, each iteration of the scheme consisted of a full parallel-update of the
top-most layer followed by a full parallel-update of the penultimate layer. In models with lateral
connections, each iteration consisted of a full parallel-update of the top-most layer followed by 50
rounds of sequential stochastic updates of each unit in the penultimate layer, under the inﬂuence of
the previously sampled top-layer states. (A different random ordering of units was drawn in each
update-round.) After running this Markov Chain we then performed an ancestral generative pass
down to the data layer.
In the case of models with no lateral connections, this simply involved
sampling from the factorial conditional distribution at each layer. In the case of models with lateral
connections we performed 50 rounds of randomly-ordered, sequential stochastic updates under the
inﬂuence of the top-down inputs from the layer above. In both cases, on the ﬁnal hidden layer update
before generating the pixel values, mean-ﬁeld updates were used so that the data was generated using
the real-valued probabilities in the ﬁrst hidden layer rather than stochastic-binary states.

(A)

(C)

(B)

(D)

Figure 3: (A) Samples from a model without lateral connections. (B) Samples from a model with
lateral connections. (C) Examples of actual data, drawn at random. (D) Examples of actual data,
chosen to have closest cosine distance to samples from panel (B).

Figure 3 shows that adding undirected lateral interactions within each intermediate hidden layer of
a deep belief net greatly improves the model’s ability to generate image patches that look realistic.
It is evident from the ﬁgure that the samples from the model with lateral connections are much more
similar to the real data and contain much more coherent, long-range structure. Belief networks with
only directed connections have difﬁculty capturing spatial constraints between the parts of an image
because, in a directed network, the only way to enforce constraints is by using observed descendants.
Unobserved ancestors can only be used to model shared sources of variation.

6.1.2 Marginal and pairwise statistics

In addition to the largely subjective comparisons from the previous section, if we perform some
simple aggregate analyses of the synthesized data we see that the samples from the model with
lateral connections are objectively well matched to those from true natural images. In the right-hand
column of ﬁgure 4 we show histograms of pixel inensities for real data and for data generated by the

6

two models. The kurtosis is 8.3 for real data, 7.3 for the model with lateral connections, and 3.4 for
the model with no lateral connections. If we make a histogram of the outputs of all of the ﬁlters in
the ﬁrst hidden layer of the model, we discover that the kurtosis is 10.5 on real data, 10.3 on image
patches generated by the model with lateral connections, and 3.8 on patches generated by the other
model.

Columns one through ﬁve of ﬁgure 4 show the distributions of the response of one ﬁlter conditional
on the response of a second ﬁlter. Again, for image patches generated with lateral connections the
statistics are similar to the data and without lateral connections they are quite different.

Figure 4: Each row shows statistics computed from a different set of 10,000 images. The ﬁrst
row is for real data. The second row is for image patches generated by the model with lateral
interactions. The third row is for patches generated without lateral interactions. Column six shows
histograms of pixel intensities. Columns 1-5 show conditional ﬁlter responses, in the style suggested
in (Wainwright and Simoncelli, 2000), for two different gabor ﬁlters applied to the sampled images.
In columns 1-3 the ﬁlters are 2, 4, or 8 pixels apart. In column 4 they are at the same location but
orthogonal orientations. In column 5 they are at the same location and orientation but one octave
apart in spatial frequency.

7 Discussion

Our results demonstrate the advantages of using semi-restricted Boltzmann machines as the building
blocks when building deep belief nets. The model with lateral connections is very good at capturing
the statistical structure of natural image patches. In future work we hope to exploit this in a number
of image processing tasks that require a good prior over image patches.

The models presented in this paper had complete lateral connectivity — largely for simplicity in
MATLAB. Such a strategy would not be feasible were we to signiﬁcantly scale up our networks.
Fortunately, there is an obvious solution to this — we can simply restrict the majority of lateral
interactions to a local neighbourhood and concomittently have the hidden units focus their attention
on spatially localised regions of the image. A topographic ordering would then exist throughout the
various layers of the hierarchy. This would greatly reduce the computational load and it corresponds
to a sensible prior over image structures, especially if the local regions get larger as we move up the
hierarchy. Furthermore, it would probably make the process of settling within a layer much faster.

One limitation of the model we have described is that the top-down effects can only change the
effective biases of the units in the Markov random ﬁeld at each level. The model becomes much

7

more powerful if top-down effects can modulate the interactions. For example, an “edge” can be
viewed as a breakdown in the local correlational structure of the image: pixel intensities cannot be
predicted from neighbors on the other side of an object boundary. A hidden unit that can modulate
the pairwise interactions rather than just the biases can form a far more abstract representation of an
edge that is not tied to any particular contrast or intensity (Geman and Geman, 1984). Extending our
model to this type of top-down modulation is fairly straightforward. Instead of using weights wij
that contribute energies −vivjwij we use weights wijk that contribute energies −vivjhkwijk. This
allows the binary state of hk to gate the effective weight between visible units i and j. Memisevic
and Hinton (2007) show that the same learning methods can be applied with a single hidden layer and
there is no reason why such higher-order semi-restricted Boltzmann machines cannot be composed
into deep belief nets.

Although we have focussed on the challenging task of modeling patches of natural images, we
believe the ideas presented here are of much more general applicability. DBN’s without lateral con-
nections have produced state of the art results in a number of domains including document retrieval
(Hinton and Salakhutdinov, 2006), character recognition (Hinton et al., 2006), lossy image compres-
sion (Hinton and Salakhutdinov, 2006), and the generation of human motion (Taylor et al., 2007).
Lateral connections may help in all of these domains.
Acknowledgments

We are grateful to the members of the machine learning group at the University of Toronto for
helpful discussions. This work was supported by NSERC, CFI and CIFAR. GEH is a fellow of
CIFAR and holds a CRC chair.
References
Bell, A. J. and Sejnowski, T. J. (1997). The ”independent components” of natural scenes are edge

ﬁlters. Vision Research, 37(23):3327–3338.

Bengio, Y., Lamblin, P., Popovici, D., and Larochelle, H. (2007). Greedy layer-wise training of
In B., S., Platt, J., and Hoffman, T., editors, Advances in Neural Information

deep networks.
Processing Systems 19. MIT Press, Cambridge, MA.

Geman, S. and Geman, D. (1984). Stochastic relaxation, gibbs distributions and the bayesian restora-

tion of images. IEEE Trans. Pattern Anal. Mach. Intell, 6.

Hinton, G. E. (2002). Training products of experts by minimizing contrastive divergence. Neural

Computation, 14(8):1711–1800.

Hinton, G. E., Osindero, S., and Teh, Y. W. (2006). A fast learning algorithm for deep belief nets.

Neural Computation, 18.

Hinton, G. E. and Salakhutdinov, R. (2006). Reducing the dimensionality of data with neural net-

works. Science, 313.

Karklin, Y. and Lewicki, M. (2005). A hierarchical bayesian model for learning nonlinear statistical

regularities in nonstationary natural signals. Neural Computation, 17(2).

Lyu, S. and Simoncelli, E. (2006). Statistical modeling of images with ﬁelds of gaussian scale

mixtures. In Advances Neural Information Processing Systems, volume 19.

Memisevic, R. F. and Hinton, G. E. (2007). Unsupervised learning of image transformations. In

Computer Vision and Pattern Recognition. IEEE Computer Society.

Olshausen, B. A. and Field, D. J. (1996). Emergence of simple-cell receptive ﬁeld properties by

learning a sparse code for natural images. Nature, 381(6583):607–609. JUN 13 NATURE.

Osindero, S., Welling, M., and Hinton, G. E. (2006). Topographic product models applied to natural

scene statistics. Neural Computation, 18(2).

Taylor, G. W., Hinton, G. E., and Roweis, S. (2007). Modeling human motion using binary latent
variables. In B., S., Platt, J., and Hoffman, T., editors, Advances in Neural Information Processing
Systems 19. MIT Press, Cambridge, MA.

Wainwright, M. and Simoncelli, E. (2000). Scale mixtures of Gaussians and the statistics of natural

images. In Advances Neural Information Processing Systems, volume 12, pages 855–861.

Welling, M. and Hinton, G. E. (2002). A new learning algorithm for mean ﬁeld boltzmann machines.

In International Joint Conference on Neural Networks, Madrid.

8

"
988,2007,CPR for CSPs: A Probabilistic Relaxation of Constraint Propagation,"This paper proposes constraint propagation relaxation (CPR), a probabilistic approach to classical constraint propagation that provides another view on the whole parametric family of survey propagation algorithms SP(&#961;), ranging from belief propagation (&#961; = 0) to (pure) survey propagation(&#961; = 1). More importantly, the approach elucidates the implicit, but fundamental assumptions underlying SP(&#961;), thus shedding some light on its effectiveness and leading to applications beyond k-SAT.","CPR for CSPs: A Probabilistic Relaxation of

Constraint Propagation

Luis E. Ortiz

ECE Dept, Univ. of Puerto Rico, Mayag¨uez, PR 00681-9042

leortiz@ece.uprm.edu

Abstract

This paper proposes constraint propagation relaxation (CPR), a probabilistic ap-
proach to classical constraint propagation that provides another view on the whole
parametric family of survey propagation algorithms SP(ρ). More importantly, the
approach elucidates the implicit, but fundamental assumptions underlying SP(ρ),
thus shedding some light on its effectiveness and leading to applications beyond
k-SAT.

1 Introduction

Survey propagation (SP) is an algorithm for solving k-SAT recently developed in the physics com-
munity [1, 2] that exhibits excellent empirical performance on “hard” instances. To understand the
behavior of SP and its effectiveness, recent work (see Maneva et al. [3] and the references therein)
has concentrated on establishing connections to belief propagation (BP) [4], a well-known approxi-
mation method for computing posterior probabilities in probabilistic graphical models. Instead, this
paper argues that it is perhaps more natural to establish connections to constraint propagation (CP),
another message-passing algorithm tailored to constraint satisfaction problems (CSPs) that is well-
known in the AI community. The ideas behind CP were ﬁrst proposed by Waltz [5] 1 Yet, CP has
received considerably less attention than BP lately.
This paper reconnects BP to CP in the context of CSPs by proposing a probabilistic relaxation
of CP that generalizes it. Through the approach, it is easy to see the exact, implicit underlying
assumptions behind the entire family of survey propagation algorithms SP(ρ). (Here, the approach
is presented in the context of k-SAT; it will be described in full generality in a separate document.)
In short, the main point of this paper is that survey propagation algorithms are instances of a natural
generalization of constraint propagation and have simple interpretations in that context.

2 Constraint Networks and Propagation

This section presents a brief introduction to the graphical representation of CSPs and CP, and con-
centrates on the aspects that are relevant to this paper. 2
A constraint network (CN) is the graphical model for CSPs used in the AI community. Of interest
here is the CN based on the hidden transformation. (See Bacchus et al. [9] for more information
on the different transformations and their properties.) It has a bipartite graph where every variable
and constraint is each represented by a node or vertex in the graph and there is an edge between a
variable i and a constraint a if and only if a is a function of i (see ﬁgure 1). From now on, a CN with
a tree graph is referred to as a tree CN, and a CN with an arbitrary graph as an arbitrary CN.

1See also Pearl [4], section 4.1.1, and the ﬁrst paragraph of section 4.1.2.
2Please refer to Russell and Norvig [6] for a general introduction, Kumar [7] for a tutorial and Dechter [8]

for a more comprehensive treatment of these topics and additional references.

1

1

4

2

a

variables
3

Constraint propagation is typically used as part
of a depth-ﬁrst search algorithm for solving
CSPs. The search algorithm works by extend-
ing partial assignments, usually one variable
at a time, during the search. The algorithm
is called backtracking search because one can
backtrack and change the value of a previously
assigned variable when the search reaches an
illegal assignment.
CP is often applied either as a preprocessing
step or after an assignment to a variable is
made. The objective is to reduce the domains
of the variables by making them locally consis-
tent with the current partial assignment. The
propagation process starts with the belief that
for every value assignment vi in the domain of
each variable i there exists a solution with vi as-
signed to i. The process then attempts to correct
this a priori belief by locally propagating con-
straint information.
It is well-known that CP,
unlike BP, always converges, regardless of the
structure of the CN graph. This is because no
possible solution is ignored at the start and none
ever removed during the process. In the end, CP
produces potentially reduced variable domains
that are in fact locally consistent. In turn, the
resulting search space is at worst no larger than the original but potentially smaller while still con-
taining all possible solutions. The computational efﬁciency and effectiveness of CP in practice has
made it a popular algorithm in the CSP community.

Figure 1: The graph of the constraint network cor-
responding to the 3-SAT formula f(x) = (x1 ∨
x2 ∨ x3) ∧ (x2 ∨ ¯x3 ∨ x4), which has four vari-
ables and two clauses; the ﬁrst and second clause
are denoted in the ﬁgure by a and b, respectively.
Following the convention of the SP community,
clause and variable nodes are drawn as boxes and
circles, respectively; also, if a variable appears as
a negative literal in a clause (e.g., variable 3 in
clause b), the edge between them is drawn as a
dashed line.

clauses

b

3 Terminology and Notation

3

4

1

2

fb→2

a(i) and C u

Let V (a) be the set of variables that appear in
constraint a and C(i) the set of constraints in
which variable i appears. Let also Vi(a) ≡
V (a) − {i} and Ca(i) ≡ C(i) − {a}. In k-
SAT, the constraints are the clauses, each vari-
able is binary, with domain {0, 1}, and a solu-
tion corresponds to a satisfying assignment. If
i ∈ V (a), denote by sa,i the value assignment
to variable i that guarantees the satisﬁability of
clause a; and denote the other possible assign-
a (i)
ment to i by ua,i. Finally, let C s
be the set of clauses in Ca(i) where variable i
appears in the same and different literal form as
it does in clause a, respectively.
The k-SAT formula under consideration is de-
noted by f.
It is convenient to introduce no-
tation for formulae associated to the CN that
results from removing variables or constraints
from f. Let fa be the function that results from
removing clause a from f (see ﬁgure 2), and
similarly, abusing notation, let fi be the function that results from removing variable i from f. Let
fa→i be the function that corresponds to the connected component of the CN graph for fa that con-
tains variable i ∈ V (a), and let fi→a be the function that corresponds to the connected component
of the CN graph for fi that contains a ∈ C(i). (Naturally, if node a is not a separator of the CN
graph for f, fa has a single connected component, which leads to fa→i = fa; similarly for fi.)

Figure 2: The graph inside the continuous curve is
the CN graph for the formula fb that results from
removing clause b from f. The graph inside the
dashed curve is the CN graph for fb→2, which cor-
responds to the formula for the connected compo-
nent of the CN graph for fb that contains variable
2.

fb

a

b

2

clausesvariablesIt is convenient to use a simple, if perhaps unusual, representation of sets in order to track the
domains of the variables during the propagation process. Each subset A of a set S of size m is
represented as a bit array of m elements where component k in the array is set to 1 if k is in A and
to 0 otherwise. For instance, if S = {0, 1}, then the array [00] represents ∅, and similarly, [01], [10]
and [11] represent {0}, {1} and {0, 1}, respectively.
It is also useful to introduce the concept of (globally) consistent domains of variables and SAT
functions. Let Sf = {x|x satisﬁes f} be the set of assignments that satisfy f. Given a complete
assignment x, denote by x−i the assignments to all the variables except i; thus, x = (x1, . . . , xn) =
(xi, x−i). Let the set Wi be the consistent domain of variable i in f if Wi = {xi|x = (xi, x−i) ∈
Sf for some x−i}; that is, Wi contains the set of all possible values that variable i can take in an
assignment that satisﬁes f. Let the set W be the consistent domain of f if W = ×n
i=1Wi and, for
all i, Wi is the consistent domain of variable i in f.
Finally, some additional terminology classiﬁes variables of a SAT function given a satisfying assign-
ment. Given a function f and a satisfying assignment x, let variable i be ﬁxed if changing only its
assignment xi in x does not produce another satisfying assignment for f; and be free otherwise.

4 Propagation Algorithms for Satisﬁability

Constraint Propagation.
In CP for k-SAT, the message Ma→i that clause a sends to variable i
is an array of binary values indexed by the elements of the domain of i; similarly, for the message
Mi→a that variable i sends to clause a. Intuitively, for all xi ∈ {0, 1}, Mi→a(xi) = 1 if and only
if assigning value xi to variable i is “ok” with all clauses other than a. Formally, Mi→a(xi) = 1
if and only if fa→i has a satisfying assignment with xi assigned to variable i (or in other words,
xi is in the consistent domain of i in fa→i). Similarly, Ma→i(xi) = 1 if and only if clause a
is “ok” with assigning value xi to variable i; or formally, Ma→i(xi) = 1 if and only if fi→a
has a satisfying assignment with xi assigned to variable i, or assigning xi to variable i by itself
satisﬁes a. It is convenient to denote Mi→a(xi) and Mi→a(xi) by M xi
a→i, respectively.
In addition, M sa,i
i→a, M s
a→i and
a→i, respectively.
M u
In summary, we can write CP for k-SAT as follows.
• Messages that clause a sends to variable i:

a→i are simply denoted by M s

a→i and M xi
i→a, M u

a→i and M ua,i

i→a, M ua,i

i→a, M sa,i

a→i = 1 if and only if xi = sa,i or, there exists j ∈ Vi(a), s.t. M s
M xi

j→a = 1.

• Messages that variable i sends to clause a:

i→a = 1 if and only if for all b ∈ Ca(i), M xi
M xi

b→i = 1.

(1)

(2)

It is convenient to express CP mathematically as follows.

• Messages that clause a sends to variable i:

(cid:26) 1,
1 −(cid:81)

M xi

a→i =

• Messages that variable i sends to clause a: M xi

j∈Vi(a)(1 − M s

j→a),

i→a =(cid:81)

if xi = sa,i,
if xi = ua,i.

b∈Ca(i) M xi

b→i.

a→i = 1, and naturally, M s

i→a =
In order to guarantee convergence, the message values in CP are initialized as M s
1, M u
a→i = 1. This initialization encodes the a priori belief that every
assignment is a solution. CP attempts to “correct” or update this belief through the local propagation
of constraint information. In fact, the expressions in CP force the messages to be locally consistent.
By being initially conservative about the consistent domains, no satisfying assignment is discarded
during the propagation process.
for
Once CP converges,

its
becomes
a→i = 1} ∈ 2{0,1}. For general CSPs,
M u
CP is usually very effective because it can signiﬁcantly reduce the original domain of the variables,

a→i = 1} = {xi|(cid:81)

variable
i,
a∈C(i):xi=ua,i

{xi|(cid:81)

locally-consistent

i→a = 1, M u

a∈C(i) M xi

domain

each

3

leading to a smaller search space of possible assignments. It should be noted that in the particular
case of k-SAT with arbitrary CNs, CP is usually only effective after some variables have already
being assigned during the search, because those (partial) assignments can lead to “boundary
conditions.” Without such boundary conditions, however, CP never reduces the domain of the
variables in k-SAT, as can be easily seen from the expressions above.
On the other hand, when CP is applied to tree CNs, it exhibits additional special properties. For
example, convergence is actually guaranteed regardless of how the messages are initialized, because
of the boundary conditions imposed by the leaves of the tree. Also, the ﬁnal messages are in fact
globally consistent (i.e., all the messages are consistent with their deﬁnition). Therefore, the locally-
consistent domains are in fact the consistent domains. Whether the formula is satisﬁable, or not,
can be determined immediately after applying CP. If the formula is not satisﬁable, the consistent
domains will be empty sets. If the formula is in fact satisﬁable, applying depth-ﬁrst search always
ﬁnds a satisfying assignment without the need to backtrack.
We can express CP in a way that looks closer to SP and BP. Using the reparametrization Γa→i =
1 − M u

a→i, we get the following expression of CP.

• Message that clause a sends to variable i: Γa→i =(cid:81)
i→a =(cid:81)

• Message that variable i sends to clause a: M s

j∈Vi(a)(1 − M s
b∈Cu

j→a).
a (i)(1 − Γb→i).

Survey Propagation. Survey propagation has become a very popular propagation algorithm for
k-SAT. It was developed in the physics community by M´ezard et al. [2]. The excitement around
SP comes from its excellent empirical performance on hard satisﬁability problems; that is, k-SAT
formulae with a ratio α of the number of clauses to the number of variables near the so called
satisﬁability threshold αc.
The following is a description of an SP-inspired family of message-passing procedures, parametrized
by ρ ∈ [0, 1]. It is often denoted by SP(ρ), and contains BP (ρ = 0) and (pure) SP (ρ = 1).

• Message that clause a sends to variable i:

ηa→i =(cid:81)

Πu
j→a
j→a+Π∗
j→a+Πs

Πu

j→a

j∈Vi(a)
• Messages that variable i sends to clause a:

i→a =

(cid:16)
i→a = (cid:81)
i→a = (cid:81)

Πu

Πs
Π∗

1 − ρ(cid:81)
(cid:16)
a (i)(1 − ηb→i)(cid:81)
a (i)(1 − ηb→i)

(cid:17)(cid:81)
1 −(cid:81)
a (i)(1 − ηb→i)
a(i)(1 − ηb→i) =(cid:81)

b∈Cs
a(i)(1 − ηb→i)

(cid:17)
a(i)(1 − ηb→i)
b∈Ca(i)(1 − ηb→i)

b∈Cu
b∈Cu

b∈Cu

b∈Cs

b∈Cs

SP was originally derived via arguments and concepts from physics. A simple derivation based on a
probabilistic interpretation of CP is given in the next section of the paper. The derivation presented
here elucidates the assumptions that SP algorithms make about the satisﬁability properties and struc-
ture of k-SAT formulae. However, it is easy to establish strong equivalence relations between the
different propagation algorithms even at the basic level, before introducing the probabilistic inter-
pretation (details omitted).

5 A Probabilistic Relaxation of Constraint Propagation for Satisﬁability

The main idea behind constraint propagation relaxation (CPR) is to introduce a probabilistic model
for the k-SAT formula and view the messages as random variables in that model. If the formula f
has n variables, the sample space Ω = (2{0,1})n is the set of the n-tuple whose components are
subsets of the set of possible values that each variable i can take (i.e., subsets of {0, 1}). The “true
probability law” Pf of a SAT formula f that corresponds to CP is deﬁned in terms of the consistent
domain of f: for all W ∈ Ω,

(cid:26) 1,

Pf (W) =

0, otherwise.

if W is the consistent domain of f,

4

Clearly, if we could compute the consistent domains of the remaining variables after each variable
assignment during the search, there would be no need to backtrack. But, while it is easy to compute
consistent domains for tree CNs, it is actually hard in general for arbitrary CNs. Thus, it is generally
hard to compute Pf . (CNs with graphs of bounded tree-width are a notable exception.)
However, the probabilistic interpretation will allow us to introduce “bias” on Ω, which leads to a
heuristic for dynamically ordering both the variables and their values during search. As shown in
this section, it turns out that for arbitrary CNs, survey propagation algorithms attempt to compute
different “approximations” or “relaxations” of Pf by making different assumptions about its “prob-
abilistic structure.”
Let us now view each message M s
i→a for each variable i and clause
a as a (Bernoulli) random variable in some probabilistic model with sample space Ω and a, now
arbitrary, probability law P. 3 Formally, for each clause a, variable i and possible assignment value
xi ∈ {0, 1}, we deﬁne

i→a, and M u

a→i, M u

a→i, M s

a→i ∼ Bernoulli(pxi
M xi
a→i = 1) and pxi

a→i) and M xi

i→a ∼ Bernoulli(pxi

i→a)

i = P(M u

i→a = P(M xi

a→i = P(M xi

i ≡ P(M xi

a→i = 1 for all a ∈ C−(i)) and p0

a→i because it is always 1, by the deﬁnition of M s

i→a = 1). This is a distribution over all possible
where pxi
subsets (i.e., the power set) of the domain of each variable, not just over the variable’s domain itself.
Also, clearly we do not need to worry about ps
a→i.
The following is a description of how we can use those probabilities during search.
In the SP
community, the resulting heuristic search is called “decimation” [1, 2]. If we believe that P “closely
a→i = 1 for all a ∈ C(i)) that xi is in
approximates” Pf , and know the probability pxi
the consistent domain for variable i of f, for every variable i, clause a and possible assignment
xi, we can use them to dynamically order both the variables and the values they can take during
a→i =
search. Speciﬁcally, we ﬁrst compute p1
1 for all a ∈ C +(i)) for each variable i, where C +(i) and C−(i) are the sets of clauses where
variable i appears as a positive and a negative literal, respectively. Using those probability values,
we then compute what the SP community calls the “bias” of i: |p1
i|. The variable to assign next
i − p0
is the one with the largest bias. 4 We would set that variable to the value of largest probability; for
instance, if variable i has the largest bias, then we set i next, to 1 if p1
i .
i < p0
The objective is then to compute or estimate those probabilities.
The following are (independence) assumptions about the random variables (i.e., messages) used in
this section. The assumptions hold for tree CNs and, as formally shown below, are inherent to the
survey propagation process.
j→a for all j ∈ Vi(a) are
Assumption 1. For each clause a and variable i, the random variables M s
independent.
b→i for all clauses b ∈
Assumption 2. For each clause a and variable i, the random variables M u
C u
b→i for all clauses b ∈
Assumption 3. For each clause a and variable i, the random variables M u
C s

a (i) are independent.

i , and to 0 if p1

i = P(M u

a(i) are independent.

i > p0

Without any further assumptions, we can derive the following, by applying assumption 1 and the
expression for M u

a→i that results from 1:

a→i = P(M u
pu

j∈Vi(a) P(M s
Similarly, by assumption 2 and the expression for M s

a→i = 1) = 1 −(cid:81)
i→a = 1) =(cid:81)

i→a = P(M s
ps

j→a = 0) = 1 −(cid:81)
b→i = 1) =(cid:81)

j∈Vi(a)(1 − ps
i→a that results from 2, we derive
b→i.

b∈Cu

a (i) pu

j→a).

Using the reparametrization ηa→i = P(M u
passing procedure.

a (i) P(M u

b∈Cu
a→i = 0) = 1 − pu

a→i, we obtain the following message-

3Given clause a and variable i of SAT formula f, let Dj
i→a(W) = 1 iff Wj ⊂ Dj

a→i be the (globally) consistent domain of fa→i
for variable j. The random variables corresponding to the messages from variable i to clause a are deﬁned as
M xi
a→i. The other random variables are
j→a(W)) for all W.
then deﬁned as M s

a→i for every variable j of fa→i; and xi ∈ Di
j∈Vi(a)(1 − M s

a→i(W) = 1 −Q

a→i(W) = 1 and M u

4For both variable and value ordering, we can break ties uniformly at random. Also, the description of
SP(ρ) used often, sets a fraction β of the variables that remained unset during search. While clearly this
speeds up the process of getting a full assignment, the effect that heuristic might have on the completeness of
the search procedure is unclear, even in practice.

5

• Message that clause a sends to variable i: ηa→i =(cid:81)
i→a =(cid:81)

• Message that variable i sends to clause a: ps

j∈Vi(a)(1 − ps
b∈Cu

i→a)
a (i)(1 − ηb→i)

i→a as(cid:81)

We can then use assumption 3 to estimate pu
Note that this message-passing procedure is exactly “classical” CP if we initialize ηa→i = 0 and
i→a = 1 for all variables i and clause a. However, the version here allows the messages to be in
ps
[0, 1]. At the same time, for tree CNs, this algorithm is the same as classical CP (i.e., produces the
same result), regardless of how the messages ηa→i and ps
i→a are initialized. In fact, in the tree case,
the ﬁnal messages uniquely identify P = Pf .

b∈Cs

a(i)(1 − ηb→i).

Making Assumptions about Satisﬁability. Let us make the following assumption about the
“probabilistic satisﬁability structure” of the k-SAT formula.
Assumption 4. For some ρ ∈ [0, 1], for each clause a and variable i,

P(M s

i→a = 0, M u

i→a = 0) = (1 − ρ)P(M s

i→a = 1, M u

i→a = 1).

i→a = 0, M u

i→a = 0, M u
i→a = 1), which, interestingly, is equivalent to the condition P(M s

For ρ = 1, the last assumption essentially says that fa→i has a satisfying assignment;
i.e.,
P(M s
i→a = 0) = 0. For ρ = 0, it essentially says that the likelihood that fa→i does
not have a satisfying assignment is the same as the likelihood that fa→i has a satisfying assignment
i→a =
where variable i is free. Formally, in this case, we have P(M s
1, M u
i→a =
1) = 1.
Let us introduce a ﬁnal assumption about the random variables associated to the messages from
variables to clauses.
Assumption 5. For each clause a and variable i, the random variables M s
independent.

i→a = 0) = P(M s
i→a = 1) + P(M u

i→a and M u

i→a are

Note that assumptions 2, 3 and 5 hold (simultaneously) if and only if for each clause a and variable
i, the random variables M u
The following theorem is the main result of this paper.
Theorem 1. (Sufﬁcient Assumptions) Let assumptions 1, 2 and 3 hold. The message-passing
procedure that results from CPR as presented above is

b→i for all clauses b ∈ Ca(i) are independent.

1. belief propagation (i.e., SP(0)), if assumption 4, with ρ = 0, holds, and
2. a member of the family of survey propagation algorithms SP(ρ), with 0 < ρ ≤ 1, if

assumption 4, with the given ρ, and assumption 5 hold.

These assumptions are also necessary in a strong sense (details omitted), Assumptions 1, 2, 3, and
even 5 might be obvious to some readers, but assumption 4 might not be, and it is essential.

Proof. As in the last subsection, assumption 1 leads to pu
assumptions 2 and 3 lead to ps
b→i and pu

b∈Cu

a (i) pu

Note also that assumption 4 is equivalent to ps
allows us to express

i→a + pu

i→a − ρ P(M s

j→a), while

j∈Vi(a)(1 − ps
a(i) pu

b→i.
b∈Cs
i→a = 1, M u

i→a = 1) = 1. This

i→a =(cid:81)

a→i = 1 −(cid:81)
i→a =(cid:81)

P(M s

i→a = 1) = ps

i→a =

ps
i→a
i→a − ρ P(M s

i→a + pu
ps

i→a = 1, M u

i→a = 1) ,

which implies

P(M s

i→a = 0) =

i→a − ρ P(M s
pu

i→a − ρ P(M s
pu

i→a = 1, M u

i→a = 1)
i→a = 1) + ps

i→a = 1, M u

.

i→a

If ρ = 0, then the last expression simpliﬁes to

P(M s

i→a = 0) =

pu
i→a
i→a + ps
pu

i→a

.

6

a→i = 0) = 1 − pu
Using the reparametrization ηa→i ≡ P(M u
i→a, leads to BP (i.e., SP(0)).
i→a = 1) = ps
and Πs
Otherwise, if 0 < ρ ≤ 1, then using the reparametrization ηa→i ≡ P(M u

i→a ≡ P(M s

i→a + Π∗

a→i, Πu

a→i = 0),

i→a ≡ P(M u

i→a = 1) = pu

i→a

i→a = 1) − ρ P(M s
i→a = 0, M u
i→a = 1, M u
i→a = 1, M u
and applying assumption 5 leads to SP(ρ).

i→a ≡ P(M u
Πu
= P(M s
i→a ≡ P(M s
Πs
i→a ≡ P(M s
Π∗

i→a = 1, M u

i→a = 1)

i→a = 1) + (1 − ρ)P(M s
i→a = 0), and
i→a = 1),

i→a = 1, M u

i→a = 1),

The following are some remarks that can be easily derived using CPR.

On the Relationship Between SP and BP.
SP essentially assumes that every sub-formula fa→i
has a satisfying assignment, while BP assumes that for every clause a and variable i ∈ V (a), variable
i is equally likely not to have a satisfying assignment or being free in fa→i, as it is easy to see from
assumption 4. The parameter ρ just modulates the relative scaling of those two likelihoods. While
the same statement about pure SP is not novel, the statement about BP, and more generally, the class
SP(ρ) for 0 ≤ ρ < 1, seems to be.

On the Solutions of SAT formula f. Note that Pf may not satisfy all or any of the assumptions.
Yet, satisfying an assumption imposes constraints on what Pf actually is and thus on the solution
space of f. For example, if Pf satisﬁes assumption 4 for any ρ < 1, which includes BP when ρ = 0,
i→a = 0, M u
and for all clauses a and variables i, then Pf (M s
i→a =
1) = 0 and therefore either Pf (M s
i→a = 1) = 1
i→a = 0) = 1 or Pf (M s
i→a = 1, M u
holds, but not both of course. That implies f must have a unique solution!

i→a = 0) = Pf (M s

i→a = 0, M u

i→a = 1, M u

i→a = 0, M u

On SP. This result provides additional support to previous informal conjectures as to why SP is
so effective near the satisﬁability threshold: SP concentrates all its efforts on ﬁnding a satisfying
assignment when they are scarce and “scattered” across the space of possible assignments. Thus, SP
assumes that the set of satisfying assignments has in fact special structure.
i→a = 0) = 0
To see that, note that assumptions 4, with ρ = 1, and 5 imply that P(M s
or P(M s
i→a = 1) = 0 must hold. This says that in every assignment that satisﬁes
fa→i, variable i is either free or always has the same value assignment. This observation is relevant
because it has been argued that as we approach the satisﬁability threshold, the set of satisfying
assignments decomposes into many “local” or disconnected subsets.
It follows easily from the
discussion here that SP assumes such a structure, therefore potentially making it most effective
under those conditions (see Maneva et al. [3] for more information).
Similarly, it has also been empirically observed that SP is more effective for ρ close to, but strictly
less than 1. The CPR approach suggests that such behavior might be because, with respect to any
P that satisﬁes assumption 4, unlike pure SP, for such values of ρ < 1, SP(ρ) guards against the
possibility that fa→i is not satisﬁable, while still being somewhat optimistic by giving more weight
to the event that variable i is free in fa→i. Naturally, BP, which is the case of ρ = 0, might be too
pessimistic in this sense.

i→a = 1, M u

i→a = 0, M u

i→a = 1, M u

On BP. For BP (ρ = 0), making the additional assumption that the formula fa→i is satisﬁable
(i.e., P(M s
i→a = 0) = 0) implies that there are no assignments with free variables (i.e.,
P(M s
i→a = 1) = 0). Therefore, the only possible consistent domain is the singleton
{sa,i} or {ua,i} (i.e., P(M s
i→a = 1) = 1). Thus,
either 0 or 1 can possibly be a consistent value assignment, but not both. This suggests that BP is
concentrating its efforts on ﬁnding satisfying assignments without free variables.

i→a = 0) + P(M s

i→a = 1, M u

i→a = 0, M u

On Variable and Value Ordering. To complete the picture of the derivation of SP(ρ) via CPR,
we need to compute p0
i for all variables i to use for variable and value ordering during search.
We can use the following, slightly stronger versions of assumptions 2 and 3 for that.
a→i for all clauses a ∈ C−(i) are
Assumption 6. For each variable i, the random variables M u
independent.

i and p1

7

Assumption 7. For each variable i, the random variables M u
independent.

(cid:81)
Using assumptions 6 and 7, we can easily derive that p1
a∈C+(i)(1 − ηa→i), respectively.

i = (cid:81)

a→i for all clauses a ∈ C +(i) are

a∈C−(i)(1 − ηa→i) and p0

i =

On Generalizations. The approach provides a general, simple and principled way to introduce
possibly uncertain domain knowledge into the problem by making assumptions about the structure
of the set of satisfying assignments and incorporating them through P. That can lead to more
effective propagation algorithms for speciﬁc contexts.

Related Work. Dechter and Mateescu [10] also connect BP to CP but in the context of the in-
ference problem of assessing zero posterior probabilities. Hsu and McIlraith [11] give an intuitive
explanation of the behavior of SP and BP from the perspective of traditional local search methods.
They provide a probabilistic interpretation, but the distribution used there is over the biases.
Braunstein and Zecchina [12] showed that pure SP is equivalent to BP on a particular MRF over
an extended domain on the variables of the SAT formula, which adds a so called “joker” state.
Maneva et al. [3] generalized that result by showing that SP(ρ) is only one of many families of
algorithms that are equivalent to performing BP on a particular MRF. In both cases, one can easily
interpret those MRFs as ultimately imposing a distribution over Ω, as deﬁned here, where the joker
state corresponds to the domain {0, 1}. Here, the only particular distribution explicitly deﬁned is
Pf , the “optimal” distribution. This paper does not make any explicit statements about any speciﬁc
distribution P for which applying CPR leads to SP(ρ).

6 Conclusion

This paper strongly connects survey and constraint propagation. In fact, the paper shows how survey
propagation algorithms are instances of CPR, the probabilistic generalization of classical constraint
propagation proposed here. The general approach presented not only provides a new view on survey
propagation algorithms, which can lead to a better understanding of them, but can also be used to
easily develop potentially better algorithms tailored to speciﬁc classes of CSPs.

References
[1] A. Braunstein, M. M´ezard, and R. Zecchina. Survey propagation: An algorithm for satisﬁability. Random

Structures and Algorithms, 27:201, 2005.

[2] M. M´ezard, G. Parisi, and R. Zecchina. Analytic and Algorithmic Solution of Random Satisﬁability

Problems. Science, 297(5582):812–815, 2002.

[3] E. Maneva, E. Mossel, and M. J. Wainwright. A new look at survey propagation and its generalizations.

ACM, 54(4):2–41, July 2007.

[4] J. Pearl. Probabilistic Reasoning in Intelligent Systems. Networks of Plausible Inference. Morgan Kauf-

mann, 1988.

[5] D. L. Waltz. Generating semantic descriptions from drawings of scenes with shadows. Technical Report

271, MIT AI Lab, Nov. 1972. PhD Thesis.

[6] S. Russell and P. Norvig. Artiﬁcial Intelligence: A Modern Approach, chapter 5, pages 137–160. Prentice

Hall, second edition, 1995.

[7] V. Kumar. Algorithms for constraint-satisfaction problems: A survey. AI Magazine, 13(1):32–44, 1992.
[8] R. Dechter. Constraint Processing. Morgan Kaufmann, 2003.
[9] F. Bacchus, X. Chen, P. van Beek, and T. Walsh. Binary vs. non-binary constraints. AI, 140(1-2):1–37,

Sept. 2002.

[10] R. Dechter and R. Mateescu. A simple insight into iterative belief propagation’s success. In UAI, 2003.
[11] E. I. Hsu and S. A. McIlraith. Characterizing propagation methods for boolean satisﬁability. In SAT,

2006.

[12] A. Braunstein and R. Zecchina. Survey propagation as local equilibrium equations. JSTAT, 2004.

8

"
976,2007,A New View of Automatic Relevance Determination,"Automatic relevance determination (ARD), and the closely-related sparse Bayesian learning (SBL) framework, are effective tools for pruning large numbers of irrelevant features. However, popular update rules used for this process are either prohibitively slow in practice and/or heuristic in nature without proven convergence properties. This paper furnishes an alternative means of optimizing a general ARD cost function using an auxiliary function that can naturally be solved using a series of re-weighted L1 problems. The result is an efficient algorithm that can be implemented using standard convex programming toolboxes and is guaranteed to converge to a stationary point unlike existing methods. The analysis also leads to additional insights into the behavior of previous ARD updates as well as the ARD cost function. For example, the standard fixed-point updates of MacKay (1992) are shown to be iteratively solving a particular min-max problem, although they are not guaranteed to lead to a stationary point. The analysis also reveals that ARD is exactly equivalent to performing MAP estimation using a particular feature- and noise-dependent \textit{non-factorial} weight prior with several desirable properties over conventional priors with respect to feature selection. In particular, it provides a tighter approximation to the L0 quasi-norm sparsity measure than the L1 norm. Overall these results suggests alternative cost functions and update procedures for selecting features and promoting sparse solutions.","A New View of Automatic Relevance Determination

David Wipf and Srikantan Nagarajan, (cid:3)
Biomagnetic Imaging Lab, UC San Francisco
fdavid.wipf, srig@mrsc.ucsf.edu

Abstract

Automatic relevance determination (ARD) and the closely-related sparse
Bayesian learning (SBL) framework are effective tools for pruning large numbers
of irrelevant features leading to a sparse explanatory subset. However, popular up-
date rules used for ARD are either dif(cid:2)cult to extend to more general problems of
interest or are characterized by non-ideal convergence properties. Moreover, it re-
mains unclear exactly how ARD relates to more traditional MAP estimation-based
methods for learning sparse representations (e.g., the Lasso). This paper furnishes
an alternative means of expressing the ARD cost function using auxiliary func-
tions that naturally addresses both of these issues. First, the proposed reformu-
lation of ARD can naturally be optimized by solving a series of re-weighted ‘1
problems. The result is an ef(cid:2)cient, extensible algorithm that can be implemented
using standard convex programming toolboxes and is guaranteed to converge to
a local minimum (or saddle point). Secondly, the analysis reveals that ARD is
exactly equivalent to performing standard MAP estimation in weight space using
a particular feature- and noise-dependent, non-factorial weight prior. We then
demonstrate that this implicit prior maintains several desirable advantages over
conventional priors with respect to feature selection. Overall these results suggest
alternative cost functions and update procedures for selecting features and promot-
ing sparse solutions in a variety of general situations. In particular, the method-
ology readily extends to handle problems such as non-negative sparse coding and
covariance component estimation.

1 Introduction
Here we will be concerned with the generative model

y = (cid:8)x + (cid:15);

(1)
where (cid:8) 2 Rn(cid:2)m is a dictionary of features, x 2 Rm is a vector of unknown weights, y is an
observation vector, and (cid:15) is uncorrelated noise distributed as N ((cid:15); 0; (cid:21)I). When large numbers
of features are present relative to the signal dimension, the estimation problem is fundamentally
ill-posed. Automatic relevance determination (ARD) addresses this problem by regularizing the
solution space using a parameterized, data-dependent prior distribution that effectively prunes away
redundant or super(cid:3)uous features [10]. Here we will describe a special case of ARD called sparse
Bayesian learning (SBL) that has been very successful in a variety of applications [15]. Later in
Section 4 we will address extensions to more general models.
The basic ARD prior incorporated by SBL is p(x; (cid:13)) = N (x; 0; diag[(cid:13)]), where (cid:13) 2 Rm
+ is a vector
of m non-negative hyperperparameters governing the prior variance of each unknown coef(cid:2)cient.
These hyperparameters are estimated from the data by (cid:2)rst marginalizing over the coef(cid:2)cients x
and then performing what is commonly referred to as evidence maximization or type-II maximum
likelihood [7, 10, 15]. Mathematically, this is equivalent to minimizing

L((cid:13)) , (cid:0) logZ p(yjx)p(x; (cid:13))dx = (cid:0) log p(y; (cid:13)) (cid:17) log j(cid:6)yj + y

T (cid:6)(cid:0)1

y y;

(2)

(cid:3)This research was supported by NIH grants R01DC04855 and R01DC006435.

where a (cid:3)at hyperprior on (cid:13) is assumed, (cid:6)y , (cid:21)I + (cid:8)(cid:0)(cid:8)T , and (cid:0) , diag[(cid:13)]. Once some (cid:13)(cid:3) =
arg min(cid:13) L((cid:13)) is computed, an estimate of the unknown coef(cid:2)cients can be obtained by setting
xARD to the posterior mean computed using (cid:13)(cid:3):

y(cid:3) y:

xARD = E[xjy; (cid:13)(cid:3)] = (cid:0)(cid:3)(cid:8)T (cid:6)(cid:0)1

(3)
Note that if any (cid:13)(cid:3);i = 0, as often occurs during the learning process, then xARD;i = 0 and the
corresponding feature is effectively pruned from the model. The resulting weight vector xARD is
therefore sparse, with nonzero elements corresponding with the ‘relevant’ features.
There are (at least) two outstanding issues related to this model which we consider to be signi(cid:2)cant.
First, while several methods exist for optimizing (2), limitations remain in each case. For example,
an EM version operates by treating the unknown x as hidden data, leading to the E-step
y y;

(cid:6) , Cov[xjy; (cid:13)] = (cid:0) (cid:0) (cid:0)(cid:8)T (cid:6)(cid:0)1

(cid:22) , E[xjy; (cid:13)] = (cid:0)(cid:8)T (cid:6)(cid:0)1

y (cid:8)(cid:0);

(4)

and the M-step

(5)
While convenient to implement, the convergence can be prohibitively slow in practice. In contrast,
the MacKay update rules are considerably faster to converge [15]. The idea here is to form the
gradient of (2), equate to zero, and then form the (cid:2)xed-point update

8i = 1; : : : ; m:

(cid:13)i ! (cid:22)2

i + (cid:6)ii;

(cid:13)i !

(cid:22)2
i
1 (cid:0) (cid:13)(cid:0)1
i (cid:6)ii

;

8i = 1; : : : ; m:

(6)

However, neither the EM nor MacKay updates are guaranteed to converge to a local minimum or
even a saddle point of L((cid:13)); both have (cid:2)xed points whenever a (cid:13)i = 0, whether at a minimizing
solution or not. Finally, a third algorithm has recently been proposed that optimally updates a single
hyperparameter (cid:13)i at a time, which can be done very ef(cid:2)ciently in closed form [16]. While extremely
fast to implement, as a greedy-like method it can sometimes be more prone to becoming trapped in
local minima when the number of features is large, e.g., m > n (results will be presented in a
forthcoming publication). Additionally, none of these methods are easily extended to more general
problems such as non-negative sparse coding, covariance component estimation, and classi(cid:2)cation
without introducing additional approximations.
A second issue pertaining to the ARD model involves its connection with more traditional maximum
a posteriori (MAP) estimation methods for extracting sparse, relevant features using (cid:2)xed, sparsity
promoting prior distributions (i.e., heavy-tailed and peaked). Presently, it is unclear how ARD,
which invokes a parameterized prior and transfers the estimation problem to hyperparameter space,
relates to MAP approaches which operate directly in x space. Nor is it intuitively clear why ARD
often works better in selecting optimal feature sets.
This paper introduces an alternative formulation of the ARD cost function using auxiliary func-
tions that naturally addresses the above issues. In Section 2, the proposed reformulation of ARD is
conveniently optimized by solving a series of re-weighted ‘1 problems. The result is an ef(cid:2)cient al-
gorithm that can be implemented using standard convex programming methods and is guaranteed to
converge to a local minimum (or saddle point) of L((cid:13)). Section 3 then demonstrates that ARD is ex-
actly equivalent to performing standard MAP estimation in weight space using a particular feature-
and noise-dependent, non-factorial weight prior. We then show that this implicit prior maintains
several desirable advantages over conventional priors with respect to feature selection. Additionally,
these results suggest modi(cid:2)cations of ARD for selecting relevant features and promoting sparse so-
lutions in a variety of general situations. In particular, the methodology readily extends to handle
problems involving non-negative sparse coding, covariance component estimation, and classi(cid:2)cation
as discussed in Section 4.

2 ARD/SBL Optimization via Iterative Re-Weighted Minimum ‘1

In this section we re-express L((cid:13)) using auxiliary functions which leads to an alternative update
procedure that circumvents the limitations of current approaches. In fact, a wide variety of alterna-
tive update rules can be derived by decoupling L((cid:13)) using upper bounding functions that are more
conveniently optimized. Here we focus on a particular instantiation of this idea that leads to an
iterative minimum ‘1 procedure. The utility of this selection being that many powerful convex pro-
gramming toolboxes have already been developed for solving these types of problems, especially
when structured dictionaries (cid:8) are being used.

2.1 Algorithm Derivation

To start we note that the log-determinant term of L((cid:13)) is concave in (cid:13) (see Section 3.1.5 of [1]),
and so can be expressed as a minimum over upper-bounding hyperplanes via

where g(cid:3)(z) is the concave conjugate of log j(cid:6)yj that is de(cid:2)ned by the duality relationship [1]

log j(cid:6)yj = min

z

T

z

(cid:13) (cid:0) g(cid:3)(z);

(7)

(8)
although for our purposes we will never actually compute g (cid:3)(z). This leads to the following upper-
bounding auxiliary cost function

g(cid:3)(z) = min

(cid:13) (cid:0) log j(cid:6)yj ;

z

T

(cid:13)

(9)
For any (cid:2)xed (cid:13), the optimal (tightest) bound can be obtained by minimizing over z. The optimal
value of z equals the slope at the current (cid:13) of log j(cid:6)yj. Therefore, we have

y y (cid:21) L((cid:13)):

(cid:13) (cid:0) g(cid:3)(z) + y

L((cid:13); z) , z

T (cid:6)(cid:0)1

T

zopt = O

y (cid:8)(cid:3) :
This formulation naturally admits the following optimization scheme:
Step 1: Initialize each zi, e.g., zi = 1; 8i.
Step 2: Solve the minimization problem

(cid:13) log j(cid:6)yj = diag(cid:2)(cid:8)T (cid:6)(cid:0)1

(cid:13) ! arg min

(cid:13)

Lz((cid:13)) , z

T

(cid:13) + y

T (cid:6)(cid:0)1

y y:

(10)

(11)

Step 3: Compute the optimal z using (10).
Step 4: Iterate Steps 2 and 3 until convergence to some (cid:13)(cid:3).
Step 5: Compute xARD = E[xjy; (cid:13)(cid:3)] = (cid:0)(cid:3)(cid:8)T (cid:6)(cid:0)1

y(cid:3) y.
Lemma 1. The objective function in (11) is convex.
This can be shown using Example 3.4 and Section 3.2.2 in [1]. Lemma 1 implies that many standard
optimization procedures can be used for the minimization required by Step 2. For example, one
attractive option is to convert the problem to an equivalent least absolute shrinkage and selector
operator or ‘Lasso’ [14] optimization problem according to the following:
Lemma 2. The objective function in (11) can be minimized by solving the weighted convex ‘1-
regularized cost function

x(cid:3) = arg min

x

ky (cid:0) (cid:8)xk2

2 + 2(cid:21)Xi

z1=2
i

jxij

(12)

and then setting (cid:13)i ! z(cid:0)1=2
The proof of Lemma 2 can be brie(cid:3)y summarized using a re-expression of the data dependent term
in (11) using
(13)

jx(cid:3);ij for all i (note that each zi will always be positive).

x2
i
(cid:13)i
This leads to an upper-bounding auxiliary function for Lz((cid:13)) given by

2 +Xi

ky (cid:0) (cid:8)xk2

y y = min

T (cid:6)(cid:0)1

1
(cid:21)

y

:

i

x

Lz((cid:13); x) , Xi

(cid:18)zi(cid:13)i +

x2
i

(cid:13)i (cid:19) +

1
(cid:21)

ky (cid:0) (cid:8)xk2

2 (cid:21) Lz((cid:13));

(14)

i

which is jointly convex in x and (cid:13) (see Example 3.4 in [1]) and can be globally minimized by
jxij minimizes Lz((cid:13); x). When substituted into
solving over (cid:13) and then x. For any x, (cid:13)i = z(cid:0)1=2
(14) we obtain (12). When solved for x, the global minimum of (14) yields the global minimum of
(11) via the stated transformation.
In summary then, by iterating the above algorithm using Lemma 2 to implement Step 2, a conve-
nient optimization method is obtained. Moreover, we do not even need to globally solve for x (or
equivalently (cid:13)) at each iteration as long as we strictly reduce (11) at each iteration. This is read-
ily achievable using a variety of simple strategies. Additionally, if z is initialized to a vector of
ones, then the starting point (assuming Step 2 is computed in full) is the exact Lasso estimator. The
algorithm then re(cid:2)nes this estimate through the speci(cid:2)ed re-weighting procedure.

2.2 Global Convergence Analysis

00) < L((cid:13)

0) for all (cid:13)

+ the subset of Rm

+ which satis(cid:2)es
Let A((cid:1)) denote a mapping that assigns to every point in Rm
Steps 2 and 3 of the proposed algorithm. Such a mapping can be implemented via the methodology
described above. We allow A((cid:1)) to be a point-to-set mapping to handle the case where the global
minimum of (11) is not unique, which could occur, for example, if two columns of (cid:8) are identical.
Theorem 1. From any initialization point (cid:13)(0) 2 Rm
+ the sequence of hyperparameter estimates
f(cid:13)(k)g generated via (cid:13)(k+1) 2 A((cid:13)(k+1)) is guaranteed to converge monotonically to a local mini-
mum (or saddle point) of (2).
The proof is relatively straightforward and stems directly from the Global Convergence Theorem
(see for example [6]). A sketch is as follows: First, it must be shown that the the mapping A((cid:1))
is compact. This condition is satis(cid:2)ed because if any element of (cid:13) is unbounded, L((cid:13)) diverges to
in(cid:2)nity. If fact, for any (cid:2)xed y, (cid:8) and (cid:21), there will always exist a radius r such that for any k(cid:13)(0)k (cid:20)
r, k(cid:13)(k)k (cid:20) r for all k. Second, we must show that for any non-minimizing point of L((cid:13)) denoted
0 the auxiliary cost function
0, L((cid:13)
(cid:13)
0 ((cid:13)) obtained from Step 3 will be strictly tangent to L((cid:13)) at (cid:13)
0. It will therefore necessarily have
Lz
0 is nonzero by de(cid:2)nition. Moreover, because the log j (cid:1) j
a minimum elsewhere since the slope at (cid:13)
function is strictly concave, at this minimum the actual cost function will be reduced still further.
Consequently, the proposed updates represent a valid descent function. Finally, it must be shown
that A((cid:1)) is closed at all non-stationary points. This follows from related arguments. The algorithm
could of course theoretically converge to a saddle point, but this is rare and any minimal perturbation
leads to escape.
Both EM and MacKay updates provably fail to satisfy one or more of the above criteria and so global
convergence cannot be guaranteed. With EM, the failure occurs because the associated updates do
not always strictly reduce L((cid:13)). Rather, they only ensure that L((cid:13)
0) at all points. In
contrast, the MacKay updates do not even guarantee cost function decrease. Consequently, both
methods can become trapped at a solution such as (cid:13) = 0; a (cid:2)xed point of the updates but not a
stationary point or local minimum of L((cid:13)). However, in practice this seems to be more of an issue
with the MacKay updates. Related shortcomings of EM in this regard can be found in [19]. Finally,
the fast Tipping updates could potentially satisfy the conditions for global convergence, although
this matter is not discussed in [16].

0). At any non-minimizing (cid:13)

00) (cid:20) L((cid:13)

00 2 A((cid:13)

3 Relating ARD to MAP Estimation
In hierarchical models such as ARD and SBL there has been considerable debate over how to best
perform estimation and inference [8]. Do we add a hyperprior and then integrate out (cid:13) and perform
MAP estimation directly on x? Or is it better to marginalize over the coef(cid:2)cients x and optimize the
hyperparameters (cid:13) as we have described in this paper? In speci(cid:2)c cases, arguments have been made
for the merits of one over the other based on intuition or heuristic arguments [8, 15]. But we would
argue that this distinction is somewhat tenuous because, as we will now show using ideas from the
previous section, the weights obtained from the ARD type-II ML procedure can equivalently be
viewed as arising from an explicit MAP estimate in x space. This notion is made precise as follows:
Theorem 2. Let x
m ]T . Then the ARD coef(cid:2)cients
from (3) solve the MAP problem

m]T and (cid:13)

1 ; : : : ; (cid:13)(cid:0)1

(cid:0)1 , [(cid:13)(cid:0)1

1; : : : ; x2

2 , [x2

xARD = arg min
2) is the concave conjugate of h((cid:13)

x

2 + (cid:21)h(cid:3)(x

(15)
ky (cid:0) (cid:8)xk2
(cid:0)1) , (cid:0) log j(cid:6)yj and is a concave, non-decreasing

2);

where h(cid:3)(x
function of x.
This result can be established using much of the same analysis used in previous sections. Omitting
some details for the sake of brevity, using (13) we can create a strict upper bounding auxiliary
function on L((cid:13)):

(16)

L((cid:13); x) =

ky (cid:0) (cid:8)xk2

+ log j(cid:6)yj:

1
(cid:21)

2 +Xi

x2
i
(cid:13)i

If we optimize (cid:2)rst over (cid:13) instead of x (allowable), the last two terms form the stated concave
conjugate function h(cid:3)(x
2). In turn, the minimizing x, which solves (15), is identical to that obtained
by ARD. The concavity of h(cid:3)(x

2) with respect each jxij follows from similar ideas.

Corollary 1. The regularization term in (15), and hence the implicit prior distribution on x given
2)], is not generally factorable, meaning p(x) 6= Qi pi(xi). Addition-
by p(x) / exp[(cid:0) 1
ally, unlike traditional MAP procedures (e.g., Lasso, ridge regression, etc.), this prior is explicitly
dependent on both the dictionary (cid:8) and the regularization term (cid:21).

2 h(cid:3)(x

This result stems directly from the fact that h((cid:13)
(cid:21). The only exception occurs when (cid:8)T (cid:8) = I; here h(cid:3)(x
form independently of (cid:8), although (cid:21) dependency remains.

(cid:0)1) is non-factorable and is dependent on (cid:8) and
2) factors and can be expressed in closed

3.1 Properties of the implicit ARD prior

To begin at the most super(cid:2)cial level, the (cid:8) dependency of the ARD prior leads to scale invariant
solutions, meaning the value of xARD is not affected if we rescale (cid:8), i.e., (cid:8) ! (cid:8)D, where D is a
diagonal matrix. Rather, any rescaling D only affects the implicit initialization of the algorithm, not
the shape of the cost function.
More signi(cid:2)cantly, the ARD prior is particularly well-designed for (cid:2)nding sparse solutions. We
should note that concave, non-decreasing regularization functions are well-known to encourage
sparse representations. Since h(cid:3)(x
2) is such a function, it should therefore not be surprising that it
promotes sparsity to some degree. However, when selecting highly sparse subsets of features, the
factorial ‘0 quasi-norm is often invoked as the ideal regularization term given unlimited computa-
tional resources. It is expressed via kxk0 , Pi I[xi 6= 0], where I[(cid:1)] denotes the indicator function,
and so represents a count of the number of nonzero coef(cid:2)cients (and therefore features). By applying
a exp[(cid:0)1=2((cid:1))] transformation, we obtain the implicit (improper) prior distribution. The associated
MAP estimation problem (assuming the same standard Gaussian likelihood) involves solving

min

x

ky (cid:0) (cid:8)xk2

2 + (cid:21)kxk0:

(17)

The dif(cid:2)culty here is that (17) is nearly impossible to solve in general; it is NP-hard owing to a
combinatorial number of local minima and so the traditional idea is to replace k (cid:1) k0 with a tractable
approximation. For this purpose, the ‘1 norm is the optimal or tightest convex relaxation of the ‘0
quasi-norm, and therefore it is commonly used leading to the Lasso algorithm [14]. However, the
‘1 norm need not be the best relaxation in general. In Sections 3.2 and 3.3 we demonstrate that
the non-factorable, (cid:21)-dependent h(cid:3)(x
2) provides a tighter, albeit non-convex, approximation that
promotes greater sparsity than kxk1 while conveniently producing many fewer local minima than
using kxk0 directly. We also show that, in certain settings, no (cid:21)-independent, factorial regularization
term can achieve similar results. Consequently, the widely used family of ‘p quasi-norms, i.e.,
kxkp , Pi jxijp, p < 1 [2], or the Gaussian entropy measure Pi log jxij based on the Jeffreys
prior [4] provably fail in this regard.

3.2 Beneﬁts of (cid:21) dependency

To explore the properties of h(cid:3)(x
2) regarding (cid:21) dependency alone, we adopt the simplifying as-
sumption (cid:8)T (cid:8) = I. (Later we investigate the bene(cid:2)ts of a non-factorial prior.) In this special case,
h(cid:3)(x

2) is factorable and can be expressed in closed form via

2jxij

i + 4(cid:21)

+ log(cid:18)2(cid:21) + x2

h(cid:3)(x

i + 4(cid:21)(cid:19) ;

(18)

h(cid:3)(x2

i ) / Xi

2) = Xi

i + jxijqx2
i ) is shown in Figure 1 (left) below.

jxij +px2
which is independent of (cid:8). A plot of h(cid:3)(x2
The (cid:21) dependency is retained however and contributes two very desirable properties: (i) As a strictly
concave function of each jxij, h(cid:3)(x
2) more closely approximates the ‘0 quasi-norm than the ‘1 norm
while, (ii) The associated cost function (15) is unimodal unlike when (cid:21)-independent approximations,
e.g., the ‘p quasi-norm, are used. This can be explained as follows. When (cid:21) is small, the Gaussian
likelihood is highly restrictive, constraining most of its relative mass to a very localized region of x
space. Therefore, a tighter prior more closely resembling the ‘0 quasi-norm can be used without the
risk of local minima, which occur when the spines of a sparse prior overlap non-negligible portions
of the likelihood (see Figure 6 in [15] for a good 2D visual of a sparse prior with characteristic spines
running alone the coordinate axis). In the limit as (cid:21) ! 0, h(cid:3)(x
2) converges to a scaled version of the

‘0 quasi-norm, yet no local minimum exist because the likelihood in this case only permits a single
feasible solution with x = (cid:8)T
y. In contrast, when (cid:21) is large, the likelihood is less constrained and a
looser prior is required to avoid local minima troubles, which will arise whenever the now relatively
diffuse likelihood intersects the sharp spines of a highly sparse prior. In this situation h(cid:3)(x
2) more
closely resembles a scaled version of the ‘1 norm. The implicit ARD prior naturally handles this
transition becoming sparser as (cid:21) decreases and vice versa. Hence the following property, which is
easy to show [18]:
Lemma 3. When (cid:8)T (cid:8) = I, (15) has no local minima whereas (17) has 2M local minima.
2) also yields no local minima; however, it is a much looser
Use of the ‘1 norm in place of h(cid:3)(x
approximation of ‘0 and penalizes coef(cid:2)cients linearly unlike h(cid:3)(x
2). The bene(cid:2)ts of (cid:21) dependency
in this regard can be formalized and will be presented in a subsequent paper. As a (cid:2)nal point of
comparison, the actual weight estimate obtained from solving (15) when (cid:8)T (cid:8) = I is equivalent to
the non-negative garrote estimator that has been advocated for wavelet shrinkage [5, 18].

2  

1.8

1.6

1.4

1.2

2

1.6

1.2

0.8

0.4

)
i

x
(
p
g
o
l

(cid:0)

I[xi 6= 0]
jxij
ARD

PSfrag replacements
xi
(cid:0) log p(xi)
I[xi 6= 0]
jxij
ARD(cid:0)

)
x
(
p
g
o
l

)
d
e
z
i
l
a
m
r
o
n
(

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

maximally

sparse
solution

ARD
Pi jxij0:01

0  
−2

−1.5

−1

−0.5

0 
xi

0.5 

1 

1.5 

2

0
−8

−6

−4

−2

2

4

6

8

0

(cid:11)

Figure 1: Left: 1D example of the implicit ARD prior. The ‘1 and ‘0 norms are included for com-
parison. Right: Plot of the ARD prior across the feasible region as parameterized by (cid:11). A factorial
prior given by (cid:0) log p(x) / Pi jxij0:01 (cid:25) kxk0 is included for comparison. Both approximations
to the ‘0 norm retain the correct global minimum, but only ARD smooths out local minima.

PSfrag replacements

(cid:0) log p(x)

(normalized)

(cid:11)
ARD
Pi jxij0:01

x0 , arg min

kxk0

x

3.3 Beneﬁts of a non-factorial prior
In contrast, the bene(cid:2)ts the typically non-factorial nature of h(cid:3)(x
2) are most pronounced when
m > n, meaning there are more features than the signal dimension y. In a noiseless setting (with
(cid:21) ! 0), we can explicitly quantify the potential of this property of the implicit ARD prior. In this
limiting situation, the canonical sparse MAP estimation problem (17) reduces to (cid:2)nding

s.t. y = (cid:8)x:

(19)
By simple extension of results in [18], the global minimum of (15) in the limit as (cid:21) ! 0 will
equal x0, assuming the latter is unique. The real distinction then is regarding the number of local
minimum. In this capacity the ARD MAP problem is superior to any possible factorial variant:
In the limit as (cid:21) ! 0 and assuming m > n, no factorial prior p(x) =
Theorem 3.
Qi exp[(cid:0)1=2fi(xi)] exists such that
the corresponding MAP problem minx ky (cid:0) (cid:8)xk2
2 +
(cid:21)Pi fi(xi) is: (i) Always globally minimized by a maximally sparse solution x0 and, (ii) Has
fewer local minima than when solving (15).
A sketch of the proof is as follows. First, for any factorial prior and associated regularization term
Pi fi(xi), the only way to satisfy (i) is if @fi(xi)=@xi ! 1 as xi ! 0. Otherwise, it will always be
possible to have a (cid:8) and y such that x0 is not the global minimum. It is then straightforward to show
that any fi(xi) with this property will necessarily have between (cid:2)(cid:0)m(cid:0)1
n(cid:1)(cid:3) local minimum.
Using results from [18], this is provably an upper bound on the number of local minimum to (15).
Moreover, with the exception of very contrived situations, the number of ARD local minima will
be considerably less. In general, this result speaks directly to the potential limitations of restricting
oneself to factorial priors when maximal feature pruning is paramount.
While generally dif(cid:2)cult to visualize, in restricted situations it is possible to explicitly illustrate
the type of smoothing over local minima that is possible using non-factorial priors. For example,

n (cid:1) + 1;(cid:0)m

consider the case where m = n + 1 and Rank((cid:8)) = n, implying that (cid:8) has a null-space dimension
of one. Consequently, any feasible solution to y = (cid:8)x can be expressed as x = x
0 + (cid:11)v, where
v 2 Null((cid:8)), (cid:11) is any real-valued scalar, and x
0 is any (cid:2)xed, feasible solution (e.g., the minimum
norm solution). We can now plot any prior distribution p(x), or equivalently (cid:0) log p(x), over the
1D feasible region of x space as a function of (cid:11) to view the local minima pro(cid:2)le.
To demonstrate this idea, we chose n = 10, m = 11 and generated a (cid:8) matrix using iid N (0; 1)
entries. We then computed y = (cid:8)x0, where kx0k0 = 9 and nonzero entries are also iid unit
Gaussian. Figure 1 (right) displays the plots of two example priors in the feasible region of y = (cid:8)x:
2 Pi jxijp), p = 0:01. The
(i) the non-factorial implicit ARD prior, and (ii) the prior p(x) / exp((cid:0) 1
later is a factorial prior which converges to the ideal sparsity penalty when p ! 0. From the (cid:2)gure,
we observe that, while both priors peak at the x0, the ARD prior has substantially smoothed away
local minima. While the implicit Lasso prior (which is equivalent to the assumption p = 1) also
smooths out local minima, the global minimum may be biased away from the maximally sparse
solution in many situations, unlike the ARD prior which provides a non-convex approximation with
its global minimum anchored at x0.

4 Extensions

Thus far we have restricted attention to one particularly useful ARD-based model. But much of the
analysis can be extended to handle a variety of alternative data likelihoods and priors. A particularly
useful adaptation relevant to compressed sensing [17], manifold learning [13], and neuroimaging
[12, 18] is as follows. First, the data y can be replaced with a n (cid:2) t observation matrix Y which is
generated via an unknown coef(cid:2)cient matrix X. The assumed likelihood model and prior are
d(cid:13)
p(Y jX) / exp(cid:18)(cid:0)
(cid:13)iCi:
(20)
Here each of the d(cid:13) matrices Ci’s are known covariance components of which the irrelevant ones
are pruned by minimizing the analogous type-II likelihood function

x X(cid:3)(cid:19) ; (cid:6)x ,

trace(cid:2)X T (cid:6)(cid:0)1

kY (cid:0) (cid:8)Xk2

F(cid:19) ;

p(X) / exp(cid:18)(cid:0)

1
2

Xi=1

1
2(cid:21)

L((cid:13)) = log j(cid:21)I + (cid:8)(cid:6)x(cid:8)T j + trace(cid:20) 1

t

XX T (cid:0)(cid:21)I + (cid:8)(cid:6)x(cid:8)T(cid:1)

(cid:0)1(cid:21) :

(21)

With minimal effort, this extension can be solved using the methodology described herein. The
primary difference is that Step 2 becomes a second-order cone (SOC) optimization problem for
which a variety of techniques exist for its minimization [2, 9].
Another very useful adaptation involves adding a non-negativity constraint on the coef(cid:2)cients x,
e.g., non-negative sparse coding. This is easily incorporated into the MAP cost function (15) and
optimization problem (12); performance is often signi(cid:2)cantly better than the non-negative Lasso.
Results will be presented in a subsequent paper. It may also be possible to develop an effective
variant for handling classi(cid:2)cation problems that avoids additional approximations such as those
introduced in [15].

5 Discussion
While ARD-based approaches have enjoyed remarkable success in a number of disparate (cid:2)elds, they
remain hampered to some degree by implementational limitations and a lack of clarity regarding the
nature of the cost function and existing update rules. This paper addresses these issues by presenting
a principled alternative algorithm based on auxiliary functions and a dual representation of the ARD
objective. The resulting algorithm is initialized at the well-known Lasso solution and then iterates
via a globally convergent re-weighted ‘1 procedure that in many ways approximates ideal subset
selection using the ‘0 norm. Preliminary results using this methodology on toy problems as well
as large neuroimaging simulations with m (cid:25) 100; 000 are very promising (and will be reported in
future papers). A good (highly sparse) solution is produced at every iteration and so early stopping is
always feasible if desired. This produces a highly ef(cid:2)cient, global competition among features that
is potentially superior to the sequential (greedy) updates of [16] in terms of local minima avoidance
in certain cases when (cid:8) is highly overcomplete (i.e., m (cid:29) n). Moreover, it is also easily extended
to handle additional constraints (e.g., non-negativity) or model complexity as occurs with general
covariance component estimation. A related optimization strategy has also been reported in [3].

The analysis used in deriving this algorithm reveals that ARD is exactly equivalent to performing
MAP estimation in x space using a principled, sparsity-inducing prior that is non-factorable and
dependent on both the feature set and noise parameter. We have shown that these qualities allow it
to promote maximally sparse solutions at the global minimum while relenting drastically fewer local
minima than competing priors. This might possibly explain the superior performance of ARD/SBL
over Lasso in a variety of disparate disciplines where sparsity is crucial [11, 12, 18]. These ideas
raise a key question: If we do not limit ourselves to factorable, (cid:8)- and (cid:21)-independent regularization
terms/priors as is commonly done, then what is the optimal prior p(x) in the context of feature
selection? Perhaps there is a better choice that does not neatly (cid:2)t into current frameworks linked
to empirical priors based on the Gaussian distribution. Note that the ‘1 re-weighting scheme for
optimization can be applied to a broad family of non-factorial, sparsity-inducing priors.

References
[1] S. Boyd and L. Vandenberghe, Convex Optimization, Cambridge University Press, 2004.
[2] S.F. Cotter, B.D. Rao, K. Engan, and K. Kreutz-Delgado, (cid:147)Sparse solutions to linear inverse
problems with multiple measurement vectors,(cid:148) IEEE Trans. Signal Processing, vol. 53, no. 7,
pp. 2477(cid:150)2488, April 2005.

[3] M. Fazel, H. Hindi, and S. Boyd (cid:147)Log-Det Heuristic for Matrix Rank Minimization with Appli-
cations to Hankel and Euclidean Distance Matrices,(cid:148) Proc. American Control Conf., vol. 3, pp.
2156(cid:150)2162, June 2003.

[4] M.A.T. Figueiredo, (cid:147)Adaptive sparseness using Jeffreys prior,(cid:148) Advances in Neural Information

Processing Systems 14, pp. 697(cid:150)704, 2002.

[5] H. Gao, (cid:147)Wavelet shrinkage denoising using the nonnegative garrote,(cid:148) Journal of Computational

and Graphical Statistics, vol. 7, no. 4, pp. 469(cid:150)488, 1998.

[6] D.G. Luenberger, Linear and Nonlinear Programming, Addison(cid:150)Wesley, Reading, Mas-

sachusetts, 2nd ed., 1984.

[7] D.J.C. MacKay, (cid:147)Bayesian interpolation,(cid:148) Neural Comp., vol. 4, no. 3, pp. 415(cid:150)447, 1992.
[8] D.J.C. MacKay, (cid:147)Comparison of approximate methods for handling hyperparameters,(cid:148) Neural

Comp., vol. 11, no. 5, pp. 1035(cid:150)1068, 1999.

[9] D.M. Malioutov, M. C‚ etin, and A.S. Willsky, (cid:147)Sparse signal reconstruction perspective for
source localization with sensor arrays,(cid:148) IEEE Trans. Signal Processing, vol. 53, no. 8, pp.
3010(cid:150)3022, August 2005.

[10] R.M. Neal, Bayesian Learning for Neural Networks, Springer-Verlag, New York, 1996.
[11] R. Pique-Regi, E.S. Tsau, A. Ortega, R.C. Seeger, and S. Asgharzadeh, (cid:147)Wavelet footprints
and sparse Bayesian learning for DNA copy number change analysis,(cid:148) Int. Conf. Acoustics
Speech and Signal Processing, April 2007.

[12] R.R. Ram·(cid:17)rez, Neuromagnetic Source Imaging of Spontaneous and Evoked Human Brain

Dynamics, PhD Thesis, New York University, 2005.

[13] J.G. Silva, J.S. Marques, and J.M. Lemos, (cid:147)Selecting landmark points for sparse manifold

learning,(cid:148) Advances in Neural Information Processing Systems 18, pp. 1241(cid:150)1248, 2006.

[14] R. Tibshirani, (cid:147)Regression shrinkage and selection via the Lasso,(cid:148) Journal of the Royal

Statistical Society, vol. 58, no. 1, pp. 267(cid:150)288, 1996.

[15] M.E. Tipping, (cid:147)Sparse Bayesian learning and the relevance vector machine,(cid:148) Journal of

Machine Learning Research, vol. 1, pp. 211(cid:150)244, 2001.

[16] M.E. Tipping and A.C. Faul, (cid:147)Fast marginal likelihood maximisation for sparse Bayesian

models,(cid:148) Ninth Int. Workshop Artiﬁcial Intelligence and Statistics, Jan. 2003.

[17] M.B. Wakin, M.F. Duarte, S. Sarvotham, D. Baron, and R.G. Baraniuk, (cid:147)Recovery of jointly
sparse signals from a few random projections,(cid:148) Advances in Neural Information Processing
Systems 18, pp. 1433(cid:150)1440, 2006.

[18] D.P. Wipf, (cid:147)Bayesian Methods for Finding Sparse Representations,(cid:148) PhD Thesis, UC San

Diego, 2006.

[19] C.F. Wu, (cid:147)On the convergence properties of the EM algorithm,(cid:148) The Annals of Statistics, vol.

11, pp. 95(cid:150)103, 1983.

"
188,2007,"HM-BiTAM: Bilingual Topic Exploration, Word Alignment, and Translation","We present a novel paradigm for statistical machine translation (SMT), based on joint modeling of word alignment and the topical aspects underlying bilingual document pairs via a hidden Markov Bilingual Topic AdMixture (HM-BiTAM). In this new paradigm, parallel sentence-pairs from a parallel document-pair are coupled via a certain semantic-flow, to ensure coherence of topical context in the alignment of matching words between languages, during likelihood-based training of topic-dependent translational lexicons, as well as topic representations in each language. The resulting trained HM-BiTAM can not only display topic patterns like other methods such as LDA, but now for bilingual corpora; it also offers a principled way of inferring optimal translation in a context-dependent way. Our method integrates the conventional IBM Models based on HMM --- a key component for most of the state-of-the-art SMT systems, with the recently proposed BiTAM model, and we report an extensive empirical analysis (in many way complementary to the description-oriented of our method in three aspects: word alignment, bilingual topic representation, and translation.","HM-BiTAM: Bilingual Topic Exploration, Word

Alignment, and Translation

Bing Zhao

IBM T. J. Watson Research

zhaob@us.ibm.com

Eric P. Xing

Carnegie Mellon University
epxing@cs.cmu.edu

Abstract

We present a novel paradigm for statistical machine translation (SMT), based on
a joint modeling of word alignment and the topical aspects underlying bilingual
document-pairs, via a hidden Markov Bilingual Topic AdMixture (HM-BiTAM).
In this paradigm, parallel sentence-pairs from a parallel document-pair are cou-
pled via a certain semantic-ﬂow, to ensure coherence of topical context in the
alignment of mapping words between languages, likelihood-based training of
topic-dependent translational lexicons, as well as in the inference of topic rep-
resentations in each language. The learned HM-BiTAM can not only display
topic patterns like methods such as LDA [1], but now for bilingual corpora; it
also offers a principled way of inferring optimal translation using document con-
text. Our method integrates the conventional model of HMM — a key component
for most of the state-of-the-art SMT systems, with the recently proposed BiTAM
model [10]; we report an extensive empirical analysis (in many ways complemen-
tary to the description-oriented [10]) of our method in three aspects: bilingual
topic representation, word alignment, and translation.

1 Introduction
Most contemporary SMT systems view parallel data as independent sentence-pairs whether or
not they are from the same document-pair. Consequently, translation models are learned only at
sentence-pair level, and document contexts – essential factors for translating documents – are gen-
erally overlooked. Indeed, translating documents differs considerably from translating a group of
unrelated sentences. A sentence, when taken out of the context from the document, is generally more
ambiguous and less informative for translation. One should avoid destroying a coherent document
by simply translating it into a group of sentences which are indifferent to each other and detached
from the context.

Developments in statistics, genetics, and machine learning have shown that latent semantic aspects
of complex data can often be captured by a model known as the statistical admixture (or mixed
membership model [4]). Statistically, an object is said to be derived from an admixture if it consists
of a bag of elements, each sampled independently or coupled in a certain way, from a mixture
model. In the context of SMT, each parallel document-pair is treated as one such object. Depending
on the chosen modeling granularity, all sentence-pairs or word-pairs in a document-pair correspond
to the basic elements constituting the object, and the mixture from which the elements are sampled
can correspond to a collection of translation lexicons and monolingual word frequencies based on
different topics (e.g., economics, politics, sports, etc.). Variants of admixture models have appeared
in population genetics [6] and text modeling [1, 4].
Recently, a Bilingual Topic-AdMixture (BiTAM) model was proposed to capture the topical aspects
of SMT [10]; word-pairs from a parallel document-pair follow the same weighted mixtures of trans-
lation lexicons, inferred for the given document-context. The BiTAMs generalize over IBM Model-
1; they are efﬁcient to learn and scalable for large training data. However, they do not capture locality

1

constraints of word alignment, i.e., words “close-in-source” are usually aligned to words “close-in-
target”, under document-speciﬁc topical assignment. To incorporate such constituents, we integrate
the strengths of both HMM and BiTAM, and propose a Hidden Markov Bilingual Topic-AdMixture
model, or HM-BiTAM, for word alignment to leverage both locality constraints and topical context
underlying parallel document-pairs.

In the HM-BiTAM framework, one can estimate topic-speciﬁc word-to-word translation lexicons
(lexical mappings), as well as the monolingual topic-speciﬁc word-frequencies for both languages,
based on parallel document-pairs. The resulting model offers a principled way of inferring optimal
translation from a given source language in a context-dependent fashion. We report an extensive
empirical analysis of HM-BiTAM, in comparison with related methods. We show our model’s ef-
fectiveness on the word-alignment task; we also demonstrate two application aspects which were
untouched in [10]: the utility of HM-BiTAM for bilingual topic exploration, and its application for
improving translation qualities.

2 Revisit HMM for SMT

An SMT system can be formulated as a noisy-channel model [2]:

e∗ = arg max

P (e|f ) = arg max

P (f |e)P (e),

e

e

(1)

where a translation corresponds to searching for the target sentence e∗ which explains the source
sentence f best. The key component is P (f |e), the translation model; P (e) is monolingual language
model. In this paper, we generalize P (f |e) with topic-admixture models.
An HMM implements the “proximity-bias” assumption — that words “close-in-source” are aligned
to words “close-in-target”, which is effective for improving word alignment accuracies, especially
for linguistically close language-pairs [8]. Following [8], to model word-to-word translation, we
introduce the mapping j → aj, which assigns a French word fj in position j to an English word
ei in position i = aj denoted as eaj . Each (ordered) French word fj is an observation, and it is
generated by an HMM state deﬁned as [eaj , aj], where the alignment indicator aj for position j is
considered to have a dependency on the previous alignment aj−1. Thus a ﬁrst-order HMM for an
alignment between e ≡ e1:I and f ≡ f1:J is deﬁned as:

p(f1:J |e1:I ) = X

Y

p(fj|eaj )p(aj|aj−1),

J

a1:J

j=1

(2)

where p(aj|aj−1) is the state transition probability; J and I are sentence lengths of the French and
English sentences, respectively. The transition model enforces the proximity-bias. An additional
pseudo word ”NULL” is used at the beginning of English sentences for HMM to start with. The
HMM implemented in GIZA++ [5] is used as our baseline, which includes reﬁnements such as
special treatment of a jump to a NULL word. A graphical model representation for such an HMM
is illustrated in Figure 1 (a).

Im,n

em,i

B = p(f |e)

α

θm

zm,n

βk

K

Im,n

em,i

fm,1

fm,2

fm,3

fJm,n

fm,1

fm,2

fm,3

fJm,n

am,1

am,2

am,3

aJm,n

am,1

am,2

am,3

aJm,n

Nm

M

Bk

K

Nm
M

Ti,i′

(a) HMM for Word Alignment

Ti,i′

(b) HM-BiTAM

Figure 1: The graphical model representations of (a) HMM, and (b) HM-BiTAM, for parallel corpora. Circles
represent random variables, hexagons denote parameters, and observed variables are shaded.

2

3 Hidden Markov Bilingual Topic-AdMixture
We assume that in training corpora of bilingual documents, the document-pair boundaries are
known, and indeed they serve as the key information for deﬁning document-speciﬁc topic weights
underlying aligned sentence-pairs or word-pairs. To simplify the outline, the topics here are sam-
pled at sentence-pair level; topics sampled at word-pair level can be easily derived following the
outlined algorithms, in the same spirit of [10]. Given a document-pair (F, E) containing N parallel
sentence-pairs (en, fn), HM-BiTAM implements the following generative scheme.

3.1 Generative Scheme of HM-BiTAM
Given a conjugate prior Dirichlet(α), the topic-weight vector (hereafter, TWV), θm for each
document-pair (Fm, Em), is sampled independently. Let the non-underscripted θ denote the TWV
of a typical document-pair (F, E), a collection of topic-speciﬁc translation lexicons be B ≡ {Bk},
where Bi,j,k=P (f =fj|e=ei, z=k) is the conditional probability of translating e into f under a
given topic indexed by z; the topic-speciﬁc monolingual model β ≡ {βk}, which can be the usual
LDA-style monolingual unigrams. The sentence-pairs {fn, en} are drawn independently from a
mixture of topics. Speciﬁcally (as illustrated also in Fig. 1 (b)):

1. θ ∼ Dirichlet(α)
2. For each sentence-pair (fn, en),

(a) zn ∼ Multinomial(θ)
(b) en,1:In |zn ∼ P (en|zn; β)

model (e.g., an unigram model),

sample the topic

sample all English words from a monolingual topic

(c) For each position jn = 1, . . . , Jn in fn,

i. ajn ∼ P (ajn |ajn−1;T )

process,

sample an alignment link ajn from a ﬁrst-order Markov

ii. fjn ∼ P (fjn |en, ajn , zn; B)

speciﬁc translation lexicon.

sample a foreign word fjn according to a topic

Under an HM-BiTAM model, each sentence-pair consists of a mixture of latent bilingual topics;
each topic is associated with a distribution over bilingual word-pairs. Each word f is generated by
two hidden factors: a latent topic z drawn from a document-speciﬁc distribution over K topics, and
the English word e identiﬁed by the hidden alignment variable a.

3.2 Extracting Bilingual Topics from HM-BiTAM

Because of the parallel nature of the data, the topics of English and the foreign language will share
similar semantic meanings. This assumption is captured in our model. Shown in Figure 1(b), both
the English and foreign topics are sampled from the same distribution θ, which is a document-
speciﬁc topic-weight vector.

Although there is an inherent asymmetry in the bilingual topic representation in HM-BiTAM (that
the monolingual topic representations β are only deﬁned for English, and the foreign topic represen-
tations are implicit via the topical translation models), it is not difﬁcult to retrieve the monolingual
topic representations of the foreign language via a marginalization over hidden word alignment. For
example, the frequency (i.e., unigram) of foreign word fw under topic k can be computed by

P (fw|k) = X

P (fw|e, Bk)P (e|βk).

e

(3)

As a result, HM-BiTAM can actually be used as a bilingual topic explorer in the LDA-style and
beyond. Given paired documents, it can extract the representations of each topic in both languages
in a consistent fashion (which is not guaranteed if topics are extracted separately from each language
using, e.g., LDA), as well as the lexical mappings under each topics, based on a maximal likelihood
or Bayesian principle. In Section 5.2, we demonstrate outcomes of this application.

We expect that, under the HM-BiTAM model, because bilingual statistics from word alignment a
are shared effectively across different topics, a word will have much less translation candidates due
to constraints by the hidden topics; therefore the topic speciﬁc translation lexicons are much smaller
and sharper, which give rise to a more parsimonious and unambiguous translation model.

3

4 Learning and Inference
We sketch a generalized mean-ﬁeld approximation scheme for inferring latent variables in HM-
BiTAM, and a variational EM algorithm for estimating model parameters.

p(F, E, θ, ~z, ~a|α, β, T, B) = p(θ|α)P (~z|θ)P (~a|T )P (F|~a, ~z, E, B)P (E|~z, β),

4.1 Variational Inference
Under HM-BiTAM, the complete likelihood of a document-pair (F, E) can be expressed as follows:
(4)
where P (~a|T )= QN
j=1 P (ajn |ajn−1; T ) represents the probability of a sequence of align-
ment jumps; P (F|~a, ~z, E, B)= QN
j=1 P (fjn |ajn , en, zn, B) is the document-level translation
probability; and P (E|~z, β) is the topic-conditional likelihood of the English document based on a
topic-dependent unigram as used in LDA. Apparently, exact inference under this model is infeasible
as noted in earlier models related to, but simpler than, this one [10].

n=1 QJn

n=1 QJn

To approximate the posterior p(~a, θ, ~z|F, E), we employ a generalized mean ﬁeld approach and
adopt the following factored approximation to the true posterior: q(θ, ~z, ~a) = q(θ|~γ)q(~z|~φ)q(~a|~λ),
where q(θ|~γ), q(~z|~φ), and q(~a|~λ) are re-parameterized Dirichlet, multinomial, and HMM, respec-
tively, determined by some variational parameters that correspond to the expected sufﬁcient statis-
tics of the dependent variables of each factor [9].

As well known in the variational inference literature, solutions to the above variational param-
eters can be obtained by minimizing the Kullback-Leibler divergence between q(θ, ~z, ~a) and
p(θ, ~z, ~a|F, E), or equivalently, by optimizing the lower-bound of the expected (over q()) log-
likelihood deﬁned by Eq.(4), via a ﬁxed-point iteration. Due to space limit, we forego a detailed
derivation, and directly give the ﬁxed-point equations below:

ˆγk = αk +

N

X

n=1

φn,k,

ˆφn,k ∝ exp“Ψ(γk) − Ψ(

K

X

k=1

γk)” · exp“

In

X

i=1

Jn

X

j=1

λn,j,i log βk,ein”

Jn ,In

1(fjn , f )1(ein , e)λn,j,ilog Bf ,e,k”,

× exp“

X

X

X

j,i=1

f ∈VF

e∈VE

(5)

(6)

ˆλn,j,i ∝ exp“

In

X

′ =1
i

λn,j−1,i

′ log Ti,i

′” × exp“

In

X

i”=1

λn,j+1,i” log Ti”,i”

× exp“X

X

f ∈VF

e∈VE

1(fjn ,f )1(ein ,e)

K

X

k=1

φn,k log Bf ,e,k” × exp“

K

X

k=1

φn,k log βk,ein”,

(7)

where 1(·, ·) denotes an indicator function, and Ψ(·) represents the digamma function.
The vector ˆφn ≡ ( ˆφn,1, . . . , ˆφn,K) given by Eq. (6) represents the approximate posterior of the
topic weights for each sentence-pair (fn, en). The topical information for updating ˆφn is collected
from three aspects: aligned word-pairs weighted by the corresponding topic-speciﬁc translation lex-
icon probabilities, topical distributions of monolingual English language model, and the smoothing
factors from the topic prior.

Equation (7) gives the approximate posterior probability for alignment between the j-th word in
fn and the i-th word in en, in the form of an exponential model. Intuitively, the ﬁrst two terms
represent the messages corresponding to the forward and the backward passes in HMM; The third
term represents the emission probabilities, and it can be viewed as a geometric interpolation of the
strengths of individual topic-speciﬁc lexicons; and the last term provides further smoothing from
monolingual topic-speciﬁc aspects.

Inference of optimum word-alignment One of the translation model’s goals is to infer the op-
timum word alignment: a∗ = arg maxa P (a|F, E). The variational inference scheme described
above leads to an approximate alignment posterior q(~a|~λ), which is in fact a reparameterized HMM.
Thus, extracting the optimum alignment amounts to applying an Viterbi algorithm on q(~a|~λ).

4

4.2 Variational EM for parameter estimation

To estimate the HM-BiTAM parameters, which include the Dirichlet hyperparameter α,
the
transition matrix T , the topic-speciﬁc monolingual English unigram {~βk}, and the topic-speciﬁc
translation lexicon {Bk}, we employ an variational EM algorithm which iterates between com-
puting variational distribution of the hidden variables (the E-step) as described in the previous
subsection, and optimizing the parameters with respect to the variational likelihood (the M-step).
Here are the update equations for the M-step:

ˆTi”,i

′ ∝

N

X

n=1

Jn

X

j=1

λn,j,i”λn,j−1,i

′ ,

Bf,e,k ∝

N

X

n=1

Jn

X

j=1

In

K

X

i=1

X

k=1

1(fjn , f )1(ein , e)λn,j,iφn,k,

βk,e ∝

N

In

X

n=1

X

i=1

Jn

X

j=1

1ei ,eλnjiφn,k.

(8)

(9)

(10)

For updating Dirichlet hyperparameter α, which is a corpora-level parameter, we resort to gradient
accent as in [7]. The overall computation complexity of the model is linear to the number of topics.

5 Experiments
In this section, we investigate three main aspects of the HM-BiTAM model, including word align-
ment, bilingual topic exploration, and machine translation.

Train

#Doc.

#Sent.

#Tokens

English

Chinese

TreeBank
Sinorama04
Sinorama02
Chnews.2005
FBIS.BEIJING
XinHua.NewsStory

316
6367
2373
1001
6111
17260

4172
282176
103252
10317
99396
98444

133,598
10,321,061
3,810,664
326,347
4,199,030
3,807,884

ALL

33,428

22,598,584
Table 1: Training data statistics.

597,757

105,331
10,027,095
3,146,014
270,274
3,527,786
3,915,267

20,991,767

The training data is a collection of parallel document-pairs, with document boundaries explicitly
given. As shown in Table 1, our training corpora are general newswire, covering topics mainly about
economics, politics, educations and sports. For word-alignment evaluation, our test set consists of
95 document-pairs, with 627 manually-aligned sentence-pairs and 14,769 alignment-links in total,
from TIDES’01 dryrun data. Word segmentations and tokenizations were ﬁxed manually for optimal
word-alignment decisions. This test set contains relatively long sentence-pairs, with an average
sentence length of 40.67 words. The long sentences introduce more ambiguities for alignment tasks.

For testing translation quality, TIDES’02 MT evaluation data is used as development data, and
ten documents from TIDES’04 MT-evaluation are used as the unseen test data. BLEU scores are
reported to evaluate translation quality with HM-BiTAM models.

5.1 Empirical Validation
Word Alignment Accuracy We trained HM-BiATMs with ten topics using parallel corpora of
sizes ranging from 6M to 22.6M words; we used the F-measure, the harmonic mean of precision
and recall, to evaluate word-alignment accuracy. Following the same logics for all BiTAMs in [10],
we choose HM-BiTAM in which topics are sampled at word-pair level over sentence-pair level. The
baseline IBM models were trained using a 18h543 scheme 2. Reﬁned alignments are obtained from
both directions of baseline models in the same way as described in [5].

Figure 2 shows the alignment accuracies of HM-BiTAM, in comparison with that of the baseline-
HMM, the baseline BiTAM, and the IBM Model-4. Overall, HM-BiTAM gives signiﬁcantly better
F-measures over HMM, with absolute margins of 7.56%, 5.72% and 6.91% on training sizes of

2Eight iterations for IBM Model-1, ﬁve iterations for HMM, and three iterations for IBM Model-4 (with

deﬁcient EM: normalization factor is computed using sampled alignment neighborhood in E-step)

5

HMM

BiTAM

IBM-4

HM-BiTAM

66

64

62

60

58

56

54

52

50

5000

4000

3000

2000

1000

5000

4000

3000

2000

1000

c
o
d
 
r
e
p
 
)
d
o
o
h

i
l

e
k

i
l
(
g
o
−

l

 
:

i

M
A
T
B
−
M
H

c
o
d
 
r
e
p
 
)
d
o
o
h

i
l

e
k

i
l
(
g
o
−

l

 
:

i

M
A
T
B
−
M
H

Negative log−likehood: HM−BiTAM (y−axis) vs IBM Model−4 (x−axis) & HMM (x−axis)

500

1000

1500

2000

2500

3000

3500

4000

4500

5000

IBM Model−4 (with deficient EM)

500

1000

1500

2000

2500

3000

3500

4000

4500

5000

HMM (with forward−backward EM)

6M

11M

22.6M

Figure 2: Alignment accuracy (F-measure) of differ-
ent models trained on corpora of different sizes.

Figure 3: Comparison of likelihoods of data under
different models. Top: HM-BiTAM v.s. IBM Model-
4; bottom: HM-BiTAM v.s. HMM.

6 M, 11 M and 22.6 M words, respectively. In HM-BiTAM, two factors contribute to narrowing
down the word-alignment decisions: the position and the lexical mapping. The position part is
the same as the baseline-HMM, implementing the “proximity-bias”. Whereas the emission lexical
probability is different, each state is a mixture of topic-speciﬁc translation lexicons, of which the
weights are inferred using document contexts. The topic-speciﬁc translation lexicons are sharper
and smaller than the global one used in HMM. Thus the improvements of HM-BiTAM over HMM
essentially resulted from the extended topic-admixture lexicons. Not surprisingly, HM-BiTAM also
outperforms the baseline-BiTAM signiﬁcantly, because BiTAM captures only the topical aspects
and ignores the proximity bias.

Notably, HM-BiTAM also outperforms IBM Model-4 by a margin of 3.43%, 3.64% and 2.73%,re-
spectively. Overall, with 22.6 M words, HM-BiTAM outperforms HMM, BiTAM, IBM-4 signiﬁ-
cantly, p=0.0031, 0.0079, 0.0121, respectively. IBM Model-4 already integrates the fertility and
distortion submodels on top of HMM, which further narrows the word-alignment choices. However,
IBM Model-4 does not have a scheme to adjust its lexicon probabilities speciﬁc to document topical-
context as in HM-BiTAM. In a way, HM-BiTAM wins over IBM-4 by leveraging topic models that
capture the document context.
Likelihood on Training and Unseen Documents Figure 3 shows comparisons of the likelihoods
of document-pairs in the training set under HM-BiTAM with those under IBM Model-4 or HMM.
Each point in the ﬁgure represents one document-pair; the y-coordinate corresponds to the negative
log-likelihood under HM-BiTAM, and the x-coordinate gives the counterparts under IBM Model-4
or HMM. Overall the likelihoods under HM-BiTAM are signiﬁcantly better than those under HMM
and IBM Model-4, revealing the better modeling power of HM-BiTAM.

We also applied HM-BiTAM to ten document-pairs selected from MT04, which were not included in
the training. These document-pairs contain long sentences and diverse topics. As shown in Table 2,
the likelihoods of HM-BiTAM on these unseen data dominates signiﬁcantly over that of HMM,
BiTAM, and IBM Models in every case, conﬁrming that HM-BiTAM indeed offers a better ﬁt and
generalizability for the bilingual document-pairs.

Publishers

Genre

IBM-1

HMM

IBM-4

BiTAM

HM-BiTAM

AgenceFrance(AFP)
AgenceFrance(AFP)
AgenceFrance(AFP)
ForeignMinistryPRC
HongKongNews
People’s Daily
United Nation
XinHua News
XinHua News
ZaoBao News

Avg. Perplexity

news
news
news
speech
speech
editorial
speech
news
news
editorial

-3752.94
-3341.69
-2527.32
-2313.28
-2198.13
-2485.08
-2134.34
-2425.09
-2684.85
-2376.12

-3388.72
-2899.93
-2124.75
-1913.29
-1822.25
-2094.90
-1755.11
-2030.57
-2326.39
-2047.55

-3448.28
-3005.80
-2161.31
-1963.24
-1890.81
-2184.23
-1821.29
-2114.39
-2352.62
-2116.42

123.83

60.54

68.41

-3602.28
-3139.95
-2323.11
-2144.12

-2035
-2377.1
-1949.39
-2192.9
-2527.78
-2235.79

107.57

-3188.90
-2595.72
-2063.69
-1669.22
-1423.84
-1867.13
-1431.16
-1991.31
-2317.47
-1943.25

43.71

Table 2: Likelihoods of unseen documents under HM-BiTAMs, in comparison with competing models.

5.2 Application 1: Bilingual Topic Extraction
Monolingual topics: HM-BiTAM facilitates inference of the latent LDA-style representations of
topics [1] in both English and the foreign language (i.e., Chinese) from a given bilingual corpora.
The English topics (represented by the topic-speciﬁc word frequencies) can be directly read-off
from HM-BiTAM parameters β. As discussed in § 3.2, even though the topic-speciﬁc distributions

6

of words in the Chinese corpora are not directly encoded in HM-BiTAM, one can marginalize over
alignments of the parallel data to synthesize them based on the monolingual English topics and the
topic-speciﬁc lexical mapping from English to Chinese.

Figure 4 shows ﬁve topics, in both English and Chinese, learned via HM-BiTAM. The top-ranked
frequent words in each topic exhibit coherent semantic meanings; and there are also consistencies
between the word semantics under the same topic indexes across languages. Under HM-BiTAM,
the two respective monolingual word-distributions for the same topic are statistically coupled due
to sharing of the same topic for each sentence-pair in the two languages. Whereas if one merely
apply LDA to the corpora in each language separately, such coupling can not be exploited. This
coupling enforces consistency between the topics across languages. However, like general clustering
algorithms, topics in HM-BiTAM, are not necessarily to present obvious semantic labels.

“sports” 

(cid:1166)(people)

(cid:8543)(cid:11154)(handicapped)

(cid:1319)(cid:13958)(sports)
(cid:1119)(cid:1006)(career)

(cid:8712)(water)

(cid:1002)(cid:11040)(world)
(cid:2318)(region) 

(cid:7044)(cid:2338)(cid:12050)(Xinhua)

(cid:19443)(cid:2604)(team member) 

(cid:16772)(cid:13785)(reporter)

“housing”

(cid:1315)(cid:6163)(house)
(cid:6163)(house)

(cid:1073)(cid:8755) (JiuJiang) 
(cid:5326)(cid:16786)(construction)

(cid:9607)(cid:19388)(macao) 

(cid:1815)(Yuan)

(cid:13856)(cid:5049)(workers) 
(cid:11458)(cid:2081)(current) 
(cid:3281)(cid:4490)(national) 
(cid:11477)(province)

“stocks”

(cid:9157)(cid:3335)(shenzhen)
(cid:9157)(shen zhen) 
(cid:7044)(Singarpore) 

(cid:1815)(Yuan)
(cid:13941)(stock) 

(cid:20333)(cid:9219)(Hongkong)
(cid:3281)(cid:7389)(state-owned) 

(cid:3818)(cid:17176)(foreign
investiment) 

(cid:7044)(cid:2338)(cid:12050)(Xinhua)
(cid:15713)(cid:17176)(refinancing)

“energy”

(cid:1856)(cid:2508)(company) 

(cid:3837)(cid:9994)(cid:8680)(gas)

(cid:1016)(two)

(cid:3281)(countries)
(cid:13666)(cid:3281)(U.S.)

(cid:16772)(cid:13785)(reporters)
(cid:1863)(cid:13007)(relations)

(cid:1432)(Russian)
(cid:8873)(France) 

(cid:18337)(cid:5210)(ChongQing)

“takeover”

(cid:3281)(cid:4490)(countries)
(cid:18337)(cid:5210)(ChongQing)

(cid:2390)(Factory) 
(cid:3837)(cid:8953)(TianJin) 

(cid:6931)(cid:5232)(Government)

(cid:20045)(cid:11458)(project) 
(cid:3281)(cid:7389)(national) 
(cid:9157)(cid:3335)(Shenzhen)
(cid:1872)(cid:5194)(take over) 

(cid:6922)(cid:17153)(buy)

Figure 4: Monolingual topics of both languages learned from parallel data. It appears that the English topics
(on the left panel) are highly parallel to the Chinese ones (annotated with English gloss, on the right panel).

Topic-Speciﬁc Lexicon Mapping: Table 3 shows two examples of topic-speciﬁc lexicon mapping
learned by HM-BiTAM. Given a topic assignment, a word usually has much less translation candi-
dates, and the topic-speciﬁc translation lexicons are generally much smaller and sharper. Different
topic-speciﬁc lexicons emphasize different aspects of translating the same source words, which can
not be captured by the IBM models or HMM. This effect can be observed from Table 3.

TopCand

TopCand

Topics

Topic-1
Topic-2
Topic-3
Topic-4
Topic-5
Topic-6
Topic-7
Topic-8
Topic-9
Topic-10
IBM Model-1
HMM
IBM Model-4

“meet”
Meaning

sports meeting

to satisfy
to adapt
to adjust

to see someone

-

to satisfy

sports meeting

-

to see someone
sports meeting
sports meeting
sports meeting

Probability
0.508544
0.160218
0.921168
0.996929
0.693673

-

0.467555
0.487728

-

0.551466
0.590271
0.72204
0.608391

“power”
Meaning

electric power

electricity factory

to be relevant

strength
strength

-

Electric watt

power

to generate

strength

power plant

strength
strength

Probability
0.565666

0.656

0.985341
0.410503
0.997586

-

0.613711

1.0

0.50457

1.0

0.314349
0.51491
0.506258

Table 3: Topic-speciﬁc translation lexicons learned by HM-BiTAM. We show the top candidate (TopCand)
lexicon mappings of “meet” and “power” under ten topics. (The symbol “-” means inexistence of signiﬁcant
lexicon mapping under that topic.) Also shown are the semantic meanings of the mapped Chinese words, and
the mapping probability p(f |e, k).

5.3 Application 2: Machine Translation
The parallelism of topic-assignment between languages modeled by HM-BiTAM, as shown in § 3.2
and exempliﬁed in Fig. 4, enables a natural way of improving translation by exploiting semantic
consistency and contextual coherency more explicitly and aggressively. Under HM-BiTAM, given
a source document DF , the predictive probability distribution of candidate translations of every
source word, P (e|f, DF ), must be computed by mixing multiple topic-speciﬁc translation lexicons
according to the topic weights p(z|DF ) determined from monolingual context in DF . That is:

P (e|f, DF ) ∝ P (f |e, DF )P (e|DF )=

K

X

k=1

P (f |e, z = k)P (e|z = k)P (z = k|DF ).

(11)

We used p(e|f, DF ) to score the bilingual phrase-pairs in a state-of-the-art GALE translation system
trained with 250 M words. We kept all other parameters the same as those used in the baseline. Then
decoding of the unseen ten MT04 documents in Table 2 was carried out.

7

Ä¬
÷v
A
(cid:18)
¬„
÷v
Ä¬
¬„
Ä¬
Ä¬
Ä¬

-

-

>å
>‚
(cid:21)9
å
å

¢å
Ñ
å
>‚
å
å

-

Systems
Hiero Sys.
Gale Sys.
HM-BiTAM
Ground Truth

1-gram 2-gram 3-gram 4-gram
13.84
73.92
14.30
75.63
14.56
76.77
76.10
15.73

40.57
42.71
42.99
43.85

23.21
25.00
25.42
26.70

BLEUr4n4

30.70
32.78
33.19
34.17

Table 4: Decoding MT04 10-documents. Experiments using the topic assignments inferred from ground truth
and the ones inferred via HM-BITAM; ngram precisions together with ﬁnal BLEUr4n4 scores are evaluated.

Table 4 shows the performance of our in-house Hiero system (following [3]), the state-of-the-art
Gale-baseline (with a better BLEU score), and our HM-BiTAM model, on the NIST MT04 test
set. If we know the ground truth of translation to infer the topic-weights, improvement is from
32.78 to 34.17 BLEU points. With topical inference from HM-BiTAM using monolingual source
document, improved N-gram precisions in the translation were observed from 1-gram to 4-gram.
The largest improved precision is for unigram: from 75.63% to 76.77%. Intuitively, unigrams have
potentially more ambiguities for translations than the higher order ngrams, because the later ones
encode already contextual information. The overall BLEU score improvement of HM-BiTAM over
other systems, including the state-of-the-art, is from 32.78 to 33.19, an slight improvement with
p = 0.043.
6 Discussion and Conclusion
We presented a novel framework, HM-BiTAM, for exploring bilingual topics, and generalizing over
traditional HMM for improved word-alignment accuracies and translation quality. A variational in-
ference and learning procedure was developed for efﬁcient training and application in translation.
We demonstrated signiﬁcant improvement of word-alignment accuracy over a number of existing
systems, and the interesting capability of HM-BiTAM to simultaneously extract coherent monolin-
gual topics from both languages. We also report encouraging improvement of translation quality
over current benchmarks; although the margin is modest, it is noteworthy that the current version of
HM-BiTAM remains a purely autonomously trained system. Future work also includes extensions
with more structures for word-alignment such as noun phrase chunking.

References

[1] David Blei, Andrew NG, and Michael I. Jordon. Latent dirichlet allocation.

Learning Research, volume 3, pages 1107–1135, 2003.

In Journal of Machine

[2] Peter F. Brown, Stephen A. Della Pietra, Vincent. J. Della Pietra, and Robert L. Mercer. The mathematics
In Computational Linguistics, volume 19(2),

of statistical machine translation: Parameter estimation.
pages 263–331, 1993.

[3] David Chiang. A hierarchical phrase-based model for statistical machine translation. In Proceedings of
the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 263–270, Ann
Arbor, Michigan, June 2005. Association for Computational Linguistics.

[4] Elena Erosheva, Steve Fienberg, and John Lafferty. Mixed membership models of scientiﬁc publications.

In Proceedings of the National Academy of Sciences, volume 101 of Suppl. 1, April 6 2004.

[5] Franz J. Och and Hermann Ney. The alignment template approach to statistical machine translation. In

Computational Linguistics, volume 30, pages 417–449, 2004.

[6] J. Pritchard, M. Stephens, and P. Donnell. Inference of population structure using multilocus genotype

data. In Genetics, volume 155, pages 945–959, 2000.

[7] K. Sj¨olander, K. Karplus, M. Brown, R. Hughey, A. Krogh, I.S. Mian, and D. Haussler. Dirichlet mix-
tures: A method for improving detection of weak but signiﬁcant protein sequence homology. Computer
Applications in the Biosciences, 12, 1996.

[8] Stephan. Vogel, Hermann Ney, and Christoph Tillmann. HMM based word alignment in statistical ma-
chine translation. In Proc. The 16th Int. Conf. on Computational Lingustics, (Coling’96), pages 836–841,
Copenhagen, Denmark, 1996.

[9] Eric P. Xing, M.I. Jordan, and S. Russell. A generalized mean ﬁeld algorithm for variational inference
in exponential families. In Meek and Kjaelff, editors, Uncertainty in Artiﬁcial Intelligence (UAI2003),
pages 583–591. Morgan Kaufmann Publishers, 2003.

[10] Bing Zhao and Eric P. Xing. Bitam: Bilingual topic admixture models for word alignment. In Proceedings

of the 44th Annual Meeting of the Association for Computational Linguistics (ACL’06), 2006.

8

"
673,2007,Optimistic Linear Programming gives Logarithmic Regret for Irreducible MDPs,"We present an algorithm called Optimistic Linear Programming (OLP) for learning to optimize average reward in an irreducible but otherwise unknown Markov decision process (MDP). OLP uses its experience so far to estimate the MDP. It chooses actions by optimistically maximizing estimated future rewards over a set of next-state transition probabilities that are close to the estimates: a computation that corresponds to solving linear programs. We show that the total expected reward obtained by OLP up to time $T$ is within $C(P)\log T$ of the reward obtained by the optimal policy, where $C(P)$ is an explicit, MDP-dependent constant. OLP is closely related to an algorithm proposed by Burnetas and Katehakis with four key differences: OLP is simpler, it does not require knowledge of the supports of transition probabilities and the proof of the regret bound is simpler, but our regret bound is a constant factor larger than the regret of their algorithm. OLP is also similar in flavor to an algorithm recently proposed by Auer and Ortner. But OLP is simpler and its regret bound has a better dependence on the size of the MDP.","Optimistic Linear Programming gives Logarithmic

Regret for Irreducible MDPs

Ambuj Tewari

Computer Science Division

Univeristy of California, Berkeley

Berkeley, CA 94720, USA

ambuj@cs.berkeley.edu

Computer Science Division and Department of Statistics

Peter L. Bartlett

University of California, Berkeley

Berkeley, CA 94720, USA

bartlett@cs.berkeley.edu

Abstract

We present an algorithm called Optimistic Linear Programming (OLP) for learn-
ing to optimize average reward in an irreducible but otherwise unknown Markov
decision process (MDP). OLP uses its experience so far to estimate the MDP. It
chooses actions by optimistically maximizing estimated future rewards over a set
of next-state transition probabilities that are close to the estimates, a computation
that corresponds to solving linear programs. We show that the total expected re-
ward obtained by OLP up to time T is within C(P ) log T of the reward obtained
by the optimal policy, where C(P ) is an explicit, MDP-dependent constant. OLP
is closely related to an algorithm proposed by Burnetas and Katehakis with four
key differences: OLP is simpler, it does not require knowledge of the supports
of transition probabilities, the proof of the regret bound is simpler, but our regret
bound is a constant factor larger than the regret of their algorithm. OLP is also
similar in ﬂavor to an algorithm recently proposed by Auer and Ortner. But OLP
is simpler and its regret bound has a better dependence on the size of the MDP.

1 Introduction

Decision making under uncertainty is one of the principal concerns of Artiﬁcial Intelligence and
Machine Learning. Assuming that the decision maker or agent is able to perfectly observe its own
state, uncertain systems are often modeled as Markov decision processes (MDPs). Given complete
knowledge of the parameters of an MDP, there are standard algorithms to compute optimal policies,
i.e., rules of behavior such that some performance criterion is maximized. A frequent criticism of
these algorithms is that they assume an explicit description of the MDP which is seldom available.
The parameters constituting the description are themselves estimated by simulation or experiment
and are thus not known with complete reliability. Taking this into account brings us to the well
known exploration vs. exploitation trade-off. On one hand, we would like to explore the system as
well as we can to obtain reliable knowledge about the system parameters. On the other hand, if we
keep exploring and never exploit the knowledge accumulated, we will not behave optimally.
Given a policy π, how do we measure its ability to handle this trade-off? Suppose the agent gets a
numerical reward at each time step and we measure performance by the accumulated reward over
time. Then, a meaningful quantity to evaluate the policy π is its regret over time. To understand
what regret means, consider an omniscient agent who knows all parameters of the MDP accurately
and behaves optimally. Let VT be the expected reward obtained by this agent up to time T . Let V π
T
denote the corresponding quantity for π. Then the regret Rπ
T measures how much π is
hurt due to its incomplete knowledge of the MDP up to time T . If we can show that the regret Rπ
T
grows slowly with time T , for all MDPs in a sufﬁciently big class, then we can safely conclude that
π is making a judicious trade-off between exploration and exploitation. It is rather remarkable that

T = VT − V π

1

T = O(log T ). Thus the per-step regret Rπ

for this notion of regret, logarithmic bounds have been proved in the literature [1,2]. This means that
there are policies π with Rπ
T /T goes to zero very quickly.
Burnetas and Katehakis [1] proved that for any policy π (satisfying certain reasonable assumptions)
T ≥ CB(P ) log T where they identiﬁed the constant CB(P ). This constant depends on the tran-
Rπ
sition function P of the MDP1. They also gave an algorithm (we call it BKA) that achieves this rate
and is therefore optimal in a very strong sense. However, besides assuming that the MDP is irre-
ducible (see Assumption 1 below) they assumed that the support sets of the transition distributions
pi(a) are known for all state-action pairs. In this paper, we not only get rid of this assumption but
our optimistic linear programming (OLP) algorithm is also computationally simpler. At each step,
OLP considers certain parameters in the vicinity of the estimates. Like BKA, OLP makes optimistic
choices among these. But now, making these choices only involves solving linear programs (LPs)
to maximize linear functions over L1 balls. BKA instead required solving non-linear (though con-
vex) programs due to the use of KL-divergence. Another beneﬁt of using the L1 distance is that
it greatly simpliﬁes a signiﬁcant part of the proof. The price we pay for these advantages is that
the regret of OLP is C(P ) log T asymptotically, for a constant C(P ) ≥ CB(P ). We should note
here that a number of algorithms in the literature have been inspired by the “optimism in the face of
uncertainty” principle [3]–[7].
The algorithm of Auer and Ortner (we refer to it as AOA) is another logarithmic regret algorithm for
irreducible2 MDPs. AOA does not solve an optimization problem at every time step but only when
a conﬁdence interval is halved. But then the optimization problem they solve is more complicated
because they ﬁnd a policy to use in the next few time steps by optimizing over a set of MDPs. The
regret of AOA is CA(P ) log T where

|S|5|A|Tw(P )κ(P )2

,

CA(P ) = c

(1)
for some universal constant c. Here |S|,|A| denote the state and action space size, Tw(P ) is the worst
case hitting time over deterministic policies (see Eqn. (12)) and ∆∗(P ) is the difference between
the long term average return of the best policy and that of the next best policy. The constant κ(P ) is
also deﬁned in terms of hitting times. Under Auer and Ortner’s assumption of bounded rewards, we
can show that the constant for OLP satisﬁes

∆∗(P )2

C(P ) ≤ 2|S||A|T (P )2

Φ∗(P )

.

(2)

Here T (P ) is the hitting time of an optimal policy is therefore necessarily smaller than Tw(P ). We
get rid of the dependence on κ(P ) while replacing Tw(P ) with T (P )2. Most importantly, we signif-
icantly improve the dependence on the state space size. The constant Φ∗(P ) can roughly be thought
of as the minimum (over states) difference between the quality of the best and the second best ac-
tion (see Eqn. (9)). The constants ∆∗(P ) and Φ∗(P ) are similar though not directly comparable.
Nevertheless, note that C(P ) depends inversely on Φ∗(P ) not Φ∗(P )2.

2 Preliminaries
Consider an MDP (S,A, R, P ) where S is the set of states, A = ∪i∈SA(i) is the set of actions
(A(i) being the actions available in state i), R = {r(i, a)}i∈S,a∈A(i) are the rewards and P =
{pi,j(a)}i,j∈S,a∈A(i) are the transition probabilities. For simplicity of analysis, we assume that
the rewards are known to us beforehand. We do not assume that we know the support sets of the
distributions pi(a).
The history σt up to time t is a sequence i0, k0, . . . , it−1, kt−1, it such that ks ∈ A(is) for all s < t.
A policy π is a sequence {πt} of probability distributions on A given σt such that πt(A(st)|σt) = 1
where st denotes the random variable representing the state at time t. The set of all policies is
denoted by Π. A deterministic policy is simply a function µ : S → A such that µ(i) ∈ A(i).
Denote the set of deterministic policies by ΠD. If D is a subset of A, let Π(D) denote the set of

1Notation for MDP parameters is deﬁned in Section 2 below.
2Auer & Ortner prove claims for unichain MDPs but their usage seems non-standard. The MDPs they call

unichain are called irreducible in standard textbooks (for example, see [9, p. 348])

2

policies that take actions in D. Probability and expectation under a policy π, transition function P
and starting state i0 will be denoted by Pπ,P
respectively. Given history σt, let Nt(i),
Nt(i, a) and Nt(i, a, j) denote the number of occurrences of the state i, the pair (i, a) and the triplet
(i, a, j) respectively in σt.
We make the following irreducibility assumption regarding the MDP.
Assumption 1. For all µ ∈ ΠD, the transition matrix P µ = (pi,j(µ(i)))i,j∈S is irreducible (i.e. it
is possible to reach any state from any other state).

and Eπ,P

i0

i0

Consider the rewards accumulated by the policy π before time T ,

T−1X

T (i0, P ) := Eπ,P
V π

i0

[

r(st, at)] ,

where at is the random variable representing the action taken by π at time t. Let VT (i0, P ) be the
maximum possible sum of expected rewards before time T ,

t=0

VT (i0, P ) := sup
π∈Π

T (i0, P ) .
V π

The regret of a policy π at time T is a measure of how well the expected rewards of π compare with
the above quantity,

T (i0, P ) := VT (i0, P ) − V π
Rπ

T (i0, P ) .

Deﬁne the long term average reward of a policy π as

Under assumption 1, the above limit exists and is independent of the starting state i0. Given a
restricted set D ⊆ A of actions, the gain or the best long term average performance is

λπ(i0, P ) := lim inf
T→∞

T (i0, P )
V π

.

T

λ(P,D) := sup
π∈Π(D)

λπ(i0, P ) .

As a shorthand, deﬁne λ∗(P ) := λ(P,A).

2.1 Optimality Equations
A restricted problem (P,D) is obtained from the original MDP by choosing subsets D(i) ⊆ A(i)
and setting D = ∪i∈SD(i). The transition and reward functions of the restricted problems are
simply the restrictions of P and r to D. Assumption 1 implies that there is a bias vector h(P,D) =
{h(i; P,D)}i∈S such that the gain λ(P,D) and bias h(P,D) are the unique solutions to the average
reward optimality equations:

[r(i, a) + hpi(a), h(P,D)i] .

∀i ∈ S, λ(P,D) + h(i; P, D) = max
a∈D(i)

(3)
We will use h∗(P ) to denote h(P,A). Also, denote the inﬁnity norm kh∗(P )k∞ by H∗(P ). Note
that if h∗(P ) is a solution to the optimality equations and e is the vector of ones, then h∗(P ) + ce
is also a solution for any scalar c. We can therefore assume ∃i∗ ∈ S, h∗(i∗; P ) = 0 without any loss
of generality.
It will be convenient to have a way to denote the quantity inside the ‘max’ that appears in the
optimality equations. Accordingly, deﬁne

L(i, a, p, h) := r(i, a) + hp, hi ,

L∗(i; P,D) := max
a∈D(i)

L(i, a, pi(a), h(P, D)) .

To measure the degree of suboptimality of actions available at a state, deﬁne
φ∗(i, a; P ) = L∗(i; P,A) − L(i, a, pi(a), h∗(P )) .

Note that the optimal actions are precisely those for which the above quantity is zero.

Any policy in O(P,D) is an optimal policy, i.e.,

O(i; P,D) := {a ∈ D(i) : φ∗(i, a; P ) = 0} ,

O(P,D) := Πi∈SO(i; P,D) .

∀µ ∈ O(P,D), λµ(P ) = λ(P,D) .

3

2.2 Critical pairs

From now on, ∆+ will denote the probability simplex of dimension determined by context. For a
suboptimal action a /∈ O(i; P,A), the following set contains probability distributions q such that if
pi(a) is changed to q, the quality of action a comes within  of an optimal action. Thus, q makes a
look almost optimal:

MakeOpt(i, a; P, ) := {q ∈ ∆+ : L(i, a, q, h∗(P )) ≥ L∗(i; P,A) − } .

(4)
Those suboptimal state-action pairs for which MakeOpt is never empty, no matter how small  is,
play a crucial role in determining the regret. We call these critical state-action pairs,

Crit(P ) := {(i, a) : a /∈ O(i; P,A) ∧ (∀ > 0, MakeOpt(i, a; P, ) 6= ∅)} .

(5)

Deﬁne the function,

Ji,a(p; P, ) := inf{kp − qk2

1 : q ∈ MakeOpt(i, a; P, )} .

(6)
To make sense of this deﬁnition, consider p = pi(a). The above inﬁmum is then the least distance
(in the L1 sense) one has to move away from pi(a) to make the suboptimal action a look -optimal.
Taking the limit of this as  decreases gives us a quantity that also plays a crucial role in determining
the regret,

(7)
Intuitively, if K(i, a; P ) is small, it is easy to confuse a suboptimal action with an optimal one and
so it should be difﬁcult to achieve small regret. The constant that multiplies log T in the regret bound
of our algorithm OLP (see Algorithm 1 and Theorem 4 below) is the following:

K(i, a; P ) := lim
→0

Ji,a(pi(a); P, ) .

C(P ) := X

(i,a)∈Crit(P )

2φ∗(i, a; P )
K(i, a; P ) .

(8)

This deﬁnition might look a bit hard to interpret, so we give an upper bound on C(P ) just in terms
of the inﬁnity norm H∗(P ) of the bias and Φ∗(P ). This latter quantity is deﬁned below to be the
minimum degree of suboptimality of a critical action.
Proposition 2. Suppose A(i) = A for all i ∈ S. Deﬁne
(i,a)∈Crit(P )

φ∗(i, a; P ) .

Φ∗(P ) :=

min

(9)

Then, for any P ,

See the appendix for a proof.

2.3 Hitting times

C(P ) ≤ 2|S||A|H∗(P )2

Φ∗(P )

.

It turns out that we can bound the inﬁnity norm of the bias in terms of the hitting time of an optimal
policy. For any policy µ deﬁne its hitting time to be the worst case expected time to reach one state
from another:

Tµ(P ) := max
i6=j

Eµ,P
j

[min{t > 0 : st = i}] .

The following constant is the minimum hitting time among optimal policies:

T (P ) := min

µ∈O(P,D)

Tµ(P ) .

(11)

The following constant is deﬁned just for comparison with results in [2]. It is the worst case hitting
time over all policies:

(10)

(12)

We can now bound C(P ) just in terms of the hitting time T (P ) and φ∗(P ).
Proposition 3. Suppose A(i) = A for all i ∈ S and that r(i, a) ∈ [0, 1] for all i ∈ S, a ∈ A. Then
for any P ,

Tw(P ) := max
µ∈ΠD

Tµ(P ) .

C(P ) ≤ 2|S||A|T (P )2

.

Φ∗(P )

See the appendix for a proof.

4

3 The optimistic LP algorithm and its regret bound

Algorithm 1 Optimistic Linear Programming
1: for t = 0, 1, 2, . . . do
st ← current state
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23: end for

end if

else

t

t

i,j(a) ← 1+Nt(i,a,j)
|A(i)|+Nt(i,a)

. Compute solution for “empirical MDP” excluding “undersampled” actions
∀i, j ∈ S, a ∈ A(i), ˆpt
∀i ∈ S, Dt(i) ← {a ∈ A(i) : Nt(i, a) ≥ log2 Nt(i)}
ˆht, ˆλt ← solution of the optimality equations (3) with P = ˆP t,D = Dt
. Compute indices of all actions for the current state
∀a ∈ A(st), Ut(st, a) ← supq∈∆+{r(st, a) + hq, ˆhti : kˆpt

Nt(st,a)}
. Optimal actions (for the current problem) that are about to become “undersampled”
t ← {a ∈ O(st; ˆP t,Dt) : Nt(st, a) < log2(Nt(st) + 1)}
Γ1
. The index maximizing actions
t ← arg maxa∈A(st) Ut(st, a)
Γ2
t = O(st; ˆP t,Dt) then
if Γ1
at ← any action in Γ1
at ← any action in Γ2

(a) − qk1 ≤q 2 log t

st

Algorithm 1 is the Optimistic Linear Programming algorithm. It is inspired by the algorithm of
Burnetas and Katehakis [1] but uses L1 distance instead of KL-divergence. At each time step t,
the algorithm computes the empirical estimates for transition probabilities. It then forms a restricted
problem ignoring relatively undersampled actions. An action a ∈ A(i) is considered “undersam-
pled” if Nt(i, a) < log2 Nt(i). The solutions ˆht, ˆλt might be misleading due to estimation errors.
To avoid being misled by empirical samples we compute optimistic “indices” Ut(st, a) for all legal
actions a ∈ A(st) where st is the current state. The index for action a is computed by looking at
(a) and choosing a probability distribution q that max-
an L1-ball around the empirical estimate ˆpt
st
imizes L(i, a, q, ˆht). Note that if the estimates were perfect, we would take an action maximizing
L(i, a, ˆpt
(a), ˆht). Instead, we take an action that maximizes the index. There is one case where we
are forced not to take an index-maximizing action. It is when all the optimal actions of the current
problem are about to become undersampled at the next time step. In that case, we take one of these
actions (steps 18–22). Note that both steps 7 and 10 can be done by solving LPs. The LP for solving
optimality equations can be found in several textbooks (see, for example, [9, p. 391]). The LP in step
10 is even simpler: the L1 ball has only 2|S| vertices and so we can maximize over them efﬁciently.
Like the original Burnetas-Katehakis algorithm, the modiﬁed one also satisﬁes a logarithmic regret
bound as stated in the following theorem. Unlike the original algorithm, OLP does not need to know
the support sets of the transition distributions.
Theorem 4. Let β denote the policy implemented by Algorithm 1. Then we have, for all i0 ∈ S and
for all P satisfying Assumption 1,

st

lim sup
T→∞

Rβ

T (i0, P )
log T

≤ C(P ) ,

where C(P ) is the MDP-dependent constant deﬁned in (8).

Proof. From Proposition 1 in [1], it follows that

[NT (i, a)]φ∗(i, a; P ) + O(1) .

(13)

T (i0, P ) =X

Rβ

X

i∈S

a /∈O(i;P,A)

Eβ,P
i0

5

Deﬁne the event

Deﬁne,

At := {kˆht − h∗(P )k∞ ≤  ∧ O( ˆP t,Dt) ⊆ O(P )} .

(14)

N 1

T (i, a; ) :=

N 2

T (i, a; ) :=

N 3

T () :=

1 [(st, at) = (i, a) ∧ At ∧ Ut(i, a) ≥ L∗(i; P,A) − 2] ,

1 [(st, at) = (i, a) ∧ At ∧ Ut(i, a) < L∗(i; P,A) − 2] ,

1(cid:2) ¯At

(cid:3) ,

t=0

T−1X
T−1X
T−1X

t=0

t=0

where ¯At denotes the complement of At. For all  > 0,
T (i, a; ) + N 2

NT (i, a) ≤ N 1

(15)
The result then follows by combining (13) and (15) with the following three propositions and then
letting  → 0 sufﬁciently slowly.
Proposition 5. For all P and i0 ∈ S, we have
Eβ,P
i0

T (i, a; ) + N 3

T (i, a; )]

X

X

T () .

φ∗(i, a; P ) ≤ C(P ) .

lim
→0

lim sup
T→∞

i∈S

a /∈O(i;P,A)

[N 1
log T

Proposition 6. For all P , i0, i ∈ S, a /∈ O(i; P,A) and  sufﬁciently small, we have

Proposition 7. For all P satisfying Assumption 1, i0 ∈ S and  > 0, we have

Eβ,P
i0

[N 2

T (i, a; )] = o(log T ) .

Eβ,P
i0

[N 3

T ()] = o(log T ) .

4 Proofs of auxiliary propositions

We prove Propositions 5 and 6. The proof of Proposition 7 is almost the same as that of Proposition 5
in [1] and therefore omitted (for details, see Chapter 6 in the ﬁrst author’s thesis [8]). The proof of
Proposition 6 is considerably simpler (because of the use of L1 distance rather than KL-divergence)
than the analogous Proposition 4 in [1].
Proof of Proposition 5. There are two cases depending on whether (i, a) ∈ Crit(P ) or not.
If
(i, a) /∈ Crit(P ), there is an 0 > 0 such that MakeOpt(i, a; P, 0) = ∅. On the event At (recall the
deﬁnition given in (14)), we have |hq, ˆhti − hq, h∗(P )i| ≤  for any q ∈ ∆+. Therefore,

{r(i, a) + hq, ˆhti}
{r(i, a) + hq, h∗(P )i} + 

Ut(i, a) ≤ sup
q∈∆+
≤ sup
q∈∆+
< L∗(i; P,A) − 0 + 
< L∗(i; P,A) − 2 provided that 3 < 0

[∵ MakeOpt(i, a; P, 0) = ∅]

Therefore for  < 0/3, N 1
Now suppose (i, a) ∈ Crit(P ). The event Ut(i, a) ≥ L∗(i; P,A) − 2 is equivalent to

T (i, a; ) = 0.

∃q ∈ ∆+ s.t.

kˆpt

i(a) − qk2

r(i, a) + hq, ˆhti ≥ L∗(i; P,A) − 2

.

(cid:17)

On the event At, we have |hq, ˆhti − hq, h∗(P )i| ≤  and thus the above implies

∃q ∈ ∆+ s.t.

kˆpt

i(a) − qk2

1 ≤ 2 log t
Nt(i, a)

∧ (r(i, a) + hq, h∗(P )i ≥ L∗(i; P,A) − 3) .

(cid:18)
(cid:18)

∧(cid:16)

(cid:19)
1 ≤ 2 log t
Nt(i, a)
(cid:19)

6

Recalling the deﬁnition (6) of Ji,a(p; P, ), we see that this implies

Ji,a(ˆpt

i(a); P, 3) ≤ 2 log t
Nt(i, a) .

We therefore have,

N 1

1

T (i, a; ) ≤ T−1X
≤ T−1X
T−1X

t=0

t=0

1

+

(cid:21)

(st, at) = (i, a) ∧ Ji,a(ˆpt

(cid:20)
i(a); P, 3) ≤ 2 log t
(cid:20)
Nt(i, a)
(st, at) = (i, a) ∧ Ji,a(pi(a); P, 3) ≤ 2 log t
1(cid:2)(st, at) = (i, a) ∧ Ji,a(pi(a); P, 3) > Ji,a(ˆpt
Nt(i, a)

+ δ

(cid:21)
i(a); P, 3) + δ(cid:3)

t=0

(16)

where δ > 0 is arbitrary. Each time the pair (i, a) occurs Nt(i, a) increases by 1, so the ﬁrst count
is no more than

2 log T

Ji,a(pi(a); P, 3) − δ

.

(17)

To control the expectation of the second sum, note that continuity of Ji,a in its ﬁrst argument implies
that there is a function f such that f(δ) > 0 for δ > 0, f(δ) → 0 as δ → 0 and Ji,a(pi(a); P, 3) >
i(a)k1 > f(δ). By a Chernoff-type bound, we have,
Ji,a(ˆpt
for some constant C1,

i(a); P, 3) + δ implies that kpi(a) − ˆpt

Pβ,P
i0

[kpi(a) − ˆpt

i(a)k1 > f(δ)| Nt(i, a) = m] ≤ C1 exp(−mf(δ)2) .

and so the expectation of the second sum is no more than

Eβ,P
i0

[

C1 exp(−Nt(i, a)f(δ)2)] ≤

t=0

m=1

C1 exp(−mf(δ)2) =

C1

1 − exp(−f(δ)2) .

(18)

Combining the bounds (17) and (18) and plugging them into (16), we get

Eβ,P
i0

[N 1

T (i, a; )] ≤

Ji,a(pi(a); P, 3) − δ
Letting δ → 0 sufﬁciently slowly, we get that for all  > 0,
2 log T

2 log T

Eβ,P
i0

[N 1

T (i, a; )] ≤

Ji,a(pi(a); P, 3)

+ o(log T ) .

+

C1

1 − exp(−f(δ)2) .

Therefore,

lim
→0

lim sup
T→∞

Eβ,P
i0

T (i, a; )]

[N 1
log T

≤ lim
→0

2

Ji,a(pi(a); P, 3)

=

2

K(i, a; P ) ,

where the last equality follows from the deﬁnition (7) of K(i, a; P ). The result now follows by
summing over (i, a) pairs in Crit(P ).

T−1X

∞X

Proof of Proposition 6. Deﬁne the event

t(i, a; ) := {(st, at) = (i, a) ∧ At ∧ Ut(i, a) < L∗(i; P,A) − 2} ,
A0

so that we can write

Note that on A0
taken at time t, so it must have been in Γ2
optimal actions a∗ ∈ O(i; P,A), we have, on the event A0

t(i, a; ), we have Γ1

t(i, a; ),

t(i, a; )] .

T (i, a; ) =
(19)
N 2
t ⊆ O(i; ˆP t,Dt) ⊆ O(i; P,A). So, a /∈ O(i; P,A). But a was
t which means it maximized the index. Therefore, for all

t=0

Ut(i, a∗) ≤ Ut(i, a) < L∗(i; P,A) − 2 .

T−1X

1 [A0

7

Since L∗(i; P,A) = r(i, a∗) + hpi(a∗), h∗(P )i, this implies

s

∀q ∈ ∆+, kq − ˆpt

i(a∗)k1 ≤

⇒ hq, ˆhti < hpi(a∗), h∗(P )i − 2 .

2 log t
Nt(i, a∗)

Moreover, on the event At, |hq, ˆhti − hq, h∗(P )i| ≤ . We therefore have, for any a∗ ∈ O(i; P,A),

t(i, a; ) ⊆
A0

∀q ∈ ∆+, kq − ˆpt

i(a)k1 ≤

⇒ hq, h∗(P )i < hpi(a), h∗(P )i − 

)

)

⊆

(
(
(
⊆ t[

⊆

m=1

s
s

2 log t
Nt(i, a)

2 log t
Nt(i, a)

s

)

∀q ∈ ∆+, kq − ˆpt

i(a)k1 ≤

⇒ kq − pi(a)k1 >



kh∗(P )k∞

kˆpt

i(a) − pi(a)k1 >
(



h∗(P )

+

2 log t
Nt(i, a)

Nt(i, a) = m ∧ kˆpt

i(a) − pi(a)k1 >



kh∗(P )k∞

+

s

)

2 log t
Nt(i, a)

Using a Chernoff-type bound, we have, for some constant C1,

Pβ,P
i0

[kˆpt

i(a) − pi(a)k1 > δ | Nt(i, a) = m] ≤ C1 exp(−mδ2/2) .

Using a union bound, we therefore have,

t(i, a; )] ≤ tX

Pβ,P
i0

[A0

 

− m
(cid:18)

2



kh∗(P )k∞

C1 exp

m=1

≤ C1
t

∞X

m=1

exp

− m2
2kh∗(P )k2∞

!2
r2 log t
(cid:19)

m

+
√
2m log t
− 
kh∗(P )k∞

= o

(cid:18)1

(cid:19)

t

.

Combining this with (19) proves the result.

References

[1] Burnetas, A.N. & Katehakis, M.N. (1997) Optimal adaptive policies for Markov decision processes. Math-
ematics of Operations Research 22(1):222–255
[2] Auer, P. & Ortner, R. (2007) Logarithmic online regret bounds for undiscounted reinforcement learning.
Advances in Neural Information Processing Systems 19. Cambridge, MA: MIT Press.
[3] Lai, T.L. & Robbins, H. (1985) Asymptotically efﬁcient adaptive allocation rules. Advances in Applied
Mathematics 6(1):4–22.
[4] Brafman, R.I. & Tennenholtz, M. (2002) R-MAX - a general polynomial time algorithm for near-optimal
reinforcement learning. Journal of Machine Learning Research 3:213–231.
[5] Auer, P. (2002) Using conﬁdence bounds for exploitation-exploration trade-offs. Journal of Machine Learn-
ing Research 3:397–422.
[6] Auer, P., Cesa-Bianchi, N. & and Fischer, P. (2002) Finite-time analysis of the multiarmed bandit problem.
Machine Learning 47(2-3):235-256.
[7] Strehl, A.L. & Littman, M. (2005) A theoretical analysis of model-based interval estimation. In Proceedings
of the Twenty-Second International Conference on Machine Learning, pp. 857-864. ACM Press.
[8] Tewari, A. (2007) Reinforcement Learning in Large or Unknown MDPs. PhD thesis, Department of Elec-
trical Engineering and Computer Sciences, University of California at Berkeley.
[9] Puterman, M.L. (1994) Markov Decision Processes: Discrete Stochastic Dynamic Programming. New
York: John Wiley and Sons.

8

"
56,2007,Topmoumoute Online Natural Gradient Algorithm,"Guided by the goal of obtaining an optimization algorithm that is both fast and yielding good generalization, we study the descent direction maximizing the decrease in generalization error or the probability of not increasing generalization error. The surprising result is that from both the Bayesian and frequentist perspectives this can yield the natural gradient direction. Although that direction can be very expensive to compute we develop an efficient, general, online approximation to the natural gradient descent which is suited to large scale problems. We report experimental results showing much faster convergence in computation time and in number of iterations with TONGA (Topmoumoute Online natural Gradient Algorithm) than with stochastic gradient descent, even on very large datasets.","Topmoumoute online natural gradient algorithm

Nicolas Le Roux

University of Montreal

nicolas.le.roux@umontreal.ca

Pierre-Antoine Manzagol

University of Montreal

manzagop@iro.umontreal.ca

Yoshua Bengio

University of Montreal

yoshua.bengio@umontreal.ca

Abstract

Guided by the goal of obtaining an optimization algorithm that is both fast and
yields good generalization, we study the descent direction maximizing the de-
crease in generalization error or the probability of not increasing generalization
error. The surprising result is that from both the Bayesian and frequentist perspec-
tives this can yield the natural gradient direction. Although that direction can be
very expensive to compute we develop an ef(cid:2)cient, general, online approximation
to the natural gradient descent which is suited to large scale problems. We re-
port experimental results showing much faster convergence in computation time
and in number of iterations with TONGA (Topmoumoute Online natural Gradient
Algorithm) than with stochastic gradient descent, even on very large datasets.

Introduction

An ef(cid:2)cient optimization algorithm is one that quickly (cid:2)nds a good minimum for a given cost func-
tion. An ef(cid:2)cient learning algorithm must do the same, with the additional constraint that the func-
tion is only known through a proxy. This work aims to improve the ability to generalize through
more ef(cid:2)cient learning algorithms.
Consider the optimization of a cost on a training set with access to a validation set. As the end
objective is a good solution with respect to generalization, one often uses early stopping: optimizing
the training error while monitoring the validation error to (cid:2)ght over(cid:2)tting. This approach makes
the underlying assumption that over(cid:2)tting happens at the later stages. A better perspective is that
over(cid:2)tting happens all through the learning, but starts being detrimental only at the point it overtakes
the (cid:147)true(cid:148) learning. In terms of gradients, the gradient of the cost on the training set is never collinear
with the true gradient, and the dot product between the two actually eventually becomes negative.
Early stopping is designed to determine when that happens. One can thus wonder: can one limit
over(cid:2)tting before that point? Would this actually postpone that point?
From this standpoint, we discover new justi(cid:2)cations behind the natural gradient [1]. Depending on
certain assumptions, it corresponds either to the direction minimizing the probability of increasing
generalization error, or to the direction in which the generalization error is expected to decrease
the fastest. Unfortunately, natural gradient algorithms suffer from poor scaling properties, both with
respect to computation time and memory, when the number of parameters becomes large. To address
this issue, we propose a generally applicable online approximation of natural gradient that scales
linearly with the number of parameters (and requires computation time comparable to stochastic
gradient descent). Experiments show that it can bring signi(cid:2)cant faster convergence and improved
generalization.

1

1 Natural gradient

encountered and can be quite dif(cid:2)cult. There exist various techniques to tackle it, their ef(cid:2)ciency
depending on L and p. In the case of non-convex optimization, gradient descent is a successful

Let eL be a cost de(cid:2)ned as eL((cid:18)) =Z L(x; (cid:18))p(x)dx where L is a loss function over some parameters
(cid:18) and over the random variable x with distribution p(x). The problem of minimizing eL over (cid:18) is often
technique. The approach consists in progressively updating (cid:18) using the gradienteg = d eL
[1] showed that the parameter space is a Riemannian space of metric eC (the covariance of the
The natural gradient direction is therefore given by eC (cid:0)1eg. The Riemannian space is known to

correspond to the space of functions represented by the parameters (instead of the space of the
parameters themselves).
The natural gradient somewhat resembles the Newton method. [6] showed that, in the case of a mean
squared cost function, the Hessian is equal to the sum of the covariance matrix of the gradients and
of an additional term that vanishes to 0 as the training error goes down. Indeed, when the data are
generated from the model, the Hessian and the covariance matrix are equal. There are two important

gradients), and introduced the natural gradient as the direction of steepest descent in this space.

differences: the covariance matrix eC is positive-de(cid:2)nite, which makes the technique more stable,

but contains no explicit second order information. The Hessian allows to account for variations in
the parameters. The covariance matrix accounts for slight variations in the set of training samples. It
also means that, if the gradients highly disagree in one direction, one should not go in that direction,
even if the mean suggests otherwise. In that sense, it is a conservative gradient.

d(cid:18) .

2 A new justi(cid:2)cation for natural gradient

Until now, we supposed we had access to the true distribution p. However, this is usually not the
case and, in general, the distribution p is only known through the samples of the training set. These
samples de(cid:2)ne a cost L (resp. a gradient g) that, although close to the true cost (resp. gradient), is
not equal to it. We shall refer to L as the training error and to eL as the generalization error. The
danger is then to over(cid:2)t the parameters (cid:18) to the training set, yielding parameters that are not optimal
with respect to the generalization error.
A simple way to (cid:2)ght over(cid:2)tting consists in determining the point when the continuation of the
optimization on L will be detrimental to eL. This can be done by setting aside some samples to
form a validation set that will provide an independent estimate of eL. Once the error starts increasing

on the validation set, the optimization should be stopped. We propose a different perspective on
over(cid:2)tting. Instead of only monitoring the validation error, we consider using as descent direction
an estimate of the direction that maximizes the probability of reducing the generalization error. The
goal is to limit over(cid:2)tting at every stage, with the hope that the optimal point with respect to the
validation should have lower generalization error.

(for a reasonably small step) when stepping in the direction of v. Likewise, if v T g is negative then
the training error drops. Since the learning objective is to minimize generalization error, we would

Consider a descent direction v. We know that if vTeg is negative then the generalization error drops
like vTeg as small as possible, or at least always negative.
nXi=1

By de(cid:2)nition, the gradient on the training set is g =
and n is the
number of training samples. With a rough approximation, one can consider the gis as draws from the
true gradient distribution and assume all the gradients are independent and identically distributed.
The central limit theorem then gives

gi where gi =

@L(xi; (cid:18))

1
n

@(cid:18)

g (cid:24) N eg; eC
n!

@(cid:18) wrt p(x).

where eC is the true covariance matrix of @L(x;(cid:18))

2

(1)

We will now show that, both in the Bayesian setting (with a Gaussian prior) and in the frequentist
setting (with some restrictions over the type of gradient considered), the natural gradient is optimal
in some sense.

2.1 Bayesian setting

In the Bayesian setting,eg is a random variable. We would thus like to de(cid:2)ne a posterior overeg given
the samples gi in order to have a posterior distribution over vTeg for any given direction v. The prior
overeg will be a Gaussian centered in 0 of variance (cid:27)2I. Thus, using eq. 1, the posterior overeg given
the gis (assuming the only information overeg given by the gis is through g and C) is

Denoting eC(cid:27) = I +

n(cid:27)2 , we therefore have
eC

egjg;eC (cid:24) N0@ I + eC
(cid:27)2 + neC (cid:0)1(cid:19)(cid:0)11A
n(cid:27)2!(cid:0)1
g;(cid:18) I
!
vTegjg;eC (cid:24) N vTeC (cid:0)1
vTeC (cid:0)1
(cid:27) eCv

(cid:27) g;

n

(2)

(3)

Using this result, one can choose between several strategies, among which two are of particular
interest:

maximize the immediate gain). In this setting, the direction v to choose is

(cid:15) choosing the direction v such that the expected value of v Teg is the lowest possible (to
If (cid:27) < 1, this is the regularized natural gradient. In the case of (cid:27) = 1, eC(cid:27) = I and this
(cid:15) choosing the direction v to minimize the probability of vTeg to be positive. This is equiva-

v / (cid:0)eC (cid:0)1

is the batch gradient descent.

lent to (cid:2)nding

(cid:27) g:

(4)

argminv

(we dropped n for the sake of clarity, since it does not change the result). If we square this

quantity and take the derivative with respect to v, we (cid:2)nd 2eC (cid:0)1
2eC (cid:0)1
(cid:27) eCv(vT eC (cid:0)1
one is in the span of eC (cid:0)1
(since eC and eC(cid:27) are invertible), i.e.

(cid:27) g(vTeC (cid:0)1
(cid:27) g)2 at the numerator. The (cid:2)rst term is in the span of eC (cid:0)1

(cid:27) eCv)(cid:0)
(cid:27) eCv. Hence, for the derivative to be zero, we must have g / eCv

(cid:27) g)(vTeC (cid:0)1

(cid:27) g and the second

(5)

This direction is the natural gradient and does not depend on the value of (cid:27).

v / (cid:0)eC (cid:0)1g:

(cid:27) g

vTeC (cid:0)1
pvTeC (cid:0)1
(cid:27) eCv

2.2 Frequentist setting

eC

n(cid:17), we have

consider (as all second-order methods do) the directions v of the form v = M T g (i.e. we are only
allowed to go in a direction which is a linear function of g).

In the frequentist setting,eg is a (cid:2)xed unknown quantity. For the sake of simplicity, we will only
Since g (cid:24) N(cid:16)eg;
The matrix M (cid:3) which minimizes the probability of vTeg to be positive satis(cid:2)es

vTeg = gT M g (cid:24) N egT Meg;egT M TeCMeg
M (cid:3) = argminM egT Meg
egT M T CMeg

!

(6)

(7)

n

3

The numerator of the derivative of this quantity isegegT M TeCMegegT (cid:0) 2eCMegegT MegegT . The (cid:2)rst
term is in the span ofeg and the second one is in the span of eCMeg. Thus, for this derivative to be
0 for alleg, one must have M / eC (cid:0)1 and we obtain the same result as in the Bayesian case: the

natural gradient represents the direction minimizing the probability of increasing the generalization
error.

3 Online natural gradient
The previous sections provided a number of justi(cid:2)cations for using the natural gradient. However,
the technique has a prohibitive computational cost, rendering it impractical for large scale problems.
Indeed, considering p as the number of parameters and n as the number of examples, a direct batch
implementation of the natural gradient is O(p2) in space and O(np2 + p3) in time, associated re-
spectively with the gradients’ covariance storage, computation and inversion. This section reviews
existing low complexity implementations of the natural gradient, before proposing TONGA, a new
low complexity, online and generally applicable implementation suited to large scale problems. In

the previous sections we assumed the true covariance matrix eC to be known. In a practical algorithm

we of course use an empirical estimate, and here this estimate is furthermore based on a low-rank
approximation denoted C (actually a sequence of estimates Ct).

3.1 Low complexity natural gradient implementations
[9] proposes a method speci(cid:2)c to the case of multilayer perceptrons. By operating on blocks of
the covariance matrix, this approach attains a lower computational complexity1. However, the tech-
nique is quite involved, speci(cid:2)c to multilayer perceptrons and requires two assumptions: Gaussian
distributed inputs and a number of hidden units much inferior to that of input units. [2] offers a more
general approach based on the Sherman-Morrison formula used in Kalman (cid:2)lters: the technique
maintains an empirical estimate of the inversed covariance matrix that can be updated in O(p2). Yet
the memory requirement remains O(p2). It is however not necessary to compute the inverse of the
gradients’ covariance, since one only needs its product with the gradient. [10] offers two approaches
to exploit this. The (cid:2)rst uses conjugate gradient descent to solve Cv = g. The second revisits
[9] thereby achieving a lower complexity. [8] also proposes an iterative technique based on the
minimization of a different cost. This technique is used in the minibatch setting, where Cv can be
computed cheaply through two matrix vector products. However, estimating the gradient covariance
only from a small number of examples in one minibatch yields unstable estimation.

3.2 TONGA
Existing techniques fail to provide an implementation of the natural gradient adequate for the large
scale setting. Their main failings are with respect to computational complexity or stability. TONGA
was designed to address these issues, which it does this by maintaining a low rank approximation of
the covariance and by casting both problems of (cid:2)nding the low rank approximation and of computing
the natural gradient in a lower dimensional space, thereby attaining a much lower complexity. What
we exploit here is that although a covariance matrix needs many gradients to be estimated, we can
take advantage of an observed property that it generally varies smoothly as training proceeds and
moves in parameter space.

3.2.1 Computing the natural gradient direction between two eigendecompositions
Even though our motivation for the use of natural gradient implied the covariance matrix of the em-
pirical gradients, we will use the second moment (i.e. the uncentered covariance matrix) throughout
the paper (and so did Amari in his work). The main reason is numerical stability. Indeed, in the
batch setting, we have (assuming C is the centered covariance matrix and g the mean) v = C (cid:0)1g,
thus Cv = g. But then, (C + ggT )v = g + ggT v = g(1 + gT v) and

(C + ggT )(cid:0)1g =

v

1 + gT v

= (cid:22)v

(8)

1Though the technique allows for a compact representation of the covariance matrix, the working memory

requirement remains the same.

4

1

kgk cos(g;v).

Even though the direction is the same, the scale changes and the norm of the direction is bounded
by
Since TONGA operates using a low rank estimate of the gradients’ non-centered covariance, we
must be able to update cheaply. When presented with a new gradient, we integrate its information
using the following update formula2:

(9)
where C0 = 0 and ^Ct(cid:0)1 is the low rank approximation at time step t (cid:0) 1. Ct is now likely of
greater rank, and the problem resides in computing its low rank approximation ^Ct. Writing ^Ct(cid:0)1 =
Xt(cid:0)1X T

Ct = (cid:13) ^Ct(cid:0)1 + gtgT

t

t(cid:0)1,

Ct = XtX T

t with Xt = [p(cid:13)Xt(cid:0)1

gt]

With such covariance matrices, computing the (regularized) natural direction vt is equal to

vt = (Ct + (cid:21)I)(cid:0)1gt = (XtX T
vt = (XtX T

t + (cid:21)I)(cid:0)1Xtyt with yt = [0; : : : 0; 1]T :

t + (cid:21)I)(cid:0)1gt

Using the Woodbury identity with positive de(cid:2)nite matrices [7], we have

(10)
(11)

(12)
If Xt is of size p (cid:2) r (with r < p, thus yielding a covariance matrix of rank r), the cost of this
computation is O(pr2 + r3). However, since the Gram matrix Gt = X T

t Xt + (cid:21)I)(cid:0)1yt

vt = Xt(X T

Gt =(cid:18) (cid:13)X T

p(cid:13)gT

t(cid:0)1Xt(cid:0)1 p(cid:13)X T
t(cid:0)1gt
gT
t Xt(cid:0)1
t gt

(cid:19) =(cid:18)

(cid:13)Gt(cid:0)1

p(cid:13)gT

t Xt(cid:0)1

t Xt can be rewritten as
p(cid:13)X T
t(cid:0)1gt
gT
t gt

(cid:19) ;

(13)

the cost of computing Gt using Gt(cid:0)1 reduces to O(pr + r3). This stresses the need to keep r small.

3.2.2 Updating the low-rank estimate of Ct
To keep a low-rank estimate of Ct = XtX T
the (cid:2)rst k eigenvectors. This can be made at low cost using its relation to that of Gt:

t , we can compute its eigendecomposition and keep only

Gt = V DV T
Ct = (XtV D(cid:0) 1

(14)
The cost of such an eigendecomposition is O(kr2 + pkr) (for the computation of the eigendecom-
position of the Gram matrix and the computation of the eigenvectors, respectively). Since the cost of
computing the natural direction is O(pr + r3), it is computationally more ef(cid:2)cient to let the rank of
Xt grow for several steps (using formula 12 in between) and then compute the eigendecomposition
using

2 )D(XtV D(cid:0) 1

2 )T

Ct+b = Xt+bX T

b(cid:0)1

2 gt+1;

(cid:13)

: : :

(cid:13)

1

2 gt+b(cid:0)1;

(cid:13)

with Ut the unnormalized eigenvectors computed during the previous eigendecomposition.

t+b with Xt+b =h(cid:13)Ut;

t+b

2 gt+b]i

3.2.3 Computational complexity
The computational complexity of TONGA depends on the complexity of updating the low rank
approximation and on the complexity of computing the natural gradient. The cost of updating the
approximation is in O(k(k + b)2 + p(k + b)k) (as above, using r = k + b). The cost of computing
the natural gradient vt is in O(p(k + b) + (k + b)3) (again, as above, using r = k + b). Assuming

k + b (cid:28)p(p) and k (cid:20) b, TONGA’s total computational cost per each natural gradient computation

is then O(pb).
Furthermore, by operating on minibatch gradients of size b0, we end up with a cost per example of
b0 ). Choosing b = b0, yields O(p) per example, the same as stochastic gradient descent. Empiri-
O( bp
cal comparison of cpu time also shows comparable CPU time per example, but faster convergence.
In our experiments, p was in the tens of thousands, k was less than 5 and b was less than 50.
The result is an approximate natural gradient with low complexity, general applicability and (cid:3)exi-
bility over the tradoff between computations and the quality of the estimate.

2The second term is not weighted by 1(cid:0)(cid:13) so that the in(cid:3)uence of gt in Ct is the same for all t, even t = 0.To
keep the magnitude of the matrix constant, one must use a normalization constant equal to 1 + (cid:13) + : : : + (cid:13) t.

5

4 Block-diagonal online natural gradient for neural networks

One might wonder if there are better approximations of the covariance matrix C than computing its
(cid:2)rst k eigenvectors. One possibility is a block-diagonal approximation from which to retain only
the (cid:2)rst k eigenvectors of every block (the value of k can be different for each block). Indeed, [4]
showed that the Hessian of a neural network with one hidden layer trained with the cross-entropy
cost converges to a block diagonal matrix during optimization. These blocks are composed of the
weights linking all the hidden units to one output unit and all the input units to one hidden unit.
Given the close relationship between the Hessian and the covariance matrices, we can assume they
have a similar shape during the optimization.
Figure 1 shows the correlation between the standard stochastic gradients of the parameters of a
16 (cid:0) 50 (cid:0) 26 neural network. The (cid:2)rst blocks represent the weights going from the input units to
each hidden unit (thus 50 blocks of size 17, bias included) and the following represent the weights
going from the hidden units to each output unit (26 blocks of size 51). One can see that the block-
diagonal approximation is reasonable. Thus, instead of selecting only k eigenvectors to represent
the full covariance matrix, we can select k eigenvectors for every block, yielding the same total cost.
However, the rank of the approximation goes from k to k(cid:2)number of blocks. In the matrices shown
in (cid:2)gure 1, which are of size 2176, a value of k = 5 yields an approximation of rank 380.

(a) Stochastic gradient

(b) TONGA

(c) TONGA - zoom

Figure 1: Absolute correlation between the standard stochastic gradients after one epoch in a neural
network with 16 input units, 50 hidden units and 26 output units when following stochastic gradient
directions (left) and natural gradient directions (center and right).

Figure 2 shows the ratio of Frobenius norms kC(cid:0) (cid:22)Ck2
for different types of approximations (cid:22)C (full
kCk2
or block-diagonal). We can (cid:2)rst notice that approximating only the blocks yields a ratio of :35 (in
comparison, taking only the diagonal of C yields a ratio of :80), even though we considered only
82076 out of the 4734976 elements of the matrix (1:73% of the total). This ratio is almost obtained
with k = 6. We can also notice that, for k < 30, the block-diagonal approximation is much better
(in terms of the Frobenius norm) than the full approximation. The block diagonal approximation is
therefore very cost effective.

F

F

s
m
r
o
n

i

 
s
u
n
e
b
o
r
F
d
e
r
a
u
q
s
 

 

e
h

t
 
f

o
o

 

i
t

a
R

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

 

200

400

Full matrix approximation
Block diagonal approximation

 

s
m
r
o
n

i

 
s
u
n
e
b
o
r
F
d
e
r
a
u
q
s
 

 

e
h

t
 
f

o

 

o

i
t

a
R

Full matrix approximation
Block diagonal approximation

 

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

800

600
1600
Number k of eigenvectors kept

1000

1400

1200

1800

2000

0.1

 

5

10

15

30
Number k of eigenvectors kept

25

20

35

40

(a) Full view

(b) Zoom

Figure 2: Quality of the approximation (cid:22)C of the covariance C depending on the number of eigenvec-
tors kept (k), in terms of the ratio of Frobenius norms kC(cid:0) (cid:22)Ck2
, for different types of approximation
kCk2
(cid:22)C (full matrix or block diagonal)

F

F

6

This shows the block diagonal approximation constitutes a powerful and cheap approximation of the
covariance matrix in the case of neural networks. Yet this approximation also readily applies to any
mixture algorithm where we can assume independence between the components.

5 Experiments

We performed a small number of experiments with TONGA approximating the full covariance ma-
trix, keeping the overhead of the natural gradient small (ie, limiting the rank of the approximation).
Regrettably, TONGA performed only as well as stochastic gradient descent, while being rather sen-
sitive to the hyperparameter values. The following experiments, on the other hand, use TONGA
with the block diagonal approximation and yield impressive results. We believe this is a re(cid:3)ection
of the phenomenon illustrated in (cid:2)gure 2: the block diagonal approximation makes for a very cost
effective approximation of the covariance matrix. All the experiments have been made optimizing
hyperparameters on a validation set (not shown here) and selecting the best set of hyperparameters
for testing, trying to keep small the overhead due to natural gradient calculations.
One could worry about the number of hyperparameters of TONGA. However, default values of
k = 5, b = 50 and (cid:13) = :995 yielded good results in every experiment. When (cid:21) goes to in(cid:2)nity,
TONGA becomes the standard stochastic gradient algorithm. Therefore, a simple heuristic for (cid:21) is
to progressively tune it down. In our experiments, we only tried powers of ten.

5.1 MNIST dataset

The MNIST digits dataset consists of 50000 training samples, 10000 validation samples and 10000
test samples, each one composed of 784 pixels. There are 10 different classes (one for every digit).

0.06

0.05

0.04

0.03

0.02

0.01

r
o
r
r
e

 

n
o

i
t

a
c
i
f
i
s
s
a
C

l

0

 
0

500

1000

 

Block diagonal TONGA
Stochastic batchsize=1
Stochastic batchsize=400
Stochastic batchsize=1000
Stochastic batchsize=2000

0.06

0.055

0.05

0.045

0.04

0.035

0.03

0.025

0.02

r
o
r
r
e

 

n
o

i
t

a
c
i
f
i
s
s
a
C

l

 

Block diagonal TONGA
Stochastic batchsize=1
Stochastic batchsize=400
Stochastic batchsize=1000
Stochastic batchsize=2000

0.2

0.18

0.16

0.14

0.12

0.1

0.08

0.06

0.04

0.02

d
o
o
h

i
l

e
k

i
l

−
g
o

l
 

e
v
i
t

a
g
e
N

Block diagonal TONGA
Stochastic batchsize=1
Stochastic batchsize=400
Stochastic batchsize=1000
Stochastic batchsize=2000

 

0.2

d
o
o
h

i
l

e
k

i
l

−
g
o

l
 

e
v
i
t

a
g
e
N

0.15

0.1

 

Block diagonal TONGA
Stochastic batchsize=1
Stochastic batchsize=400
Stochastic batchsize=1000
Stochastic batchsize=2000

2000

1500
3000
CPU time (in seconds)

2500

3500

4000

4500

0.015

 
0

500

1000

2000

1500
3000
CPU time (in seconds)

2500

3500

4000

4500

0

 
0

500

1000

2000

1500
3000
CPU time (in seconds)

2500

3500

4000

4500

0.05

 
0

500

1000

2000

1500
3000
CPU time (in seconds)

2500

3500

4000

4500

(a) Train class error

(b) Test class error

(c) Train NLL

(d) Test NLL

Figure 3: Comparison between stochastic gradient and TONGA on the MNIST dataset (50000 train-
ing examples), in terms of training and test classi(cid:2)cation error and Negative Log-Likelihood (NLL).
The mean and standard error have been computed using 9 different initializations.

Figure 3 shows that in terms of training CPU time (which includes the overhead due to TONGA),
TONGA allows much faster convergence in training NLL, as well as in testing classi(cid:2)cation error
and testing NLL than ordinary stochastic and minibatch gradient descent on this task. One can also
note that minibatch stochastic gradient is able to pro(cid:2)t from matrix-matrix multiplications, but this
advantage is mainly seen in training classi(cid:2)cation error.

5.2 Rectangles problem

The Rectangles-images task has been proposed in [5] to compare deep belief networks and support
vector machines. It is a two-class problem and the inputs are 28(cid:2) 28 grey-level images of rectangles
located in varying locations and of different dimensions. The inside of the rectangle and the back-
ground are extracted from different real images. We used 900,000 training examples and 10,000 val-
idation examples (no early stopping was performed, we show the whole training/validation curves).
All the experiments are performed with a multi-layer network with a 784-200-200-100-2 architec-
ture (previously found to work well on this dataset). Figure 4 shows that in terms of training CPU
time, TONGA allows much faster convergence than ordinary stochastic gradient descent on this
task, as well as lower classi(cid:2)cation error.

7

0.55

t

e
s
 

0.5

i

g
n
n
a
r
t
 

i

e
h

t
 

 

n
o
d
o
o
h

i
l

e
k

i
l

−
g
o

l
 

e
v
i
t

a
g
e
N

0.45

0.4

0.35

0.3

0.25

0.2

 
0

0.5

1

Stochastic gradient
Block diagonal TONGA

 

Stochastic gradient
Block diagonal TONGA

 

0.2

0.18

0.16

0.14

0.12

0.1

0.08

t

e
s
 
t
s
e

t
 

e
h

t
 

n
o

 

d
o
o
h

i
l

e
k

i
l

−
g
o

l
 

e
v
i
t

a
g
e
N

0.5

0.45

0.4

0.35

0.3

0.25

0.2

Stochastic gradient
Block diagonal TONGA

 

t

e
s
 
t
s
e

t
 

e
h

t
 

n
o

 
r
o
r
r
e
n
o

 

i
t

a
c
i
f
i
s
s
a
C

l

Stochastic gradient
Block diagonal TONGA

 

0.2

0.18

0.16

0.14

0.12

0.1

0.08

t

e
s
 
g
n
n
a
r
t
 

i

i

e
h

t
 

n
o

 
r
o
r
r
e
n
o

 

i
t

a
c
i
f
i
s
s
a
C

l

1.5

2

CPU time (in seconds)

2.5

3

3.5
x 104

0.06

 
0

0.5

1

1.5

2

CPU time (in seconds)

2.5

3

3.5
x 104

 
0

0.5

1

1.5

2

CPU time (in seconds)

2.5

3

3.5
x 104

0.06

 
0

0.5

1

1.5

2

CPU time (in seconds)

2.5

3

3.5
x 104

(a) Train NLL error

(b) Test NLL error

(c) Train class error

(d) Test class error

Figure 4: Comparison between stochastic gradient descent and TONGA w.r.t. NLL and classi(cid:2)ca-
tion error, on training and validation sets for the rectangles problem (900,000 training examples).
6 Discussion
[3] reviews the different gradient descent techniques in the online setting and discusses their re-
spective properties. Particularly, he states that a second order online algorithm (i.e., with a search
direction of is v = M g with g the gradient and M a positive semide(cid:2)nite matrix) is optimal (in terms
of convergence speed) when M converges to H (cid:0)1. Furthermore, the speed of convergence depends
(amongst other things) on the rank of the matrix M. Given the aforementioned relationship between
the covariance and the Hessian matrices, the natural gradient is close to optimal in the sense de(cid:2)ned
above, provided the model has enough capacity. On mixture models where the block-diagonal ap-
proximation is appropriate, it allows us to maintain an approximation of much higher rank than a
standard low-rank approximation of the full covariance matrix.

Conclusion and future work
We bring two main contributions in this paper. First, by looking for the descent direction with either
the greatest probability of not increasing generalization error or the direction with the largest ex-
pected increase in generalization error, we obtain new justi(cid:2)cations for the natural gradient descent
direction. Second, we present an online low-rank approximation of natural gradient descent with
computational complexity and CPU time similar to stochastic gradientr descent. In a number of
experimental comparisons we (cid:2)nd this optimization technique to beat stochastic gradient in terms of
speed and generalization (or in generalization for a given amount of training time). Even though de-
fault values for the hyperparameters yield good results, it would be interesting to have an automatic
procedure to select the best set of hyperparameters.

References
[1] S. Amari. Natural gradient works ef(cid:2)ciently in learning. Neural Computation, 10(2):251(cid:150)276, 1998.
[2] S. Amari, H. Park, and K. Fukumizu. Adaptive method of realizing natural gradient learning for multilayer

perceptrons. Neural Computation, 12(6):1399(cid:150)1409, 2000.

[3] L. Bottou. Stochastic learning. In O. Bousquet and U. von Luxburg, editors, Advanced Lectures on Ma-
chine Learning, number LNAI 3176 in Lecture Notes in Arti(cid:2)cial Intelligence, pages 146(cid:150)168. Springer
Verlag, Berlin, 2004.

[4] R. Collobert. Large Scale Machine Learning. PhD thesis, Universit·e de Paris VI, LIP6, 2004.
[5] H. Larochelle, D. Erhan, A. Courville, J. Bergstra, and Y. Bengio. An empirical evaluation of deep
architectures on problems with many factors of variation. In Twenty-fourth International Conference on
Machine Learning (ICML’2007), 2007.

[6] Y. LeCun, L. Bottou, G. Orr, and K.-R. M¤uller. Ef(cid:2)cient backprop. In G. Orr and K.-R. M¤uller, editors,

Neural Networks: Tricks of the Trade, pages 9(cid:150)50. Springer, 1998.

[7] K. B. Petersen and M. S. Pedersen. The matrix cookbook, feb 2006. Version 20051003.
[8] N. N. Schraudolph. Fast curvature matrix-vector products for second-order gradient descent. Neural

Computation, 14(7):1723(cid:150)1738, 2002.

[9] H. H. Yang and S. Amari. Natural gradient descent for training multi-layer perceptrons. Submitted to

IEEE Tr. on Neural Networks, 1997.

[10] H. H. Yang and S. Amari. Complexity issues in natural gradient descent method for training multi-layer

perceptrons. Neural Computation, 10(8):2137(cid:150)2157, 1998.

8

"
133,2007,Learning the structure of manifolds using random projections,We present a simple variant of the k-d tree which automatically adapts to intrinsic low dimensional structure in data.,"Learning the structure of manifolds using random

projections

Yoav Freund ∗
UC San Diego

Sanjoy Dasgupta †

UC San Diego

Mayank Kabra
UC San Diego

Nakul Verma
UC San Diego

Abstract

We present a simple variant of the k-d tree which automatically adapts to intrinsic
low dimensional structure in data.

1 Introduction

The curse of dimensionality has traditionally been the bane of nonparametric statistics, as reﬂected
for instance in convergence rates that are exponentially slow in dimension. An exciting way out of
this impasse is the recent realization by the machine learning and statistics communities that in many
real world problems the high dimensionality of the data is only superﬁcial and does not represent
the true complexity of the problem. In such cases data of low intrinsic dimension is embedded in a
space of high extrinsic dimension.

For example, consider the representation of human motion generated by a motion capture system.
Such systems typically track marks located on a tight-ﬁtting body suit. The number of markers, say
N, is set sufﬁciently large in order to get dense coverage of the body. A posture is represented by a
(3N)-dimensional vector that gives the 3D location of each of the N marks. However, despite this
seeming high dimensionality, the number of degrees of freedom is relatively small, corresponding
to the dozen-or-so joint angles in the body. The marker positions are more or less deterministic
functions of these joint angles. Thus the data lie in R3N , but on (or very close to) a manifold [4] of
small dimension.

In the last few years, there has been an explosion of research investigating methods for learning in
the context of low-dimensional manifolds. Some of this work (for instance, [2]) exploits the low
intrinsic dimension to improve the convergence rate of supervised learning algorithms. Other work
(for instance, [12, 11, 1]) attempts to ﬁnd an embedding of the data into a low-dimensional space,
thus ﬁnding an explicit mapping that reduces the dimensionality.
In this paper, we describe a new way of modeling data that resides in RD but has lower intrinsic
dimension d < D. Unlike many manifold learning algorithms, we do not attempt to ﬁnd a single
uniﬁed mapping from RD to Rd. Instead, we hierarchically partition RD into pieces in a manner
that is provably sensitive to low-dimensional structure. We call this spatial data structure a random
projection tree (RP tree). It can be thought of as a variant of the k-d tree that is provably manifold-
adaptive.

k-d trees, RP trees, and vector quantization
Recall that a k-d tree [3] partitions RD into hyperrectangular cells. It is built in a recursive manner,
splitting along one coordinate direction at a time. The succession of splits corresponds to a binary
tree whose leaves contain the individual cells in RD. These trees are among the most widely-used
methods for spatial partitioning in machine learning and computer vision.

∗Corresponding author: yfreund@cs.ucsd.edu.
†Dasgupta and Verma acknowledge the support of NSF, under grants IIS-0347646 and IIS-0713540.

1

Figure 1: Left: A spatial partitioning of R2 induced by a k-d tree with three levels. The dots are data
vectors; each circle represents the mean of the vectors in one cell. Right: Partitioning induced by an
RP tree.

On the left part of Figure 1 we illustrate a k-d tree for a set of vectors in R2. The leaves of the tree
partition RD into cells; given a query point q, the cell containing q is identiﬁed by traversing down
the k-d tree. Each cell can be thought of as having a representative vector: its mean, depicted in the
ﬁgure by a circle. The partitioning together with these mean vectors deﬁne a vector quantization
(VQ) of R2: a mapping from R2 to a ﬁnite set of representative vectors (called a “codebook” in the
context of lossy compression methods). A good property of this tree-structured vector quantization
is that a vector can be mapped efﬁciently to its representative. The design goal of VQ is to minimize
the error introduced by replacing vectors with their representative.

We quantify the VQ error by the average squared Euclidean distance between a vector in the set and
the representative vector to which it is mapped. This error is closely related (in fact, proportional) to
the average diameter of cells, that is, the average squared distance between pairs of points in a cell.1
As the depth of the k-d tree increases the diameter of the cells decreases and so does the VQ error.
However, in high dimension, the rate of decrease of the average diameter can be very slow. In fact,
as we show in the supplementary material, there are data sets in RD for which a k-d tree requires D
levels in order to halve the diameter. This slow rate of decrease of cell diameter is ﬁne if D = 2 as
in Figure 1, but it is disastrous if D = 1000. Constructing 1000 levels of the tree requires 21000 data
points! This problem is a real one that has been observed empirically: k-d trees are prone to a curse
of dimensionality.

What if the data have low intrinsic dimension? In general, k-d trees will not be able to beneﬁt from
this; in fact the bad example mentioned above has intrinsic dimension d = 1. But we show that
a simple variant of the k-d tree does indeed decrease cell diameters much more quickly. Instead
of splitting along coordinate directions, we use randomly chosen unit vectors, and instead of split-
ting data exactly at the median, we use a more carefully chosen split point. We call the resulting
data structure a random projection tree (Figure 1, right) and we show that it admits the following
theoretical guarantee (formal statement is in the next section).

Pick any cell C in the RP tree, and suppose the data in C have intrinsic dimension
d. Pick a descendant cell ≥ d levels below; then with constant probability, this
descendant has average diameter at most half that of C.2

There is no dependence at all on the extrinsic dimensionality (D) of the data. We thus have a
vector quantization construction method for which the diameter of the cells depends on the intrinsic
dimension, rather than the extrinsic dimension of the data.

A large part of the beneﬁt of RP trees comes from the use of random unit directions, which is
rather like running k-d trees with a preprocessing step in which the data are projected into a random

1This is in contrast to the max diameter, the maximum distance between two vectors in a cell.
2Here the probability is taken over the randomness in constructing the tree.

2

low-dimensional subspace. In fact, a recent experimental study of nearest neighbor algorithms [8]
observes that a similar pre-processing step improves the performance of nearest neighbor schemes
based on spatial data structures. Our work provides a theoretical explanation for this improvement
and shows both theoretically and experimentally that this improvement is signiﬁcant. The explana-
tion we provide is based on the assumption that the data has low intrinsic dimension.

Another spatial data structure based on random projections is the locality sensitive hashing scheme
[6].

Manifold learning and near neighbor search

The fast rate of diameter decrease in random projection trees has many consequences beyond the
quality of vector quantization. In particular, the statistical theory of tree-based statistical estimators
— whether used for classiﬁcation or regression — is centered around the rate of diameter decrease;
for details, see for instance Chapter 20 of [7]. Thus RP trees generically exhibit faster convergence
in all these contexts.

Another case of interest is nearest neighbor classiﬁcation. If the diameter of cells is small, then it
is reasonable to classify a query point according to the majority label in its cell. It is not necessary
to ﬁnd the nearest neighbor; after all, the only thing special about this point is that it happens to be
close to the query. The classical work of Cover and Hart [5] on the Bayes risk of nearest neighbor
methods applies equally to the majority vote in a small enough cell.

Figure 2: Distributions with low intrinsic dimension. The purple areas in these ﬁgures indicate re-
gions in which the density of the data is signiﬁcant, while the complementary white areas indicate ar-
eas where data density is very low. The left ﬁgure depicts data concentrated near a one-dimensional
manifold. The ellipses represent mean+PCA approximations to subsets of the data. Our goal is to
partition data into small diameter regions so that the data in each region is well-approximated by its
mean+PCA. The right ﬁgure depicts a situation where the dimension of the data is variable. Some of
the data lies close to a one-dimensional manifold, some of the data spans two dimensions, and some
of the data (represented by the red dot) is concentrated around a single point (a zero-dimensional
manifold).

Finally, we return to our original motivation: modeling data which lie close to a low-dimensional
manifold. In the literature, the most common way to capture this manifold structure is to create a
graph in which nodes represent data points and edges connect pairs of nearby points. While this is
a natural representation, it does not scale well to very large datasets because the computation time
of closest neighbors grows like the square of the size of the data set. Our approach is fundamentally
different.
Instead of a bottom-up strategy that starts with individual data points and links them
together to form a graph, we use a top-down strategy that starts with the whole data set and partitions
it, in a hierarchical manner, into regions of smaller and smaller diameter. Once these individual cells
are small enough, the data in them can be well-approximated by an afﬁne subspace, for instance that
given by principal component analysis. In Figure 2 we show how data in two dimensions can be
approximated by such a set of local ellipses.

2 The RP tree algorithm

2.1 Spatial data structures
In what follows, we assume the data lie in RD, and we consider spatial data structures built by
recursive binary splits. They differ only in the nature of the split, which we deﬁne in a subroutine

3

called CHOOSERULE. The core tree-building algorithm is called MAKETREE, and takes as input a
data set S ⊂ RD.

procedure MAKETREE(S)
if |S| < M inSize
then return (Leaf)

Rule ← CHOOSERULE(S)

else

Lef tT ree ← MAKETREE({x ∈ S : Rule(x) = true})
RightT ree ← MAKETREE({x ∈ S : Rule(x) = false})
return ([Rule, Lef tT ree, RightT ree])

A natural way to try building a manifold-adaptive spatial data structure is to split each cell along its
principal component direction (for instance, see [9]).

procedure CHOOSERULE(S)
comment: PCA tree version
let u be the principal eigenvector of the covariance of S
Rule(x) := x · u ≤ median({z · u : z ∈ S})
return (Rule)

This method will do a good job of adapting to low intrinsic dimension (details omitted). However,
it has two signiﬁcant drawbacks in practice. First, estimating the principal eigenvector requires a
signiﬁcant amount of data; recall that only about 1/2k fraction of the data winds up at a cell at level
k of the tree. Second, when the extrinsic dimension is high, the amount of memory and computation
required to compute the dot product between the data vectors and the eigenvectors becomes the
dominant part of the computation. As each node in the tree is likely to have a different eigenvector
this severely limits the feasible tree depth. We now show that using random projections overcomes
these problems while maintaining the adaptivity to low intrinsic dimension.

2.2 Random projection trees

We shall see that the key beneﬁts of PCA-based splits can be realized much more simply, by picking
random directions. To see this pictorially, consider data that is concentrated on a subspace, as in the
following ﬁgure. PCA will of course correctly identify this subspace, and a split along the principal
eigenvector u will do a good job of reducing the diameter of the data. But a random direction v will
also have some component in the direction of u, and splitting along the median of v will not be all
that different from splitting along u.

Figure 3: Intuition: a random direction is almost as good as the principal eigenvector.

Now only medians need to be estimated, not principal eigenvectors; this signiﬁcantly reduces the
data requirements. Also, we can use the same random projection in different places in the tree; all
we need is to choose a large enough set of projections that, with high probability, there is be a good
projection direction for each node in the tree. In our experience setting the number of projections
equal to the depth of the tree is sufﬁcient. Thus, for a tree of depth k, we use only k projection
vectors v, as opposed to 2k with a PCA tree. When preparing data to train a tree we can compute
the k projection values before building the tree. This also reduces the memory requirements for
the training set, as we can replace each high dimensional data point with its k projection values
(typically we use 10 ≤ k ≤ 20).
We now deﬁne RP trees formally. For a cell containing points S, let ∆(S) be the diameter of S (the
distance between the two furthest points in the set), and ∆A(S) the average diameter, that is, the

4

average distance between points of S:

X

X

∆2

A(S) =

1
|S|2

kx − yk2 =

2
|S|

kx − mean(S)k2.

x∈S
We use two different types of splits: if ∆2(S) is less than c∆2
A(S) (for some constant c) then we
use the hyperplane split discussed above. Otherwise, we split S into two groups based on distance
from the mean.

x,y∈S

procedure CHOOSERULE(S)
comment: RP tree version
if ∆2(S) ≤ c · ∆2

A(S)



choose a random unit direction v
sort projection values: a(x) = v · x ∀x ∈ S, generating the list a1 ≤ a2 ≤ ··· ≤ an
for i = 1, . . . , n − 1 compute
j=1 aj, µ2 = 1
µ1 = 1
n−i
i

(
Pi
Pn
j=1(aj − µ1)2 +Pn
ci =Pi
j=i+1(aj − µ2)2

j=i+1 aj

then

ﬁnd i that minimizes ci and set θ = (ai + ai+1)/2
Rule(x) := v · x ≤ θ

else {Rule(x) := kx − mean(S)k ≤ median{kz − mean(S)k : z ∈ S}
return (Rule)

In the ﬁrst type of split, the data in a cell are projected onto a random direction and an appropriate
split point is chosen. This point is not necessarily the median (as in k-d trees), but rather the position
that maximally decreases average squared interpoint distance. In Figure 4.4, for instance, splitting
the bottom cell at the median would lead to a messy partition, whereas the RP tree split produces
two clean, connected clusters.

Figure 4: An illustration of the RP-Tree algorithm. 1: The full data set and the PCA ellipse that
approximates it. 2: The ﬁrst level split. 3: The two PCA ellipses corresponding to the two cells after
the ﬁrst split. 4: The two splits in the second level. 5: The four PCA ellipses for the cells at the third
level. 6: The four splits at the third level. As the cells get smaller, their individual PCAs reveal 1D
manifold structure. Note: the ellipses are for comparison only; the RP tree algorithm does not look
at them.

The second type of split, based on distance from the mean of the cell, is needed to deal with cases in
which the cell contains data at very different scales. In Figure 2, for instance, suppose that the vast
majority of data is concentrated at the singleton “0-dimensional” point. If only splits by projection
were allowed, then a large number of splits would be devoted to uselessly subdividing this point
mass. The second type of split separates it from the rest of the data in one go. For a more concrete
example, suppose that the data are image patches. A large fraction of them might be “empty”
background patches, in which case they’d fall near the center of the cell in a very tight cluster. The

5

remaining image patches will be spread out over a much larger space. The effect of the split is then
to separate out these two clusters.

2.3 Theoretical foundations

1 ≥ σ2

2 ≥ ··· ≥ σ2

In analyzing RP trees, we consider a statistical notion of dimension: we say set S has local covari-
ance dimension (d, ) if (1− ) fraction of the variance is concentrated in a d-dimensional subspace.
To make this precise, start by letting σ2
D denote the eigenvalues of the covariance
matrix; these are the variances in each of the eigenvector directions.
Deﬁnition 1 S ⊂ RD has local covariance dimension (d, ) if the largest d eigenvalues of its
D =
covariance matrix satisfy σ2
(1/2)∆2
Now, suppose an RP tree is built from a data set X ⊂ RD, not necessarily ﬁnite. Recall that there
are two different types of splits; let’s call them splits by distance and splits by projection.

d ≥ (1 − ) · (σ2

D). (Note that σ2

A(S).)

1 + ··· + σ2

1 + ··· + σ2

1 + ··· + σ2

Theorem 2 There are constants 0 < c1, c2, c3 < 1 with the following property. Suppose an RP
tree is built using data set X ⊂ RD. Consider any cell C for which X ∩ C has local covariance
dimension (d, ), where  < c1. Pick a point x ∈ S ∩ C at random, and let C0 be the cell that
contains it at the next level down.

• If C is split by distance then

E [∆(S ∩ C0)] ≤ c2∆(S ∩ C).

• If C is split by projection, then

A(S ∩ C0)(cid:3) ≤(cid:16)
E(cid:2)∆2

(cid:17)

1 − c3
d

A(S ∩ C).
∆2

In both cases, the expectation is over the randomization in splitting C and the choice of
x ∈ S ∩ C.

As a consequence, the expected average diameter of cells is halved every O(d) levels. The proof of
this theorem is in the supplementary material, along with even stronger results for different notions
of dimension.

3 Experimental Results

3.1 A streaming version of the algorithm

The version of the RP algorithm we use in practice differs from the one above in three ways. First
of all, both splits operate on the projected data; for the second type of split (split by distance), data
that fall in an interval around the median are separated from data outside that interval. Second,
the tree is built in a streaming manner: that is, the data arrive one at a time, and are processed (to
update the tree) and immediately discarded. This is managed by maintaining simple statistics at
each internal node of the tree and updating them appropriately as the data streams by (more details
in the supplementary matter). The resulting efﬁciency is crucial to the large-scale applications we
have in mind. Finally, instead of choosing a new random projection in each cell, a dictionary of a
few random projections is chosen at the outset. In each cell, every one of these projections is tried
out and the best one (that gives the largest decrease in ∆2
A(S)) is retained. This last step has the
effect of boosting the probability of a good split.

3.2 Synthetic datasets

We start by considering two synthetic datasets that illustrate the shortcomings of k-d trees. We
will see that RP trees adapt well to such cases. For the ﬁrst dataset, points x1, . . . , xn ∈ RD are
generated by the following process: for each point xi,

6

Figure 5: Performance of RP trees with k-d trees on ﬁrst synthetic dataset (left) and the second
synthetic dataset (right)

• choose pi uniformly at random from [0, 1], and
• select each coordinate xij independently from N(pi, 1).

For the second dataset, we choose n points from two D-dimensional Gaussians (with equal proba-
bility) with means at (−1,−1, . . . ,−1) and (1, 1, . . . , 1), and identity covariances.
We compare the performance of different trees according to the average VQ error they incur at
various levels. We consider four types of trees: (1) k-d trees in which the coordinate for a split is
chosen at random; (2) k-d trees in which at each split, the best coordinate is chosen (the one that
most improves VQ error); (3) RP trees; and (4) for reference, PCA trees.
Figure 5 shows the results for the two datasets (D = 1,000 and n = 10,000) averaged over 15 runs.
In both cases, RP trees outperform both k-d tree variants and are close to the performance of PCA
trees without having to explicitly compute any principal components.

3.3 MNIST dataset

We next demonstrate RP trees on the all-familiar MNIST dataset of handwritten digits. This dataset
consists of 28 × 28 grayscale images of the digits zero through nine, and is believed to have low
intrinsic dimension (for instance, see [10]). We restrict our attention to digit 1 for this discussion.
Figure 6 (top) shows the ﬁrst few levels of the RP tree for the images of digit 1. Each node is
represented by the mean of the datapoints falling into that cell. Hence, the topmost node shows the
mean of the entire dataset; its left and the right children show the means of the points belonging to
their respective partitions, and so on. The bar underneath each node shows the fraction of points
going to the left and to the right, to give a sense of how balanced each split is. Alongside each mean,
we also show a histogram of the 20 largest eigenvalues of the covariance matrix, which reveal how
closely the data in the cell is concentrated near a low-dimensional subspace. The last bar in the
histogram is the variance unaccounted for.

Notice that most of the variance lies in a small number of directions, as might be expected. And
this rapidly becomes more pronounced as we go further down in the tree. Hence, very quickly, the
cell means become good representatives of the dataset: an experimental corroboration that RP trees
adapt to the low intrinsic dimension of the data.

This is also brought out in Figure 6 (bottom), where the images are shown projected onto the plane
deﬁned by their top two principal components. (The outer ring of images correspond to the linear
combinations of the two eigenvectors at those locations in the plane.) The left image shows how the
data was split at the topmost level (dark versus light). Observe that this random cut is actually quite
close to what the PCA split would have been, corroborating our earlier intuition (recall Figure 3).
The right image shows the same thing, but for the ﬁrst two levels of the tree: data is shown in four
colors corresponding to the four different cells.

7

1234595010001050110011501200125013001350LevelsAvg VQ Error  k−d Tree (random coord)k−d Tree (max var coord)RP TreePCA Tree12345100012001400160018002000LevelsAvg VQ Error  k−d Tree (random coord)k−d Tree (max var coord)RP TreePCA TreeFigure 6: Top: Three levels of the RP tree for MNIST digit 1. Bottom: Images projected onto the
ﬁrst two principal components. Colors represent different cells in the RP tree, after just one split
(left) or after two levels of the tree (right).

References
[1] M. Belkin and P. Niyogi. Laplacian eigenmaps for dimensionality reduction and data representation.

Neural Computation, 15(6):1373–1396, 2003.

[2] M. Belkin, P. Niyogi, and V. Sindhwani. On manifold regularization. Conference on AI and Statistics,

2005.

[3] J. Bentley. Multidimensional binary search trees used for associative searching. Communications of the

ACM, 18(9):509–517, 1975.

[4] W. Boothby. An Introduction to Differentiable Manifolds and Riemannian Geometry. Academic Press,

2003.

[5] T. M. Cover and P. E. Hart. Nearest neighbor pattern classiﬁcations. IEEE Transactions on Information

Theory, 13(1):21–27, 1967.

[6] M. Datar, N. Immorlica, P. Indyk, and V. Mirrokni. Locality sensitive hashing scheme based on p-stable

distributions. Symposium on Computational Geometry, 2004.

[7] L. Devroye, L. Gyorﬁ, and G. Lugosi. A Probabilistic Theory of Pattern Recognition. Springer, 1996.
[8] T. Liu, A. Moore, A. Gray, and K. Yang. An investigation of practical approximate nearest neighbor

algorithms. Advances in Neural Information Processing Systems, 2004.

[9] J. McNames. A fast nearest neighbor algorithm based on a principal axis search tree. IEEE Transactions

on Pattern Analysis and Machine Intelligence, 23(9):964–976, 2001.

[10] M. Raginsky and S. Lazebnik. Estimation of intrinsic dimensionality using high-rate vector quantization.

Advances in Neural Information Processing Systems, 18, 2006.

[11] S. Roweis and L. Saul. Nonlinear dimensionality reduction by locally linear embedding. Science,

290:2323–2326, 2000.

[12] J. Tenenbaum, V. de Silva, and J. Langford. A global geometric framework for nonlinear dimensionality

reduction. Science, 290(5500):2319–2323, 2000.

8

"
1072,2007,Multiple-Instance Active Learning,"In a multiple instance (MI) learning problem, instances are naturally organized into bags and it is the bags, instead of individual instances, that are labeled for training. MI learners assume that every instance in a bag labeled negative is actually negative, whereas at least one instance in a bag labeled positive is actually positive. We present a framework for active learning in the multiple-instance setting. In particular, we consider the case in which an MI learner is allowed to selectively query unlabeled instances in positive bags. This approach is well motivated in domains in which it is inexpensive to acquire bag labels and possible, but expensive, to acquire instance labels. We describe a method for learning from labels at mixed levels of granularity, and introduce two active query selection strategies motivated by the MI setting. Our experiments show that learning from instance labels can significantly improve performance of a basic MI learning algorithm in two multiple-instance domains: content-based image recognition and text classification.","Multiple-Instance Active Learning

Burr Settles Mark Craven

University of Wisconsin
Madison, WI 5713 USA

{bsettles@cs,craven@biostat}.wisc.edu

Soumya Ray

Oregon State University
Corvallis, OR 97331 USA

sray@eecs.oregonstate.edu

Abstract

We present a framework for active learning in the multiple-instance (MI) setting.
In an MI learning problem, instances are naturally organized into bags and it is
the bags, instead of individual instances, that are labeled for training. MI learners
assume that every instance in a bag labeled negative is actually negative, whereas
at least one instance in a bag labeled positive is actually positive. We consider
the particular case in which an MI learner is allowed to selectively query unla-
beled instances from positive bags. This approach is well motivated in domains
in which it is inexpensive to acquire bag labels and possible, but expensive, to
acquire instance labels. We describe a method for learning from labels at mixed
levels of granularity, and introduce two active query selection strategies moti-
vated by the MI setting. Our experiments show that learning from instance labels
can signiﬁcantly improve performance of a basic MI learning algorithm in two
multiple-instance domains: content-based image retrieval and text classiﬁcation.

1 Introduction

A limitation of supervised learning is that it requires a set of instance labels which are often difﬁcult
or expensive to obtain. The multiple-instance (MI) learning framework [3] can, in some cases, ad-
dress this handicap by relaxing the granularity at which labels are given. In the MI setting, instances
are grouped into bags (i.e., multi-sets) which may contain any number of instances. A bag is labeled
negative if and only if it contains all negative instances. A bag is labeled positive, however, if at
least one of its instances is positive. Note that positive bags may also contain negative instances.
The MI setting was formalized by Dietterich et al. in the context of drug activity prediction [3], and
has since been applied to a wide variety of tasks including content-based image retrieval [1, 6, 8],
text classiﬁcation [1, 9], stock prediction [6], and protein family modeling [10].
Figure 1 illustrates how the MI representation can be applied to (a) content-based image retrieval
(CBIR) and (b) text classiﬁcation tasks. For the CBIR task, images are represented as bags and in-
stances correspond to segmented regions of the image. A bag representing a given image is labeled
positive if the image contains some object of interest. The multiple-instance paradigm is well suited
to this task because only a few regions of an image may represent the object of interest, such as the
gold medal in Figure 1(a). An advantage of the MI representation here is that it is signiﬁcantly easier
to label an entire image than it is to label each segment. For text classiﬁcation, documents are repre-
sented as bags and instances correspond to short passages (e.g., paragraphs) in the documents. This
formulation is useful in classiﬁcation tasks for which document labels are freely available or cheaply
obtained, but the target concept is represented by only a few passages. For example, consider the
task of classifying articles according whether or not they contain information about the sub-cellular
location of proteins. The article in Figure 1(b) is labeled by the Mouse Genome Database [4] as a
citation for the protein catalase that speciﬁes its sub-cellular location. However, the text that states
this is only a short passage on the second page of the article. The MI approach is therefore com-
pelling because document labels can be cheaply obtained (say from the Mouse Genome Database),
but the labeling is not readily available at the most appropriate level of granularity (passages).

bag: image = { instances: segments }

bag: document = { instances: passages }

catalase-containing 

The 
structures 
represent  peroxisomes  as  concluded 
from the co-localization with the peroxi-
somal membrane marker, PMP70.

(a)

(b)

Figure 1: Motivating examples for multiple- instance active learning. (a) In content- based image retrieval,
images are represented as bags and instances correspond to segmented image regions. An active MI learner
may query which segments belong to the object of interest, such as the gold medal shown in this image. (b) In
text classiﬁcation, documents are bags and the instances represent passages of text. In MI active learning, the
learner may query speciﬁc passages to determine if they are representative of the positive class at hand.

The main challenge of multiple- instance learning is that, to induce an accurate model of the tar-
get concept, the learner must determine which instances in positive bags are actually positive, even
though the ratio of negatives to positives in these bags can be arbitrarily high. For many MI prob-
lems, such as the tasks illustrated in Figure 1, it is possible to obtain labels both at the bag level
and directly at the instance level. Fully labeling all instances, however, is expensive. As mentioned
above, the rationale for formulating the learning task as an MI problem is that it allows us to take
advantage of coarse labelings that may be available at low cost, or even for free. The approach
that we consider here is one that involves selectively obtaining the labels of certain instances in the
context of MI learning. In particular, we consider obtaining labels for selected instances in positive
bags, since the labels for instances in negative bags are known.
In active learning [2], the learner is allowed to ask queries about unlabeled instances. In this way, the
oracle (or human annotator) is required to label only instances that are assumed to be most valuable
for training. In the standard supervised setting, pool- based active learning typically begins with an
initial learner trained with a small set of labeled instances. Then the learner can query instances
from a large pool of unlabeled instances, re- train, and repeat. The goal is to reduce the total amount
of labeling effort required for the learner to achieve a certain level of accuracy.
We argue that whereas multiple- instance learning reduces the burden of labeling data by getting
labels at a coarse level of granularity, we may also beneﬁt from selectively labeling some part of
the training data at a ﬁner level of granularity. Hence, we explore the approach of multiple- instance
active learning as a way to efﬁciently overcome the ambiguity of the MI framework while keeping
labeling costs low.
There are several MI active learning scenarios we might consider. The ﬁrst, which is analogous to
standard supervised active learning, is simply to allow the learner to query for the labels of unlabeled
bags. A second scenario is one in which all bags in the training set are labeled and the learner is
allowed to query for the labels of selected instances from positive bags. For example, the learner
might query on particular image segments or passages of text in the CBIR and text classiﬁcation
domains, respectively. If an instance- query result is positive, the learner now has direct evidence
for the positive class.
If the query result is negative, the learner knows to focus its attention to
other instances from that bag, also reducing ambiguity. A third scenario involves querying selected
positive bags rather than instances, and obtaining labels for any (or all) instances in such bags. For
example, the learner might query a positive image in the CBIR domain, and ask the oracle to label as
many segments as desired. A ﬁnal scenario would assume that some bags are labeled and some are
not, and the learner would be able to query on (i) unlabeled bags, (ii) unlabeled instances in positive

bags, or (iii) some combination thereof. In the present work, we focus on the second formulation
above, where the learner queries selected unlabeled instances from labeled, positive bags.
The rest of this paper is organized as follows. First, we describe the algorithms we use to train
MI classiﬁers and select instance queries for active learning. Then, we describe our experiments to
evaluate these approaches on two data sets in the CBIR and text classiﬁcation domains. Finally, we
discuss the results of our experiments and offer some concluding remarks.

2 Algorithms

MI Logistic Regression. We train probabilistic models for multiple-instance tasks using a gen-
eralization of the Diverse Density framework [6]. For MI classiﬁcation, we seek the conditional
probability that the label yi is positive for bag Bi given n constituent instances: P (yi = 1|Bi =
{Bi1, Bi2, . . . , Bin}). If a classiﬁer can provide an equivalent probability P (yij = 1|Bij) for in-
stance Bij, we can use a combining function (such as softmax or noisy-or) to combine posterior
probabilities of all the instances in a bag and estimate its posterior probability P (yi = 1|Bi). The
combining function here explicitly encodes the MI assumption. If the model ﬁnds an instance likely
to be positive, the output of the combining function should ﬁnd its corresponding bag likely to be
positive as well.
In our work, we train classiﬁers using multiple-instance logistic regression (MILR) which has been
shown to be a state-of-the-art MI learning algorithm, and appears to be a competitive method for
text classiﬁcation and CBIR tasks [9]. MILR uses logistic regression with parameters θ = (w, b) to
estimate conditional probabilities for each instance:

oij = P (yij = 1|Bij) =

1

1 + e−(w·Bij +b)

.

Here Bij represents a vector of feature values representing the jth instance in the ith bag, and w
is a vector of weights associated with the features. In order to combine these class probabilities for
instances into a class probability for a bag, MILR uses the softmax function:

oi = P (yi = 1|Bi) = softmaxα(oi1, . . . , oin) =

Pn
Pn

j=1 oijeαoij
j=1 eαoij

,

where α is a constant that determines the extent to which softmax approximates a hard max function.
In the general MI setting we do not know the labels of instances in positive bags. Because the equa-
P
tions above represent smooth functions of the model parameters θ, however, we can learn parameter
values using a gradient-based optimization method and an appropriate objective function. In the
i(yi − oi)2, where yi ∈ {0, 1}
present work, we minimize squared error over the bags E(θ) = 1
2
is the known label of bag Bi. While we describe our MI active learning methods below in terms
of this formulation of MILR, it is important to note that they generalize to any classiﬁer that out-
puts instance-level probabilities used with differentiable combining and objective functions. Diverse
Density [6], for example, couples a Gaussian instance model with a noisy-or combining function.
Learning from Labels at Mixed Granularities. Suppose our active MI learner queries instance
Bij and the corresponding instance label yij is provided by the oracle. We would like to include a
direct training signal for this instance in the optimization procedure above. However, E(θ) is deﬁned
in terms of bag-level error, not instance-level error. Consider, though, that in MI learning a labeled
instance is effectively the same as a labeled bag that contains only that instance. So when the label
for instance Bij is known, we transform the training set for each query by adding a new training tuple
h{Bij}, yiji, where {Bij} is a new singleton bag containing only a copy of the queried instance,
and yij is the corresponding label. A copy of the query instance Bij also remains in the original bag
Bi, enabling the learner to compute the remaining instance gradients as described below.
Since the objective function will guide the learner toward classifying the singleton query instance
Bij in the positive tuple h{Bij}, 1i as positive, it will tend to classify the original bag Bi positive as
well. Conversely, if we add the negative tuple h{Bij}, 0i, the learner will tend to classify the instance

negative in the original bag, which will affect the other instance gradients via the combining function
and guides the learner to focus on other potentially positive instances in that bag.
It may seem that this effect on the original bag could be achieved by clamping the instance output oij
to yij during training, but this has the undesirable property of eliminating the training signal for the
bag and the instance. If yij = 1, the combining function output would be extremely high, making
bag error nearly zero, thus minimizing the objective function without any actual parameter updates.
If yij = 0, the instance would output nothing to the combining function, thus the learner would get
no training signal for this instance (though in this case the learner can still focus on other instances
in the bag). It is possible to combine clamped instance outputs with our singleton bag approach to
overcome this problem, but our experiments indicate that this has no practical advantage over adding
singleton bags alone.
Also note that simply adding singleton bags will alter the objective function by adding weight, albeit
indirectly, to bags that have been queried more often. To control this effect, we uniformly weight
each bag and all its queried singleton bags to sum to 1 when computing the value and gradient for
the objective function during training. For example, an unqueried bag has weight 1, a bag with one
instance query and its derived singleton bag each have weight 0.5, and so on.
Uncertainty Sampling. Now we turn our attention to strategies for selecting query instances for
labeling. A common approach to active learning in the standard supervised setting is uncertainty
sampling [5]. For probabilistic classiﬁers, this involves applying the classiﬁer to each unlabeled
instance and querying those with most uncertainty about the class label. Recall that the learned
model estimates oij = P (yij = 1|Bij), the probability that instance Bij is positive. We represent
the uncertainty U(Bij) by the Gini measure:

U(Bij) = 2oij(1 − oij).

Note that the particular measure we use here is not critical; the important properties are that its
minima are at zero and one, its maximum is at 0.5, and it is symmetric about 0.5.
MI Uncertainty (MIU). We argue that when doing active learning in a multiple-instance setting,
the selection criterion should take into account not just uncertainty about a given instance’s class
label, but also the extent to which the learner can adequately “explain” the bag to which the instance
belongs. For example, the instance that the learner ﬁnds most uncertain may belong to the same
bag as the instance it ﬁnds most positive. In this case, the learned model will have a high value of
P (yi = 1|Bi) for the bag because the value computed by the combining function will be dominated
by the output of the positive-looking instance. We propose an uncertainty-based query strategy that
weights the uncertainty of Bij in terms of how much it contributes to the classiﬁcation of bag Bi.
As such, we deﬁne the MI Uncertainty (MIU) of an instance to be the derivative of bag output with
respect to instance output (i.e., the derivative of the softmax combining function) times instance
uncertainty:

M IU(Bij) = ∂oi
∂oij

U(Bij).

Expected Gradient Length (EGL). Another query strategy we consider is to identify the instance
that would impart the greatest change to the current model if we knew its label. Since we train
MILR with gradient descent, this involves querying the instance which, if h{Bij}, yiji is added to
the training set, would create the greatest change in the gradient of the objective function (i.e., the
largest gradient vector used to re-estimate values for θ). Let ∇E(θ) be the gradient of E with respect
to θ, which is a vector whose components are the partial derivatives of E with respect to each model
parameter: ∇E(θ) = [ ∂E
ij(θ) be the new gradient obtained by adding the positive tuple h{Bij}, 1i to the training
Now let ∇E+
set, and likewise let ∇E−
ij(θ) be the new gradient if a query results in the negative tuple h{Bij}, 0i
being added. Since we do not know which label the oracle will provide in advance, we instead
calculate the expected length of the gradient based on the learner’s current belief oij in each outcome.
More precisely, we deﬁne the Expected Gradient Length (EGL) to be:
ij(θ)k + (1 − oij)k∇E−

EGL(Bij) = oijk∇E+

ij(θ)k.

∂θ1

, ∂E
∂θ2

, . . . , ∂E
∂θm

].

Note that this selection strategy does not explicitly encode the MI bias. Instead, it employs class
probabilities to determine the expected label for candidate queries, with the goal of maximizing
parameter changes to what happens to be an MI learning algorithm. This strategy can be generalized
to query for other properties in non-MI active learning as well. For example, Zhu et al. [11] use
a related approach to determine the expected label of candidate query instances when combining
active learning with graph-based semi-supervised learning. Rather than trying to maximize the
expected change in the learning model, however, they select for the expected reduction in estimated
error over unlabeled instances.

3 Data and Experiments

Since no MI data sets with instance-level labels previously existed, we augmented an existing MI
data set by manually adding instance labels. SIVAL1 is a collection for content-based image retrieval
that includes 1500 images, each labeled with one of 25 class labels. The images contain complex
objects photographed in a variety of positions, orientations, locations, and lighting conditions. The
images (bags) have been transformed and segmented into approximately 30 segments (instances)
each. Each segment is represented by a 30-dimensional feature vector describing color and texture
attributes of the segment and its neighbors. For more details, see Rahmani & Goldman [8]. We
modiﬁed the collection by manually annotating the instance segments that belong to the labeled
object for each image using a graphical interface we developed.
We also created a semi-synthetic MI data set for text classiﬁcation, using the 20 Newsgroups2 corpus
as a base. This corpus was chosen because it is an established benchmark for text classiﬁcation,
and because the source texts—newsnet posts from the early 1990s—are relatively short (in the MI
setting, instances are usually paragraphs or short passages [1, 9]). For each of the 20 news categories,
we generate artiﬁcial bags of approximately 50 posts (instances) each by randomly sampling from
the target class (i.e., newsgroup category) at a rate of 3% for positive bags, with remaining instances
(and all instances for negative bags) drawn uniformly from the other classes. The texts are processed
with stemming, stop-word removal, and information-gain ranked feature selection. The TFIDF
values of the top 200 features are used to represent the instance texts. We construct a data set of 100
bags (50 positives and 50 negatives) for each class.
We compare our MI Uncertainty (MIU) and Expected Gradient Length (EGL) selection strategies
from Section 2 against two baselines: Uncertainty (using only the instance-model’s uncertainty), and
instances chosen uniformly at Random from positive bags (to evaluate the advantage of “passively”
labeling instances). The MILR model uses α = 2.5 for the softmax function and is trained by
minimizing squared loss via L-BFGS [7]. The instance-labeled MI data sets and MI learning source
code used in these experiments are available online3.
We evaluate our methods by constructing learning curves that plot the area under the ROC curve
(AUROC) as a function of instances queried for each data set and selection strategy. The initial point
in all experiments is the AUROC for a model trained on labeled bags from the training set without
any instance queries. Following previous work on the CBIR problem [8], we average results for
SIVAL over 20 independent runs for each image class, where the learner begins with 20 randomly
drawn positive bags (from which instances may be queried) and 20 random negative bags. The
model is then evaluated on the remainder of the unlabeled bags, and labeled query instances are
added to the training set in batches of size q = 2. For 20 Newsgroups, we average results using
10-fold cross-validation for each newsgroup category, using a query batch size of q = 5.
Due to lack of space, we cannot show learning curves for every task. Figure 2 shows three represen-
tative learning curves for each of the two data sets. In Table 1 we summarize all curves by reporting
the average improvement made by each query selection strategy over the initial MILR model (before
any instance queries) for various points along the learning curve. Table 2 presents a more detailed
comparison of the initial model against each query selection method at a ﬁxed point early on in
active learning (10 query batches).

1http://www.cs.wustl.edu/accio/
2http://people.csail.mit.edu/jrennie/20Newsgroups/
3http://pages.cs.wisc.edu/˜bsettles/amil/

Figure 2: Sample learning curves from SIVAL (top row) and 20 Newsgroups (bottom row) tasks.

Table 1: Summary of learning curves. The average AUROC improvement over the initial MI model (before
any instance queries) is reported for each selection strategy. Numbers are averaged across all tasks in each data
set at various points during active learning. The winning algorithm at each point is indicated with a box.

Instance
Queries

Random Uncert.

SIVAL Tasks
EGL

10
20
50
80
100

+0.023
+0.033
+0.057
+0.065
+0.068

+0.043
+0.065
+0.084
+0.088
+0.092

+0.039
+0.063
+0.085
+0.093
+0.095

MIU

+0.050
+0.070
+0.087
+0.090
+0.090

4 Discussion of Results

20 Newsgroups Tasks

Random Uncert.

EGL

-0.001
-0.002
+0.002
+0.003
+0.008

+0.002
+0.015
+0.046
+0.052
+0.055

+0.002
+0.015
+0.045
+0.056
+0.055

MIU

+0.009
+0.029
+0.051
+0.056
+0.058

We can draw several interesting conclusions from these results. First and most germane to MI
active learning is that MI learners beneﬁt from instance-level labels. With the exception of random
selection on 20 Newsgroups data, instance-level labels almost always improve the accuracy of the
learner, often with statistical signiﬁcance after only a few queries.
Second, we see that active query strategies (e.g., Uncertainty, EGL, and MIU) perform better than
passive (random) instance labeling. On SIVAL tasks, random querying steadily improves accuracy,
but very slowly. As Table 1 shows, random selection at 100 queries fails to be competitive with the
three active query strategies after half as many queries. On 20 Newsgroups tasks, random selection
has a slight negative effect (if any) early on, possibly because it lacks a focused search for positive
instances (of which there are only one or two per bag). All three active selection methods, on the
other hand, show signiﬁcant gains fairly quickly on both data sets.
Finally, MIU appears to be a well-suited query strategy for this formulation of MI active learning.
On both data sets, it consistently improves the initial MI learner, usually with statistical signiﬁcance,
and often approaches the asymptotic level of accuracy with fewer labeled instances than the other
two active methods. Uncertainty and EGL seem to perform quite comparably, with EGL performing
slightly better between the two. MIU’s gains over these other query strategies are not usually statis-
tically signiﬁcant, however, and in the long run it is generally matched or slightly surpassed by them.
MIU shows the greatest advantage early in the active instance-querying process, perhaps because it
is the only method we tested that explicitly encodes the MI assumption by taking advantage of the
combining function in its estimation of value to the learner.

 0.5 0.6 0.7 0.8 0.9 1 0 20 40 60 80 100 120 140AUROCNumber of Instance QueriesMIUEGLUncertaintyRandom 0.5 0.6 0.7 0.8 0.9 1 0 20 40 60 80 100 120 140Number of Instance QueriesMIUEGLUncertaintyRandom 0.5 0.6 0.7 0.8 0.9 1 0 20 40 60 80 100 120 140Number of Instance QueriesMIUEGLUncertaintyRandomrec.autossci.crypttalk.politics.misc 0.5 0.6 0.7 0.8 0.9 1 0 10 20 30 40 50AUROCMIUEGLUncertaintyRandomwd40can 0.5 0.6 0.7 0.8 0.9 1 0 10 20 30 40 50MIUEGLUncertaintyRandomtranslucentbowl 0.5 0.6 0.7 0.8 0.9 1 0 10 20 30 40 50MIUEGLUncertaintyRandomspritecanTable 2: Detailed comparison of the initial MI learner against various query strategies after 10 query batches
(20 instances for SIVAL, 50 instances for 20 Newsgroups). Average AUROC values are shown for each al-
gorithm on each task. Statistically signiﬁcant gains over the initial learner (using a two-tailed t-test at 95%)
are shown in bold. The winning algorithm for each task is indicated with a box, and a tally of wins for each
algorithm is reported below each column.

Task

Initial

Random Uncert.

EGL

MIU

ajaxorange
apple
banana
bluescrunge
candlewithholder
cardboardbox
checkeredscarf
cokecan
dataminingbook
dirtyrunningshoe
dirtyworkgloves
fabricsoftenerbox
feltﬂowerrug
glazedwoodpot
goldmedal
greenteabox
juliespot
largespoon
rapbook
smileyfacedoll
spritecan
stripednotebook
translucentbowl
wd40can
woodrollingpin

alt.atheism
comp.graphics
comp.os.ms-windows.misc
comp.sys.ibm.pc.hardware
comp.sys.mac.hardware
comp.windows.x
misc.forsale
rec.autos
rec.motorcycles
rec.sport.baseball
rec.sport.hockey
sci.crypt
sci.electronics
sci.med
sci.space
soc.religion.christian
talk.politics.guns
talk.politics.mideast
talk.politics.misc
talk.religion.misc

0.547
0.431
0.440
0.410
0.623
0.430
0.662
0.668
0.445
0.620
0.455
0.417
0.743
0.444
0.496
0.563
0.479
0.436
0.478
0.556
0.670
0.477
0.548
0.599
0.416

0.812
0.720
0.772
0.716
0.716
0.835
0.769
0.768
0.844
0.838
0.918
0.770
0.719
0.827
0.822
0.768
0.847
0.791
0.789
0.759

TOTAL NUMBER OF WINS

4

0.564
0.418
0.463
0.426
0.662
0.437
0.749
0.727
0.480
0.701
0.497
0.534
0.754
0.464
0.544
0.595
0.490
0.403
0.455
0.612
0.711
0.478
0.614
0.658
0.435

0.836
0.690
0.768
0.690
0.728
0.827
0.748
0.785
0.844
0.846
0.918
0.770
0.751
0.819
0.824
0.780
0.855
0.793
0.797
0.773

3

0.633
0.469
0.514
0.508
0.646
0.451
0.765
0.693
0.505
0.703
0.491
0.617
0.794
0.528
0.622
0.614
0.571
0.406
0.463
0.675
0.749
0.486
0.678
0.687
0.420

0.863
0.789
0.764
0.687
0.861
0.888
0.758
0.872
0.871
0.871
0.966
0.887
0.731
0.837
0.901
0.769
0.860
0.874
0.878
0.785

0.638
0.455
0.511
0.470
0.656
0.442
0.772
0.713
0.522
0.697
0.496
0.594
0.799
0.515
0.602
0.619
0.580
0.394
0.454
0.640
0.746
0.519
0.665
0.700
0.426

0.839
0.783
0.742
0.694
0.855
0.894
0.777
0.872
0.879
0.869
0.962
0.893
0.733
0.845
0.905
0.771
0.870
0.880
0.866
0.773

0.627
0.459
0.507
0.491
0.677
0.454
0.765
0.736
0.519
0.708
0.497
0.634
0.792
0.526
0.605
0.639
0.564
0.408
0.457
0.655
0.750
0.489
0.702
0.707
0.429

0.877
0.819
0.714
0.707
0.878
0.882
0.771
0.860
0.883
0.899
0.964
0.913
0.725
0.862
0.893
0.789
0.858
0.876
0.856
0.793

9

12

19

It is also interesting to note that in an earlier version of our learning algorithm, we did not normalize
weights for bags and instance-query singleton bags when learning with labels at mixed granularities.
Instead, all such bags were weighted equally and the objective function was slightly altered. In
those experiments, MIU’s accuracy was roughly equivalent to the ﬁgures reported here, although
the improvement for all other query strategies (especially random selection) were lower.

5 Conclusion

We have presented multiple-instance active learning, a novel framework for reducing the labeling
burden by obtaining labels at a coarse granularity, and then selectively labeling at ﬁner levels. This
approach is useful when bag labels are easily acquired, and instance labels can be obtained but are
expensive. In the present work, we explored the case where an MI learner may query unlabeled
instances from positively labeled bags in order reduce the inherent ambiguity of the MI representa-
tion, while keeping label costs low. We also described a simple method for learning from labels at
both the bag-level and instance-level, and showed that querying instance-level labels through active
learning is beneﬁcial in content-based image retrieval and text categorization problems. In addition,
we introduced two active query selection strategies motivated by this work, MI Uncertainty and
Expected Gradient Length, and demonstrated that they are well-suited to MI active learning.
In future work, we plan to investigate the other MI active learning scenarios mentioned in Section 1.
Of particular interest is the setting where, initially, some bags are labeled and others are not, and
the learner is allowed to query on (i) unlabeled bags, (ii) unlabeled instances from positively labeled
bags, or (iii) some combination thereof. We also plan to investigate other selection methods for
different query formats, such as “label any or all positive instances in this bag,” which may be more
natural for some MI learning problems.

Acknowledgments

This research was supported by NSF grant IIS-0093016 and NIH grants T15-LM07359 and R01-
LM07050-05.

References
[1] S. Andrews, I. Tsochantaridis, and T. Hofmann. Support vector machines for multiple-instance learning.

In Advances in Neural Information Processing Systems (NIPS), pages 561–568. MIT Press, 2003.

[2] D. Cohn, L. Atlas, and R. Ladner. Improving generalization with active learning. Machine Learning,

15(2):201–221, 1994.

[3] T. Dietterich, R. Lathrop, and T. Lozano-Perez. Solving the multiple-instance problem with axis-parallel

rectangles. Artiﬁcial Intelligence, 89:31–71, 1997.

[4] J.T. Eppig, C.J. Bult, J.A. Kadin, J.E. Richardson, J.A. Blake, and the members of the Mouse Genome
Database Group. The Mouse Genome Database (MGD): from genes to mice–a community resource for
mouse biology. Nucleic Acids Research, 33:D471–D475, 2005. http://www.informatics.jax.org.

[5] D. Lewis and J. Catlett. Heterogeneous uncertainty sampling for supervised learning. In Proceedings of

the International Conference on Machine Learning (ICML), pages 148–156. Morgan Kaufmann, 1994.

[6] O. Maron and T. Lozano-Perez. A framework for multiple-instance learning.

Information Processing Systems (NIPS), pages 570–576. MIT Press, 1998.

In Advances in Neural

[7] J. Nocedal and S.J. Wright. Numerical Optimization. Springer, 1999.
[8] R. Rahmani and S.A. Goldman. MISSL: Multiple-instance semi-supervised learning. In Proceedings of

the International Conference on Machine Learning (ICML), pages 705–712. ACM Press, 2006.

[9] S. Ray and M. Craven. Supervised versus multiple instance learning: An empirical comparison.

In
Proceedings of the International Conference on Machine Learning (ICML), pages 697–704. ACM Press,
2005.

[10] Q. Tao, S.D. Scott, and N.V. Vinodchandran. SVM-based generalized multiple-instance learning via
approximate box counting. In Proceedings of the International Conference on Machine Learning (ICML),
pages 779–806. Morgan Kaufmann, 2004.

[11] X. Zhu, J. Lafferty, and Z. Ghahramani. Combining active learning and semi-supervised learning using
gaussian ﬁelds and harmonic functions. In Proceedings of the ICML Workshop on the Continuum from
Labeled to Unlabeled Data, pages 58–65, 2003.

"
637,2007,Mining Internet-Scale Software Repositories,"Large repositories of source code create new challenges and opportunities for statistical machine learning. Here we first develop an infrastructure for the automated crawling, parsing, and database storage of open source software. The infrastructure allows us to gather Internet-scale source code. For instance, in one experiment, we gather 4,632 java projects from SourceForge and Apache totaling over 38 million lines of code from 9,250 developers. Simple statistical analyses of the data first reveal robust power-law behavior for package, SLOC, and method call distributions. We then develop and apply unsupervised author-topic, probabilistic models to automatically discover the topics embedded in the code and extract topic-word and author-topic distributions. In addition to serving as a convenient summary for program function and developer activities, these and other related distributions provide a statistical and information-theoretic basis for quantifying and analyzing developer similarity and competence, topic scattering, and document tangling, with direct applications to software engineering. Finally, by combining software textual content with structural information captured by our CodeRank approach, we are able to significantly improve software retrieval performance, increasing the AUC metric to 0.86-- roughly 10-30% better than previous approaches based on text alone.","Mining Internet-Scale Software Repositories

Erik Linstead, Paul Rigor, Sushil Bajracharya, Cristina Lopes and Pierre Baldi

Donald Bren School of Information and Computer Science

University of California, Irvine

Irvine, CA 92697-3435

{elinstea,prigor,sbajrach,lopes,pfbaldi}@ics.uci.edu

Abstract

Large repositories of source code create new challenges and opportunities for sta-
tistical machine learning. Here we ﬁrst develop Sourcerer, an infrastructure for
the automated crawling, parsing, and database storage of open source software.
Sourcerer allows us to gather Internet-scale source code. For instance, in one ex-
periment, we gather 4,632 java projects from SourceForge and Apache totaling
over 38 million lines of code from 9,250 developers. Simple statistical analyses
of the data ﬁrst reveal robust power-law behavior for package, SLOC, and lexical
containment distributions. We then develop and apply unsupervised author-topic,
probabilistic models to automatically discover the topics embedded in the code
and extract topic-word and author-topic distributions. In addition to serving as
a convenient summary for program function and developer activities, these and
other related distributions provide a statistical and information-theoretic basis for
quantifying and analyzing developer similarity and competence, topic scattering,
and document tangling, with direct applications to software engineering. Finally,
by combining software textual content with structural information captured by our
CodeRank approach, we are able to signiﬁcantly improve software retrieval per-
formance, increasing the AUC metric to 0.84– roughly 10-30% better than pre-
vious approaches based on text alone. Supplementary material may be found at:
http://sourcerer.ics.uci.edu/nips2007/nips07.html.

1 Introduction

Large repositories of private or public software source code, such as the open source projects avail-
able on the Internet, create considerable new opportunities and challenges for statistical machine
learning, information retrieval, and software engineering. Mining such repositories is important, for
instance, to understand software structure, function, complexity, and evolution, as well as to improve
software information retrieval systems and identify relationships between humans and the software
they produce. Tools to mine source code for functionality, structural organization, team structure,
and developer contributions are also of interest to private industry, where these tools can be applied
to such problems as in-house code reuse and project stafﬁng. While some progress has been made
in the application of statistics and machine learning techniques to mine software corpora, empirical
studies have typically been limited to small collections of projects, often on the order of one hundred
projects or less, several orders of magnitude smaller than publicly available repositories(eg. [1]).
Mining large software repositories requires leveraging both the textual and structural aspects of soft-
ware data, as well as any relevant meta data. Here we develop Sourcerer, a large-scale infrastructure
to explore such aspects. We ﬁrst identify a number of robust power-law behaviors by simple statisti-
cal analyses. We then develop and apply unsupervised author-topic probabilistic models to discover
the topics embedded in the code and extract topic-word and author-topic distributions. Finally, we
leverage the dual textual and graphical nature of software to improve code search and retrieval.

2

Infrastructure and Data

To allow for the Internet-scale analysis of source code we have built Sourcerer, an extensive infras-
tructure designed for the automated crawling, downloading, parsing, organization, and storage of
large software repositories in a relational database. A highly conﬁgurable crawler allows us to spec-
ify the number and types of projects desired, as well as the host databases that should be targeted,
and to proceed with incremental updates in an automated fashion. Once target projects are down-
loaded, a depackaging module uncompresses archive ﬁles while saving useful metadata (project
name, version, etc). While the infrastructure is general, we apply it here to a sample of projects
in Java. Speciﬁcally, for the results reported, we download 12,151 projects from Sourceforge and
Apache and ﬁlter out distributions packaged without source code (binaries only). The end result is
a repository consisting of 4,632 projects, containing 244,342 source ﬁles, with 38.7 million lines
of code, written by 9,250 developers. For the software author-topic modeling approach we also
employ the Eclipse 3.0 source code as a baseline. Though only a single project, Eclipse is a large,
active open source effort that has been widely studied. In this case, we consider 2,119 source ﬁles,
associated with about 700,000 lines of code, a vocabulary of 15,391 words, and 59 programmers.
Methods for extracting and assigning words and programmers to documents are described in the
next sections. A complete list of all the projects contained in our repository is available from the
supplementary materials web pages.

3 Statistical Analysis

During the parsing process our system performs a static analysis on project source code ﬁles to
extract code entities and their relationships, storing them in a relational database. For java these en-
tities consist of packages, classes, interfaces, methods, and ﬁelds, as well as more speciﬁc constructs
such as constructors and static initializers. Relations capture method calls, inheritance, and encap-
sulation, to name a few. The populated database represents a substantial foundation on which to
base statistical analysis of source code. Parsing the multi-project repository described above yields
a repository of over 5 million entities organized into 48 thousand packages, 560 thousand classes,
and 3.2 million methods, participating in over 23.4 million relations. By leveraging the query capa-
bilities of the underlying database we can investigate other interesting statistics. For example, table
1 contains the frequencies of Java keywords across all 4,632 projects. Upon examining this data we
can see that the ’default’ keyword occurs about 6 percent less frequently than the ’switch’ keyword,
despite the fact that best practice typically mandates all switch statements contain a default block.
Moreover, the ’for’ loop is about twice as pervasive as the ’while’ loop, suggesting that the bound on
the number of iterations is more likely to be known or based on the size of a known data structure.

Table 1: Frequency of java keyword occurrence

Percentage Keyword
boolean

Keyword

public

if
new
return
import

int
null
void
private
static
ﬁnal
else
throws

12.53
8.44
8.39
7.69
6.89
6.54
5.52
4.94
3.66
3.16
3.01
2.33
2.16

Percentage

2.12
1.69
1.60
1.60
1.36
1.33
1.33
1.22
1.22
1.16
0.96
0.93
0.89

Keyword

this
break
while
super

instanceof

double
long

implements

char
ﬂoat

abstract

synchronized

short

false
case
true
class

protected

catch
for
try
throw
package

byte

extends

Percentage Keyword
switch
interface
continue
ﬁnally
default
native
transient

do

assert
enum
volatile
strictfp

0.89
0.85
0.63
0.57
0.56
0.55
0.54
0.43
0.30
0.28
0.25
0.25
0.20

Percentage

0.19
0.17
0.15
0.14
0.13
0.08
0.06
0.05
0.03
0.02
0.04

2.49E-06

Finally, statistical analyses of distributions also identify several power-law distributions. We have
observed power-law distributions governing package, SLOC, and inside relation (lexical contain-

ment) counts. For instance, Figure 1 shows the log-log plots for the number of packages across
projects. Similar graphs for other distributions are available from the supplemental materials page.

Figure 1: Approximate power-law distribution for packages over projects

4 Topic and Author-Topic Probabilistic Modeling of Source Code

Automated topic and author-topic modeling have been successfully used in text mining and infor-
mation retrieval where they have been applied, for instance, to the problem of summarizing large
text corpora. Recent techniques include Latent Dirichlet Allocation (LDA), which probabilistically
models text documents as mixtures of latent topics, where topics correspond to key concepts pre-
sented in the corpus [2] (see also [3]). Author-Topic (AT) modeling is an extension of topic modeling
that captures the relationship of authors to topics in addition to extracting the topics themselves. An
extension of LDA to probabilistic AT modeling has been developed in [4]. In the literature [5],
these more recent approaches have been found to produce better results than more traditional meth-
ods such as latent semantic analysis (LSA) [6]. Despite previous work in classifying code based
on concepts [1], applications of LDA and AT models have been limited to traditional text corpora
such as academic publications, news reports, corporate emails, and historical documents [7, 8]. At
the most basic level, however, a code repository can be viewed as a text corpus, where source ﬁles
are analogous to documents and developers to authors. Though vocabulary, syntax, and conventions
differentiate a programming language from a natural language, the tokens present in a source ﬁle
are still indicative of its function (ie. its topics). Thus here we develop and apply probabilistic AT
models to software data.
In AT models for text, the data consists of a set of documents. The authors of each documents are
known and each document is treated as a bag of words. We let A be the total number of authors, W
the total number of distinct words (vocabulary size), and T the total number of topics present in the
documents. While non-parametric Bayesian [9] and other [10] methods exist to try to infer T from
the data, here we assume that T is ﬁxed (e.g. T = 100), though we explore different values.
As in [7], our model assumes that each topic t is associated with a multinomial distribution φ•t over
words w, and each author a is associated with a multinomial distribution θ•a over topics. More
precisely, the parameters are given by two matrices: a T × A matrix Θ = (θta) of author-topic
distributions, and a W × T matrix Φ = (φwt) of topic-word distributions. Given a document d
containing Nd words with known authors, in generative mode each word is assigned to one of the
authors a of the document uniformly, then the corresponding θ•a is sampled to derive a topic t, and
ﬁnally the corresponding φ•t is sampled to derive a word w. A fully Bayesian model is derived by
putting symmetric Dirichlet priors with hyperparameters α and β over the distributions θ•a and φ•t.
So for instance the prior on θ•a is given by

and similarly for φ•t. If A is the set of authors of the corpus and document d has Ad authors, it is
easy to see that under these assumptions the likelihood of a document is given by:

Dα(θ•a) =

Γ(T α)
(Γ(α))T

θα−1

ta

T(cid:89)

t=1

P (d|Θ, Φ,A) =

φwitθta

Nd(cid:89)

(cid:88)

T(cid:88)

1
Ad

i=1

a

t=1

100101102103104100101102103Number of PackagesRankDistribution of Packages over Projectswhich can be integrated over φ and θ and their Dirichlet distributions to get P (d|α, β,A). The
posterior can be sampled efﬁciently using Markov Chain Monte Carlo Methods (Gibbs sampling)
and, for instance, the Θ and Φ parameter matrices can be estimated by MAP or MPE methods.
Once the data is obtained, applying this basic AT model to software requires the development of
several tools to facilitate the processing and modeling of source code. In addition to the crawling
infrastructure described above, the primary functions of the remaining tools are to extract and resolve
author names from source code, as well as convert the source code to the bag-of-words format.

4.1

Information Extraction from Source Code

Author-Document: The author-document matrix is produced from the output of our author extrac-
tion tool. It is a binary matrix where entry [i,j]=1 if author i contributed to document j, and 0
otherwise. Extracting author information is ultimately a matter of tokenizing the code and associat-
ing developer names with ﬁle (document) names when this information is available. This process is
further simpliﬁed for java software due to the prevalence of javadoc tags which present this metadata
in the form of attribute-value pairs.
Exploratory analysis of the Eclipse 3.0 code base, however, shows that most source ﬁles are credited
to “The IBM Corporation” rather than speciﬁc developers. Thus, to generate a list of authors for
speciﬁc source ﬁles, we parsed the Eclipse bug data available in [11]. After pruning ﬁles not
associated with any author, this input dataset consists of 2,119 Java source ﬁles, comprising 700,000
lines of code, from a total of 59 developers.
While leveraging bug data is convenient (and necessary) to generate the developer list for Eclipse
3.0, it is also desirable to develop a more ﬂexible approach that uses only the source code itself,
and not other data sources. Thus to extract author names from source code we also develop a
lightweight parser that examines the code for javadoc ’@author’ tags, as well as free form labels such
as ’author’ and ’developer.’ Occurrences of these labels are used to isolate and identify developer
names. Ultimately author identiﬁers may come in the form of full names, email addresses, url’s,
or CVS account names. This multitude of formats, combined with the fact that author names are
typically labeled in the code header, is key to our decision to extract developer names using our own
parsing utilities, rather than part-of-speech taggers [12] leveraged in other text mining projects.
A further complication for author name extraction is the fact that the same developer may write
his name in several different ways. For example, “John Q. Developer” alternates between “John
Developer,” “J. Q. Developer,” or simply “Developer.” To account for this effect, we implement
also a two-tiered approach to name resolution using the q-gram algorithm [13]. When an individual
project is parsed, a list of contributing developers (and the ﬁles they modiﬁed) is created. A pairwise
comparison of author-names is then performed using q-gram similarity, and pairs of names whose
similarity is greater than a threshold t1 are merged. This process continues until all pairwise simi-
larities are below the threshold, and the project list is then added to a global list of authors. When
parsing is complete for all projects, the global author list is resolved using the same process, but
with a new threshold, t2, such that t2 > t1. This approach effectively implements more conser-
vative name resolution across projects in light of the observation that the scope of most developer
activities is limited to a relatively small number (1 in many cases) of open source efforts. In prac-
tice, we set t1 = .65 and t2 = .75. Running our parser on the multi-project repository yields 9,250
distinct authors respectively.
Word-Document: To produce the word-document matrix for our input data we have developed a
comprehensive tokenization tool tuned to the Java programming language. This tokenizer includes
language-speciﬁc heuristics that follow the commonly practiced naming conventions. For example,
the Java class name “QuickSort” will generate the words “quick” and “sort”. All punctuation is
ignored. As an important step in processing source ﬁles our tool removes commonly occurring stop
words. We augment a standard list of stop words used for the English language (e.g. and, the, but,
etc) to include the names of all classes from the Java SDK (eg. ArrayList, HashMap, etc). This is
done to speciﬁcally avoid extracting common topics relating to the Java collections framework.We
run the LDA-based AT algorithm on the input matrices and set the total number of topics (100)
and the number of iterations by experimentation. For instance, the number of iterations, i, to run the
algorithm is determined empirically by analyzing results for i ranging from 500 to several thousands.
The results presented in the next section are derived using 3,000 iterations, which were found to

produce interpretable topics in a reasonable amount of time (a week or so). Because the algorithm
contains a stochastic component we also veriﬁed the stability of the results across multiple runs.

4.2 Topic and Author-Topic Modeling Results

A representative subset of 6 topics extracted via Author-Topic modeling on the selected 2,119 source
ﬁles from Eclipse 3.0 is given in Table 2. Each topic is described by several words associated with
the topic concept. To the right of each topic is a list of the most likely authors for each topic with
their probabilities. Examining the topic column of the table it is clear that various functions of the
Eclipse framework are represented. For example, topic 1 clearly corresponds to unit testing, topic
2 to debugging, topic 4 to building projects, and topic 6 to automated code completion. Remaining
topics range from package browsing to compiler options.

Table 2: Representative topics and authors from Eclipse 3.0

#

1

2

3

Topic
junit
run

listener
item
suite
target
source
debug

breakpoint
location

ast

button
cplist
entries
astnode

Author Probabilities

egamma 0.97065
wmelhem 0.01057

darin 0.00373

krbarnes 0.00144
kkolosow 0.00129
jaburns 0.96894
darin 0.02101

lbourlier 0.00168
darins 0.00113
jburns 0.00106
maeschli 0.99161
mkeller 0.00097
othomann 0.00055
tmaeder 0.00055
teicher 0.00046

#

4

5

6

Topic
nls-1
ant

manager
listener
classpath

type
length
names
match
methods
token

completion

current
identiﬁer

assist

Author Probabilities

darins 0.99572
dmegert 0.00044

nick 0.00044

kkolosow 0.00036
maeschli 0.00031
kjohnson 0.59508
jlanneluc 0.32046

darin 0.02286
johna 0.00932
pmulet 0.00918
daudel 0.99014
teicher 0.00308
jlanneluc 0.00155
twatson 0.00084
dmegert 0.00046

Table 3 presents 6 representative author-topic assignments from the multi-project repository. This
dataset yields a substantial increase in topic diversity. Topics representing major sub-domains of
software development are clearly represented, with the ﬁrst topic corresponding to web applica-
tions, the second to databases, the third to network applications, and the fourth to ﬁle processing.
Topics 5 and 6 are especially interesting, as they correspond to common examples of crosscutting
concerns from aspect-oriented programming [14], namely security and logging. Topic 5 is also
demonstrative of the inherent difﬁculty of resolving author names, and the shortcomings of the q-
gram algorithm, as the developer “gert van ham” and the developer “hamgert” are most likely the
same person documenting their name in different ways.
Several trends reveal themselves when all results are considered. Though the majority of topics
can be intuitively mapped to their corresponding domains, some topics are too noisy to be able to
associate any functional description to them. For example, one topic extracted from our repository
consists of Spanish words unrelated to software engineering which seem to represent the subset
of source ﬁles with comments in Spanish. Other topics appear to be very project speciﬁc, and
while they may indeed describe a function of code, they are not easily understood by those who
are only casually familiar with the software artifacts in the codebase. This is especially true with
Eclipse, which is limited in both the number and diversity of source ﬁles. In general noise appears to
diminish as repository size grows. Noise can be controlled to some degree with tuning the number
of topics to be extracted, but of course can not be eliminated completely.
Examining the author assignments (and probabilities) for the various topics provides a simple means
by which to discover developer contributions and infer their competencies. It should come as no
surprise that the most probable developer assigned to the JUnit framework topic is “egamma”, or
Erich Gamma.
In this case, there is a 97% chance that any source ﬁle in our dataset assigned
to this topic will have him as a contributor. Based on this rather high probability, we can also
infer that he is likely to have extensive knowledge of this topic. This is of course a particularly

Table 3: Representative topics and authors from the multi-project repository

#

1

2

3

Topic
servlet
session
response
request

http
sql

column

jdbc
type
result
packet
type
session

snmpwalkmv

address

Author Probabilities

craig r mcclanahan 0.19147
remy maucherat 0.08301
peter rossbach 0.04760
greg wilkins 0.04251

amy roh 0.03100

mark matthews 0.33265

ames 0.02640

mike bowler 0.02033

manuel laﬂamme 0.02027

gavin king 0.01813
brian weaver 0.14015

apache directory project 0.10066

opennms 0.08667

matt whitlock 0.06508

trustin lee 0.04752

#

4

5

6

Topic
ﬁle
path
dir

directory
stream
token
key

security
param
cert

service

str
log

conﬁg
result

matthew hawthorne 0.01170

lk 0.01106

werner dittmann 0.09409

apache software foundation 0.06117

gert van ham 0.05153

hamgert 0.05144

jcetaglib.sourceforge.net 0.05133

Author Probabilities
adam murdoch 0.02466
peter donald 0.02056
ludovic claude 0.01496

wayne m osse 0.44638
dirk mascher 0.07339
david irwin 0.04928

linke 0.02823
jason 0.01505

attractive example because Erich Gamma is widely known for being a founder of the JUnit project,
a fact which lends credibility to the ability of the topic modeling algorithm to assign developers to
reasonable topics. One can interpret the remaining author-topic assignments along similar lines. For
example, developer “daudel” is assigned to the topic corresponding to automatic code completion
with probability .99. Referring back to the Eclipse bug data it is clear that the overwhelming majority
of bug ﬁxes for the codeassist framework were made by this developer. One can infer that this is
likely to be an area of expertise of the developer.
In addition to determining developer contributions, one may also be curious to know the scope
of a developer’s involvement. Does a developer work across application areas, or are his contri-
butions highly focused? How does the breadth of one developer compare to another? These are
natural questions that arise in the software development process. To answer these questions within
the framework of author-topic models, we can measure the breadth of an author a by the entropy
t θta log θta of the corresponding distribution over topics. Applying the measure to
our multi-project dataset, we ﬁnd that the average measure is 2.47 bits. The developer with the low-
est entropy is “thierry danard,” with .00076 bits. The developer with the highest entropy is “wdi”
with 4.68 bits, with 6.64 bits being the maximum possible score for 100 topics. While the entropy

H(a) = −(cid:80)

Figure 2: All 59 Eclipse 3.0 authors clustered by KL divergence

of the distribution of an author over topics measures the author’s breadth, the similarity between two
authors can be measured by comparing their respective distributions over topics. Several metrics
are possible for this purpose, but one of the most natural measures is provided by the symmetrized
Kullback-Leibler (KL) divergence. Multidimensional scaling (MDS) is employed to further visual-

egammajeromelkjohnsondmegertkmaetzelcwongptfflbourlierjfogellprapicaudwilsonjburnsmaeschlimannkkolosowbbaumgartakiezundaudelmkellermrenniejaburnsdarinsothomannmfarajsfranklinjohnajeemdejantmaederaweinandmvanmeektodbbokowskitwatsonkhornedpollockoliviertbbiggsdarinjeffdbirsankrbarnesffusierikhelifisxenospmuletjdesriviereswmelhemschanrchavesmaeschlidjcmartisarsenaukentteicherjlanneluctwidmerdbaeumernickize author similarities, resulting in Figure 2 for the Eclipse project. The boxes represent individual
developers, and are arranged such that developers with similar topic distributions are nearest one an-
other. A similar ﬁgure, displaying only a subset of the 4,500 SourceForge and Apache authors due to
space and legibility constraints, is available in the supplementary materials. This information is es-
pecially useful when considering how to form a development team, choosing suitable programmers
to perform code updates, or deciding to whom to direct technical questions. Two other important
distributions that can be retrieved from the AT modeling approach are the distribution of topics
across documents, and the distribution of documents across topics (not shown). The corresponding
entropies provide an automated and novel way to precisely formalize and measure topic scattering
and document tangling, two fundamental concepts of software design [14], which are important to
software architects when performing activities such as code refactoring.

5 Code Search and Retrieval

Sourcerer relies on a deep analysis of code to extract pertinent textual and structural features that can
be used to improve the quality and performance of source code search, as well as augment the ways
in which code can be searched. By combining standard text information retrieval techniques with
source-speciﬁc heuristics and a relational representation of code, we have available a comprehensive
platform for searching software components. While there has been progress in developing source-
code-speciﬁc search engines in recent years (e.g. Koders, Krugle, and Google’s CodeSearch), these
systems continue to focus strictly on text information retrieval, and do not appear to leverage the
copious relations that can be extracted and analyzed from code.
Programs are best modeled as graphs, with code entities comprising the nodes and various relations
the edges. As such, it is worth exploring possible ranking methods that leverage the underlying
graphs. A natural starting point is Google’s PageRank [15], which considers hyperlinks to formulate
a notion of popularity among web pages. This can be applied to source as well, as it is likely that a
code entity referenced by many other entities are more robust than those with few references.
We used Google’s PageRank [15] almost verbatim. The Code Rank of a code entity (package, class,
or method) A is given by: CR(A) = (1 − d) + d(CR(T1)/C(T1) + ... + CR(Tn)/C(Tn)) where
T1...Tn are the code entities referring to A, C(A) is the number of outgoing links of A, and d is a
damping factor.
Using the CodeRank algorithm as a basis it is possible to devise many ranking schemes by building
graphs from the many entities and relations stored in our database, or subsets thereof. For example,
one may consider the graph of only method call relationships, package dependencies, or inheritance
hierarchies. Moreover, graph-based techniques can be combined with a variety of heuristics to
further improve code search. For example, keyword hits to the right of the fully-qualiﬁed name can
be boosted, hits in comments can be discounted, and terms indicative of test articles can be ignored.
We are conducting detailed experiments to assess the effectiveness of graph-based algorithms in con-
junction with standard IR techniques to search source code. Current evidence strongly indicates that
best results are ultimately obtained by combining term-based ranking with source-speciﬁc heuris-
tics and coderank. After deﬁning a set of 25 control queries with known ”best” hits, we compared
performances using standard information retrieval metrics, such as area under curve (AUC). Queries
were formulated to represent users searching for speciﬁc algorithms, such as ’depth ﬁrst search,’ as
well as users looking to reuse complete components, such as ’database connection manager.’ Best
hits were determined manually with a team of 3 software engineers serving as human judges of re-
sult quality, modularity, and ease of reuse. Results clearly indicate that the general Google search
engine is ineffective for locating relevant source code, with a mean AUC of .31 across the queries.
By restricting its corpus to code alone, Google’s code search engine yields substantial improvement
with an AUC of approximately .66. Despite this improvement this system essentially relies only
on regular expression matching of code keywords. Using a Java-speciﬁc keyword and comment
parser our infrastructure yields an immediate improvement with an AUC of .736. By augmenting
this further with the heuristics above and CodeRank (consisting of class and method relations), the
mean AUC climbs to .841. At this time we have conducted extensive experiments for 12 ranking
schemes corresponding to various combinations of graph-based and term-based heuristics, and have
observed similar improvements. While space does not allow their inclusion, additional results are
available from our supplementary materials page.

6 Conclusion

Here we have leveraged a comprehensive code processing infrastructure to facilitate the mining of
large-scale software repositories. We conduct a statistical analysis of source code on a previously un-
reported scale, identifying robust power-law behavior among several code entities. The development
and application of author-topic probabilistic modeling to source code allows for the unsupervised ex-
traction of program organization, functionality, developer contributions, and developer similarities,
thus providing a new direction for research in this area of software engineering. The methods de-
veloped are applicable at multiple scales, from single projects to Internet-scale repositories. Results
indicate that the algorithm produces reasonable and interpretable automated topics and author-topic
assignments. The probabilistic relationships between author, topics, and documents that emerge
from the models naturally provide an information-theoretic basis to deﬁne and compare developer
and program similarity, topic scattering, and document tangling with potential applications in soft-
ware engineering ranging from bug ﬁx assignments and stafﬁng to software refactoring. Finally,
by combining term-based information retrieval techniques with graphical information derived from
program structure, we are able to signiﬁcantly improve software search and retrieval performance.
Acknowledgments: Work in part supported by NSF MRI grant EIA-0321390 and a Microsoft
Faculty Research Award to PB, as well as NSF grant CCF-0725370 to CL and PB.

References
[1] S. Ugurel, R. Krovetz, and C. L. Giles. What’s the code?: automatic classiﬁcation of source code archives.
In KDD ’02: Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery
and data mining, pages 632–638, New York, NY, USA, 2002. ACM Press.

[2] D.M. Blei, A.Y. Ng, and M.I. Jordan. Latent dirichlet allocation. Journal of Machine Learning Research,

3:993–1022, January 2003.

[3] W. Buntine. Open source search: a data mining platform. SIGIR Forum, 39(1):4–10, 2005.
[4] M. Steyvers, P. Smyth, M. Rosen-Zvi, and T. Grifﬁths. Probabilistic author-topic models for information
discovery. In KDD ’04: Proceedings of the tenth ACM SIGKDD international conference on Knowledge
discovery and data mining, pages 306–315, New York, NY, USA, 2004. ACM Press.

[5] D. Newman, C. Chemudugunta, P. Smyth, and M. Steyvers. Analyzing entities and topics in news articles

using statistical topic models. In ISI, pages 93–104, 2006.

[6] S. Deerwester, S. Dumais, T. Landauer, G. Furnas, and R. Harshman. Indexing by latent semantic analysis.

Journal of the American Society of Information Science, 41(6):391–407, 1990.

[7] Michal Rosen-Zvi, Thomas Grifﬁths, Mark Steyvers, and Padhraic Smyth. The author-topic model for
In UAI ’04: Proceedings of the 20th conference on Uncertainty in artiﬁcial

authors and documents.
intelligence, pages 487–494, Arlington, Virginia, United States, 2004. AUAI Press.

[8] D. Newman and S. Block. Probabilistic topic decomposition of an eighteenth-century american newspa-

per. J. Am. Soc. Inf. Sci. Technol., 57(6):753–767, 2006.

[9] Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei. Hierarchical Dirichlet processes. Journal of the

American Statistical Association, 101(476):1566–1581, 2006.

[10] T. L. Grifﬁths and M. Steyvers. Finding scientiﬁc topics. Proc Natl Acad Sci U S A, 101 Suppl 1:5228–

5235, April 2004.

[11] A. Schr¨oter, T. Zimmermann, R. Premraj, and A. Zeller. If your bug database could talk. . . . In Proceedings
of the 5th International Symposium on Empirical Software Engineering, Volume II: Short Papers and
Posters, pages 18–20, September 2006.

[12] E. Brill. Some advances in transformation-based part of speech tagging.

Artiﬁcial Intelligence, pages 722–727, 1994.

In National Conference on

[13] E. Ukkonen. Approximate string-matching with q-grams and maximal matches. Theor. Comput. Sci.,

92(1):191–211, 1992.

[14] G. Kiczales, J. Lamping, A. Menhdhekar, C. Maeda, C. Lopes, J. Loingtier, and J. Irwin. Aspect-oriented
programming. In Mehmet Aks¸it and Satoshi Matsuoka, editors, Proceedings European Conference on
Object-Oriented Programming, volume 1241, pages 220–242. Springer-Verlag, Berlin, Heidelberg, and
New York, 1997.

[15] R. Motwani L. Page, S. Brin and T. Winograd.

The pagerank citation ranking: Bringing or-
Stanford Digital Library working paper SIDL-WP-1999-0120 of 11/11/1999 (see:

der to the web.
http://dbpubs.stanford.edu/pub/1999-66).

"
38,2007,Optimal models of sound localization by barn owls,"Sound localization by barn owls is commonly modeled as a matching procedure where localization cues derived from auditory inputs are compared to stored templates. While the matching models can explain properties of neural responses, no model explains how the owl resolves spatial ambiguity in the localization cues to produce accurate localization near the center of gaze. Here, we examine two models for the barn owl's sound localization behavior. First, we consider a maximum likelihood estimator in order to further evaluate the cue matching model. Second, we consider a maximum a posteriori estimator to test if a Bayesian model with a prior that emphasizes directions near the center of gaze can reproduce the owl's localization behavior. We show that the maximum likelihood estimator can not reproduce the owl's behavior, while the maximum a posteriori estimator is able to match the behavior. This result suggests that the standard cue matching model will not be sufficient to explain sound localization behavior in the barn owl. The Bayesian model provides a new framework for analyzing sound localization in the barn owl and leads to predictions about the owl's localization behavior.","Optimal models of sound localization by barn owls

Brian J. Fischer
Division of Biology

California Institute of Technology

Pasadena, CA

fischerb@caltech.edu

Abstract

Sound localization by barn owls is commonly modeled as a matching procedure
where localization cues derived from auditory inputs are compared to stored tem-
plates. While the matching models can explain properties of neural responses, no
model explains how the owl resolves spatial ambiguity in the localization cues to
produce accurate localization for sources near the center of gaze. Here, I exam-
ine two models for the barn owl’s sound localization behavior. First, I consider
a maximum likelihood estimator in order to further evaluate the cue matching
model. Second, I consider a maximum a posteriori estimator to test whether a
Bayesian model with a prior that emphasizes directions near the center of gaze
can reproduce the owl’s localization behavior. I show that the maximum likeli-
hood estimator can not reproduce the owl’s behavior, while the maximum a poste-
riori estimator is able to match the behavior. This result suggests that the standard
cue matching model will not be sufﬁcient to explain sound localization behavior in
the barn owl. The Bayesian model provides a new framework for analyzing sound
localization in the barn owl and leads to predictions about the owl’s localization
behavior.

1 Introduction

Barn owls, the champions of sound localization, show systematic errors when localizing sounds.
Owls localize broadband noise signals with great accuracy for source directions near the center of
gaze [1]. However, localization errors increase as source directions move to the periphery, consistent
with an underestimate of the source direction [1]. Behavioral experiments show that the barn owl
uses the interaural time difference (ITD) for localization in the horizontal dimension and the interau-
ral level difference (ILD) for localization in the vertical dimension [2]. Direct measurements of the
sounds received at the ears for sources at different locations in space show that disparate directions
are associated with very similar localization cues. Speciﬁcally, there is a similarity between ILD
and ITD cues for directions near the center of gaze and directions with eccentric elevations on the
vertical plane. How does the owl resolve this ambiguity in the localization cues to produce accurate
localization for sound sources near the center of gaze?

Theories regarding the use of localization cues by the barn owl are drawn from the extensive knowl-
edge of processing in the barn owl’s auditory system. Neurophysiological and anatomical studies
show that the barn owl’s auditory system contains specialized circuitry that is devoted to extracting
spectral ILD and ITD cues and processing them to derive source direction information [2]. It has
been suggested that a spectral matching operation between ILD and ITD cues computed from audi-
tory inputs and preferred ILD and ITD spectra associated with spatially selective auditory neurons
underlies the derivation of spatial information from the auditory cues [3–6]. The spectral matching
models reproduce aspects of neural responses, but none reproduces the sound localization behavior
of the barn owl. In particular, the spectral matching models do not describe how the owl resolves am-
biguities in the localization cues. In addition to spectral matching of localization cues, it is possible

1

that the owl incorporates prior experience or beliefs into the process of deriving direction estimates
from the auditory input signals. These two approaches to sound localization can be formalized using
the language of estimation theory as maximum likelihood (ML) and Bayesian solutions, respectively.

Here, I examine two models for the barn owl’s sound localization behavior in order to further evalu-
ate the spectral matching model and to test whether a Bayesian model with a prior that emphasizes
directions near the center of gaze can reproduce the owl’s localization behavior. I begin by viewing
the sound localization problem as a statistical estimation problem. Maximum likelihood and max-
imum a posteriori (MAP) solutions to the estimation problem are compared with the localization
behavior of a barn owl in a head turning task.

2 Observation model

To deﬁne the localization problem, we must specify an observation model that describes the infor-
mation the owl uses to produce a direction estimate. Neurophysiological and behavioral experiments
suggest that the barn owl derives direction estimates from ILD and ITD cues that are computed at an
array of frequencies [2, 7, 8]. Note that when computed as a function of frequency, the ITD is given
by an interaural phase difference (IPD).

Here I consider a model where the observation made by the owl is given by the ILD and IPD spectra
derived from barn owl head-related transfer functions (HRTFs) after corruption with additive noise.
For a source direction (θ, φ), the observation vector r is expressed mathematically as

r = (cid:20) rILD

rIPD (cid:21) = (cid:20) ILDθ,φ

IPDθ,φ (cid:21) +(cid:20) ηILD
ηIPD (cid:21)

(1)

where the ILD spectrum ILDθ,φ = [ILDθ,φ(ω1), ILDθ,φ(ω2), . . . , ILDθ,φ(ωNf )] and the IPD spec-
trum IPDθ,φ = [IPDθ,φ(ω1), IPDθ,φ(ω2), . . . , IPDθ,φ(ωNf )] are speciﬁed at a ﬁnite number of fre-
quencies. The ILD and IPD cues are computed directly from the HRTFs as

and

ILDθ,φ(ω) = 20 log10

|ˆhR(θ,φ)(ω)|
|ˆhL(θ,φ)(ω)|

IPDθ,φ(ω) = ϕR(θ,φ)(ω) − ϕL(θ,φ)(ω),

(2)

(3)

where the left and right HRTFs are written as ˆhL(θ,φ)(ω) = |ˆhL(θ,φ)(ω)|eiϕL(θ,φ)(ω) and
ˆhR(θ,φ)(ω) = |ˆhR(θ,φ)(ω)|eiϕR(θ,φ)(ω), respectively.
The noise corrupting the ILD spectrum is modeled as a Gaussian random vector with independent
and identically distributed (i.i.d.) components, ηILD(ωj) ∼ N (0, σ). The IPD spectrum noise vector
is assumed to have i.i.d. components where each element has a von Mises distribution with parame-
ter κ. The von Mises distribution can be viewed as a 2π-periodic Gaussian distribution for large κ
and is a uniform distribution for κ = 0 [9]. I assume that the ILD and IPD noise terms are mutually
independent.

With this noise model, the likelihood function has the form

where the ILD likelihood function is given by

pr|Θ,Φ(r|θ, φ) = prILD|Θ,Φ(rILD|θ, φ)prIPD|Θ,Φ(rIPD|θ, φ)

prILD|Θ,Φ(rILD|θ, φ) =

1

(2πσ2)Nf /2 exp[−

1
2σ2

Nf

Xj=1

(rILD(ωj) − ILDθ,φ(ωj))2]

and the IPD likelihood function is given by

(4)

(5)

prIPD|Θ,Φ(rIPD|θ, φ) =

1

(2πI0(κ))Nf

exp[κ

Nf

Xj=1

cos(rIPD(ωj) − IPDθ,φ(ωj))]

(6)

where I0(κ) is a modiﬁed Bessel function of the ﬁrst kind of order 0. The likelihood function will
have peaks at directions where the expected spectral cues ILDθ,φ and IPDθ,φ are near the observed
values rILD and rIPD.

2

3 Model performance measure

I evaluate maximum likelihood and maximum a posteriori methods for estimating the source direc-
tion from the observed ILD and IPD cues by computing an expected localization error and compar-
ing the results to an owl’s behavior. The performance of each estimation procedure at a given source
direction is quantiﬁed by the expected absolute angular error E[|ˆθ(r) − θ| + | ˆφ(r) − φ| | θ, φ]. This
measure of estimation error is directly compared to the behavioral performance of a barn owl in
a head turning localization task [1]. The expected absolute angular error is approximated through
Monte Carlo simulation as

E[|ˆθ(r) − θ| + | ˆφ(r) − φ| | θ, φ] ≈ µ({|ˆθ(ri) − θ|}N

(7)
i=1) is the circular mean of the angles
where the ri are drawn from pr|Θ,Φ(r|θ, φ) and µ({θi}N
i=1. The error is computed using HRTFs for two barn owls [10] and is calculated for direc-
{θi}N
tions in the frontal hemisphere with 5◦ increments in azimuth and elevation, as deﬁned using double
polar coordinates.

i=1) + µ({| ˆφ(ri) − φ|}N

i=1)

4 Maximum likelihood estimate

The maximum likelihood direction estimate is derived from the observed noisy ILD and IPD cues
by ﬁnding the source direction that maximizes the likelihood function, yielding

(ˆθML(r), ˆφML(r)) = arg max
(θ,φ)

pr|Θ,Φ(r|θ, φ).

(8)

This procedure amounts to a spectral cue matching operation. Each direction in space is associated
with a particular ILD and IPD spectrum, as derived from the HRTFs. The direction with associated
cues that are closest to the observed cues is designated as the estimate. This estimator is of particular
interest because of the claim that salience in the neural map of auditory space in the barn owl can be
described by a spectral cue matching operation [3, 4, 6].

The maximum likelihood estimator was unable to reproduce the owl’s localization behavior. The
performance of the maximum likelihood estimator depends on the two likelihood function parame-
ters σ and κ, which determine the ILD and IPD noise variances, respectively. For noise variances
large enough that the error increased at peripheral directions, in accordance with the barn owl’s
behavior, the error also increased signiﬁcantly for directions near the center of the interaural coordi-
nate system (Figure 1). This pattern of error as a function of eccentricity, with a large central peak,
is not consistent with the performance of the barn owl in the head turning task [1]. Additionally,
directions near the center of gaze were often confused with directions in the periphery leading to a
high variability in the direction estimates, which is not seen in the owl’s behavior.

5 Maximum a posteriori estimate

In the Bayesian framework, the direction estimate depends on both the likelihood function and the
prior distribution over source directions through the posterior distribution. Using Bayes’ rule, the
posterior density is proportional to the product of the likelihood function and the prior,

pΘ,Φ|r(θ, φ|r) ∝ pr|Θ,Φ(r|θ, φ)pΘ,Φ(θ, φ).

(9)
The prior distribution is used to summarize the owl’s belief about the most likely source directions
before an observation of ILD and IPD cues is made. Based on the barn owl’s tendency to underes-
timate source directions [1], I use a prior that emphasizes directions near the center of gaze. The
prior is given by a product of two one-dimensional von Mises distributions, yielding the probability
density function

pΘ,Φ(θ, φ) =

exp[κ1 cos(θ) + κ2 cos(φ)]

(2π)2I0(κ1)I0(κ2)

(10)

where I0(κ) is a modiﬁed Bessel function of the ﬁrst kind of order 0. The maximum a posteriori
source direction estimate is computed for a given observation by ﬁnding the source direction that
maximizes the posterior density, yielding

(ˆθMAP(r), ˆφMAP(r)) = arg max
(θ,φ)

pΘ,Φ|r(θ, φ|r).

(11)

3

Figure 1: Estimation error in the model for the maximum likelihood (ML) and maximum a posteriori
(MAP) estimates. HRTFs were used from owls 884 (top) and 880 (bottom). Left column: Estimation
error at 685 locations in the frontal hemisphere plotted in double polar coordinates. Center column:
Estimation error on the horizontal plane along with the estimation error of a barn owl in a head
turning task [1]. Right column: Estimation error on the vertical plane along with the estimation
error of a barn owl in a head turning task. Note that each plot uses a unique scale.

4

Figure 2: Estimates for the MAP estimator on the horizontal plane (left) and the vertical plane (right)
using HRTFs from owl 880. The box extends from the lower quartile to the upper quartile of the
sample. The solid line is the identity line. Like the owl, the MAP estimator underestimates the
source direction.

In the MAP case, the estimate depends on spectral matching of observations with expected cues for
each direction, but with a penalty on the selection of peripheral directions.

It was possible to ﬁnd a MAP estimator that was consistent with the owl’s localization behavior
(Figures 1,2). For the example MAP estimators shown in Figures 1 and 2, the error was smallest in
the central region of space and increased at the periphery. The largest errors occurred at the vertical
extremes. This pattern of error qualitatively matches the pattern of error displayed by the owl in a
head turning localization task [1].

The parameters that produced a behaviorally consistent MAP estimator correspond to a likelihood
and prior with large variances. For the estimators shown in Figure 1, the likelihood function para-
meters were given by σ = 11.5 dB and κ = 0.75 for owl 880 and σ = 10.75 dB and κ = 0.8 for owl
884. For comparison, the range of ILD values normally experienced by the barn owl falls between
± 30 dB [10]. The prior parameters correspond to an azimuthal width parameter κ1 of 0.25 for owl
880 and 0.2 for owl 884 and an elevational width parameter κ2 of 0.25 for owl 880 and 0.18 for owl
884.

The implication of this model for implementation in the owl’s auditory system is that the spectral
localization cues ILD and IPD do not need to be computed with great accuracy and the emphasis on
central directions does not need to be large in order to produce the barn owl’s behavior.

6 Discussion

6.1 A new approach to modeling sound localization in the barn owl

The simulation results show that the maximum likelihood model considered here can not reproduce
the owl’s behavior, while the maximum a posteriori solution is able to match the behavior. This
result suggests that the standard spectral matching model will not be sufﬁcient to explain sound lo-
calization behavior in the barn owl. Previously, suggestions have been made that sound localization
by the barn owl can be described using the Bayesian framework [11, 12], but no speciﬁc models
have been proposed. This paper demonstrates that a Bayesian model can qualitatively match the
owl’s localization behavior. The Bayesian approach described here provides a new framework for
analyzing sound localization in the owl.

6.2 Failure of the maximum likelihood model

The maximum likelihood model fails because of the nature of spatial ambiguity in the ILD and
IPD cues. The existence of spatial ambiguity has been noted in previous descriptions of barn owl
HRTFs [3, 10, 13]. As expected, directions near each other have similar cues. In addition to sim-

5

ilarity of cues between proximal directions, distant directions can have similar ILD and IPD cues.
Most signiﬁcantly, there is a similarity between the ILD and IPD cues at the center of gaze and at
peripheral directions on the vertical plane. The consequence of such ambiguity between distant di-
rections is that noise in measuring localization cues can lead to large errors in direction estimation,
as seen in the ML estimate. The results of the simulations suggest that a behaviorally accurate so-
lution to the sound localization problem must include a mechanism that chooses between disparate
directions which are associated with similar localization cues in such a way as to limit errors for
source directions near the center of gaze. This work shows that a possible mechanism for choosing
between such directions is to incorporate a bias towards directions at the center of gaze through a
prior distribution and utilize the Bayesian estimation framework. The use of a prior that emphasizes
directions near the center of gaze is similar to the use of central weighting functions in models of
human lateralization [14].

6.3 Predictions of the Bayesian model

The MAP estimator predicts the underestimation of peripheral source directions on the horizontal
and vertical planes (Figure 2). The pattern of error displayed by the MAP estimator qualitatively
matches the owl’s behavioral performance by showing increasing error as a function of eccentricity.
Our evaluation of the model performance is limited, however, because there is little behavioral data
for directions outside ± 70 deg [15,16]. For the owl whose performance is displayed in Figure 1, the
largest errors on the vertical and horizontal planes were less than 20 deg and 11 deg, respectively.
The model produces much larger errors for directions beyond 70 deg, especially on the vertical plane.
The large errors in elevation result from the ambiguity in the localization cues on the vertical plane
and the shape of the prior distribution. As discussed above, for broadband noise stimuli, there is a
similarity between the ILD and IPD cues for central and peripheral directions on the vertical plane
[3, 10, 13]. The presence of a prior distribution that emphasizes central directions causes direction
estimates for both central and peripheral directions to be concentrated near zero deg. Therefore,
estimation errors are minimal for sources at the center of gaze, but approach the magnitude of the
source direction for peripheral source directions. Behavioral data shows that localization accuracy
is the greatest near the center of gaze [1], but there is no data for localization performance at the
most eccentric directions on the vertical plane. Further behavioral experiments must be performed
to determine if the owl’s error increases greatly at the most peripheral directions.

There is a signiﬁcant spatial ambiguity in the localization cues when target sounds are narrow-
band. It is well known that spatial ambiguity arises from the way that interaural time differences are
processed at each frequency [17–19]. The owl measures the interaural time difference for each fre-
quency of the input sound as an interaural phase difference. Therefore, multiple directions in space
that differ in their associated interaural time difference by the period of a tone at that frequency are
consistent with the same interaural phase difference and can not be distinguished. Behavioral exper-
iments show that the owl may localize a phantom source in the horizontal dimension when the signal
is a tone [20]. Based on the presence of a prior that emphasizes directions near the center of gaze,
I predict that for low frequency tones where phase equivalent directions lie near the center of gaze
and at directions greater than 80 deg, confusion will always lead to an estimate of a source direction
near zero degrees. This prediction can not be evaluated from available data because localization of
tonal signals has only been systematically studied using 5 kHz tones with target directions at ± 20
deg [19]. Because the prior is broad, the target direction of ± 20 deg and the phantom direction of
± 50 deg may both be considered central.
The ILD cue also displays a signiﬁcant ambiguity at high frequencies. At frequencies above 7 kHz,
the ILD is non-monotonically related to the vertical position of a sound source [3, 10] (Figure 3).
Therefore, for narrowband sounds, the owl can not uniquely determine the direction of a sound
source from the ITD and ILD cues.
I predict that for tonal signals above 7 kHz, there will be
multiple directions on the vertical plane that are confused with directions near zero deg. I predict
that confusion between source directions near zero deg and eccentric directions will always lead to
estimates of directions near zero deg. There is no available data to evaluate this prediction.

6

Figure 3: Model predictions for localization of tones on the vertical plane. (A) ILD as a function of
elevation at 8 kHz, computed from HRTFs of owl 880 recorded by Keller et al. (1998). (B) Given
an ILD of 0 dB, a likelihood function (dots) based on matching cues to expected values would be
multimodal with three equal peaks. If the target is at any of the three directions, there will be large
localization errors because of confusion with the other directions. If a prior emphasizing frontal
space (dashed) is included, a posterior density equal to the product of the likelihood and the prior
would have a main peak at 0 deg elevation. Using a maximum a posteriori estimate, large errors
would be made if the target is above or below. However, few errors would be observed when the
target is near 0 deg.

6.4 Testing the Bayesian model

Further head turning localization experiments with barn owls must be performed to test predictions
generated by the Bayesian hypothesis and to provide constraints on a model of sound localization.
Experiments should test the localization accuracy of the owl for broadband noise sources and tonal
signals at directions covering the frontal hemisphere. The Bayesian model will be supported if, ﬁrst,
localization accuracy is high for both tonal and broadband noise sources near the center of gaze
and, second, peripherally located sources are confused for targets near the center of gaze, leading
to large localization errors. Additionally, a Bayesian model should be ﬁt to the data, including
points away from the horizontal and vertical planes, using a nonparametric prior [21, 22]. While the
model presented here, using a von Mises prior, qualitatively matches the performance of the owl, the
performance of the Bayesian model may be improved by removing assumptions about the structure
of the prior distribution.

6.5 Implications for neural processing

The analysis presented here does not directly address the neural implementation of the solution
to the localization problem. However, our abstract analysis of the sound localization problem has
implications for neural processing. Several models exist that reproduce the basic properties of ILD,
ITD, and space selectivity in ICx and OT neurons using a spectral matching procedure [3, 5, 6].
These results suggest that a Bayesian model is not necessary to describe the responses of individual
ICx and OT neurons. It may be necessary to look in the brainstem motor targets of the optic tectum
to ﬁnd neurons that resolve the ambiguity present in sound stimuli and show responses that reﬂect
the MAP solution. This implies that the prior distribution is not employed until the ﬁnal stage of
processing. The prior may correspond to the distribution of best directions of space-speciﬁc neurons
in ICx and OT, which emphasizes directions near the center of gaze [23].

6.6 Conclusion

This analysis supports the Bayesian model of the barn owl’s solution to the localization problem
over the maximum likelihood model. This result suggests that the standard spectral matching model
will not be sufﬁcient to explain sound localization behavior in the barn owl. The Bayesian model

7

provides a new framework for analyzing sound localization in the owl. The simulation results using
the MAP estimator lead to testable predictions that can be used to evaluate the Bayesian model of
sound localization in the barn owl.

Acknowledgments

I thank Kip Keller, Klaus Hartung, and Terry Takahashi for providing the head-related transfer
functions and Mark Konishi and Jos´e Luis Pe˜na for comments and support.

References

[1] E.I. Knudsen, G.G. Blasdel, and M. Konishi. Sound localization by the barn owl (Tyto alba) measured

with the search coil technique. J. Comp. Physiol., 133:1–11, 1979.

[2] M. Konishi. Coding of auditory space. Annu. Rev. Neurosci., 26:31–55, 2003.
[3] M.S. Brainard, E.I. Knudsen, and S.D. Esterly. Neural derivation of sound source location: Resolution of

spatial ambiguities in binaural cues. J. Acoust. Soc. Am., 91(2):1015–1027, 1992.

[4] B.J. Arthur. Neural computations leading to space-speciﬁc auditory responses in the barn owl. Ph.D.

thesis, Caltech, 2001.

[5] B.J. Fischer. A model of the computations leading to a representation of auditory space in the midbrain

of the barn owl. D.Sc. thesis, Washington University in St. Louis, 2005.

[6] C.H. Keller and T.T. Takahashi. Localization and identiﬁcation of concurrent sounds in the owl’s auditory

space map. J. Neurosci., 25:10446–10461, 2005.

[7] I. Poganiatz and H. Wagner. Sound-localization experiments with barn owls in virtual space: inﬂuence of
broadband interaural level difference on head-turning behavior. J. Comp. Physiol. A, 187:225–233, 2001.
[8] D.R. Euston and T.T. Takahashi. From spectrum to space: The contribution of level difference cues to

spatial receptive ﬁelds in the barn owl inferior colliculus. J. Neurosci., 22(1):284–293, Jan. 2002.

[9] Evans M., Hastings N., and Peacock B. von Mises Distribution. In Statistical Distributions, 3rd ed., pages

189–191. Wiley, New York, 2000.

[10] C.H. Keller, K. Hartung, and T.T. Takahashi. Head-related transfer functions of the barn owl: measure-

ment and neural responses. Hearing Research, 118:13–34, 1998.

[11] R.O. Duda. Elevation dependence of the interaural transfer function, chapter 3 in Binaural and Spatial
Hearing in Real and Virtual Environments, pages 49–75. New Jersey: Lawrence Erlbaum Associates,
1997.

[12] Witten I.B. and Knudsen E.I. Why seeing is believing: Merging auditory and visual worlds. Neuron,

48:489–496, 2005.

[13] J.F Olsen, E.I. Knudsen, and S.D. Esterly. Neural maps of interaural time and intensity differences in the

optic tectum of the barn owl. J. Neurosci., 9:2591–2605, 1989.

[14] R.M. Stern and H.S. Colburn. Theory of binaural interaction based on auditory-nerve data. IV. A model

for subjective lateral position. J. Acoust. Soc. Am., 64:127–140, 1978.

[15] H. Wagner. Sound-localization deﬁcits induced by lesions in the barn owl’s auditory space map. J.

Neurosci., 13:371–386, 1993.

[16] I. Poganiatz, I. Nelken, and H. Wagner. Sound-localization experiments with barn owls in virtual space:

inﬂuence of interaural time difference on head-turning behavior. J. Ass. Res. Otolarnyg., 2:1–21, 2001.

[17] T. Takahashi and M. Konishi. Selectivity for interaural time difference in the owl’s midbrain. J. Neurosci.,

6(12):3413–3422, 1986.

[18] J.A. Mazer. How the owl resolves auditory coding ambiguity. Proc. Natl. Acad. Sci. USA, 95:10932–

10937, 1998.

[19] K. Saberi, Y. Takahashi, H. Farahbod, and M. Konishi. Neural bases of an auditory illusion and its

elimination in owls. Nature Neurosci., 2(7):656–659, 1999.

[20] E.I. Knudsen and M. Konishi. Mechanisms of sound localization in the barn owl (Tyto alba) measured

with the search coil technique. J. Comp. Phys. A, (133):13–21, 1979.

[21] Liam Paninski. Nonparametric inference of prior probabilities from Bayes-optimal behavior. In Y. Weiss,
B. Sch¨olkopf, and J. Platt, editors, Advances in Neural Information Processing Systems 18, pages 1067–
1074. MIT Press, Cambridge, MA, 2006.

[22] Stocker A.A. and Simoncelli E.P. Noise characteristics and prior expectations in human visual speed

perception. Nature Neurosci., 9(4):578–585, 2006.

[23] E.I. Knudsen and M. Konishi. A neural map of auditory space in the owl. Science, 200:795–797, 1978.

8

"
737,2007,Discriminative K-means for Clustering,"We present a theoretical study on the discriminative clustering framework, recently proposed for simultaneous subspace selection via linear discriminant analysis (LDA) and clustering. Empirical results have shown its favorable performance in comparison with several other popular clustering algorithms. However, the inherent relationship between subspace selection and clustering in this framework is not well understood, due to the iterative nature of the algorithm. We show in this paper that this iterative subspace selection and clustering is equivalent to kernel K-means with a specific kernel Gram matrix. This provides significant and new insights into the nature of this subspace selection procedure. Based on this equivalence relationship, we propose the Discriminative K-means (DisKmeans) algorithm for simultaneous LDA subspace selection and clustering, as well as an automatic parameter estimation procedure. We also present the nonlinear extension of DisKmeans using kernels. We show that the learning of the kernel matrix over a convex set of pre-specified kernel matrices can be incorporated into the clustering formulation. The connection between DisKmeans and several other clustering algorithms is also analyzed. The presented theories and algorithms are evaluated through experiments on a collection of benchmark data sets.","Discriminative K-means for Clustering

Jieping Ye

Arizona State University

Tempe, AZ 85287

Zheng Zhao

Arizona State University

Tempe, AZ 85287

jieping.ye@asu.edu

zhaozheng@asu.edu

Abstract

Mingrui Wu

mingrui.wu@tuebingen.mpg.de

MPI for Biological Cybernetics

T¨ubingen, Germany

We present a theoretical study on the discriminative clustering framework, re-
cently proposed for simultaneous subspace selection via linear discriminant analy-
sis (LDA) and clustering. Empirical results have shown its favorable performance
in comparison with several other popular clustering algorithms. However, the in-
herent relationship between subspace selection and clustering in this framework
is not well understood, due to the iterative nature of the algorithm. We show in
this paper that this iterative subspace selection and clustering is equivalent to ker-
nel K-means with a speciﬁc kernel Gram matrix. This provides signiﬁcant and
new insights into the nature of this subspace selection procedure. Based on this
equivalence relationship, we propose the Discriminative K-means (DisKmeans)
algorithm for simultaneous LDA subspace selection and clustering, as well as an
automatic parameter estimation procedure. We also present the nonlinear exten-
sion of DisKmeans using kernels. We show that the learning of the kernel matrix
over a convex set of pre-speciﬁed kernel matrices can be incorporated into the
clustering formulation. The connection between DisKmeans and several other
clustering algorithms is also analyzed. The presented theories and algorithms are
evaluated through experiments on a collection of benchmark data sets.

1 Introduction
Applications in various domains such as text/web mining and bioinformatics often lead to very high-
dimensional data. Clustering such high-dimensional data sets is a contemporary challenge, due to the
curse of dimensionality. A common practice is to project the data onto a low-dimensional subspace
through unsupervised dimensionality reduction such as Principal Component Analysis (PCA) [9]
and various manifold learning algorithms [1, 13] before the clustering. However, the projection may
not necessarily improve the separability of the data for clustering, due to the inherent separation
between subspace selection (via dimensionality reduction) and clustering.
One natural way to overcome this limitation is to integrate dimensionality reduction and clustering
in a joint framework. Several recent work [5, 10, 16] incorporate supervised dimensionality reduc-
tion such as Linear Discriminant Analysis (LDA) [7] into the clustering framework, which performs
clustering and LDA dimensionality reduction simultaneously. The algorithm, called Discrimina-
tive Clustering (DisCluster) in the following discussion, works in an iterative fashion, alternating
between LDA subspace selection and clustering. In this framework, clustering generates the class
labels for LDA, while LDA provides the subspace for clustering. Empirical results have shown the
beneﬁts of clustering in a low dimensional discriminative space rather than in the principal com-
ponent space (generative). However, the integration between subspace selection and clustering in
DisCluster is not well understood, due to the intertwined and iterative nature of the algorithm.
In this paper, we analyze this discriminative clustering framework by studying several fundamental
and important issues: (1) What do we really gain by performing clustering in a low dimensional
discriminative space? (2) What is the nature of its iterative process alternating between subspace

1

selection and clustering? (3) Can this iterative process be simpliﬁed and improved? (4) How to
estimate the parameter involved in the algorithm?
The main contributions of this paper are summarized as follows: (1) We show that the LDA pro-
jection can be factored out from the integrated LDA subspace selection and clustering formulation.
This results in a simple trace maximization problem associated with a regularized Gram matrix of
the data, which is controlled by a regularization parameter λ; (2) The solution to this trace max-
imization problem leads to the Discriminative K-means (DisKmeans) algorithm for simultaneous
LDA subspace selection and clustering. DisKmeans is shown to be equivalent to kernel K-means,
where discriminative subspace selection essentially constructs a kernel Gram matrix for clustering.
This provides new insights into the nature of this subspace selection procedure; (3) The DisKmeans
algorithm is dependent on the value of the regularization parameter λ. We propose an automatic
parameter tuning process (model selection) for the estimation of λ; (4) We propose the nonlinear
extension of DisKmeans using the kernels. We show that the learning of the kernel matrix over
a convex set of pre-speciﬁed kernel matrices can be incorporated into the clustering formulation,
resulting in a semideﬁnite programming (SDP) [15]. We evaluate the presented theories and algo-
rithms through experiments on a collection of benchmark data sets.
2 Linear Discriminant Analysis and Discriminative Clustering
Consider a data set consisting of n data points {xi}n
centered, that is,
given by xi. In clustering, we aim to group the data {xi}n
be the cluster indicator matrix deﬁned as follows:

(cid:80)n
i=1 ∈ Rm. For simplicity, we assume the data is
i=1 xi/n = 0. Denote X = [x1,··· , xn] as the data matrix whose i-th column is
j=1. Let F ∈ Rn×k

i=1 into k clusters {Cj}k

F = {fi,j}n×k, where fi,j = 1, iff xi ∈ Cj.

We can deﬁne the weighted cluster indicator matrix as follows [4]:
L = [L1, L2,··· , Lk] = F (F T F )− 1
2 .

It follows that the j-th column of L is given by

(1)

(2)

Lj = (0, . . . , 0,

(3)
where nj is the sample size of the j-th cluster Cj. Denote µj =
x/nj as the mean of the j-th
cluster Cj. The within-cluster scatter, between-cluster scatter, and total scatter matrices are deﬁned
as follows [7]:

1, . . . , 1, 0, . . . , 0)T /n

j ,
x∈Cj

Sw =

(xi − µj)(xi − µj)T , Sb =

njµjµT

j = XLLT X T , St = XX T .

(4)

k(cid:88)

(cid:88)

j=1

xi∈Cj

It follows that trace(Sw) captures the intra-cluster distance, and trace(Sb) captures the inter-cluster
distance. It can be shown that St = Sw + Sb.
Given the cluster indicator matrix F (or L), Linear Discriminant Analysis (LDA) aims to compute
a linear transformation (projection) P ∈ Rm×d that maps each xi in the m-dimensional space to
a vector ˆxi in the d-dimensional space (d < m) as follows: xi ∈ IRm → ˆxi = P T xi ∈ IRd,
such that the following objective function is maximized [7]: trace
. Since
St = Sw + Sb, the optimal transformation matrix P is also given by maximizing the following
objective function:

(P T SwP )−1P T SbP

(cid:162)

(cid:161)

(5)
For high-dimensional data, the estimation of the total scatter (covariance) matrix is often not reliable.
The regularization technique [6] is commonly applied to improve the estimation as follows:

trace

.

(P T StP )−1P T SbP

(cid:162)

(cid:161)

˜St = St + λIm = XX T + λIm,

(6)

where Im is the identity matrix of size m and λ > 0 is a regularization parameter.
In Discriminant Clustering (DisCluster) [5, 10, 16], the transformation matrix P and the weighted
cluster indicator matrix L are computed by maximizing the following objective function:

(cid:180)

f(L, P ) ≡ trace
= trace

(P T ˜StP )−1P T SbP
(P T (XX T + λIm)P )−1P T XLLT X T P

.

(7)

(cid:162)

(cid:179)
(cid:161)

2

(cid:80)

1
2

(cid:122) (cid:125)(cid:124) (cid:123)

nj

k(cid:88)

j=1

.

(cid:162)

(cid:161)

f(L, P ) = trace

The algorithm works in an intertwined and iterative fashion, alternating between the computation
of L for a given P and the computation of P for a given L. More speciﬁcally, for a given L, P is
given by the standard LDA procedure. Since trace(AB) = trace(BA) for any two matrices [8], for
a given P , the objective function f(L, P ) can be expressed as:

LT X T P (P T (XX T + λIm)P )−1P T XL

(8)
Note that L is not an arbitrary matrix, but a weighted cluster indicator matrix, as deﬁned in Eq. (3).
The optimal L can be computed by applying the gradient descent strategy [10] or by solving a kernel
K-means problem [5, 16] with X T P (P T (XX T + λIm)P )−1P T X as the kernel Gram matrix [4].
The algorithm is guaranteed to converge in terms of the value of the objective function f(L, P ), as
the value of f(L, P ) monotonically increases and is bounded from above.
Experiments [5, 10, 16] have shown the effectiveness of DisCluster in comparison with several other
popular clustering algorithms. However, the inherent relationship between subspace selection via
LDA and clustering is not well understood, and there is need for further investigation. We show
in the next section that the iterative subspace selection and clustering in DisCluster is equivalent
to kernel K-means with a speciﬁc kernel Gram matrix. Based on this equivalence relationship,
we propose the Discriminative K-means (DisKmeans) algorithm for simultaneous LDA subspace
selection and clustering.
3 DisKmeans: Discriminative K-means with a Fixed λ
Assume that λ is a ﬁxed positive constant. Let’s consider the maximization of the function in Eq. (7):
(9)
Here, P is a transformation matrix and L is a weighted cluster indicator matrix as in Eq. (3). It
follows from the Representer Theorem [14] that the optimal transformation matrix P ∈ IRm×d can
be expressed as P = XH, for some matrix H ∈ IRn×d. Denote G = X T X as the Gram matrix,
which is symmetric and positive semideﬁnite. It follows that

(P T (XX T + λIm)P )−1P T XLLT X T P

f(L, P ) = trace

(cid:161)

(cid:162)

.

H T GLLT GH

f(L, P ) = trace

H T (GG + λG) H

(10)
We show that the matrix H can be factored out from the objective function in Eq. (10), thus dramat-
ically simplifying the optimization problem in the original DisCluster algorithm. The main result is
summarized in the following theorem:
Theorem 3.1. Let G be the Gram matrix deﬁned as above and λ > 0 be the regularization param-
eter. Let L∗ and P ∗ be the optimal solution to the maximization of the objective function f(L, P ) in
Eq. (7). Then L∗ solves the following maximization problem:
In − (In +

L∗ = arg max

G)−1

(cid:182)

(cid:181)

(cid:181)

(cid:182)

trace

(11)

LT

L

.

.

(cid:162)−1

(cid:179)(cid:161)

(cid:180)

L

1
λ

Proof. Let G = UΣU T be the Singular Value Decomposition (SVD) [8] of G, where U ∈ IRn×n
is orthogonal, Σ = diag (σ1,··· , σt, 0,··· , 0) ∈ IRn×n is diagonal, and t = rank(G). Let U1 ∈
IRn×t consist of the ﬁrst t columns of U and Σt = diag (σ1,··· , σt) ∈ IRt×t . Then

(cid:179)

2 ΣtU T

G = UΣU T = U1ΣtU T
1 .

(12)
Denote R = (Σ2
1 L and let R = MΣRN T be the SVD of R, where M and
N are orthogonal and ΣR is diagonal with rank(ΣR) = rank(R) = q. Deﬁne the matrix Z as
Z = Udiag
, where diag(A, B) is a block diagonal matrix. It follows that
˜Σ 0
0
0

t + λΣt)− 1
t + λΣt)− 1

, Z T (GG + λG) Z =

Z T(cid:161)

(cid:180)
(cid:181)

2 M, In−t

GLLT G

(cid:182)

(cid:181)

(cid:182)

Z =

(13)

(Σ2

(cid:162)

It
0

0
0

,

where ˜Σ = (ΣR)2 is diagonal with non-increasing diagonal entries. It can be veriﬁed that

(cid:179)

(cid:180)

f(L, P ) ≤ trace

˜Σ

= trace

(GG + λG)+ GLLT G

(cid:180)

(cid:179)
(cid:179)
(cid:181)

(cid:181)

(cid:180)

(cid:182)

(cid:182)

= trace

= trace

LT G (GG + λG)+ GL
G)−1

In − (In +

LT

1
λ

L

,

(14)

where the equality holds when P = XH and H consists of the ﬁrst q columns of Z.

3

3.1 Computing the Weighted Cluster Matrix L
The weighted cluster indicator matrix L solving the maximization problem in Eq. (11) can be com-
puted by solving a kernel K-means problem [5] with the kernel Gram matrix given by

(cid:181)

(cid:182)−1

˜G = In −

In +

1
λ

G

.

(15)

Thus, DisCluster is equivalent to a kernel K-means problem. We call the algorithm Discriminative
K-means (DisKmeans).
3.2 Constructing the Kernel Gram Matrix via Subspace Selection
The kernel Gram matrix in Eq. (15) can be expressed as

˜G = U diag (σ1/(λ + σ1), σ2/(λ + σ2),··· , σn/(λ + σn)) U T .

(16)
Recall that the original DisCluster algorithm involves alternating LDA subspace selection and clus-
tering. The analysis above shows that the LDA subspace selection in DisCluster essentially con-
structs a kernel Gram matrix for clustering. More speciﬁcally, all the eigenvectors in G is kept
unchanged, while the following transformation is applied to the eigenvalues:

Φ(σ) = σ/(λ + σ).

This elucidates the nature of the subspace selection procedure in DisCluster. The clustering algo-
rithm is dramatically simpliﬁed by removing the iterative subspace selection. We thus address issues
(1)–(3) in Section 1. The last issue will be addressed in Section 4 below.
3.3 Connection with Other Clustering Approaches
Consider the limiting case when λ → ∞. It follows from Eq. (16) that ˜G → G/λ. The optimal L is
thus given by solving the following maximization problem:

(cid:161)

(cid:162)

arg max

trace

L

LT GL

.

The solution is given by the standard K-means clustering [4, 5].
Consider the other extreme case when λ → 0. It follows from Eq. (16) that ˜G → U T
1 U1. Note that
the columns of U1 form the full set of (normalized) principal components [9]. Thus, the algorithm
is equivalent to clustering in the (full) principal component space.
4 DisKmeansλ: Discriminative K-means with Automatically Tuned λ
Our experiments show that the value of the regularization parameter λ has a signiﬁcant impact on
the performance of DisKmeans. In this section, we show how to incorporate the automatic tuning
of λ into the optimization framework, thus addressing issue (4) in Section 1.
The maximization problem in Eq. (11) is equivalent to the minimization of the following function:

(cid:195)

(cid:181)

(cid:182)−1

(cid:33)

1
λ

trace

LT

In +

G

L

.

(17)

It is clear that a small value of λ leads to a small value of the objective function in Eq. (17). To
overcome this problem, we include an additional penalty term to control the eigenvalues of the
matrix In + 1

λ G. This leads to the following optimization problem:

g(L, λ) ≡ trace

min
L,λ

LT

In +

L

+ log det

In +

G

.

(18)

(cid:195)

(cid:181)

(cid:182)−1

(cid:33)

1
λ

G

(cid:181)

(cid:182)

1
λ

Note that the objective function in Eq. (18) is closely related to the negative log marginal likelihood
function in Gaussian Process [12] with In + 1
λ G as the covariance matrix. We have the following
main result for this section:
Theorem 4.1. Let G be the Gram matrix deﬁned above and let L be a given weighted cluster
1 be the SVD of G with Σt = diag (σ1,··· , σt)
indicator matrix. Let G = UΣU T = U1ΣtU T
as in Eq. (12), and ai be the i-th diagonal entry of the matrix U T
1 LLT U1. Then for a ﬁxed L,

4

t(cid:88)

i=1

t(cid:88)

the optimal λ∗ solving the optimization problem in Eq. (18) is given by minimizing the following
objective function:

λai
λ + σi

+ log

1 + σi
λ

.

(19)

Proof. Let U = [U1, U2], that is, U2 is the orthogonal complement of U1. It follows that

(cid:181)

(cid:195)

log det

(cid:181)

In +

G

1
λ

(cid:182)−1

= log det

(cid:195)

(cid:182)
(cid:33)

(cid:181)

1
λ

G

trace

LT

In +

L

= trace

LT U1

It +

Σt

U T

1 L

+ trace

LT U2U T

2 L

It +

Σ1

=

log (1 + σi/λ) .

(cid:161)

(20)

(cid:162)

(cid:179)

1
λ

(cid:181)

(cid:161)

(cid:180)

(cid:182)

1
λ

i=1

t(cid:88)
(cid:182)−1
(cid:161)
(cid:162)

(cid:33)

(cid:162)

=

(1 + σi/λ)−1ai + trace

LT U2U T

2 L

,

(21)

The result follows as the second term in Eq. (21), trace

i=1

LT U2U T

2 L

, is a constant.

We can thus solve the optimization problem in Eq. (18) iteratively as follows: For a ﬁxed λ, we
update L by maximizing the objective function in Eq. (17), which is equivalent to the DisKmeans
algorithm; for a ﬁxed L, we update λ by minimizing the objective function in Eq. (19), which is a
single-variable optimization and can be solved efﬁciently using the line search method. We call the
algorithm DisKmeansλ, whose solution depends on the initial value of λ.
5 Kernel DisKmeans: Nonlinear Discriminative K-means using the kernels
The DisKmeans algorithm can be easily extended to deal with nonlinear data using the kernel trick.
Kernel methods [14] work by mapping the data into a high-dimensional feature space F equipped
with an inner product through a nonlinear mapping φ : IRm → F. The nonlinear mapping can
be implicitly speciﬁed by a symmetric kernel function K, which computes the inner product of the
images of each data pair in the feature space. For a given training data set {xi}n
i=1, the kernel Gram
matrix GK is deﬁned as follows: GK(i, j) = (φ(xi), φ(xj)). For a given GK, the weighted cluster
matrix L = [L1,··· , Lk] in kernel DisKmeans is given by minimizing the following objective
function:

(cid:195)

(cid:181)

(cid:182)−1

(cid:33)

k(cid:88)

(cid:181)

(cid:182)−1

trace

LT

In +

L

=

LT
j

In +

GK

Lj.

(22)

j=1

The performance of kernel DisKmeans is dependent on the choice of the kernel Gram matrix.
Following [11], we assume that GK is restricted to be a convex combination of a given set
of kernel Gram matrices {Gi}(cid:96)
i=1 satisfy
i=1 may be

(cid:80)(cid:96)
i=1 θiGi, where the coefﬁcients {θi}(cid:96)
If L is given, the optimal coefﬁcients {θi}(cid:96)

(cid:80)(cid:96)
i=1 θitrace(Gi) = 1 and θi ≥ 0 ∀i.

computed by solving a Semideﬁnite programming (SDP) problem as follows:
Theorem 5.1. Let GK be constrained to be a convex combination of a given set of kernel matrices
{Gi}(cid:96)
i=1 θiGi satisfying the constraints deﬁned above. Then the optimal GK
minimizing the objective function in Eq. (22) is given by solving the following SDP problem:

i=1 as GK =

i=1 as GK =

(cid:80)(cid:96)

1
λ

GK

1
λ

min

t1,··· ,tk,θ

(cid:181)

In + 1
λ

θi ≥ 0 ∀i,

tj

j=1

k(cid:88)
(cid:80)(cid:96)
(cid:96)(cid:88)
(cid:162)−1

LT
j

i=1

s.t.

(cid:161)

Proof. It follows as LT
j

(cid:182)

(cid:186) 0, for j = 1,··· , k,

i=1 θi ˜Gi Lj
tj

θi trace(Gi) = 1.

(cid:181)

I + 1
λ

(cid:80)(cid:96)

i=1 θi ˜Gi Lj
LT
tj
j

(cid:182)

(23)

(cid:186) 0.

In + 1

λ GK

Lj ≤ ti is equivalent to

5

This leads to an iterative algorithm alternating between the computation of the kernel Gram matrix
GK and the computation of the cluster indicator matrix L. The parameter λ can also be incorporated
into the SDP formulation by treating the identity matrix In as one of the kernel Gram matrix as in
[11]. The algorithm is named Kernel DisKmeansλ. Note that unlike the kernel learning in [11], the
class label information is not available in our formulation.
6 Empirical Study
In this section, we empirically study the properties of DisKmeans and its variants, and evaluate the
performance of the proposed algorithms in comparison with several other representative algorithms,
including Locally Linear Embedding (LLE) [13] and Laplacian Eigenmap (Leigs) [1].
Experiment Setup:
All algorithms were implemented us-
ing Matlab and experiments were conducted on a PEN-
TIUM IV 2.4G PC with 1.5GB RAM. We test
these al-
gorithms on eight benchmark data sets.
They are ﬁve
banding, soybean, segment, satimage,
UCI data sets [2]:
pendigits; one biological data set:
leukemia (http://www.
upo.es/eps/aguilar/datasets.html) and two image
data sets: ORL (http://www.uk.research.att.com/
facedatabase.html, sub-sampled to a size of 100*100
= 10000 from 10 persons) and USPS (ftp://ftp.kyb.tuebingen.mpg.de/pub/bs/
data/). See Table 1 for more details. To make the results of different algorithms comparable,
we ﬁrst run K-means and the clustering result of K-means is used to construct the set of k initial
centroids, for all experiments. This process is repeated for 50 times with different sub-samples from
the original data sets. We use two standard measurements: the accuracy (ACC) and the normalized
mutual information (NMI) to measure the performance.

Table 1: Summary of benchmark data sets
Data Set
banding
soybean
segment
pendigits
satimage
leukemia
ORL
USPS

# CL
(k)
2
15
7
10
6
2
10
10

# DIM # INST
(m)
29
35
19
16
36
7129
10304
256

(n)
238
562
2309
10992
6435
72
100
9298

Figure 1: The effect of the regularization parameter λ on DisKmeans and Discluster.

Effect of the regularization parameter λ: Figure 1 shows the accuracy (y-axis) of DisKmeans
and DisCluster for different λ values (x-axis). We can observe that λ has a signiﬁcant impact on
the performance of DisKmeans. This justiﬁes the development of an automatic parameter tuning
process in Section 4. We can also observe from the ﬁgure that when λ → ∞, the performance of
DisKmeans approaches to that of K-means on all eight benchmark data sets. This is consistent with
our theoretical analysis in Section 3.3. It is clear that in many cases, λ = 0 is not the best choice.
Effect of parameter tuning in DisKmeansλ: Figure 2 shows the accuracy of DisKmeansλ using
4 data sets. In the ﬁgure, the x-axis denotes the different λ values used as the starting point for
DisKmeansλ. The result of DisKmeans (without parameter tuning) is also presented for comparison.
We can observe from the ﬁgure that in many cases the tuning process is able to signiﬁcantly improve
the performance. We observe similar trends on other four data sets and the results are omitted.

6

10−610−410−21001021041060.7620.7630.7640.7650.7660.7670.7680.7690.770.7710.772BandingACCl K−meansDisClusterDisKmeans10−610−410−21001021041060.6240.6260.6280.630.6320.6340.6360.6380.640.6420.644soybeanACCl K−meansDisClusterDisKmeans10−610−410−21001021041060.630.640.650.660.670.680.69segmentACCl K−meansDisClusterDisKmeans10−610−410−21001021041060.680.6850.690.6950.7pendigitsACCl K−meansDisClusterDisKmeans10−610−410−21001021041060.610.620.630.640.650.660.670.680.690.70.71satimageACCl K−meansDisClusterDisKmeans10−610−410−21001021041060.7350.740.7450.750.7550.760.7650.770.7750.78leukemiaACCl K−meansDisClusterDisKmeans10−510010510100.7350.7360.7370.7380.7390.740.7410.7420.7430.7440.745ORLACCl K−meansDisClusterDisKmeans10−610−410−21001021041060.540.560.580.60.620.640.660.680.70.72USPSACCl K−meansDisClusterDisKmeansFigure 2: The effect of the parameter tuning in DisKmeansλ using 4 data sets. The x-axis denotes the different
λ values used as the starting point for DisKmeansλ.

Figure 2 also shows that the tuning process is dependent on the initial value of λ due to its non-
convex optimization, and when λ → ∞, the effect of the tuning process become less pronounced.
Our results show that a value of λ, which is neither too large nor too small works well.

Figure 3: Comparison of the trace value achieved by DisKmean and DisCluster. The x-axis denotes the
number of iterations in Discluster. The trace value of DisCluster is bounded from above by that of DisKmean.

DisKmean versus DisCluster: Figure 3 compares the trace value achieved by DisKmean and
the trace value achieved in each iteration of DisCluster on 4 data sets for a ﬁxed λ.
It is clear
that the trace value of DisCluster increases in each iteration but is bounded from above by that of
DisKmean. We observe a similar trend on the other four data sets and the results are omitted. This is
consistent with our analysis in Section 3 that both algorithms optimize the same objective function,
and DisKmean is a direct approach for the trace maximization without the iterative process.
Clustering evaluation: Table 2 presents the accuracy (ACC) and normalized mutual information
(NMI) results of various algorithms on all eight data sets. In the table, DisKmeans (or DisCluster)
with “max” and “ave” stands for the maximal and average performance achieved by DisKmeans and
DisCluster using λ from a wide range between 10−6 and 106. We can observe that DisKmeansλ is
competitive with other algorithms. It is clear that the average performance of DisKmeansλ is robust
against different initial values of λ. We can also observe that the average performance of DisKmeans
and DisCluster is quite similar, while DisCluster is less sensitive to the value of λ.

7 Conclusion
In this paper, we analyze the discriminative clustering (DisCluster) framework, which integrates
subspace selection and clustering. We show that the iterative subspace selection and clustering in
DisCluster is equivalent to kernel K-means with a speciﬁc kernel Gram matrix. We then propose the
DisKmeans algorithm for simultaneous LDA subspace selection and clustering, as well as an auto-
matic parameter tuning procedure. The connection between DisKmeans and several other clustering
algorithms is also studied. The presented analysis and algorithms are veriﬁed through experiments
on a collection of benchmark data sets.
We present the nonlinear extension of DisKmeans in Section 5. Our preliminary studies have shown
the effectiveness of Kernel DisKmeansλ in learning the kernel Gram matrix. However, the SDP
formulation is limited to small-sized problems. We plan to explore efﬁcient optimization techniques
for this problem. Partial label information may be incorporated into the proposed formulations. This
leads to semi-supervised clustering [3]. We plan to examine various semi-learning techniques within
the proposed framework and their effectiveness for clustering from both labeled and unlabeled data.

7

10−610−410−21001021041060.60.620.640.660.680.70.72satimageACCl DisKmeansDisKmeansl10−610−410−21001021041060.680.6820.6840.6860.6880.690.6920.6940.6960.6980.7pendigitsACCl DisKmeansDisKmeansl10−510010510100.730.7320.7340.7360.7380.740.7420.7440.7460.7480.75ORLACCl DisKmeansDisKmeanl10−610−410−21001021041060.540.560.580.60.620.640.660.680.70.72USPSACCl DisKmeansDisKmeansl123456780.0840.0860.0880.090.0920.0940.0960.098 satimageTRACEl DisClusterDisKmeans123450.3410.3420.3430.3440.3450.3460.347pendigitsTRACEl DisClusterDisKmeans12345670.2140.2160.2180.220.2220.2240.2260.2280.23segmentTRACEl DisClusterDisKmeans11.522.533.544.550.0250.02550.0260.02650.0270.0275USPSTRACEl DisClusterDisKmeansTable 2: Accuracy (ACC) and Normalized Mutual Information (NMI) results on 8 data sets. “max” and “ave”
stand for the maximal and average performance achieved by DisKmeans and DisCluster using λ from a wide
range of values between 10−6 and 106. We present the result of DisKmeansλ with different initial λ values.
LLE stands for Local Linear Embedding and LEI for Laplacian Eigenmap. “AVE” stands for the mean of ACC
or NMI on 8 data sets for each algorithm.

DisKmeans

max

ave

DisCluster

max

ave

ACC

Data Sets

banding
soybean
segment
pendigits
satimage
leukemia
ORL
USPS
AVE

banding
soybean
segment
pendigits
satimage
leukemia
ORL
USPS
AVE

0.771
0.641
0.687
0.699
0.701
0.775
0.744
0.712
0.716

0.225
0.707
0.632
0.669
0.593
0.218
0.794
0.647
0.561

0.768
0.634
0.664
0.690
0.651
0.763
0.738
0.628
0.692

0.221
0.701
0.612
0.656
0.537
0.199
0.789
0.544
0.532

0.771
0.633
0.676
0.696
0.654
0.738
0.739
0.692
0.700

0.225
0.698
0.615
0.660
0.551
0.163
0.789
0.629
0.541

10−2

0.771
0.639
0.664
0.700
0.696
0.738
0.749
0.684
0.705

0.225
0.706
0.629
0.661
0.597
0.163
0.800
0.612
0.549

DisKmeansλ
10−1
100

0.771
0.639
0.659
0.696
0.712
0.753
0.743
0.702
0.709

0.225
0.707
0.625
0.658
0.608
0.185
0.795
0.637
0.555

0.771
0.638
0.671
0.696
0.696
0.738
0.748
0.680
0.705

0.225
0.704
0.628
0.658
0.596
0.163
0.801
0.609
0.548

101

0.771
0.637
0.680
0.697
0.683
0.738
0.748
0.684
0.705

0.225
0.704
0.632
0.660
0.586
0.163
0.800
0.612
0.548

LLE

LEI

0.648
0.630
0.594
0.599
0.627
0.714
0.733
0.631
0.647

0.093
0.691
0.539
0.577
0.493
0.140
0.784
0.569
0.486

0.764
0.649
0.663
0.697
0.663
0.686
0.317
0.700
0.642

0.213
0.709
0.618
0.645
0.548
0.043
0.327
0.640
0.468

0.767
0.632
0.672
0.690
0.642
0.738
0.738
0.683
0.695

0.219
0.696
0.608
0.654
0.541
0.163
0.788
0.613
0.535

NMI

Acknowledgments

This research is sponsored by the National Science Foundation Grant IIS-0612069.

References
[1] M. Belkin and P. Niyogi. Laplacian eigenmaps for dimensionality reduction and data representation. In

NIPS, 2003.

[2] C.L. Blake and C.J. Merz. UCI repository of machine learning databases, 1998.
[3] O. Chapelle, B. Sch¨olkopf, and A. Zien. Semi-Supervised Learning. The MIT Press, 2006.
[4] I. S. Dhillon, Y. Guan, and B. Kulis. A uniﬁed view of kernel k-means, spectral clustering and graph

partitioning. Technical report, Department of Computer Sciences, University of Texas at Austin, 2005.

[5] C. Ding and T. Li. Adaptive dimension reduction using discriminant analysis and k-means clustering. In

ICML, 2007.

[6] J. H. Friedman. Regularized discriminant analysis. JASA, 84(405):165–175, 1989.
[7] K. Fukunaga. Introduction to Statistical Pattern Classiﬁcation. Academic Press.
[8] G. H. Golub and C. F. Van Loan. Matrix Computations. The Johns Hopkins Univ. Press, 1996.
[9] I.T. Jolliffe. Principal Component Analysis. Springer; 2nd edition, 2002.
[10] F. De la Torre Frade and T. Kanade. Discriminative cluster analysis. In ICML, pages 241–248, 2006.
[11] G.R.G. Lanckriet, N. Cristianini, P. Bartlett, L. E. Ghaoui, and M. I. Jordan. Learning the kernel matrix

with semideﬁnite programming. JMLR, 5:27–72, 2004.

[12] C.E. Rasmussen and C.K.I. Williams. Gaussian Processes for Machine Learning. The MIT Press, 2006.
[13] S. T. Roweis and L. K. Saul. Nonlinear dimensionality reduction by locally linear embedding. Science,

290:2323–2326, 2000.

[14] B. Sch¨olkopf and A. Smola. Learning with Kernels: Support Vector Machines, Regularization, Optimiza-

tion and Beyond. MIT Press, 2002.

[15] L. Vandenberghe and S. Boyd. Semideﬁnite programming. SIAM Review, 38:49–95, 1996.
[16] J. Ye, Z. Zhao, and H. Liu. Adaptive distance metric learning for clustering. In CVPR, 2007.

8

"
238,2007,On higher-order perceptron algorithms,"A new algorithm for on-line learning linear-threshold functions is proposed which efficiently combines second-order statistics about the data with the logarithmic behavior"" of multiplicative/dual-norm algorithms. An initial theoretical analysis is provided suggesting that our algorithm might be viewed as a standard Perceptron algorithm operating on a transformed sequence of examples with improved margin properties. We also report on experiments carried out on datasets from diverse domains, with the goal of comparing to known Perceptron algorithms (first-order, second-order, additive, multiplicative). Our learning procedure seems to generalize quite well, and converges faster than the corresponding multiplicative baseline algorithms.""","On Higher-Order Perceptron Algorithms ∗

Cristian Brotto

DICOM, Universit`a dell’Insubria
cristian.brotto@gmail.com

Claudio Gentile

DICOM, Universit`a dell’Insubria

claudio.gentile@uninsubria.it

Fabio Vitale

DICOM, Universit`a dell’Insubria

fabiovdk@yahoo.com

Abstract

A new algorithm for on-line learning linear-threshold functions is proposed which
efﬁciently combines second-order statistics about the data with the ”logarithmic
behavior” of multiplicative/dual-norm algorithms. An initial theoretical analysis is
provided suggesting that our algorithm might be viewed as a standard Perceptron
algorithm operating on a transformed sequence of examples with improved mar-
gin properties. We also report on experiments carried out on datasets from diverse
domains, with the goal of comparing to known Perceptron algorithms (ﬁrst-order,
second-order, additive, multiplicative). Our learning procedure seems to general-
ize quite well, and converges faster than the corresponding multiplicative baseline
algorithms.

1 Introduction and preliminaries

The problem of on-line learning linear-threshold functions from labeled data is one which have
spurred a substantial amount of research in Machine Learning. The relevance of this task from
both the theoretical and the practical point of view is widely recognized: On the one hand, linear
functions combine ﬂexiblity with analytical and computational tractability, on the other hand, on-
line algorithms provide efﬁcient methods for processing massive amounts of data. Moreover, the
widespread use of kernel methods in Machine Learning (e.g., [24]) have greatly improved the scope
of this learning technology, thereby increasing even further the general attention towards the speciﬁc
task of incremental learning (generalized) linear functions. Many models/algorithms have been
proposed in the literature (stochastic, adversarial, noisy, etc.) : Any list of references would not do
justice of the existing work on this subject. In this paper, we are interested in the problem of on-
line learning linear-threshold functions from adversarially generated examples. We introduce a new
family of algorithms, collectively called the Higher-order Perceptron algorithm (where ”higher”
means here ”higher than one”, i.e., ”higher than ﬁrst-order” descent algorithms such as gradient-
descent or standard Perceptron-like algorithms”). Contrary to other higher-order algorithms, such
as the ridge regression-like algorithms considered in, e.g., [4, 7], Higher-order Perceptron has the
ability to put together in a principled and ﬂexible manner second-order statistics about the data with
the ”logarithmic behavior” of multiplicative/dual-norm algorithms (e.g., [18, 19, 6, 13, 15, 20]). Our
algorithm exploits a simpliﬁed form of the inverse data matrix, lending itself to be easily combined
with the dual norms machinery introduced by [13] (see also [12, 23]). As we will see, this has also
computational advantages, allowing us to formulate an efﬁcient (subquadratic) implementation.

Our contribution is twofold. First, we provide an initial theoretical analysis suggesting that our
algorithm might be seen as a standard Perceptron algorithm [21] operating on a transformed se-
quence of examples with improved margin properties. The same analysis also suggests a simple
(but principled) way of switching on the ﬂy between higher-order and ﬁrst-order updates. This is
∗The authors gratefully acknowledge partial support by the PASCAL Network of Excellence under EC grant

n. 506778. This publication only reﬂects the authors’ views.

byt ∈ {−1, 1}. Then the true label yt is disclosed. In the case whenbyt 6= yt we say that the algorithm

especially convenient when we deal with kernel functions, a major concern being the sparsity of the
computed solution. The second contribution of this paper is an experimental investigation of our
algorithm on artiﬁcial and real-world datasets from various domains: We compared Higher-order
Perceptron to baseline Perceptron algorithms, like the Second-order Perceptron algorithm deﬁned in
[7] and the standard (p-norm) Perceptron algorithm, as in [13, 12]. We found in our experiments that
Higher-order Perceptron generalizes quite well. Among our experimental ﬁndings are the follow-
ing: 1) Higher-order Perceptron is always outperforming the corresponding multiplicative (p-norm)
baseline (thus the stored data matrix is always beneﬁcial in terms of convergence speed); 2) When
dealing with Euclidean norms (p = 2), the comparison to Second-order Perceptron is less clear and
depends on the speciﬁc task at hand.
Learning protocol and notation. Our algorithm works in the well-known mistake bound model
of on-line learning, as introduced in [18, 2], and further investigated by many authors (e.g., [19, 6,
13, 15, 7, 20, 23] and references therein). Prediction proceeds in a sequence of trials. In each trial
t = 1, 2, . . . the prediction algorithm is given an instance vector in Rn (for simplicity, all vectors are
normalized, i.e., ||xt|| = 1, where || · || is the Euclidean norm unless otherwise speciﬁed), and then
guesses the binary label yt ∈ {−1, 1} associated with xt. We denote the algorithm’s prediction by
has made a prediction mistake. We call an example a pair (xt, yt), and a sequence of examples S
any sequence S = (x1, y1), (x2, y2), . . . , (xT , yT ). In this paper, we are competing against the
class of linear-threshold predictors, parametrized by normal vectors u ∈ {v ∈ Rn : ||v|| = 1}. In
this case, a common way of measuring the (relative) prediction performance of an algorithm A is
to compare the total number of mistakes of A on S to some measure of the linear separability of S.
One such measure (e.g., [24]) is the cumulative hinge-loss (or soft-margin) Dγ(u;S) of S w.r.t. a
t=1 max{0, γ − ytu>xt} (observe
that Dγ(u;S) vanishes if and only if u separates S with margin at least γ.
A mistake-driven algorithm A is one which updates its internal state only upon mistakes. One
can therefore associate with the run of A on S a subsequence M = M(S, A) ⊆ {1, . . . , T} of
mistaken trials. Now, the standard analysis of these algorithms allows us to restrict the behavior
of the comparison class to mistaken trials only and, as a consequence, to reﬁne Dγ(u;S) so as to
t∈M max{0, γ − ytu>xt}. This gives bounds on A’s
performance relative to the best u over a sequence of examples produced (or, actually, selected)
by A during its on-line functioning. Our analysis in Section 3 goes one step further: the number
of mistakes of A on S is contrasted to the cumulative hinge loss of the best u on a transformed
sequence ˜S = ((˜xi1, yi1), (˜xi2, yi2), . . . , (˜xim , yim)), where each instance xik gets transformed
into ˜xik through a mapping depending only on the past behavior of the algorithm (i.e., only on
examples up to trial t = ik−1). As we will see in Section 3, this new sequence ˜S tends to be ”more
separable” than the original sequence, in the sense that if S is linearly separable with some margin,
then the transformed sequence ˜S is likely to be separable with a larger margin.

linear classiﬁer u at a given margin value γ > 0: Dγ(u;S) =PT

include only trials in M: Dγ(u;S) = P

2 The Higher-order Perceptron algorithm

The algorithm (described in Figure 1) takes as input a sequence of nonnegative parameters ρ1, ρ2, ...,
and maintains a product matrix Bk (initialized to the identity matrix I) and a sum vector vk (ini-
tialized to 0). Both of them are indexed by k, a counter storing the current number of mistakes
(plus one). Upon receiving the t-th normalized instance vector xt ∈ Rn, the algorithm computes
t )
while vector vk−1 is updated additively through the standard Perceptron rule vk = vk−1 + yt xt.

its binary prediction valuebyt as the sign of the inner product between vector Bk−1vk−1 and vector
Bk−1xt. Ifbyt 6= yt then matrix Bk−1 is updates multiplicatively as Bk = Bk−1 (I − ρk xtx>
The new matrix Bk and the new vector vk will be used in the next trial. Ifbyt = yt no update is
behavior be substantially different from Perceptron’s) we need to ensureP∞

performed (hence the algorithm is mistake driven). Observe that ρk = 0 for any k makes this algo-
rithm degenerate into the standard Perceptron algorithm [21]. Moreover, one can easily see that, in
order to let this algorithm exploit the information collected in the matrix B (and let the algorithm’s
k=1 ρk = ∞. In the
sequel, our standard choice will be ρk = c/k, with c ∈ (0, 1). See Sections 3 and 4.
Implementing Higher-Order Perceptron can be done in many ways. Below, we quickly describe
three of them, each one having its own merits.
1) Primal version. We store and update an n×n matrix Ak = B>

k Bk and an n-dimensional column

Parameters: ρ1, ρ2, ... ∈ [0, 1).
Initialization: B0 = I; v0 = 0; k = 1.
Repeat for t = 1, 2, . . . , T :

||xt|| = 1;

1. Get instance xt ∈ Rn,
3. Get label yt ∈ {−1, +1};

2. Predictbyt = SGN(w>
4. ifbyt 6= yt then:

k−1xt) ∈ {−1, +1}, where wk−1 = B>

k−1Bk−1vk−1;

vk = vk−1 + yt xt
Bk = Bk−1 (I − ρk xtx>
t )
k ← k + 1.

Figure 1: The Higher-order Perceptron algorithm (for p = 2).

(cid:21)

i,j=1 d(k)

−ρk b
>
k

d(k)
k,k

, where bk = Dk−1X>

k−1xk, and d(k)

k−1x is equal to g>

k−1X>

1 , ..., h(k)
j , it is not hard to show that the margin value w>

vector vk. Matrix Ak is updated as Ak = Ak−1− ρAk−1xx>− ρxx>Ak−1 + ρ2(x>Ak−1x)xx>,
taking O(n2) operations, while vk is updated as in Figure 1. Computing the algorithm’s margin
v>Ax can then be carried out in time quadratic in the dimension n of the input space.
2) Dual version. This implementation allows us the use of kernel functions (e.g., [24]). Let us
denote by Xk the n × k matrix whose columns are the n-dimensional instance vectors x1, ..., xk
where a mistake occurred so far, and yk be the k-dimensional column vector of the corresponding
labels. We store and update the k × k matrix Dk = [d(k)
i,j=1, the k × k diagonal matrix Hk =
i,j ]k
DIAG{hk}, hk = (h(k)
k )> = X>
k Xk yk, and the k-dimensional column vector gk = yk +
If we interpret the primal matrix Ak above as Ak =
Dk Hk 1k, being 1k a vector of k ones.
i,j xix>
k−1x,
and can be computed through O(k) extra inner products. Now, on the k-th mistake, vector g can
be updated with O(k2) extra inner products by updating D and H in the following way. We let
D0 and H0 be empty matrices. Then, given Dk−1 and Hk−1 = DIAG{hk−1}, we have1 Dk =
k. On

I +Pk
(cid:20) Dk−1 −ρk bk
This amounts to say that matrix Ak = I +Pk

the other hand, Hk = DIAG{hk−1 + yk X>
Observe that on trials when ρk = 0 matrix Dk−1 is padded with a zero row and a zero column.
j , is not updated, i.e., Ak = Ak−1. A
closer look at the above update mechanism allows us to conclude that the overall extra inner prod-
ucts needed to compute gk is actually quadratic only in the number of past mistaken trials having
ρk > 0. This turns out to be especially important when using a sparse version of our algorithm
which, on a mistaken trial, decides whether to update both B and v or just v (see Section 4).
3) Implicit primal version and the dual norms algorithm. This is based on the simple observation
that for any vector z we can compute Bkz by unwrapping Bk as in Bkz = Bk−1(I − ρxx>)z =
Bk−1z0, where vector z0 = (z − ρx x>z) can be calculated in time O(n). Thus computing
the margin v>B>
k−1Bk−1x actually takes O(nk). Maintaining this implicit representation for the
product matrix B can be convenient when an efﬁcient dual version is likely to be unavailable,
as is the case for the multiplicative (or, more generally, dual norms) extension of our algorithm.
We recall that a multiplicative algorithm is useful when learning sparse target hyperplanes (e.g.,
[18, 15, 3, 12, 11, 20]). We obtain a dual norms algorithm by introducing a norm parameter p ≥ 2,
and the associated gradient mapping2 g : θ ∈ Rn → ∇θ||θ||2
p / 2 ∈ Rn. Then, in Figure 1, we
normalize instance vectors xt w.r.t. the p-norm, we deﬁne wk−1 = B>
k−1g(Bk−1vk−1), and gen-
eralize the matrix update as Bk = Bk−1(I − ρkxtg(xt)>). As we will see, the resulting algorithm
combines the multiplicative behavior of the p-norm algorithms with the ”second-order” information
contained in the matrix Bk. One can easily see that the above-mentioned argument for computing
the margin g(Bk−1vk−1)>Bk−1x in time O(nk) still holds.

k Xk−1bk − 2ρk + ρ2
k−1X>

k−1xk + yk.

k,k = ρ2
k }, with h(k)

k x>
k = y>

i,j=1 d(k)

i,j xix>

k−1xk , h(k)

1Observe that, by construction, Dk is a symmetric matrix.
2This mapping has also been used in [12, 11]. Recall that setting p = O(log n) yields an algorithm similar

to Winnow [18]. Also, notice that p = 2 yields g = identity.

3 Analysis

We express the performance of the Higher-order Perceptron algorithm in terms of the hinge-loss
behavior of the best linear classiﬁer over the transformed sequence

˜S = (B0xt(1), yt(1)), (B1xt(2), yt(2)), (B2xt(3), yt(3)), . . . ,

(1)
being t(k) the trial where the k-th mistake occurs, and Bk the k-th matrix produced by the algorithm.
Observe that each feature vector xt(k) gets transformed by a matrix Bk depending on past examples
only. This is relevant to the argument that ˜S tends to have a larger margin than the original sequence
(see the discussion at the end of this section). This neat ”on-line structure” does not seem to be
shared by other competing higher-order algorithms, such as the ”ridge regression-like” algorithms
considered, e.g., in [25, 4, 7, 23]. For the sake of simplicity, we state the theorem below only in the
case p = 2. A more general statement holds when p ≥ 2.

Theorem 1 Let the Higher-order Perceptron algorithm in Figure 1 be run on a sequence of exam-
ples S = (x1, y1), (x2, y2), . . . , (xT , yT ). Let the sequence of parameters ρk satisfy 0 ≤ ρk ≤
k−1xt| , where xt is the k-th mistaken instance vector, and c ∈ (0, 1]. Then the total number m
1−c
1+|v>
of mistakes satisﬁes3

s

Dγ(u; ˜Sc))

m ≤ α

γ

+ α2
2γ2 + α

γ

α

Dγ(u; ˜Sc))

γ

+ α2
4γ2 ,

(2)

holding for any γ > 0 and any unit norm vector u ∈ Rn, where α = α(c) = (2 − c)/c.
Proof. The analysis deliberately mimics the standard Perceptron convergence analysis [21]. We ﬁx
an arbitrary sequence S = (x1, y1), (x2, y2), . . . , (xT , yT ) and let M ⊆ {1, 2, . . . , T} be the set
of trials where the algorithm in Figure 1 made a mistake. Let t = t(k) be the trial where the k-th
mistake occurred. We study the evolution of ||Bkvk||2 over mistaken trials. Notice that the matrix
B>
k Bk is positive semideﬁnite for any k. We can write

||Bkvk||2 = ||Bk−1 (I − ρk xtx>

t ) (vk−1 + yt xt)||2

(from the update rule vk = vk−1 + yt xt and Bk = Bk−1 (I − ρk xtx>
= ||Bk−1vk−1 + yt (1 − ρkytvk−1xt − ρk)Bk−1xt||2
= ||Bk−1vk−1||2 + 2 ytrk v>

k||Bk−1xt||2,

k−1Bk−1xt + r2

k−1B>

(using ||xt|| = 1)

t ) )

t Ak−1 xt, where we set Ak = B>

k ≤ (1+ρk|vk−1xt|−ρk)2 ≤ (2−c)2. Now, using yt v>

where we set for brevity rk = 1 − ρkytvk−1xt − ρk. We proceed by upper and lower bounding the
above chain of equalities. To this end, we need to ensure rk ≥ 0. Observe that ytvk−1xt ≥ 0 implies
rk ≥ 0 if and only if ρk ≤ 1/(1 + ytvk−1xt). On the other hand, if ytvk−1xt < 0 then, in order for
rk to be nonnegative, it sufﬁces to pick ρk ≤ 1. In both cases ρk ≤ (1 − c)/(1 + |vk−1xt|) implies
k−1Bk−1xt ≤ 0
rk ≥ c > 0, and also r2
||Bkvk||2 − ||Bk−1vk−1||2 ≤ (2 − c)2 ||Bk−1 xt||2 =
(combined with rk ≥ 0), we conclude that
(2 − c)2 x>
k Bk. A simple4 (and crude) upper bound on the last
t Ak−1 xt ≤ ||Ak−1||, the spectral norm (largest
term follows by observing that ||xt|| = 1 implies x>
eigenvalue) of Ak−1. Since a factor matrix of the form (I − ρ xx>) with ρ ≤ 1 and ||x|| = 1 has
t(i)||2 ≤ 1. Therefore,
spectral norm one, we have x>
summing over k = 1, . . . , m = |M| (or, equivalently, over t ∈ M) and using v0 = 0 yields the
upper bound
(3)
To ﬁnd a lower bound of the left-hand side of (3), we ﬁrst pick any unit norm vector u ∈ Rn, and
apply the standard Cauchy-Schwartz inequality: ||Bmvm|| ≥ u>Bmvm. Then, we observe that for
a generic trial t = t(k) the update rule of our algorithm allows us to write

t Ak−1 xt ≤ ||Ak−1|| ≤Qk−1

||Bmvm||2 ≤ (2 − c)2 m.

i=1 ||I − ρi xt(i)x>

k−1B>

u>Bkvk − u>Bk−1vk−1 = rk yt u>Bk−1xt ≥ rk (γ − max{0, γ − yt u>Bk−1xt}),

where the last inequality follows from rk ≥ 0 and holds for any margin value γ > 0. We sum
3The subscript c in ˜Sc emphasizes the dependence of the transformed sequence on the choice of c. Note
that in the special case c = 1 we have ρk = 0 for any k and α = 1, thereby recovering the standard Perceptron
bound for nonseparable sequences (see, e.g., [12]).
4A slightly more reﬁned bound can be derived which depends on the trace of matrices I − Ak. Details will

be given in the full version of this paper.

the above over k = 1, . . . , m and exploit c ≤ rk ≤ 2 − c after rearranging terms. This gets
||Bmvm|| ≥ u>Bmvm ≥ c γ m− (2− c)Dγ(u; ˜Sc). Combining with (3) and solving for m gives
(cid:3)
the claimed bound.

From the above result one can see that our algorithm might be viewed as a standard Perceptron
algorithm operating on the transformed sequence ˜Sc in (1). We now give a qualitative argument,
which is suggestive of the improved margin properties of ˜Sc. Assume for simplicity that all examples
(xt, yt) in the original sequence are correctly classiﬁed by hyperplane u with the same margin
γ = yt u>xt > 0, where t = t(k). According to Theorem 1, the parameters ρ1, ρ2, . . . should be
small positive numbers. Assume, again for simplicity, that all ρk are set to the same small enough
t(i)) can be approximated as
t(i). Then, to the extent that the above approximation holds, we can write:5

value ρ > 0. Then, up to ﬁrst order, matrix Bk =Qk
Bk ’ I − ρPk
yt u>Bk−1xt = yt u>(cid:0)I − ρPk−1
(cid:0)Pk−1

(cid:1)xt = yt u>(cid:0)I − ρPk−1

(cid:1)xt = γ − ρ γ yt v>

i=1 yt(i)xt(i) yt(i)x>

i=1(I − ρ xt(i)x>

i=1 yt(i) u>xt(i) yt(i)x>

t(i)

(cid:1)xt

t(i)

k−1xt.

i=1 xt(i)x>

= yt u>xt − ρ yt

i=1 xt(i)x>

t(i)

k−1B>

k−1wk−1 = v>

k−1xt ≤ 0 is more likely to imply yt v>

Now, yt v>
k−1xt is the margin of the (ﬁrst-order) Perceptron vector vk−1 over a mistaken trial for
the Higher-order Perceptron vector wk−1. Since the two vectors vk−1 and wk−1 are correlated
k−1Bk−1vk−1 = ||Bk−1vk−1||2 ≥ 0) the mistaken condition
(recall that v>
k−1xt ≤ 0 than the opposite. This tends to yield a
yt w>
margin larger than the original margin γ. As we mentioned in Section 2, this is also advantageous
from a computational standpoint, since in those cases the matrix update Bk−1 → Bk might be
skipped (this is equivalent to setting ρk = 0), still Theorem 1 would hold.
Though the above might be the starting point of a more thorough theoretical understanding of the
margin properties of our algorithm, in this paper we prefer to stop early and leave any further inves-
tigation to collecting experimental evidence.

4 Experiments

We tested the empirical performance of our algorithm by conducting a number of experiments on a
collection of datasets, both artiﬁcial and real-world from diverse domains (Optical Character Recog-
nition, text categorization, DNA microarrays). The main goal of these experiments was to compare
Higher-order Perceptron (with both p = 2 and p > 2) to known Perceptron-like algorithms, such
as ﬁrst-order [21] and second-order Perceptron [7], in terms of training accuracy (i.e., convergence
speed) and test set accuracy. The results are contained in Tables 1, 2, 3, and in Figure 2.
Task 1: DNA microarrays and artiﬁcial data. The goal here was to test the convergence proper-
ties of our algorithms on sparse target learning tasks. We ﬁrst tested on a couple of well-known DNA
microarray datasets. For each dataset, we ﬁrst generated a number of random training/test splits (our
random splits also included random permutations of the training set). The reported results are aver-
aged over these random splits. The two DNA datasets are: i. The ER+/ER− dataset from [14]. Here
the task is to analyze expression proﬁles of breast cancer and classify breast tumors according to ER
(Estrogen Receptor) status. This dataset (which we call the “Breast” dataset) contains 58 expression
proﬁles concerning 3389 genes. We randomly split 1000 times into a training set of size 47 and a
test set of size 11.
ii. The “Lymphoma” dataset [1]. Here the goal is to separate cancerous and
normal tissues in a large B-Cell lymphoma problem. The dataset contains 96 expression proﬁles
concerning 4026 genes. We randomly split the dataset into a training set of size 60 and a test set of
size 36. Again, the random split was performed 1000 times. On both datasets, the tested algorithms
have been run by cycling 5 times over the current training set. No kernel functions have been used.
We also artiﬁcially generated two (moderately) sparse learning problems with margin γ ≥ 0.005 at
labeling noise levels η = 0.0 (linearly separable) and η = 0.1, respectively. The datasets have been
generated at random by ﬁrst generating two (normalized) target vectors u ∈ {−1, 0, +1}500, where
the ﬁrst 50 components are selected independently at random in {−1, +1} and the remaining 450
5Again, a similar argument holds in the more general setting p ≥ 2. The reader should notice how important

the dependence of Bk on the past is to this argument.

components are 0. Then we set η = 0.0 for the ﬁrst target and η = 0.1 for the second one and,
corresponding to each of the two settings, we randomly generated 1000 training examples and 1000
test examples. The instance vectors are chosen at random from [−1, +1]500 and then normalized. If
u · xt ≥ γ then a +1 label is associated with xt. If u · xt ≤ −γ then a −1 label is associated with
xt. The labels so obtained are ﬂipped with probability η. If |u · xt| < γ then xt is rejected and
a new vector xt is drawn. We call the two datasets ”Artiﬁcial 0.0” and ”Artiﬁcial 0.1”. We tested
our algorithms by training over an increasing number of epochs and checking the evolution of the
corresponding test set accuracy. Again, no kernel functions have been used.
Task 2: Text categorization. The text categorization datasets are derived from the ﬁrst 20,000
newswire stories in the Reuters Corpus Volume 1 (RCV1, [22]). A standard TF-IDF bag-of-words
encoding was used to transform each news story into a normalized vector of real attributes. We
built four binary classiﬁcation problems by “binarizing” consecutive news stories against the four
target categories 70, 101, 4, and 59. These are the 2nd, 3rd, 4th, and 5th most frequent6 categories,
respectively, within the ﬁrst 20,000 news stories of RCV1. We call these datasets RCV1x, where
x = 70, 101, 4, 59. Each dataset was split into a training set of size 10,000 and a test set of the same
size. All algorithms have been trained for a single epoch. We initially tried polynomial kernels,
then realized that kernel functions did not signiﬁcantly alter our conclusions on this task. Thus the
reported results refer to algorithms with no kernel functions.
Task 3: Optical character recognition (OCR). We used two well-known OCR benchmarks: the
USPS dataset and the MNIST dataset [16] and followed standard experimental setups, such as the
one in [9], including the one-versus-rest scheme for reducing a multiclass problem to a set of binary
tasks. We used for each algorithm the standard Gaussian and polynomial kernels, with parameters
chosen via 5-fold cross validation on the training set across standard ranges. Again, all algorithms
have been trained for a single epoch over the training set. The results in Table 3 only refer to the
best parameter settings for each kernel.
Algorithms. We implemented the standard Perceptron algorithm (with and without kernels), the
Second-order Perceptron algorithm, as described in [7] (with and without kernels), and our Higher-
order Perceptron algorithm. The implementation of the latter algorithm (for both p = 2 and p > 2)
was ”implicit primal” when tested on the sparse learning tasks, and in dual variables for the other two
tasks. When using Second-order Perceptron, we set its parameter a (see [7] for details) by testing
on a generous range of values. For brevity, only the settings achieving the best results are reported.
On the sparse learning tasks we tried Higher-order Perceptron with norm p = 2, 4, 7, 10, while on
the other two tasks we set p = 2. In any case, for each value of p, we set7 ρk = c/k, with c =
0, 0.2, 0.4, 0.6, 0.8. Since c = 0 corresponds to a standard p-norm Perceptron algorithm [13, 12] we
tried to emphasize the comparison c = 0 vs. c > 0. Finally, when using kernels on the OCR tasks,
we also compared to a sparse dual version of Higher-order Perceptron. On a mistaken round t =
t(k), this algorithm sets ρk = c/k if yt vk−1xt ≥ 0, and ρk = 0 otherwise (thus, when yt vk−1xt <
0 the matrix Bk−1 is not updated). For the sake of brevity, the standard Perceptron algorithm is
called FO (”First Order”), the Second-order algorithm is denoted by SO (”Second Order”), while the
Higher-order algorithm with norm parameter p and ρk = c/k is abbreviated as HOp(c). Thus, for
instance, FO = HO2(0).
Results and conclusions. Our Higher-order Perceptron algorithm seems to deliver interesting
results. In all our experiments HOp(c) with c > 0 outperforms HOp(0). On the other hand, the
comparison HOp(c) vs. SO depends on the speciﬁc task. On the DNA datasets, HOp(c) with c > 0 is
clearly superior in Breast. On Lymphoma, HOp(c) gets worse as p increases. This is a good indica-
tion that, in general, a multiplicative algorithm is not suitable for this dataset. In any case, HO2 turns
out to be only slightly worse than SO. On the artiﬁcial datasets HOp(c) with c > 0 is always better
than the corresponding p-norm Perceptron algorithm. On the text categorization tasks, HO2 tends to
perform better than SO. On USPS, HO2 is superior to the other competitors, while on MNIST it per-
forms similarly when combined with Gaussian kernels (though it turns out to be relatively sparser),
while it is slightly inferior to SO when using polynomial kernels. The sparse version of HO2 cuts
the matrix updates roughly by half, still maintaining a good performance. In all cases HO2 (either
sparse or not) signiﬁcantly outperforms FO.
In conclusion, the Higher-order Perceptron algorithm is an interesting tool for on-line binary clas-

6We did not use the most frequent category because of its signiﬁcant overlap with the other ones.
7Notice that this setting fulﬁlls the condition on ρk stated in Theorem 1.

Table 1: Training and test error on the two datasets ”Breast” and ”Lymphoma”. Training error is
the average total number of updates over 5 training epochs, while test error is the average fraction
of misclassiﬁed patterns in the test set, The results refer to the same training/test splits. For each
algorithm, only the best setting is shown (best training and best test setting coincided in these ex-
periments). Thus, for instance, HO2 differs from FO because of the c parameter. We emphasized
the comparison HO7(0) vs. HO7(c) with best c among the tested values. According to Wilcoxon
signed rank test, an error difference of 0.5% or larger might be considered signiﬁcant. In bold are
the smallest ﬁgures achieved on each row of the table.
HO4
24.5

HO7(0)
47.4

BREAST

HO2
21.7

FO
45.2

HO7
24.5
23.4% 16.4% 13.3% 15.7% 12.0%
20.0

HO10
32.4
13.5
23.1
11.8% 10.0% 10.0% 11.5% 11.5% 11.9%

23.0

SO
29.6
15.0%
19.3
9.6%

LYMPHOMA

22.1

19.6

18.9

TRAIN
TEST
TRAIN
TEST

Figure 2: Experiments on the two artiﬁcial datasets (Artiﬁcial0.0, on the left, and Artiﬁcial0.1, on
the right). The plots give training and test behavior as a function of the number of training epochs.
Notice that the test set in Artiﬁcial0.1 is affected by labelling noise of rate 10%. Hence, a visual
comparison between the two plots at the bottom can only be made once we shift down the y-axis of
the noisy plot by 10%. On the other hand, the two training plots (top) are not readily comparable.
The reader might have difﬁculty telling apart the two kinds of algorithms HOp(0.0) and HOp(c) with
c > 0. In practice, the latter turned out to be always slightly superior in performance to the former.

siﬁcation, having the ability to combine multiplicative (or nonadditive) and second-order behavior
into a single inference procedure. Like other algorithms, HOp can be extended (details omitted due
to space limitations) in several ways through known worst-case learning technologies, such as large
margin (e.g., [17, 11]), label-efﬁcient/active learning (e.g., [5, 8]), and bounded memory (e.g., [10]).

References
[1] A. Alizadeh, et al. (2000). Distinct types of diffuse large b-cell lymphoma identiﬁed by gene expression

proﬁling. Nature, 403, 503–511.

[2] D. Angluin (1988). Queries and concept learning. Machine Learning, 2(4), 319–342.
[3] P. Auer & M.K. Warmuth (1998). Tracking the best disjunction. Machine Learning, 32(2), 127–150.
[4] K.S. Azoury & M.K. Warmuth (2001). Relative loss bounds for on-line density estimation with the

exponential familiy of distributions. Machine Learning, 43(3), 211–246.

[5] A. Bordes, S. Ertekin, J. Weston, & L. Bottou (2005). Fast kernel classiﬁers with on-line and active

learning. JMLR, 6, 1579–1619.

[6] N. Cesa-Bianchi, Y. Freund, D. Haussler, D.P. Helmbold, R.E. Schapire, & M.K. Warmuth (1997). How

to use expert advice. J. ACM, 44(3), 427–485.

1510205321# of training updates600800500400300Training updates vs training epochs on Artificial      # of training epochs0.0700HO  (0.0)*SO    (a = 0.2)7HO  (0.4)2HO  (0.4)4HO    (0.4)7FO = HO  (0.0)*******21510205321# of training updates160024001200800400Training updates vs training epochs on Artificial      # of training epochs0.12000HO  (0.0)*SO    (a = 0.2)7HO  (0.4)2HO  (0.4)4HO  (0.4)7FO = HO  (0.0)*******21510205321Test error rates18%26%14%10%  6%Test error rates vs training epochs on Artificial      # of training epochs0.022%HO  (0.0)*SO    (a = 0.2)7HO  (0.4)2HO  (0.4)4HO    (0.4)7FO = HO  (0.0)**2*****1510205321Test error rates (minus 10%)18%26%14%10%  6%Test error rates vs training epochs on Artificial      # of training epochs0.122%HO  (0.0)*SO    (a = 0.2)7HO  (0.4)2HO  (0.4)4HO    (0.4)7FO = HO  (0.0)**2*****Table 2: Experimental results on the four binary classiﬁcation tasks derived from RCV1. ”Train”
denotes the number of training corrections, while ”Test” gives the fraction of misclassiﬁed patterns
in the test set. Only the results corresponding to the best test set accuracy are shown. In bold are the
smallest ﬁgures achieved for each of the 8 combinations of dataset (RCV1x, x = 70, 101, 4, 59) and
phase (training or test).

FO

HO2

SO

RCV170
RCV1101
RCV14
RCV159

TRAIN
993
673
803
767

TEST
7.20%
6.39%
6.14%
6.45%

TRAIN
941
665
783
762

TEST
6.83%
5.81%
5.94%
6.04%

TRAIN
880
677
819
760

TEST
6.95%
5.48%
6.05%
6.84%

Table 3: Experimental results on the OCR tasks. ”Train” denotes the total number of training cor-
rections, summed over the 10 categories, while ”Test” denotes the fraction of misclassiﬁed patterns
in the test set. Only the results corresponding to the best test set accuracy are shown. For the sparse
version of HO2 we also reported (in parentheses) the number of matrix updates during training. In
bold are the smallest ﬁgures achieved for each of the 8 combinations of dataset (USPS or MNIST),
kernel type (Gaussian or Polynomial), and phase (training or test).

USPS

GAUSS
POLY

MNIST GAUSS

POLY

FO

TRAIN
1385
1609
5834
8148

TEST
6.53%
7.37%
2.10%
3.04%

HO2

TEST

Sparse HO2
TRAIN
4.76% 965 (440)
5.71% 1081 (551)
1.79% 5363 (2596)
2.27% 6476 (3311)

TEST
5.13%
5.52%
1.81%
2.28%

TRAIN
945
1090
5351
6404

SO

TRAIN
1003
1054
5684
6440

TEST
5.05%
5.53%
1.82%
2.03%

[7] N. Cesa-Bianchi, A. Conconi & C. Gentile (2005). A second-order perceptron algorithm. SIAM Journal

of Computing, 34(3), 640–668.

[8] N. Cesa-Bianchi, C. Gentile, & L. Zaniboni (2006). Worst-case analysis of selective sampling for linear-

threshold algorithms. JMLR, 7, 1205–1230.

[9] C. Cortes & V. Vapnik (1995). Support-vector networks. Machine Learning, 20(3), 273–297.
[10] O. Dekel, S. Shalev-Shwartz, & Y. Singer (2006). The Forgetron: a kernel-based Perceptron on a ﬁxed

budget. NIPS 18, MIT Press, pp. 259–266.

[11] C. Gentile (2001). A new approximate maximal margin classiﬁcation algorithm. JMLR, 2, 213–242.
[12] C. Gentile (2003). The Robustness of the p-norm Algorithms. Machine Learning, 53(3), pp. 265–299.
[13] A.J. Grove, N. Littlestone & D. Schuurmans (2001). General convergence results for linear discriminant

updates. Machine Learning Journal, 43(3), 173–210.

[14] S. Gruvberger, et al. (2001). Estrogen receptor status in breast cancer is associated with remarkably dis-

tinct gene expression patterns. Cancer Res., 61, 5979–5984.

[15] J. Kivinen, M.K. Warmuth, & P. Auer (1997). The perceptron algorithm vs. winnow: linear vs. logarithmic

mistake bounds when few input variables are relevant. Artiﬁcial Intelligence, 97, 325–343.

[16] Y. Le Cun, et al. (1995). Comparison of learning algorithms for handwritten digit recognition. ICANN

1995, pp. 53–60.

[17] Y. Li & P. Long (2002). The relaxed online maximum margin algorithm. Machine Learning, 46(1-3),

361–387.

[18] N. Littlestone (1988). Learning quickly when irrelevant attributes abound: a new linear-threshold algo-

rithm. Machine Learning, 2(4), 285–318.

[19] N. Littlestone & M.K. Warmuth (1994). The weighted majority algorithm. Information and Computation,

108(2), 212–261.

[20] P. Long & X. Wu (2004). Mistake bounds for maximum entropy discrimination. NIPS 2004.
[21] A.B.J. Novikov (1962). On convergence proofs on perceptrons. Proc. of the Symposium on the Mathe-

matical Theory of Automata, vol. XII, pp. 615–622.

[22] Reuters: 2000. http://about.reuters.com/researchandstandards/corpus/.
[23] S. Shalev-Shwartz & Y. Singer (2006). Online Learning Meets Optimization in the Dual. COLT 2006, pp.

423–437.

[24] B. Schoelkopf & A. Smola (2002). Learning with kernels. MIT Press.
[25] Vovk, V. (2001). Competitive on-line statistics. International Statistical Review, 69, 213-248.

"
853,2007,Fast Variational Inference for Large-scale Internet Diagnosis,"Web servers on the Internet need to maintain high reliability, but the cause of intermittent failures of web transactions is non-obvious. We use Bayesian inference to diagnose problems with web services. This diagnosis problem is far larger than any previously attempted: it requires inference of 10^4 possible faults from 10^5 observations. Further, such inference must be performed in less than a second. Inference can be done at this speed by combining a variational approximation, a mean-field approximation, and the use of stochastic gradient descent to optimize a variational cost function. We use this fast inference to diagnose a time series of anomalous HTTP requests taken from a real web service. The inference is fast enough to analyze network logs with billions of entries in a matter of hours.","Fast Variational Inference

for Large-scale Internet Diagnosis

John C. Platt

Emre Kıcıman

Microsoft Research

1 Microsoft Way

Redmond, WA 98052

{jplatt,emrek,dmaltz}@microsoft.com

David A. Maltz

Abstract

Web servers on the Internet need to maintain high reliability, but the cause
of intermittent failures of web transactions is non-obvious. We use approx-
imate Bayesian inference to diagnose problems with web services. This
diagnosis problem is far larger than any previously attempted: it requires
inference of 104 possible faults from 105 observations. Further, such infer-
ence must be performed in less than a second. Inference can be done at
this speed by combining a mean-ﬁeld variational approximation and the
use of stochastic gradient descent to optimize a variational cost function.
We use this fast inference to diagnose a time series of anomalous HTTP
requests taken from a real web service. The inference is fast enough to
analyze network logs with billions of entries in a matter of hours.

1 Introduction

Internet content providers, such as MSN, Google and Yahoo, all depend on the correct
functioning of the wide-area Internet to communicate with their users and provide their
services. When these content providers lose network connectivity with some of their users,
it is critical that they quickly resolve the problem, even if the failure lies outside their own
1 One challenge is that content providers have little direct visibility into the
systems.
wide-area Internet infrastructure and the causes of user request failures. Requests may fail
because of problems in the content provider’s systems or faults in the network infrastructure
anywhere between the user and the content provider, including routers, proxies, ﬁrewalls,
and DNS servers. Other failing requests may be due to denial of service attacks or bugs in
the user’s software. To compound the diagnosis problem, these faults may be intermittent:
we must use probabilistic inference to perform diagnosis, rather than using logic.

A second challenge is the scale involved. Not only do popular Internet content providers
receive billions of HTTP requests a week, but the number of potential causes of failure are
numerous. Counting only the coarse-grained Autonomous Systems (ASes) through which
users receive Internet connectivity, there are over 20k potential causes of failure. In this
paper, we show that approximate Bayesian inference scales to handle this high rate of
observations and accurately estimates the underlying failure rates of such a large number of
potential causes of failure.

To scale Bayesian inference to Internet-sized problems, we must make several simplifying
approximations. First, we introduce a bipartite graphical model using overlapping noisy-
ORs, to model the interactions between faults and observations. Second, we use mean-

1A loss of connectivity to users translates directly into lost revenue and a sullied reputation for

content providers, even if the cause of the problem is a third-party network component.

1

ﬁeld variational inference to map the diagnosis problem to a reasonably-sized optimization
problem. Third, we further approximate the integral in the variational method. Fourth, we
speed up the optimization problem using stochastic gradient descent.

The paper is structured as follows: Section 1.1 discusses related work to this paper. We
describe the graphical model in Section 2, and the approximate inference in that model
in Section 2.1, including stochastic gradient descent (in Section 3). We present inference
results on synthetic and real data in Section 4 and then draw conclusions.

1.1 Previous Work

The original application of Bayesian diagnosis was medicine. One of the original diagno-
sis network was QMR-DT [14], a bipartite graphical model that used noisy-OR to model
symptoms given diseases. Exact inference in such networks is intractable (exponential in the
number of positive symptoms,[2]), so diﬀerent approximation and sampling algorithms were
proposed. Shwe and Cooper proposed likelihood-weighted sampling [13], while Jaakkola
and Jordan proposed using a variational approximation to unlink each input to the net-
work [3]. With only thousands of possible symptoms and hundreds of diseases, QMR-DT
was considered very challenging.

More recently, researchers have applied Bayesian techniques for the diagnosis of computers
and networks [1][12][16]. This work has tended to avoid inference in large networks, due to
speed constraints. In contrast, we attack the enormous inference problem directly.

2 Graphical model of diagnosis

Bernoulli

Beta

Noisy
OR

Figure 1: The full graphical model for the diagnosis of Internet faults

The initial graphical model for diagnosis is shown in Figure 1. Starting at the bottom, we
observe a large number of binary random variables, each corresponding to the success/failure
of a single HTTP request. The failure of an HTTP request can be modeled as a noisy-OR [11]
of a set of Bernoulli-distributed binary variables, each of which models the underlying factors
that can cause a request to fail:

P (Vi = fail|Dij) = 1 − (1 − ri0)Yj

(1 − rijdij),

(1)

where rij is the probability that the observation is a failure if a single underlying fault dij
is present. The matrix rij is typically very sparse, because there are only a small number of
possible causes for the failure of any request. The ri0 parameter models the probability of a
spontaneous failure without any known cause. The rij are set by elicitation of probabilities
from an expert.

The noisy-OR models the causal structure in the network, and its connections are derivable
from the metadata associated with the HTTP request. For example, a single request can fail

2

Figure 2: Graphical model after integrating out instantaneous faults: a bipartite noisy-OR
network with Beta distributions as hidden variables

because its server has failed, or because a misconﬁgured or overloaded router can cause an
AS to lose connectivity to the content provider, or because the user agent is not compatible
with the service. All of these underlying causes are modeled independently for each request,
because possible faults in the system can be intermittent.

Each of the Bernoulli variables Dij depends on an underlying continuous fault rate variable
Fj ∈ [0, 1]:

P (Dij|Fj = µj) = µdij

j (1 − µj)1−dij ,

(2)

where µj is the probability of a fault manifesting at any time. We model the Fj as inde-
pendent Beta distributions, one for each fault:

p(Fj = µj) =

1
j , β 0
B(α0
j )

j −1

α0
µ
j

(1 − µj)β 0

j −1,

(3)

where B is the beta function. The fan-out for each of these fault rates can be diﬀerent:
some of these fault rates are connected to many observations, while less common ones are
connected to fewer.
Our goal is to model the posterior distribution P ( ~F |~V ) in order to identify hidden faults
and track them through time. The existence of the Dij random variable is a nuisance. We
do not want to estimate P ( ~D|~V ) for any Dij: the distribution of instantaneous problems is
not interesting. Fortunately, we can exactly integrate out these nuisance variables, because
they are connected to only one observation thru a noisy-OR.

After integrating out the Dij, the graphical model is shown in Figure 2. The model is now
completely analogous to the QMR-DT mode [14], but instead of the noisy-OR combining
binary random variables, they combine rate variables:

P (Vi = fail|Fj = µj) = 1 − (1 − ri0)Yj

(1 − rijµj).

(4)

One can view (4) as a generalization of a noisy-OR to continuous [0, 1] variables.

2.1 Approximations to make inference tractable

In order to scale inference up to 104 hidden variables, and 105 observations, we choose a
simple, robust approximate inference algorithm: mean-ﬁeld variational inference [4]. Mean-
ﬁeld variational inference approximates the posterior P ( ~F |~V ) with a factorized distribution.
For inferring fault rates, we choose to approximate P with a product of beta distributions

Q( ~F |~V ) =Yj

q(Fj|~V ) =Yj

1

B(αj, βj)

µαj −1

j

(1 − µj)βj −1.

(5)

3

Mean-ﬁeld variational inference maximizes a lower bound on the evidence of the model:

max
~α,~β

L =Z Q(~µ|~V ) log

P (~V |~µ)p(~µ)

Q(~µ|~V )

d~µ.

(6)

This integral can be broken into two terms: a cross-entropy between the approximate pos-
terior and the prior, and an expected log-likelihood of the observations:

max
~α,~β

L = −Z Q(~µ|~V ) log

Q(~µ|~V )

p(~µ)

d~µ +Dlog P (~V | ~F )EQ

.

(7)

The ﬁrst integral is the negative of a sum of cross-entropies between Beta distributions with
a closed form:

DKL(qj||pj) = log  B(α0

B(αj, βj)! + (αj − α0

j , β 0
j )

j )ψ(αj)

(8)

+(βj − β 0

j )ψ(βj) − (αj + βj − α0

j − β 0

j )ψ(αj + βj),

where ψ is the digamma function.

However, the expected log likelihood of a noisy-OR integrated over a product of Beta dis-
tributions does not have an analytic form. Therefore, we employ the MF(0) approximation
of Ng and Jordan [9], replacing the expectation of the log likelihood with the log likelihood
of the expectation. The second term then becomes the sum of a set of log likelihoods, one
per observation:

L(Vi) =(log(cid:16)1 − (1 − ri0)Qj[1 − rijαj/(αj + βj)](cid:17) if Vi = 1 (failure);

if Vi = 0 (success).

log(1 − ri0) +Pj log[1 − rijαj/(αj + βj)]

For the Internet diagnosis case, the MF(0) approximation is reasonable: we expect the
posterior distribution to be concentrated around its mean, due to the large amount of data
that is available. Ng and Jordan [9] have have proved accuracy bounds for MF(0) based on
the number of parents that an observation has.

(9)

The ﬁnal cost function for a minimization routine then becomes

min
~α,~β

C =Xj

DKL(qj||pj) −Xi

L(Vi).

(10)

3 Variational inference by stochastic gradient descent

In order to apply unconstrained optimization algorithms to minimize (10), we need transform
the variables: only positive αj and βj are valid, so we parameterize them by

and the gradient computation becomes

αj = eaj ,

βj = ebj .

(11)

(12)

∂C
∂aj

= αj
Xj

∂DKL(qj||pj)

∂αj

−Xi

∂L(Vi)

∂αj 
 .

with a similar gradient for bj. Note that this gradient computation can be quite computa-
tionally expensive, given that i sums over all of the observations.

For Internet diagnosis, we can decompose the observation stream into blocks, where the size
of the block is determined by how quickly the underlying rates of faults change, and how
ﬁnely we want to sample those rates. We typically use blocks of 100,000 observations, which
can make the computation of the gradient expensive. Further, we repeat the inference over
and over again, on thousands of blocks of data: we prefer a fast optimization procedure over
a highly accurate one.

Therefore, we investigated the use of stochastic gradient descent for optimizing the vari-
ational cost function. Stochastic gradient descent approximates the full gradient with a

4

Algorithm 1 Variational Gradient Descent
Require: Noisy-OR parameters rij, priors α0

j , β 0

j , observations Vi

Initialize aj = log(α0
Initialize yi, zj to 0
for k = 1 to number of epochs do

j ), bj = log(β 0
j )

for all Faults j do

αj = exp(aj), βj = exp(bj)
yj ← ξyj + (1 − ξ)∂DKL(qj||pj; αj, βj)/∂aj
zj ← ξzj + (1 − ξ)∂DKL(qj||pj; αj, βj)/∂bj
aj ← aj − ηyj
bj ← bj − ηzj

end for
for all Observations i do

for all Parent faults j of observation vi do

αj = exp(aj), βj = exp(bj)

end for
for all Parent faults j of observation vi do

yj ← ξyj − (1 − ξ)∂L(Vi; ~α, ~β)/∂aj
zj ← ξzj − (1 − ξ)∂L(Vi; ~α, ~β)/∂bj
aj ← aj − ηyj
bj ← bj − ηzj

end for

end for

end for

single term from the gradient: the state of the optimization is updated using that single
term [5]. This enables the system to converge quickly to an approximate answer. The details
of stochastic gradient descent are shown in Algorithm 1.

Estimating the sum in equation (12) with a single term adds a tremendous amount of noise
to the estimates. For example, the sign of a single L(Vi) gradient term depends only on
the sign of Vi. In order to reduce the noise in the estimate, we use momentum [15]: we
exponentially smooth the gradient with a ﬁrst-order ﬁlter before applying it to the state
variables. This momentum modiﬁcation is shown in Algorithm 1. We typically use a large
step size (η = 0.1) and momentum term (ξ = 0.99), in order to both react quickly to changes
in the fault rate and to smooth out noise.

Stochastic gradient descent can be used as a purely on-line method (where each data point
is seen only once), setting the “number of epochs” in Algorithm 1 to 1. Alternatively, it can
get higher accuracy if it is allowed to sweep through the data multiple times.

3.1 Other possible approaches

We considered and tested several other approaches to solving the approximate inference
problem.

Jaakkola and Jordan propose a variational inference method for bipartite noisy-OR net-
works [3], where one variational parameter is introduced to unlink one observation from
the network. We typically have far more observations than possible faults: this previous
approach would have forced us to solve very large optimization problems (with 100,000 pa-
rameters). Instead, we solve an optimization that has dimension equal to the number of
faults.

We originally optimized the variational cost function (10) with both BFGS and the trust-
region algorithm in the Matlab optimization toolbox. This turned out to be far worse than
stochastic gradient descent. We found that a C# implementation of L-BFGS, as described in
Nocedal and Wright [10] sped up the exact optimization by orders of magnitude. We report
on the L-BFGS performance, below:
it is within 4x the speed of the stochastic gradient
descent.

5

We experimented with Metropolis-Hastings to sample from the posterior, using a Gaussian
random walk in (aj, bj). We found that the burn-in time was very long. Also, each update
is slow, because the speed of a single update depends on the fan-out of each fault. In the
Internet diagnosis network, the fan-out is quite high (because a single fault aﬀects many
observations). Thus, Metropolis-Hastings was far slower than variational inference.

We did not try loopy belief propagation [8], nor expectation propagation [6]. Because the
Beta distribution is not conjugate to the noisy OR, the messages passed by either algorithm
do not have a closed form.

Finally, we did not try the idea of learning to predict the posterior from the observations
by sampling from the generative model and learning the reverse mapping [7]. For Internet
diagnosis, we do not know the structure of graphical model for a block of data ahead of
time: the structure depends on the metadata for the requests in the log. Thus, we cannot
amortize the learning time of a predictive model.

4 Results

We test the approximations and optimization methods used for Internet diagnosis on both
synthetic and real data.

4.1 Synthetic data with known hidden state

Testing the accuracy of approximate inference is very diﬃcult, because, for large graphical
models, the true posterior distribution is intractable. However, we can probe the reliability
of the model on a synthetic data set.

We start by generating fault rates from a prior (here, 2000 faults drawn from Beta(5e-
3,1)). We randomly generate connections from faults to observations, with probability
5 × 10−3. Each connection has a strength rij drawn randomly from [0, 1]. We generate
100,000 observations from the noisy-OR model (4). Given these observations, we predict an
approximate posterior.

Given that the number of observations is much larger than the number of faults, we expect
that the posterior distribution should tightly cluster around the rate that generated the
observations. Diﬀerence between the true rate and the mean of the approximate posterior
should reﬂect inaccuracies in the estimation.

t

e
a
m

 

i
t
s
e
e
a
r
 
f

t

o
 
r
o
r
r

E

0.08
0.06
0.04
0.02
0
-0.02
-0.04
-0.06
-0.08
-0.1
-0.12

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

Figure 3: The error in estimate of rate versus true underlying rate. Black dots are L-BFGS,
Red dots are Stochastic Gradient Descent with 20 epochs.

The results for a run is shown in Figure 3. The ﬁgure shows that the errors in the estimate
are small enough to be very useful for understanding network errors. There is a slight
systematic bias in the stochastic gradient descent, as compared to L-BFGS. However, the
improvement in speed shown in Table 1 is worth the loss of accuracy: we need inference to

6

be as fast as possible to scale to billions of samples. The run times are for a uniprocessor
Pentium 4, 3 GHz, with code in C#.

Algorithm

L-BFGS
SGD, 1 epoch
SGD, 20 epochs

Accuracy
(RMSE)
0.0033
0.0343
0.0075

Time

(CPU sec)

38
0.5
11.7

Table 1: Accuracy and speed on synthetic data set

4.2 Real data from web server logs

We then tested the algorithm on real data from a major web service. Each observation
consists of a success or failure of a single HTTP request. We selected 18848 possible faults
that occur frequently in the dataset, including the web server that received the request,
which autonomous system that originated the request, and which “user agent” (brower or
robot) generated the request.

We have been analyzing HTTP logs collected over several months with the stochastic gra-
dient descent algorithm. In this paper, we present an analysis of a short 2.5 hour window
containing an anomalously high rate of failures, in order to demonstrate that our algo-
rithm can help us understand the cause of failures based on observations in a real-world
environment.

We broke the time series of observations into blocks of 100,000 observations, and inferred
the hidden rates for each block. The initial state of the optimizer was set to be the state of
the optimizer at convergence of the previous block. Thus, for stochastic gradient descent,
the momentum variables were carried forward from block to block.

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
8:00 PM 8:29 PM 8:57 PM 9:26 PM 9:55 PM 10:24 PM 10:53 PM 11:21 PM

Figure 4: The inferred fault rate for two Autonomous Systems, as a function of time. These
are the only two faults with high rate.

The results of this tracking experiment are shown in Figure 4.
In this ﬁgure, we used
stochastic gradient descent and a Beta(0.1,100) prior. The ﬁgure shows the only two faults
whose probability went higher than 0.1 in this time interval: they correspond to two ASes in
the same city, both causing failures at roughly the same time. This could be due to a router
that is in common between them, or perhaps an denial of service attack that originated in
that city.

The speed of the analysis is much faster than real time. For a data set of 10 million
samples, L-BFGS required 209 CPU seconds, while SGD (with 3 passes of data per block)
only required 51 seconds. This allows us to go through logs containing billions of entries in
a matter of hours.

7

5 Conclusions

This paper presents high-speed variational inference to diagnose problems on the scale of
the Internet. Given observations at a web server, the diagnosis can determine whether a web
server needs rebooting, whether part of the Internet is broken, or whether the web server is
compatible with a browser or user agent.

In order to scale inference up to Internet-sized diagnosis problems, we make several ap-
proximations. First, we use mean-ﬁeld variational inference to approximate the posterior
distribution. The expected log likelihood inside of the variational cost function is approxi-
mated with the MF(0) approximation. Finally, we use stochastic gradient descent to perform
the variational optimization.

We are currently using variational stochastic gradient descent to analyze logs that contain
billions of requests. We are not aware of any other applications of variational inference at
this scale. Future publications will include conclusions of such analysis, and implications
for web services and the Internet at large.

References

[1] M. Chen, A. X. Zheng, J. Lloyd, M. I. Jordan, and E. Brewer. Failure diagnosis using

decision trees. In Proc. Int’l. Conf. Autonomic Computing, pages 36–43, 2004.

[2] D. Heckerman. A tractable inference algorithm for diagnosing multiple diseases. In

Proc. UAI, pages 163–172, 1989.

[3] T. Jaakkola and M. Jordan. Variational probabilistic inference and the QMR-DT

database. Journal of Artiﬁcial Intelligence Research, 10:291–322, 1999.

[4] M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K. Saul. An introduction to

variational methods for graphical models. Machine Learning, 37:183–233, 1999.

[5] H. J. Kushner and G. G. Yin. Stochastic Approximation and Recursive Algorithms and

Applications. Springer-Verlag, 2003.

[6] T. P. Minka. Expectation propagation for approximate bayesian inference. In Proc.

UAI, pages 362–369, 2001.

[7] Q. Morris. Recognition networks for approximate inference in BN20 networks. In Proc.

UAI, pages 370–37, 2001.

[8] K. P. Murphy, Y. Weiss, and M. I. Jordan. Loopy belief propagation for approximate

inference: An empirical study. In Proc. UAI, pages 467–475, 1999.

[9] A. Y. Ng and M. Jordan. Approximate inference algorithms for two-layer bayesian

networks. In Proc. NIPS, pages 533–539, 1999.

[10] J. Nocedal and S. J. Wright. Numerical Optimization. Springer, 2nd edition, 2006.
[11] J. Pearl. Probabilistic Reasoning In Intelligent Systems: Networks of Plausible Infer-

ence. Morgan Kaufmann, 1988.

[12] I. Rish, M. Brodie, and S. Ma. Accuracy vs. eﬃciency tradeoﬀs in probabilistic diag-

nosis. In Proc. AAAI, pages 560–566, 2001.

[13] M. A. Shwe and G. F. Cooper. An empirical analysis of likelihood-weighting simulation
on a large, multiply-connected medical belief network. Computers and Biomedical
Research, 24(5):453–475, 1991.

[14] M. A. Shwe, B. Middleton, D. E. Heckerman, M. Henrion, E. J. Horvitz, H. P. Lehmann,
and G. F. Cooper. Probabilistic diagnosis using a reformulation of the INTERNIST-
1/QMR knowledge base. Methods of Information in Medicine, 30(4):241–255, 1991.

[15] J. J. Shynk and S. Roy. The LMS algorithm with momentum updating. In Proc. Intl.

Symp. Circuits and Systems, pages 2651–2654, 1988.

[16] M. Steinder and A. Sethi. End-to-end service failure diagnosis using belief networks.

In Proc. Network Operations and Management Symposium, pages 375–390, 2002.

8

"
902,2007,Active Preference Learning with Discrete Choice Data,"We propose an active learning algorithm that learns a continuous valuation model from discrete preferences. The algorithm automatically decides what items are best presented to an individual in order to find the item that they value highly in as few trials as possible, and exploits quirks of human psychology to minimize time and cognitive burden. To do this, our algorithm maximizes the expected improvement at each query without accurately modelling the entire valuation surface, which would be needlessly expensive. The problem is particularly difficult because the space of choices is infinite. We demonstrate the effectiveness of the new algorithm compared to related active learning methods. We also embed the algorithm within a decision making tool for assisting digital artists in rendering materials. The tool finds the best parameters while minimizing the number of queries.","Active Preference Learning with Discrete Choice Data

Eric Brochu, Nando de Freitas and Abhijeet Ghosh

Department of Computer Science
University of British Columbia

{ebrochu, nando, ghosh}@cs.ubc.ca

Vancouver, BC, Canada

Abstract

We propose an active learning algorithm that learns a continuous valuation model
from discrete preferences. The algorithm automatically decides what items are
best presented to an individual in order to ﬁnd the item that they value highly in
as few trials as possible, and exploits quirks of human psychology to minimize
time and cognitive burden. To do this, our algorithm maximizes the expected
improvement at each query without accurately modelling the entire valuation sur-
face, which would be needlessly expensive. The problem is particularly difﬁcult
because the space of choices is inﬁnite. We demonstrate the effectiveness of the
new algorithm compared to related active learning methods. We also embed the
algorithm within a decision making tool for assisting digital artists in rendering
materials. The tool ﬁnds the best parameters while minimizing the number of
queries.

1 Introduction

A computer graphics artist sits down to use a simple renderer to ﬁnd appropriate surfaces for a
typical reﬂectance model. It has a series of parameters that must be set to control the simulation:
“specularity”, “Fresnel reﬂectance coefﬁcient”, and other, less-comprehensible ones. The parame-
ters interact in ways difﬁcult to discern. The artist knows in his mind’s eye what he wants, but he’s
not a mathematician or a physicist — no course he took during his MFA covered Fresnel reﬂectance
models. Even if it had, would it help? He moves the specularity slider and waits for the image
to be generated. The surface is too shiny. He moves the slider back a bit and runs the simulation
again. Better. The surface is now appropriately dull, but too dark. He moves a slider down. Now
it’s the right colour, but the specularity doesn’t look quite right any more. He repeatedly bumps the
specularity back up, rerunning the renderer at each attempt until it looks right. Good. Now, how to
make it look metallic...?
Problems in simulation, animation, rendering and other areas often take such a form, where the
desired end result is identiﬁable by the user, but parameters must be tuned in a tedious trial-and-
error process. This is particularly apparent in psychoperceptual models, where continual tuning is
required to make something “look right”. Using the animation of character walking motion as an
example, for decades, animators and scientists have tried to develop objective functions based on
kinematics, dynamics and motion capture data [Cooper et al., 2007]. However, even when expen-
sive mocap is available, we simply have to watch an animated ﬁlm to be convinced of how far we
still are from solving the gait animation problem. Unfortunately, it is not at all easy to ﬁnd a mapping
from parameterized animation to psychoperceptual plausibility. The perceptual objective function is
simply unknown. Fortunately, however, it is fairly easy to judge the quality of a walk — in fact, it is
trivial and almost instantaneous. The application of this principle to animation and other psychoper-
ceptual tools is motivated by the observation that humans often seem to be forming a mental model
of the objective function. This model enables them to exploit feasible regions of the parameter space
where the valuation is predicted to be high and to explore regions of high uncertainty. It is our the-

1

Figure 1: An illustrative example of the difference between models learned for regression vesus optimization.
The regression model ﬁts the true function better overall, but doesn’t ﬁt at the maximum better than anywhere
else in the function. The optimization model is less accurate overall, but ﬁts the area of the maximum very
well. When resources are limited, such as an active learning environment, it is far more useful to ﬁt the area
of interest well, even at the cost of overall predictive performance. Getting a good ﬁt for the maximum will
require many more samples using conventional regression.
sis that the process of tweaking parameters to ﬁnd a result that looks “right” is akin to sampling a
perceptual objective function, and that twiddling the parameters to ﬁnd the best result is, in essence,
optimization. Our objective function is the psycho-perceptual process underlying judgement — how
well a realization ﬁts what the user has in mind. Following the econometrics terminology, we refer
to the objective as the valuation. In the case of a human being rating the suitability of a simulation,
however, it is not possible to evaluate this function over the entire domain. In fact, it is in gen-
eral impossible to even sample the function directly and get a consistent response! While it would
theoretically be possible to ask the user to rate realizations with some numerical scale, such meth-
ods often have problems with validity and reliability. Patterns of use and other factors can result
in a drift effect, where the scale varies over time [Siegel and Castellan, 1988]. However, human
beings do excel at comparing options and expressing a preference for one over others [Kingsley,
2006]. This insight allows us to approach the optimization function in another way. By presenting
two or more realizations to a user and requiring only that they indicate preference, we can get far
more robust results with much less cognitive burden on the user [Kendall, 1975]. While this means
we can’t get responses for a valuation function directly, we model the valuation as a latent func-
tion, inferred from the preferences, which permits an active learning approach [Cohn et al., 1996;
Tong and Koller, 2000].
This motivates our second major insight — it is not necessary to accurately model the entire ob-
jective function. The problem is actually one of optimization, not regression (Figure 1). We can’t
directly maximize the valuation function, so we propose to use an expected improvement function
(EIF) [Jones et al., 1998; Sasena, 2002]. The EIF produces an estimate of the utility of knowing the
valuation at any point in the space. The result is a principled way of trading off exploration (showing
the user examples unlike any they have seen) and exploitation (trying to show the user improvements
on examples they have indicated preference for). Of course, regression-based learning can produce
an accurate model of the entire valuation function, which would also allow us to ﬁnd the best valua-
tion. However, this comes at the cost of asking the user to compare many, many examples that have
no practical relation what she is looking for, as we demonstrate experimentally in Sections 3 and
4. Our method tries instead to make the most efﬁcient possible use of the user’s time and cognitive
effort.
Our goal is to exploit the strengths of human psychology and perception to develop a novel frame-
work of valuation optimization that uses active preference learning to ﬁnd the point in a parameter
space that approximately maximizes valuation with the least effort to the human user. Our goal is
to ofﬂoad the cognitive burden of estimating and exploring different sets of parameters, though we
can incorporate “slider twiddling” into the framework easily. In Section 4, we present a simple, but
practical application of our model in a material design gallery that allows artists to ﬁnd particular
appearance rendering effects. Furthermore, the valuation function can be any psychoperceptual pro-
cess that lends itself to sliders and preferences: the model can support an animator looking for a
particular “cartoon physics” effect, an artist trying to capture a particular mood in the lighting of a
scene, or an electronic musician looking for a speciﬁc sound or rhythm. Though we use animation
and rendering as motivating domains, our work has a broad scope of application in music and other
arts, as well as psychology, marketing and econometrics, and human-computer interfaces.

2

regression modeloptimization modelmodeltrue function1.1 Previous Work

Probability models for learning from discrete choices have a long history in psychology and econo-
metrics [Thurstone, 1927; Mosteller, 1951; Stern, 1990; McFadden, 2001]. They have been studied
extensively for use in rating chess players, and the Elo system [ ´El˝o, 1978] was adopted by the
World Chess Federation FIDE to model the probability of one player defeating another. Glickman
and Jensen [2005] use Bayesian optimal design for adaptively ﬁnding pairs for tournaments. These
methods all differ from our work in that they are intended to predict the probability of a prefer-
ence outcome over a ﬁnite set of possible pairs, whereas we work with inﬁnite sets and are only
incidentally interested in modelling outcomes.
In Section 4, we introduce a novel “preference gallery” application for designing simulated materials
in graphics and animation to demonstrate the practical utility of our model. In the computer graphics
ﬁeld, the Design Gallery [Marks et al., 1997] for animation and the gallery navigation interface for
Bidirectional Reﬂectance Distribution Functions (BRDFs) [Ngan et al., 2006] are artist-assistance
tools most like ours. They both uses non-adaptive heuristics to ﬁnd the set of input parameters to be
used in the generation of the display. We depart from this heuristic treatment and instead present a
principled probabilistic decision making approach to model the design process.
Parts of our method are based on [Chu and Ghahramani, 2005b], which presents a prefer-
ence learning method using probit models and Gaussian processes. They use a Thurstone-
[Chu
Mosteller model, but with an innovative nonparametric model of the valuation function.
and Ghahramani, 2005a] adds active learning to the model, though the method presented there
differs from ours in that realizations are selected from a ﬁnite pool to maximize informative-
ness. More importantly, though, this work, like much other work in the ﬁeld [Seo et al., 2000;
Guestrin et al., 2005], is concerned with learning the entire latent function. As our experiments
show in Section 3, this is too expensive an approach for our setting, leading us to develop the new
active learning criteria presented here.

2 Active Preference Learning

By querying the user with a paired comparison, one can estimate statistics of the valuation function
at the query point, but only at considerable expense. Thus, we wish to make sure that the samples
we do draw will generate the maximum possible improvement.
Our method for achieving this goal iterates the following steps:

1. Present the user with a new pair and record the choice: Augment the training set of paired choices

with the new user data.

2. Infer the valuation function: Here we use a Thurstone-Mosteller model with Gaussian processes.
See Sections 2.1 and 2.2 for details. Note that we are not interested in predicting the value of the
valuation function over the entire feasible domain, but rather in predicting it well near the optimum.
3. Formulate a statistical measure for exploration-exploitation: We refer to this measure as the
expected improvement function (EIF). Its maximum indicates where to sample next. EI is a function
of the Gaussian process predictions over the feasible domain. See Section 2.3.

4. Optimize the expected improvement function to obtain the next query point: Finding the maxi-

mum of the EI corresponds to a constrained nonlinear programming problem. See Section 2.3.

2.1 Preference Learning Model

Assume we have shown the user M pairs of items. In each case, the user has chosen which item she
likes best. The dataset therefore consists of the ranked pairs D = {rk (cid:31) ck; k = 1, . . . , M}, where
the symbol (cid:31) indicates that the user prefers r to c. We use x1:N = {x1, x2, . . . , xN}, xi ∈ X ⊆ Rd,
to denote the N elements in the training data. That is, rk and ck correspond to two elements of x1:N .
Our goal is to compute the item x (not necessarily in the training data) with the highest user valuation
in as few comparisons as possible. We model the valuation functions u(·) for r and c as follows:

u(rk) = f(rk) + erk
u(ck) = f(ck) + eck,

3

(1)

2 exp(cid:0)− 1

2 f T K−1f(cid:1),

where the noise terms are Gaussian: erk ∼ N (0, σ2) and eck ∼ N (0, σ2). Following [Chu and
Ghahramani, 2005b], we assign a nonparametric Gaussian process prior to the unknown mean valua-
tion: f(·) ∼ GP (0, K(·,·)). That is, at the N training points. p(f) = |2πK|− 1
where f = {f(x1), f(x2), . . . , f(xN )} and the symmetric positive deﬁnite covariance K has en-
tries (kernels) Kij = k(xi, xj). Initially we learned these parameters via maximum likelihood, but
soon realized that this was unsound due to the scarcity of data. To remedy this, we elected to use
subjective priors using simple heuristics, such as expected dataset spread. Although we use Gaus-
sian processes as a principled method of modelling the valuation, other techniques, such as wavelets
could also be adopted.
Random utility models such as (1) have a long and inﬂuential history in psychology and the study
of individual choice behaviour in economic markets. Daniel McFadden’s Nobel Prize speech [Mc-
Fadden, 2001] provides a glimpse of this history. Many more comprehensive treatments appear in
classical economics books on discrete choice theory.
Under our Gaussian utility models, the probability that item r is preferred to item c is given by:

(cid:21)
(cid:20) f(rk) − f(ck)
(cid:82) dk−∞ exp(cid:0)−a2/2(cid:1) da is the cumulative function of the standard Normal dis-

where Φ (dk) = 1√
tribution. This model, relating binary observations to a continuous latent function, is known as the
Thurstone-Mosteller law of comparative judgement [Thurstone, 1927; Mosteller, 1951]. In statistics
it goes by the name of binomial-probit regression. Note that one could also easily adopt a logis-
tic (sigmoidal) link function ϕ (dk) = (1 + exp (−dk))−1. In fact, such choice is known as the
Bradley-Terry model [Stern, 1990]. If the user had more than two choices one could adopting a
multinomial-probit model. This multi-category extension would, for example, enable the user to
state no preference for any of the two items being presented.

P (rk (cid:31) ck) = P (u(rk) > u(ck)) = P (eck − erk < f(rk) − f(ck)) = Φ

√
2σ

2π

,

2.2

Inference

√

That is, we want to compute p(f|D) ∝ p(f)(cid:81)M

k=1 p(dk|f), where dk = f (rk)−f (ck)

Our goal is to estimate the posterior distribution of the latent utility function given the discrete data.
. Although
there exist sophisticated variational and Monte Carlo methods for approximating this distribution,
we favor a simple strategy: Laplace approximation. Our motivation for doing this is the simplicity
and computational efﬁciency of this technique. Moreover, given the amount of uncertainty in user
valuations, we believe the choice of approximating technique plays a small role and hence we expect
the simple Laplace approximation to perform reasonably in comparison to other techniques. The
application of the Laplace approximation is fairly straightforward, and we refer the reader to [Chu
and Ghahramani, 2005b] for details.
Finally, given an arbitrary test pair, the predicted utility f (cid:63) and f are jointly Gaussian. Hence, one
can obtain the conditional p(f (cid:63)|f) easily. Moreover, the predictive distribution p(f (cid:63)|D) follows by

straightforward convolution of two Gaussians: p(f (cid:63)|D) =(cid:82) p(f (cid:63)|f)p(f|D)df. One of the criticisms

of Gaussian processes, the fact that they are slow with large data sets, is not a problem for us, since
active learning is designed explicitly to minimize the number of training data.

2σ

2.3 The Expected Improvement Function

Now that we are armed with an expression for the predictive distribution, we can use it to decide
what the next query should be. In loose terms, the predictive distribution will enable us to balance the
tradeoff of exploiting and exploring. When exploring, we should choose points where the predicted
variance is large. When exploiting, we should choose points where the predicted mean is large (high
valuation).
Let x(cid:63) be an arbitrary new instance. Its predictive distribution p(f (cid:63)(x(cid:63))|D) has sufﬁcient statis-
M AP )−1k(cid:63)}, where, now, k(cid:63)T =
tics {µ(x(cid:63)) = k(cid:63)T K−1f M AP , s2(x(cid:63)) = k(cid:63)(cid:63) − k(cid:63)T (K + C−1
[k(x(cid:63), x1)··· k(x(cid:63), xN )] and k(cid:63)(cid:63) = k(x(cid:63), x(cid:63)). Also, let µmax denote the highest estimate of the
predictive distribution thus far. That is, µmax is the highest valuation for the data provided by the
individual.

4

Figure 2: The 2D test function (left), and the estimate of the function based on the results of a typical run of 12
preference queries (right). The true function has eight local and one global maxima. The predictor identiﬁes the
region of the global maximum correctly and that of the local maxima less well, but requires far fewer queries
than learning the entire function.

The probability of improvement at a point x(cid:63) is simply given by a tail probability:

p(f (cid:63)(x(cid:63)) ≤ µmax) = Φ

(cid:18) µmax − µ(x(cid:63))

(cid:19)

s(x(cid:63))

,

where f (cid:63)(x(cid:63)) ∼ N (µ(x(cid:63)), s2(x(cid:63))). This statistical measure of improvement has been widely used
in the ﬁeld of experimental design and goes back many decades [Kushner, 1964]. However, it is
known to be sensitive to the value of µmax. To overcome this problem, [Jones et al., 1998] deﬁned
the improvement over the current best point as I(x(cid:63)) = max{0, µ(x(cid:63)) − µmax}, which resulted in
an expected improvement of

(cid:26) (µmax − µ(x(cid:63)))Φ(d) + s(x(cid:63))φ(d)

EI(x(cid:63)) =

0

if s > 0
if s = 0

.

s(x(cid:63))

where d = µmax−µ(x(cid:63))
To ﬁnd the point at which to sample, we still need to maximize the constrained objective EI(x(cid:63))
over x(cid:63). Unlike the original unknown cost function, EI(·) can be cheaply sampled. Furthermore,
for the purposes of our application, it is not necessary to guarantee that we ﬁnd the global maximum,
merely that we can quickly locate a point that is likely to be as good as possible. The original EGO
work used a branch-and-bound algorithm, but we found it was very difﬁcult to get good bounds
over large regions. Instead we use DIRECT [Jones et al., 1993], a fast, approximate, derivative-
free optimization algorithm, though we conjecture that for larger dimensional spaces, sequential
quadratic programming with interior point methods might be a better alternative.

3 Experiments

The goal of our algorithm is to ﬁnd a good approximation of the maximum of a latent function using
preference queries. In order to measure our method’s effectiveness in achieving this goal, we create
a function f for which the optimum is known. At each time step, a query is generated in which
two points x1 and x2 are adaptively selected, and the preference is found, where f(x1) > f(x2) ⇔
x1 (cid:31) x2. After each preference, we measure the error, deﬁned as  = fmax − f(argmaxx f∗(x)),
that is, the difference between the true maximum of f and the value of f at the point predicted to be
the maximum. Note that by design, this does not penalize the algorithm for drawing samples from
X that are far from argmaxx, or for predicting a latent function that differs from the true function.
We are not trying to learn the entire valuation function, which would take many more queries – we
seek only to maximize the valuation, which involves accurate modelling only in the areas of high
valuation.
We measured the performance of our method on three functions – 2D, 4D and 6D. By way of demon-
stration, Figure 2 shows the actual 2D functions and the typical prediction after several queries. The
test functions are deﬁned as:

f2d = max{0, sin(x1) + x1/3 + sin(12x1) + sin(x2) + x2/3 + sin(12x2) − 1}

d(cid:88)

i=1

f4d,6d =

sin(xi) + xi/3 + sin(12xi)

5

00.20.40.60.8100.20.40.60.8100.511.522.5300.20.40.60.8100.20.40.60.81−4−202468x 10−400.20.40.60.8100.20.40.60.8100.20.40.60.8100.20.40.60.81Figure 3: The evolution of error for the estimate of the optimum on the test functions. The plot shows the error
evolution  against the number of queries. The solid line is our method; the dashed is a baseline comparison
in which each query point is selected randomly. The performance is averaged over 20 runs, with the error bars
showing the variance of .

all deﬁned over the range [0, 1]d. We selected these equations because they seem both general and
difﬁcult enough that we can safely assume that if our method works well on them, it should work on a
large class of real-world problems — they have multiple local minima to get trapped in and varying
landscapes and dimensionality. Unfortunately, there has been little work in the psychoperception
literature to indicate what a good test function would be for our problem, so we have had to rely to
an extent on our intuition to develop suitable test cases.
The results of the experiments are shown in Figure 3. In all cases, we simulate 50 queries using our
method (here called maxEI). As a baseline, we compare against 50 queries using the maximum
variance of the model (maxs), which is a common criterion in active learning for regression [Seo
et al., 2000; Chu and Ghahramani, 2005a]. We repeated each experiment 20 times and measured
the mean and variance of the error evolution. We ﬁnd that it takes far fewer queries to ﬁnd a good
result using maxEI in all cases.
In the 2D case, for example, after 20 queries, maxEI already
has better average performance than maxs achieves after 50, and in both the 2D and 4D scenarios,
maxEI steadily improves until it ﬁnd the optima, while maxs soon reaches a plateau, improving only
slightly, if at all, while it tries to improve the global ﬁt to the latent function. In the 6D scenario,
neither algorithm succeeds well in ﬁnding the optimum, though maxEI clearly comes closer. We
believe the problem is that in six dimensions, the space is too large to adequately explore with so few
queries, and variance remains quite high throughout the space. We feels that requiring more than 50
user queries in a real application would be unacceptable, so we are instead currently investigating
extensions that will allow the user to direct the search in higher dimensions.

4 Preference Gallery for Material Design

Properly modeling the appearance of a material is a necessary component of realistic image syn-
thesis. The appearance of a material is formalized by the notion of the Bidirectional Reﬂectance
Distribution Function (BRDF). In computer graphics, BRDFs are most often speciﬁed using vari-
ous analytical models observing the physical laws of reciprocity and energy conservation while also
exhibiting shadowing, masking and Fresnel reﬂectance phenomenon. Realistic models are therefore
fairly complex with many parameters that need to be adjusted by the designer. Unfortunately these
parameters can interact in non-intuitive ways, and small adjustments to certain settings may result
in non-uniform changes in appearance. This can make the material design process quite difﬁcult for
the end user, who cannot expected to be an expert in the ﬁeld of appearance modeling.
Our application is a solution to this problem, using a “preference gallery” approach, in which users
are simply required to view two or more images rendered with different material properties and
indicate which ones they prefer. To maximize the valuation, we use an implementation of the model
described in Section 2. In practice, the ﬁrst few examples will be points of high variance, since little
of the space is explored (that is, the model of user valuation is very uncertain). Later samples tend
to be in regions of high valuation, as a model of the user’s interest is learned.
We use our active preference learning model on an example gallery application for helping users
ﬁnd a desired BRDF. For the purposes of this example, we limit ourselves to isotropic materials and
ignore wavelength dependent effects in reﬂection. The gallery uses the Ashikhmin-Shirley Phong

6

102030400.01.02.03.04.04D functionǫ10203040102030400.00.20.40.60.81.02D function4.05.06.07.08.06D functionpreference queriesTable 1: Results of the user study

algorithm
latin hypercubes
maxs
maxEI

trials n (mean ± std)
18.40 ± 7.87
50
17.87 ± 8.60
50
8.56 ± 5.23
50

model [Ashikhmin and Shirley, 2000] for the BRDFs which was recently validated to be well suited
for representing real materials [Ngan et al., 2005]. The BRDFs are rendered on a sphere under high
frequency natural illumination as this has been shown to be the desired setting for human preception
of reﬂectance [Fleming et al., 2001]. Our gallery demonstration presents the user with two BRDF
images at a time. We start with four predetermined queries to “seed” the parameter space, and after
that use the learned model to select gallery images. The GP model is updated after each preference
is indicated. We use parameters of real measured materials from the MERL database [Ngan et al.,
2005] for seeding the parameter space, but can draw arbitrary parameters after that.

4.1 User Study

To evaluate the performance of our application, we have run a simple user study in which the gen-
erated images are restricted to a subset of 38 materials from the MERL database that we deemed to
be representative of the appearance space of the measured materials. The user is given the task of
ﬁnding a single randomly-selected image from that set by indicating preferences. Figure 4 shows a
typical user run, where we ask the user to use the preference gallery to ﬁnd a provided target image.
At each step, the user need only indicate the image they think looks most like the target. This would,
of course, be an unrealistic scenario if we were to be evaluating the application from an HCI stance,
but here we limit our attention to the model, where we are interested here in demonstrating that with
human users maximizing valuation is preferable to learning the entire latent function.
Using ﬁve subjects, we compared 50 trials using the EIF to select the images for the gallery (maxEI),
50 trials using maximum variance (maxs, the same criterion as in the experiments of Section 3), and
50 trials using samples selected using a randomized Latin hypercube algorithm. In each case, one of
the gallery images was the image with the highest predicted valuation and the other was selected by
the algorithm. The algorithm type for each trial was randomly selected by the computer and neither
the experimenter nor the subjects knew which of the three algorithms was selecting the images. The
results are shown in Table 1. n is the number clicks required of the user to ﬁnd the target image.
Clearly maxEI dominates, with a mean n less than half that of the competing algorithms. Interest-
ingly, selecting images using maximum variance does not perform much better than random. We
suspect that this is because maxs has a tendency to select images from the corners of the param-
eter space, which adds limited information to the other images, whereas Latin hypercubes at least
guarantees that the selected images ﬁll the space.
Active learning is clearly a powerful tool for situations where human input is required for learning.
With this paper, we have shown that understanding the task — and exploiting the quirks of human
cognition — is also essential if we are to deploy real-world active learning applications. As peo-
ple come to expect their machines to act intelligently and deal with more complex environments,
machine learning systems that can collaborate with users and take on the tedious parts of users’
cognitive burden has the potential to dramatically affect many creative ﬁelds, from business to the
arts to science.

References
[Ashikhmin and Shirley, 2000] M. Ashikhmin and P. Shirley. An anisotropic phong BRDF model. J. Graph.

Tools, 5(2):25–32, 2000.

[Chu and Ghahramani, 2005a] W. Chu and Z. Ghahramani. Extensions of Gaussian processes for ranking:

semi-supervised and active learning. In Learning to Rank workshop at NIPS-18, 2005.

[Chu and Ghahramani, 2005b] W. Chu and Z. Ghahramani. Preference learning with Gaussian processes. In

ICML, 2005.

[Cohn et al., 1996] D. A. Cohn, Z. Ghahramani, and M. I. Jordan. Active learning with statistical models.

Journal of Artiﬁcial Intelligence Research, 4:129–145, 1996.

7

T arget

1(cid:46)

3(cid:46)

2(cid:46)

4(cid:46)

Figure 4: A shorter-than-average but otherwise typical run of the preference gallery tool. At each (numbered)
iteration, the user is provided with two images generated with parameter instances and indicates the one they
think most resembles the target image (top-left) they are looking for. The boxed images are the user’s selections
at each iteration.

[Cooper et al., 2007] S. Cooper, A. Hertzmann, and Z. Popovi´c. Active learning for motion controllers. In

SIGGRAPH, 2007.

´A. ´El˝o. The Rating of Chess Players: Past and Present. Arco Publishing, New York, 1978.

[ ´El˝o, 1978]
[Fleming et al., 2001] R. Fleming, R. Dror, and E. Adelson. How do humans determine reﬂectance properties
In CVPR Workshop on Identifying Objects Across Variations in Lighting,

under unknown illumination?
2001.

[Glickman and Jensen, 2005] M. E. Glickman and S. T. Jensen. Adaptive paired comparison design. Journal

of Statistical Planning and Inference, 127:279–293, 2005.

[Guestrin et al., 2005] C. Guestrin, A. Krause, and A. P. Singh. Near-optimal sensor placements in Gaussian

processes. In Proceedings of the 22nd International Conference on Machine Learning (ICML-05), 2005.

[Jones et al., 1993] D. R. Jones, C. D. Perttunen, and B. E. Stuckman. Lipschitzian optimization without the

Lipschitz constant. J. Optimization Theory and Apps, 79(1):157–181, 1993.

[Jones et al., 1998] D. R. Jones, M. Schonlau, and W. J. Welch. Efﬁcient global optimization of expensive

black-box functions. J. Global Optimization, 13(4):455–492, 1998.

[Kendall, 1975] M. Kendall. Rank Correlation Methods. Grifﬁn Ltd, 1975.
[Kingsley, 2006] D. C. Kingsley. Preference uncertainty, preference reﬁnement and paired comparison choice

experiments. Dept. of Economics, University of Colorado, 2006.

[Kushner, 1964] H. J. Kushner. A new method of locating the maximum of an arbitrary multipeak curve in the

presence of noise. Journal of Basic Engineering, 86:97–106, 1964.

[Marks et al., 1997] J. Marks, B. Andalman, P. A. Beardsley, W. Freeman, S. Gibson, J. Hodgins, T. Kang,
B. Mirtich, H. Pﬁster, W. Ruml, K. Ryall, J. Seims, and S. Shieber. Design galleries: A general approach to
setting parameters for computer graphics and animation. Computer Graphics, 31, 1997.

[McFadden, 2001] D. McFadden. Economic choices. The American Economic Review, 91:351–378, 2001.
[Mosteller, 1951] F. Mosteller. Remarks on the method of paired comparisons: I. the least squares solution

assuming equal standard deviations and equal correlations. Psychometrika, 16:3–9, 1951.

[Ngan et al., 2005] A. Ngan, F. Durand, and W. Matusik. Experimental analysis of BRDF models. In Pro-

ceedings of the Eurographics Symposium on Rendering, pages 117–226, 2005.

[Ngan et al., 2006] A. Ngan, F. Durand, and W. Matusik. Image-driven navigation of analytical BRDF models.

In T. Akenine-M¨oller and W. Heidrich, editors, Eurographics Symposium on Rendering, 2006.

[Sasena, 2002] M. J. Sasena. Flexibility and Efﬁciency Enhancement for Constrained Global Design Opti-

mization with Kriging Approximations. PhD thesis, University of Michigan, 2002.

[Seo et al., 2000] S. Seo, M. Wallat, T. Graepel, and K. Obermayer. Gaussian process regression: active data

selection and test point rejection. In Proceedings of IJCNN 2000, 2000.

[Siegel and Castellan, 1988] S. Siegel and N. J. Castellan. Nonparametric Statistics for the Behavioral Sci-

ences. McGraw-Hill, 1988.

[Stern, 1990] H. Stern. A continuum of paired comparison models. Biometrika, 77:265–273, 1990.
[Thurstone, 1927] L. Thurstone. A law of comparative judgement. Psychological Review, 34:273–286, 1927.
[Tong and Koller, 2000] S. Tong and D. Koller. Support vector machine active learning with applications to

text classiﬁcation. In Proc. ICML-00, 2000.

8

"
160,2007,Bayesian binning beats approximate alternatives: estimating peri-stimulus time histograms,"The peristimulus time historgram (PSTH) and its more continuous cousin, the spike density function (SDF) are staples in the analytic toolkit of neurophysiologists. The former is usually obtained by binning spiketrains, whereas the standard method for the latter is smoothing with a Gaussian kernel. Selection of a bin with or a kernel size is often done in an relatively arbitrary fashion, even though there have been recent attempts to remedy this situation \cite{ShimazakiBinningNIPS2006,ShimazakiBinningNECO2007}. We develop an exact Bayesian, generative model approach to estimating PSHTs and demonstate its superiority to competing methods. Further advantages of our scheme include automatic complexity control and error bars on its predictions.","Bayesian binning beats approximate alternatives:

estimating peristimulus time histograms

Dominik Endres, Mike Oram, Johannes Schindelin and Peter F¨oldi´ak

School of Psychology

University of St. Andrews

{dme2,mwo,js108,pf2}@st-andrews.ac.uk

KY16 9JP, UK

Abstract

The peristimulus time histogram (PSTH) and its more continuous cousin, the
spike density function (SDF) are staples in the analytic toolkit of neurophysiol-
ogists. The former is usually obtained by binning spike trains, whereas the stan-
dard method for the latter is smoothing with a Gaussian kernel. Selection of a bin
width or a kernel size is often done in an relatively arbitrary fashion, even though
there have been recent attempts to remedy this situation [1, 2]. We develop an
exact Bayesian, generative model approach to estimating PSTHs and demonstate
its superiority to competing methods. Further advantages of our scheme include
automatic complexity control and error bars on its predictions.

1 Introduction

Plotting a peristimulus time histogram (PSTH), or a spike density function (SDF), from spiketrains
evoked by and aligned to a stimulus onset is often one of the ﬁrst steps in the analysis of neurophys-
iological data. It is an easy way of visualizing certain characteristics of the neural response, such
as instantaneous ﬁring rates (or ﬁring probabilities), latencies and response offsets. These measures
also implicitly represent a model of the neuron’s response as a function of time and are important
parts of their functional description. Yet PSTHs are frequently constructed in an unsystematic man-
ner, e.g. the choice of time bin size is driven by result expectations as much as by the data. Recently,
there have been more principled approaches to the problem of determining the appropriate temporal
resolution [1, 2].
We develop an exact Bayesian solution, apply it to real neural data and demonstrate its superiority
to competing methods. Note that we do in no way claim that a PSTH is a complete generative
description of spiking neurons. We are merely concerned with inferring that part of the generative
process which can be described by a PSTH in a Bayes-optimal way.

2 The model

Suppose we wanted to model a PSTH on [tmin, tmax], which we discretize into T contiguous in-
tervals of duration ∆t = (tmax − tmin)/T (see ﬁg.1, left). We select a discretization ﬁne enough
so that we will not observe more than one spike in a ∆t interval for any given spike train. This can
be achieved easily by choosing a ∆t shorter than the absolute refractory period of the neuron under
investigation. Spike train i can then be represented by a binary vector ~zi of dimensionality T . We
model the PSTH by M +1 contiguous, non-overlapping bins having inclusive upper boundaries km,
within which the ﬁring probability P (spike|t ∈ (tmin +∆t(km−1 +1), tmin +∆t(km +1)]) = fm
is constant. M is the number of bin boundaries inside [tmin, tmax]. The probability of a spike train

1

Figure 1: Left: Top: A spike train, recorded between times tmin and tmax is represented by a binary
vector ~zi. Bottom: The time span between tmin and tmax is discretized into T intervals of duration
∆t = (tmax − tmin)/T , such that interval k lasts from k × ∆t + tmin to (k + 1) × ∆t + tmin. ∆t
is chosen such that at most one spike is observed per ∆t interval for any given spike train. Then, we
model the ﬁring probabilities P (spike|t) by M + 1 = 4 contiguous, non-overlapping bins (M is the
number of bin boundaries inside the time span [tmin, tmax]), having inclusive upper boundaries km
and P (spike|t ∈ (tmin + ∆t(km−1 + 1), tmin + ∆t(km + 1)]) = fm. Right: The core iteration. To
compute the evidence contribution subEm[T −1] of a model with a bin boundary at T −1 and m bin
boundaries prior to T − 1, we sum over all evidence contributions of models with a bin boundary at
k and m − 1 bin boundaries prior to k, where k ≥ m − 1, because m bin boundaries must occupy
at least time intervals 0; . . . ; m − 1. This takes O(T ) operations. Repeat the procedure to obtain
subEm[T −2]; . . . ; subEm[m]. Since we expect T (cid:29) m, computing all subEm[k] given subEm−1[k]
requires O(T 2) operations. For details, see text.

~zi of independent spikes/gaps is then

P (~zi|{fm},{km}, M) =

MY

m=0

f s(~zi,m)
m

(1 − fm)g(~zi,m)

(1)

where s(~zi, m) is the number of spikes and g(~zi, m) is the number of non-spikes, or gaps in spike-
train ~zi in bin m, i.e. between intervals km−1 +1 and km (both inclusive). In other words, we model
the spiketrains by an inhomogeneous Bernoulli process with piecewise constant probabilities. We
also deﬁne k−1 = −1 and kM = T − 1. Note that there is no binomial factor associated with
the contribution of each bin, because we do not want to ignore the spike timing information within
the bins, but rather, we try to build a simpliﬁed generative model of the spike train. Therefore, the
probability of a (multi)set of spiketrains {~zi} = {z1, . . . , zN}, assuming independent generation, is

P ({~zi}|{fm},{km}, M) =

f s(~zi,m)
m

(1 − fm)g(~zi,m)

MY

m=0

NY
MY

i=1

where s({~zi}, m) =PN

i=1 s(~zi, m) and g({~zi}, m) =PN

m=0

i=1 g(~zi, m)

=

f s({~zi},m)

m

(1 − fm)g({~zi},m)

2.1 The priors
We will make a non-informative prior assumption for p({fm},{km}), namely

p({fm},{km}|M) = p({fm}|M)P ({km}|M).

2

(2)

(3)

P(spike|t)0k0k1k2k3=T­1tttmintmaxk=[1,0,0,1,1,1,0,1,0,1,0,0,1,0]zif1f2subEm­1[m­1]subEm­1[m]subEm­1[T­2]subEm[T­1]km­1mT­2T­1× getIEC(m,T­1,m)× getIEC(T­1,T­1,m)× getIEC(m+1,T­1,m)subEm­1[m­1]subEm­1[m]subEm[T­2]× getIEC(m,T­2,m)× getIEC(m+1,T­2,m)subEm[T­1]i.e. we have no a priori preferences for the ﬁring rates based on the bin boundary positions. Note
that the prior of the fm, being continuous model parameters, is a density. Given the form of eqn.(1)
and the constraint fm ∈ [0, 1], it is natural to choose a conjugate prior

The Beta density is deﬁned in the usual way [3]:

m=0

MY

p({fm}|M) =

B(fm; σm, γm).

B(p; σ, γ) =

Γ(σ + γ)
Γ(σ)Γ(γ) pσ(1 − p)γ.
(cid:19) .

(cid:18) T − 1

1

P ({km}|M) =

(4)

(5)

(6)

(7)

(8)

(12)

There are only ﬁnitely many conﬁgurations of the km. Assuming we have no preferences for any of
them, the prior for the bin boundaries becomes

M

where the denominator is just the number of possibilities in which M ordered bin boundaries can
be distributed across T − 1 places (bin boundary M always occupies position T − 1, see ﬁg.1,left ,
hence there are only T − 1 positions left).
3 Computing the evidence P ({~zi}|M )

To calculate quantities of interest for a given M, e.g. predicted ﬁring probabilities and their variances
or expected bin boundary positions, we need to compute averages over the posterior

p({fm},{km}|M,{~zi}) = p({~zi},{fm},{km}|M)

P ({~zi}|M)

which requires the evaluation of the evidence, or marginal likelihood of a model with M bins:

P ({~zi}|M) =

. . .

P ({~zi}|{km}, M)P ({km}|M)

T−2X

kM−1−1X

kM−1=M−1

kM−2=M−2

k1−1X

k0=0

where the summation boundaries are chosen such that the bins are non-overlapping and contiguous
and

P ({~zi}|{km}, M) =

dfM P ({~zi}|{fm},{km}, M)p({fm}|M).

(9)

Z 1

Z 1

df0

df1 . . .

0

0

By virtue of eqn.(2) and eqn.(4), the integrals can be evaluated:

P ({~zi}|{km}, M) =

Γ(s({~zi}, m) + σm)Γ(g({~zi}, m) + γm)
Γ(s({~zi}, m) + σm + g({~zi}, m) + γm)

MY

m=0

Γ(σm + γm)
Γ(σm)Γ(γm) .

(10)

Z 1
MY

0

m=0

Computing the sums in eqn.(8) quickly is a little tricky. A na¨ıve approach would suggest that a
computational effort of O(T M ) is required. However, because eqn.(10) is a product with one factor
per bin, and because each factor depends only on spike/gap counts and prior parameters in that bin,
the process can be expedited. We will use an approach very similar to that described in [4, 5] in the
context of density estimation and in [6, 7] for Bayesian function approximation: deﬁne the function

getIEC(ks, ke, m) :=

Γ(s({~zi}, ks, ke) + σm)Γ(g({~zi}, ks, ke) + γm)
Γ(s({~zi}, ks, ke) + σm + g({~zi}, ks, ke) + γm)

(11)
where s({~zi}, ks, ke) is the number of spikes and g({~zi}, ks, ke) is the number of gaps in {~zi}
between the start interval ks and the end interval ke (both included). Furthermore, collect all contri-
butions to eqn.(8) that do not depend on the data (i.e. {~zi}) and store them in the array pr[M]:

QM
(cid:18) T − 1

m=0

Γ(σm+γm)
Γ(σm)Γ(γm)

(cid:19) .

pr[M] :=

M

3

T−2X

k1−1X

MY

kM−1=M−1

k0=0

m=1

Substituting eqn.(10) into eqn.(8) and using the deﬁnitions (11) and (12), we obtain

P ({~zi}|M) ∝

. . .

getIEC(km−1 + 1, km, m)getIEC(0, k0, 0)

(13)

with kM = T − 1 and the constant of proportionality being pr[M]. Since the factors on the r.h.s.
depend only on two consecutive bin boundaries each, it is possible to apply dynamic programming
[8]: rewrite the r.h.s. by ’pushing’ the sums as far to the right as possible:

getIEC(kM−1+1, T −1, M)

getIEC(kM−2+1, kM−1, M−1)

kM−1−1X

kM−2=M−2

P ({~zi}|M) ∝ T−2X

kM−1=M−1

× . . .

k1−1X

k0=0

getIEC(k0 + 1, k1, 1)getIEC(0, k0, 0).

(14)

Evaluating the sum over k0 requires O(T ) operations (assuming that T (cid:29) M, which is likely to
be the case in real-world applications). As the summands depend also on k1, we need to repeat this
evaluation O(T ) times, i.e. summing out k0 for all possible values of k1 requires O(T 2) operations.
This procedure is then repeated for the remaining M − 1 sums, yielding a total computational
effort of O(M T 2). Thus, initialize the array subE0[k] := getIEC(0, k, 0), and iterate for all m =
1, . . . , M:

subEm[k] :=

getIEC(r + 1, k, m)subEm−1[r],

(15)

k−1X

r=m−1

A close look at eqn.(14) reveals that while we sum over kM−1, we need subEM−1[k] for k =
M − 1; . . . ; T − 2 to compute the evidence of a model with its latest boundary at T − 1. We can,
however, compute subEM−1[T − 1] with little extra effort, which is, up to a factor pr[M − 1], equal
to P ({~zi}|M − 1), i.e. the evidence for a model with M − 1 bin boundaries. Moreover, having
computed subEm[k], we do not need subEm−1[k − 1] anymore. Hence, the array subEm−1[k] can
be reused to store subEm[k], if overwritten in reverse order. In pseudo-code (E[m] contains the
evidence of a model with m bin boundaries inside [tmin, tmax] after termination):

Table 1: Computing the evidences of models with up to M bin boundaries
1. for k := 0 . . . T − 1 : subE[k] := getIEC(0, k, 0)
2. E[0] := subE[T − 1] × pr[0]
3. for m := 1 . . . M :

(a) if m = M then l := T − 1 else l := m
(b) for k := T − 1 . . . l

subE[k] :=Pk−1

(c) E[m] = subE[T − 1] × pr[m]

r:=m−1 subE[r] × getIEC(r + 1, k, m)

4. return E[]

4 Predictive ﬁring rates and variances
We will now calculate the predictive ﬁring rate P (spike|˜k,{~zi}, M). For a given conﬁguration of
{fm} and {km}, we can write

P (spike|˜k,{fm},{km}, M) =

fm1(˜k ∈ {km−1 + 1, km})

(16)

where the indicator function 1(x) = 1 iff x is true and 0 otherwise. Note that the probability
of a spike given {km} and {fm} does not depend on any observed data. Since the bins are non-
overlapping, ˜k ∈ {km−1 + 1, km} is true for exactly one summand and P (spike|˜k,{~zi},{km})
evaluates to the corresponding ﬁring rate.

m=0

4

MX

To ﬁnish we average eqn.(16) over the posterior eqn.(7). The denominator of eqn.(7) is independent
of {fm},{km} and is obtained by integrating/summing the numerator via the algorithm in table 1.
Thus, we only need to multiply the integrand of eqn.(9) (i.e. the numerator of the posterior) with
P (spike|˜k,{fm},{km}, M), thereby replacing eqn.(11) with

getIEC(ks, ke, m) :=

Γ(s({~zi}, ks, ke) + 1(˜k ∈ {ks, ke}) + σm)Γ(g({~zi}, ks, ke) + γm)
Γ(s({~zi}, ks, ke) + 1(˜k ∈ {ks, ke}) + σm + g({~zi}, ks, ke) + γm)

(17)

i.e. we are adding an additional spike to the data at ˜k. Call the array returned by this modiﬁed
algorithm E˜k[]. By virtue of eqn.(7) we then ﬁnd P (spike|˜k,{~zi}, M) = E˜k[M ]
. To evaluate the
E[M ]
m. This can be computed by adding two spikes at ˜k.
variance, we need the posterior expectation of f 2

5 Model selection vs. model averaging
To choose the best M given {~zi}, or better, a probable range of Ms, we need to determine the model
posterior

P
P (M|{~zi}) = P ({~zi}|M)P (M)
m P ({~zi}|m)P (m)

(18)

where P (M) is the prior over M, which we assume to be uniform. The sum in the denominator
runs over all values of m which we choose to include, at most 0 ≤ m ≤ T − 1.
Once P (M|{~zi}) is evaluated, we could use it to select the most probable M0. However, making this
decision means ’contriving’ information, namely that all of the posterior probability is concentrated
at M0. Thus we should rather average any predictions over all possible M, even if evaluating such
an average has a computational cost of O(T 3), since M ≤ T − 1. If the structure of the data allow,
it is possible, and useful given a large enough T , to reduce this cost by ﬁnding a range of M, such
that the risk of excluding a model even though it provides a good description of the data is low. In
analogy to the signiﬁcance levels of orthodox statistics, we shall call this risk α. If the posterior of
M is unimodal (which it has been in most observed cases, see ﬁg.3, right, for an example), we can
then choose the smallest interval of Ms around the maximum of P (M|{~zi}) such that

P (Mmin ≤ M ≤ Mmax|{~zi}) ≤ 1 − α

(19)

and carry out the averages over this range of M after renormalizing the model posterior.

6 Examples and comparison to other methods

6.1 Data acquisition

We obtained data through [9], where the experimental protocols have been described. Brieﬂy, extra-
cellular single-unit recordings were made using standard techniques from the upper and lower banks
of the anterior part of the superior temporal sulcus (STSa) and the inferior temporal cortex (IT) of
two monkeys (Macaca mulatta) performing a visual ﬁxation task. Stimuli were presented for 333
ms followed by an 333 ms inter-stimulus interval in random order. The anterior-posterior extent of
the recorded cells was from 7mm to 9mm anterior of the interaural plane consistent with previous
studies showing visual responses to static images in this region [10, 11, 12, 13]. The recorded cells
were located in the upper bank (TAa, TPO), lower bank (TEa, TEm) and fundus (PGa, IPa) of STS
and in the anterior areas of TE (AIT of [14]). These areas are rostral to FST and we collectively
call them the anterior STS (STSa), see [15] for further discussion. The recorded ﬁring patters were
turned into distinct samples, each of which contained the spikes from −300 ms before to 600 ms
after the stimulus onset with a temporal resolution of 1 ms.

6.2

Inferring PSTHs

To see the method in action, we used it to infer a PSTH from 32 spiketrains recorded from one of the
available STSa neurons (see ﬁg.2, A). Spikes times are relative to the stimulus onset. We discretized
the interval from −100ms pre-stimulus to 500ms post-stimulus into ∆t = 1ms time intervals and

5

A

B

C

D

Figure 2: Predicting a PSTH/SDF with 3 different methods. A: the dataset used in this comparison
consisted of 32 spiketrains recorded from a STSa neuron. Each tick mark represents a spike. B:
PSTH inferred with our Bayesian binning method. The thick line represents the predictive ﬁring
rate (section 4), the thin lines show the predictive ﬁring rate ±1 standard deviation. Models with
4 ≤ M ≤ 13 were included on a risk level of α = 0.1 (see eqn.(19)). C: bar PSTH (solid lines),
optimal binsize ≈ 26ms, and line PSTH (dashed lines), optimal binsize ≈ 78ms, computed by the
methods described in [1, 2]. D: SDF obtained by smoothing the spike trains with a 10ms Gaussian
kernel.

computed the model posterior (eqn.(18)) (see ﬁg.3, right). The prior parameters were equal for all
bins and set to σm = 1 and γm = 32. This choice corresponds to a ﬁring probability of ≈ 0.03 in
each 1 ms time interval (30 spikes/s), which is typical for the neurons in this study1. Models with
4 ≤ M ≤ 13 (expected bin sizes between ≈ 23ms-148ms) were included on an α = 0.1 risk level
the expected ﬁring rate,
(eqn.(19)) in the subsequent calculation of the predictive ﬁring rate (i.e.
hence the continuous appearance) and standard deviation (ﬁg.2, B). Fig.2, C, shows a bar PSTH and
a line PSTH computed with the recently developed methods described in [1, 2]. Roughly speaking,
the σm, γm which maximize of P ({~zi}|σm, γm) =
1Alternatively,
M P ({~zi}|M )P (M|σm, γm), where P ({~zi}|M ) is given by eqn.(8). Using a uniform P (M|σm, γm),

one could search for

P

we found σm ≈ 2.3 and γm ≈ 37 for the data in ﬁg.2, A

6

|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||0102030spiketrain number00.050.1P(spike)00.050.1P(spike)-1000100200300400500600time, ms after stimulus onset00.050.1P(spike)these methods try to optimize a compromise between minimal within-bin variance and maximal
between-bin variance. In this example, the bar PSTH consists of 26 bins. Graph D in ﬁg.2 depicts a
SDF obtained by smoothing the spiketrains with a 10ms wide Gaussian kernel, which is a standard
way of calculating SDFs in the neurophysiological literature.
All tested methods produce results which are, upon cursory visual inspection, largely consistent
with the spiketrains. However, Bayesian binning is better suited than Gaussian smoothing to model
steep changes, such as the transient response starting at ≈ 100ms. While the methods from [1, 2]
share this advantage, they suffer from two drawbacks: ﬁrstly, the bin boundaries are evenly spaced,
hence the peak of the transient is later than the scatterplots would suggest. Secondly, because the
bin duration is the only parameter of the model, these methods are forced to put many bins even
in intervals that are relatively constant, such as the baselines before and after the stimulus-driven
response. In contrast, Bayesian binning, being able to put bin boundaries anywhere in the time span
of interest, can model the data with less bins – the model posterior has its maximum at M = 6 (7
bins), whereas the bar PSTH consists of 26 bins.

6.3 Performance comparison

Figure 3: Left: Comparison of Bayesian Binning with competing methods by 5-fold crossvalidation.
The CV error is the negative expected log-probability of the test data. The histograms show rela-
tive frequencies of CV error differences between 3 competing methods and our Bayesian binning
approach. Gaussian: SDFs obtained by Gaussian smoothing of the spiketrains with a 10 ms kernel.
Bar PSTH and line PSTH: PSTHs computed by the binning methods described in [1, 2]. Right:
Model posterior P (M|{~zi}) (see eqn.(18)) computed from the data shown in ﬁg.2. The shape is
fairly typical for model posteriors computed from the neural data used in this paper: a sharp rise at
a moderately low M followed by a maximum (here at M = 6) and an approximately exponential
decay. Even though a maximum M of 699 would have been possible, P (M > 23|{~zi}) < 0.001.
Thus, we can accelerate the averaging process for quantities of interest (e.g. the predictive ﬁring
rate, section 4) by choosing a moderately small maximum M.

For a more rigorous method comparison, we split the data into distinct sets, each of which contained
the responses of a cell to a different stimulus. This procedure yielded 336 sets from 20 cells with at
least 20 spiketrains per set. We then performed 5-fold crossvalidation, the crossvalidation error is
given by the negative logarithm of the data (spike or gap) in the test sets:

CV error = −hlog(P (spike|t))i .

(20)
Thus, we measure how well the PSTHs predict the test data. The Gaussian SDFs were discretized
into 1 ms time intervals prior to the procedure. We average the CV error over the 5 estimates to obtain
a single estimate for each of the 336 neuron/stimulus combinations. On average, the negative log
likelihood of our Bayesian approach predicting the test data (0.04556±0.00029, mean ± SEM) was
signiﬁcantly better than any of the other methods (10ms Gaussian kernel: 0.04654 ± 0.00028; Bar
PSTH: 0.04739±0.00029; Line PSTH: 0.04658±0.00029). To directly compare the performance of
different methods we calculate the difference in the CV error for each neuron/stimulus combination.
Here a positive value indicates that Bayesian binning predicts the test data more accurately than the
alternative method. Fig.3, left, shows the relative frequencies of CV error differences between the
3 other methods and our approach. Bayesian binning predicted the data better than the three other

7

00.20.400.20.4relative frequency00.0050.010.015CV error relative to Bayesian Binning00.20.410 ms Gaussianbar PSTHline PSTH0102030M00.050.1P(M|{z i})methods in at least 295/336 cases, with a minimal difference of ≈ −0.0008, indicating the general
utility of this approach.

7 Summary

We have introduced an exact Bayesian binning method for the estimation of PSTHs. Besides treating
uncertainty – a real problem with small neurophysiological datasets – in a principled fashion, it also
outperforms competing methods on real neural data. It offers automatic complexity control because
the model posterior can be evaluated. While its computational cost is signiﬁcantly higher than that
of the methods we compared it to, it is still fast enough to be useful: evaluating the predictive
probability takes less than 1s on a modern PC2, with a small memory footprint (<10MB for 512
spiketrains).
Moreover, our approach can easily be adapted to extract other characteristics of neural responses in
a Bayesian way, e.g. response latencies or expected bin boundary positions. Our method reveals
a clear and sharp initial response onset, a distinct transition from the transient to the sustained part
of the response and a well-deﬁned offset. An extension towards joint PSTHs from simultaneous
multi-cell recordings is currently being implemented.

References
[1] H. Shimazaki and S. Shinomoto. A recipe for optimizing a time-histogram. In B. Sch¨olkopf,
J. Platt, and T. Hoffman, editors, Advances in Neural Information Processing Systems 19, pages
1289–1296. MIT Press, Cambridge, MA, 2007.

[2] H. Shimazaki and S. Shinomoto. A method for selecting the bin size of a time histogram.

Neural Computation, 19(6):1503–1527, 2007.

[3] J.O. Berger. Statistical Decision Theory and Bayesian Analysis. Springer, New York, 1985.
[4] D. Endres and P. F¨oldi´ak. Bayesian bin distribution inference and mutual information. IEEE

Transactions on Information Theory, 51(11), 2005.

[5] D. Endres. Bayesian and Information-Theoretic Tools for Neuroscience. PhD thesis, School

of Psychology, University of St. Andrews, U.K., 2006. http://hdl.handle.net/10023/162.

[6] M. Hutter.

Bayesian regression of piecewise constant functions.

arXiv:math/0606315v1, IDSIA-14-05, 2006.

Technical Report

[7] M. Hutter. Exact bayesian regression of piecewise constant functions. Journal of Bayesian

Analysis, 2(4):635–664, 2007.

[8] D. P. Bertsekas. Dynamic Programming and Optimal Control. Athena Scientiﬁc, 2000.
[9] M. W. Oram, D. Xiao, B. Dritschel, and K.R. Payne. The temporal precision of neural signals:
A unique role for response latency? Philosophical Transactions of the Royal Society, Series B,
357:987–1001, 2002.

[10] CJ Bruce, R Desimone, and CG Gross. Visual properties of neurons in a polysensory area in

superior temporal sulcus of the macaque. Journal of Neurophysiology, 46:369–384, 1981.

[11] DI Perrett, ET Rolls, and W Caan. Visual neurons responsive to faces in the monkey temporal

cortex. Expl. Brain. Res., 47:329–342, 1982.

[12] G.C. Baylis, E.T. Rolls, and C.M. Leonard. Functional subdivisions of the temporal lobe

neocortex. 1987.

[13] M. W. Oram and D. I. Perrett. Time course of neural responses discriminating different views

of the face and head. Journal of Neurophysiology, 68(1):70–84, 1992.

[14] K Tanaka, H Saito, Y Fukada, and M Moriya. Coding visual images of objects in the infer-
otemporal cortex of the macaque monkey. Journal of Neurophysiology, pages 170–189, 1991.
[15] N.E. Barraclough, D. Xiao, C.I. Baker, M.W. Oram, and D.I. Perrett. Integration of visual and
auditory information by superior temporal sulcus neurons responsive to the sight of actions.
Journal of Cognitive Neuroscience, 17, 2005.

23.2 GHz Intel XeonTM, SuSE Linux 10.0

8

"
631,2007,Online Linear Regression and Its Application to Model-Based Reinforcement Learning,"We provide a provably efficient algorithm for learning Markov Decision Processes (MDPs) with continuous state and action spaces in the online setting. Specifically, we take a model-based approach and show that a special type of online linear regression allows us to learn MDPs with (possibly kernalized) linearly parameterized dynamics. This result builds on Kearns and Singh's work that provides a provably efficient algorithm for finite state MDPs. Our approach is not restricted to the linear setting, and is applicable to other classes of continuous MDPs.","Online Linear Regression and Its Application to

Model-Based Reinforcement Learning

Alexander L. Strehl∗

Yahoo! Research
New York, NY

strehl@yahoo-inc.com

Michael L. Littman

Department of Computer Science

Rutgers University
Piscataway, NJ USA

mlittman@cs.rutgers.edu

Abstract

We provide a provably efﬁcient algorithm for learning Markov Decision Processes
(MDPs) with continuous state and action spaces in the online setting. Speciﬁcally,
we take a model-based approach and show that a special type of online linear
regression allows us to learn MDPs with (possibly kernalized) linearly parame-
terized dynamics. This result builds on Kearns and Singh’s work that provides a
provably efﬁcient algorithm for ﬁnite state MDPs. Our approach is not restricted
to the linear setting, and is applicable to other classes of continuous MDPs.

Introduction

Current reinforcement-learning (RL) techniques hold great promise for creating a general type of
artiﬁcial intelligence (AI), speciﬁcally autonomous (software) agents that learn difﬁcult tasks with
limited feedback (Sutton & Barto, 1998). Applied RL has been very successful, producing world-
class computer backgammon players (Tesauro, 1994) and model helicopter ﬂyers (Ng et al., 2003).
Many applications of RL, including the two above, utilize supervised-learning techniques for the
purpose of generalization. Such techniques enable an agent to act intelligently in new situations by
learning from past experience in different but similar situations.

Provably efﬁcient RL for ﬁnite state and action spaces is accomplished by Kearns and Singh (2002)
and hugely contributes to our understanding of the relationship between exploration and sequential
decision making. The achievement of the current paper is to provide an efﬁcient RL algorithm that
learns in Markov Decision Processes (MDPs) with continuous state and action spaces. We prove that
it learns linearly-parameterized MDPs, a model introduced by Abbeel and Ng (2005), with sample
(or experience) complexity that grows only polynomially with the number of state space dimensions.

Our new RL algorithm utilizes a special linear regresser, based on least-squares regression, whose
analysis may be of interest to the online learning and statistics communities. Although our primary
result is for linearly-parameterized MDPs, our technique is applicable to other classes of continuous
MDPs and our framework is developed speciﬁcally with such future applications in mind. The lin-
ear dynamics case should be viewed as only an interesting example of our approach, which makes
substantial progress in the goal of understanding the relationship between exploration and general-
ization in RL.

An outline of the paper follows. In Section 1, we discuss online linear regression and pose a new
online learning framework that requires an algorithm to not only provide predictions for new data
points but also provide formal guarantees about its predictions. We also develop a speciﬁc algorithm
and prove that it solves the problem. In Section 2, using the algorithm and result from the ﬁrst
section, we develop a provably efﬁcient RL algorithm. Finally, we conclude with future work.

∗Some of the work presented here was conducted while the author was at Rutgers University.

1

1 Online Linear Regression

Linear Regression (LR) is a well-known and tremendously powerful technique for prediction of
the value of a variable (called the response or output) given the value of another variable (called
the explanatory or input). Suppose we are given some data consisting of input-output pairs:
(x1, y1), (x2, y2), . . . , (xm, ym), where xi ∈ Rn and yi ∈ R for i = 1, . . . , m. Further, suppose
that the data satisﬁes a linear relationship, that is yi ≈ θT xi ∀i ∈ {1, . . . , m}, where θ ∈ Rn is an
n-dimensional parameter vector. When a new input x arrives, we would like to make a prediction
of the corresponding output by estimating θ from our data. A standard approach is to approximate
θ with the least-squares estimator ˆθ deﬁned by ˆθ = (X T X)−1X T y, where X ∈ Rm×n is a matrix
whose ith row consists of the ith input xT
i and y ∈ Rn is a vector whose ith component is the ith
output yi.
Although there are many analyses of the linear regression problem, none is quite right for an appli-
cation to model-based reinforcement learning (MBRL). In particular, in MBRL, we cannot assume
that X is ﬁxed ahead of time and we require more than just a prediction of θ but knowledge about
whether this prediction is sufﬁciently accurate. A robust learning agent must not only infer an ap-
proximate model of its environment but also maintain an idea about the accuracy of the parameters
of this model. Without such meta-knowledge, it would be difﬁcult to determine when to explore (or
when to trust the model) and how to explore (to improve the model). We coined the term KWIK
(“know what it knows”) for algorithms that have this special property. With this idea in mind, we
present the following online learning problem related to linear regression. Let ||v|| denote the Eu-
clidean norm of a vector v and let Var [X] denote the variance of a random variable X.

Deﬁnition 1 (KWIK Linear Regression Problem or KLRP) On every timestep t = 1, 2, . . . an
input vector xt ∈ Rnsatisf ying||xt|| ≤ 1 and output number yt ∈ [−1, 1] is provided. The input
xt may be chosen in any way that depends on the previous inputs and outputs (x1, y1), . . . , (xt, yt).
The output yt is chosen probabilistically from a distribution that depends only on xt and satisﬁes
E[yt] = θT xt and Var[yt] ≤ σ2, where θ ∈ Rn is an unknown parameter vector satisfying ||θ|| ≤ 1
and σ ∈ R is a known constant. After observing xt and before observing yt, the learning algorithm
must produce an output ˆyt ∈ [−1, 1] ∪ {∅} (a prediction of E[yt|xt]). Furthermore, it should be
able to provide an output ˆy(x) for any input vector x ∈ {0, 1}n.
A key aspect of our problem that distinguishes it from other online learning models is that the
algorithm is allowed to output a special value ∅ rather than make a valid prediction (an output other
than ∅). An output of ∅ signiﬁes that the algorithm is not sure of what to predict and therefore
declines to make a prediction. The algorithm would like to minimize the number of times it predicts
∅, and, furthermore, when it does make a valid prediction the prediction must be accurate, with
high probability. Next, we formalize the above intuition and deﬁne the properties of a “solution” to
KLRP.

Deﬁnition 2 We deﬁne an admissible algorithm for the KWIK Linear Regression Problem to
be one that takes two inputs 0 ≤  ≤ 1 and 0 ≤ δ < 1 and, with probability at least 1 − δ, satisﬁes
the following conditions:

1. Whenever the algorithm predicts ˆyt(x) ∈ [−1, 1], we have that |ˆyt(x) − θT x| ≤ .
2. The number of timesteps t for which ˆyt(xt) = ∅ is bounded by some function ζ(, δ, n),

polynomial in n, 1/ and 1/δ, called the sample complexity of the algorithm.

1.1 Solution

First, we present an algorithm and then a proof that it solves KLRP. Let X denote an m × n matrix
whose rows we interpret as transposed input vectors. We let X(i) denote the transpose of the ith
row of X. Since X T X is symmetric, we can write it as

X T X = U ΛU T ,

(1)
where U = [v1, . . . , vn] ∈ Rn×n, with v1, . . . , vn being a set of orthonormal eigenvectors of X T X.
Let the corresponding eigenvalues be λ1 ≥ λ2 ≥ ··· ≥ λk ≥ 1 > λk+1 ≥ ··· ≥ λn ≥ 0. Note that
Λ = diag(λ1, . . . , λn) is diagonal but not necessarily invertible. Now, deﬁne ¯U = [v1, . . . , vk] ∈

(Singular Value Decomposition)

2

Rn×k and ¯Λ = diag(λ1, . . . , λk) ∈ Rk×k. For a ﬁxed input xt (a new input provided to the
algorithm at time t), deﬁne
(2)
(3)

¯q := X ¯U ¯Λ−1 ¯U T xt ∈ Rm×n,

¯v = [0, . . . , 0, vT

k+1xt, . . . , vT

n xt]T ∈ Rn.

Algorithm 1 KWIK Linear Regression
0: Inputs: α1, α2
1: Initialize X = [ ] and y = [ ].
2: for t = 1, 2, 3,··· do
3:
4:
5:
6:

Let xt denote the input at time t.
Compute ¯q and ¯v using Equations 2 and 3.
if ||¯q|| ≤ α1 and ||¯v|| ≤ α2 then
Choose ˆθ ∈ Rn that minimizes Pi [y(i) − ¯θT X(i)]2 subject to ||¯θ|| ≤ 1, where X(i) is

the transpose of the ith row of X and y(i) is the ith component of y.
Output valid prediction xT ˆθ.

else

7:
8:
9:
10:
11:
12:
end if
13:
14: end for

Output ∅.
Receive output yt.
Append xT
Append yt as a new element to the vector y.

t as a new row to the matrix X.

Our algorithm for solving the KWIK Linear Regression Problem uses these quantities and is pro-
vided in pseudocode by Algorithm 1. Our ﬁrst main result of the paper is the following theorem.

Theorem 1 With appropriate parameter settings, Algorithm 1 is an admissible algorithm for the
KWIK Linear Regression Problem with a sample complexity bound of ˜O(n3/4).

Although the analysis of Algorithm 1 is somewhat complicated, the algorithm itself has a simple
interpretation. Given a new input xt, the algorithm considers making a prediction of the output yt
using the norm-constrained least-squares estimator (speciﬁcally, ˆθ deﬁned in line 6 of Algorithm1).
The norms of the vectors ¯q and ¯v provide a quantitative measure of uncertainty about this estimate.
When both norms are small, the estimate is trusted and a valid prediction is made. When either norm
is large, the estimate is not trusted and the algorithm produces an output of ∅.
One may wonder why ¯q and ¯v provide a measure of uncertainty for the least-squares estimate.
Consider the case when all eigenvalues of X T X are greater than 1. In this case, note that x =
X T X(X T X)−1x = X T ¯q. Thus, x can be written as a linear combination of the rows of X, whose
coefﬁcients make up ¯q, of previously experienced input vectors. As shown by Auer (2002), this
Intuitively,
particular linear combination minimizes ||q|| for any linear combination x = X T q.
if the norm of ¯q is small, then there are many previous training samples (actually, combinations
of inputs) “similar” to x, and hence our least-squares estimate is likely to be accurate for x. For
the case of ill-conditioned X T X (when X T X has eigenvalues close to 0), X(X T X)−1x may be
undeﬁned or have a large norm. In this case, we must consider the directions corresponding to small
eigenvalues separately and this consideration is dealt with by ¯v.

1.2 Analysis

We provide a sketch of the analysis of Algorithm 1. Please see our technical report for full details.
The analysis hinges on two key lemmas that we now present.

In the following lemma, we analyze the behavior of the squared error of predictions based on an
incorrect estimator ˆθ 6= θ verses the squared error of using the true parameter vector θ. Speciﬁcally,
we show that the squared error of the former is very likely to be larger than the latter when the pre-
dictions based on ˆθ (of the form ˆθT x for input x) are highly inaccurate. The proof uses Hoeffding’s
bound and is omitted.

3

Lemma 1 Let θ ∈ Rn and ˆθ ∈ Rn be two ﬁxed parameter vectors satisfying ||θ|| ≤ 1 and ||ˆθ|| ≤
1. Suppose that (x1, y1), . . . , (xm, ym) is any sequence of samples satisfying xi ∈ Rn, yi ∈ R,
||xi|| ≤ 1, yi ∈ [−1, 1], E[yi|xi] = θT xi, and Var[yi|xi] ≤ σ2. For any 0 < δ0 < 1 and ﬁxed
positive constant z, if

m

then

Xi=1

[(θ − ˆθ)T xi]2 ≥ 2p8m ln(2/δ) + z,

m

m

(4)

(5)

Xi=1

(yi − ˆθT xi)2 >

(yi − θT xi)2 + z

Xi=1
with probability at least 1 − 2δ0.
The following lemma, whose proof is fairly straight-forward and therefore omitted, relates the error
of an estimate ˆθT x for a ﬁxed input x based on an inaccurate estimator ˆθ to the quantities ||¯q||,
||¯v||, and ∆E(ˆθ) := qPm
i=1 [(θ − ˆθ)T X(i)]2. Recall that when ||¯q|| and ||¯v|| are both small, our
algorithm becomes conﬁdent of the least-squares estimate. In precisely this case, the lemma shows
that |(θ − ˆθ)T x| is bounded by a quantity proportional to ∆E(ˆθ).
Lemma 2 Let θ ∈ Rn and ˆθ ∈ Rn be two ﬁxed parameter vectors satisfying ||θ|| ≤ 1 and ||ˆθ|| ≤
1. Suppose that (x1, y1), . . . , (xm, ym) is any sequence of samples satisfying xi ∈ Rn, yi ∈ R,
||xi|| ≤ 1, yi ∈ [−1, 1]. Let x ∈ Rn be any vector. Let ¯q and ¯v be deﬁned as above. Let ∆E(ˆθ)
denote the error term qPm

i=1 [(θ − ˆθ)T xi]2. We have that

|(θ − ˆθ)T x| ≤ ||¯q||∆E(ˆθ) + 2||¯v||.

(6)

Proof sketch: (of Theorem 1)

The proof has three steps. The ﬁrst is to bound the sample complexity of the algorithm (the number
of times the algorithm makes a prediction of ∅), in terms of the input parameters α1 and α2. The
second is to choose the parameters α1 and α2. The third is to show that, with high probability, every
valid prediction made by the algorithm is accurate.
Step 1
We derive an upper bound ¯m on the number of timesteps for which either ||¯q|| > α1 holds or
||¯v|| > α2 holds. Observing that the algorithm trains on only those samples experienced during
pricisely these timesteps and applying Lemma 13 from the paper by Auer (2002) we have that

2

+

(7)

α2
1

n
α2

, and α2 = /4.

n√ln(1/(δ)) ln(n)

¯m = O(cid:18) n ln(n/α1)

2(cid:19) .
Step 2 We choose α1 = C·Q ln Q, where C is a constant and Q =
Step 3 Consider some ﬁxed timestep t during the execution of Algorithm 1 such that the algorithm
makes a valid prediction (not ∅). Let ˆθ denote the solution of the norm-constrained least-squares
minimization (line 6 in the pseudocode). By deﬁnition, since ∅ was not predicted, we have that
¯q ≤ α1 and ¯v ≤ α2. We would like to show that |ˆθT x− θT x| ≤  so that Condition 1 of Deﬁnition 2
is satisﬁed. Suppose not, namely that |(ˆθ − θ)T x| > . Using Lemma 2, we can lower bound the
quantity ∆E(ˆθ)2 = Pm
i=1[(θ − ˆθ)T X(i)]2, where m denotes the number of rows of the matrix X
(equivalently, the number of samples obtained used by the algorithm for training, which we upper-
bounded by ¯m), and X(i) denotes the transpose of the ith row of X. Finally, we would like to
apply Lemma 1 to prove that, with high probability, the squared error of ˆθ will be larger than the
squared error of predictions based on the true parameter vector θ, which contradicts the fact that ˆθ
was chosen to minimize the term Pm
i=1(yi − ˆθT X(i))2. One problem with this approach is that
Lemma 1 applies to a ﬁxed ˆθ and the least-squares computation of Algorithm 1 may choose any ˆθ in
the inﬁnite set {ˆθ ∈ Rn such that ||ˆθ|| ≤ 1}. Therefore, we use a uniform discretization to form a

4

ﬁnite cover of [−1, 1]n and apply the theorem to the member of the cover closest to ˆθ. To guarantee
that the total failure probability of the algorithm is at most δ, we apply the union bound over all
(ﬁnitely many) applications of Lemma 1. 2

1.3 Notes

In our formulation of KLRP we assumed an upper bound of 1 on the the two-norm of the inputs xi,
outputs yi, and the true parameter vector θ. By appropriate scaling of the inputs and/or outputs, we
could instead allow a larger (but still ﬁnite) bound.

Our analysis of Algorithm 1 showed that it is possible to solve KLRP with polynomial sample com-
plexity (where the sample complexity is deﬁned as the number of timesteps t that the algorithm
outputs ∅ for the current input xt), with high probability. We note that the algorithm also has poly-
nomial computational complexity per timestep, given the tractability of solving norm-constrained
least-squares problems (see Chapter 12 of the book by Golub and Van Loan (1996)).

1.4 Related Work

Work on linear regression is abundant in the statistics community (Seber & Lee, 2003). The use
of the quantities ¯v and ¯q to quantify the level of certainty of the linear estimator was introduced by
Auer (2002). Our analysis differs from that by Auer (2002) because we do not assume that the input
vectors xi are ﬁxed ahead of time, but rather that they may be chosen in an adversarial manner. This
property is especially important for the application of regression techniques to the full RL problem,
rather than the Associative RL problem considered by Auer (2002). Our analysis has a similar ﬂavor
to some, but not all, parts of the analysis by Abbeel and Ng (2005). However, a crucial difference
of our framework and analysis is the use of output ∅ to signify uncertainty in the current estimate,
which allows for efﬁcient exploration in the application to RL as described in the next section.

2 Application to Reinforcement Learning

The general reinforcement-learning (RL) problem is how to enable an agent (computer program,
robot, etc.) to maximize an external reward signal by acting in an unknown environment. To ensure
a well-deﬁned problem, we make assumptions about the types of possible worlds. To make the
problem tractable, we settle for near-optimal (rather than optimal) behavior on all but a polynomial
number of timesteps, as well as a small allowable failure probability. This type of performance
metric was introduced by Kakade (2003), in the vein of recent RL analyses (Kearns & Singh, 2002;
Brafman & Tennenholtz, 2002).

In this section, we formalize a speciﬁc RL problem where the environment is mathematically mod-
eled by a continuous MDP taken from a rich class of MDPs. We present an algorithm and prove
that it learns efﬁciently within this class. The algorithm is “model-based” in the sense that it con-
structs an explicit MDP that it uses to reason about future actions in the true, but unknown, MDP
environment. The algorithm uses, as a subroutine, any admissible algorithm for the KWIK Linear
Regression Problem introduced in Section 1. Although our main result is for a speciﬁc class of con-
tinuous MDPs, albeit an interesting and previously studied one, our technique is more general and
should be applicable to many other classes of MDPs as described in the conclusion.

2.1 Problem Formulation

The model we use is slightly modiﬁed from the model described by Abbeel and Ng (2005). The
main difference is that we consider discounted rather than undiscounted MDPs and we don’t require
the agent to have a “reset” action that takes it to a speciﬁed start state (or distribution). Let PS denote
the set of all (measurable) probability distributions over the set S. The environment is described by
a discounted MDP M = hS, A, T, R, γi, where S = RnS is the state space, A = RnA is the action
space, T : S × A → PS is the unknown transition dynamics, γ ∈ [0, 1) is the discount factor, and
R : S × A → R is the known reward function.1 For each timestep t, let xt ∈ S denote the current
1All of our results can easily be extended to the case of an unknown reward function with a suitable linearity

assumption.

5

state and ut ∈ A the current action. The transition dynamics T satisfy

xt+1 = M φ(xt, ut) + wt,

(8)
where xt+1 ∈ S, φ(·,·) : RnS +nA → Rn is a (basis or kernel) function satisfying ||φ(·,·)|| ≤ 1,
and M is an nS × n matrix. We assume that the 2-norm of each row of M is bounded by 1.2 Each
component of the noise term wt ∈ RnS is chosen i.i.d. from a normal distribution with mean 0
and variance σ2 for a known constant σ. If an MDP satisﬁes the above conditions we say that it
is linearly parameterized, because the next-state xt+1 is a linear function of the vector φ(xt, ut)
(which describes the current state and action) plus a noise term.
We assume that the learner (also called the agent) receives nS, nA, n, R, φ(·,·), σ, and γ as input,
with T initially being unknown. The learning problem is deﬁned as follows. The agent always
occupies a single state s of the MDP M . The agent is given s and chooses an action a. It then
receives an immediate reward r ∼ R(s, a) and is transported to a next state s0 ∼ T (s, a). This
procedure then repeats forever. The ﬁrst state occupied by the agent may be chosen arbitrarily.

M (s) (Qπ

A policy is any strategy for choosing actions. We assume (unless noted otherwise) that rewards all lie
in the interval [0, 1]. For any policy π, let V π
M (s, a)) denote the discounted, inﬁnite-horizon
value (action-value) function for π in M (which may be omitted from the notation) from state s.
Speciﬁcally, let st and rt be the tth encountered state and received reward, respectively, resulting
from execution of policy π in some MDP M from state s0. Then, V π
M (s) = E[P∞j=0 γjrj|s0 = s].
The optimal policy is denoted π∗ and has value functions V ∗M (s) and Q∗M (s, a). Note that a policy
cannot have a value greater than vmax := 1/(1 − γ) by the assumption of a maximum reward of 1.
2.2 Algorithm

First, we discuss how to use an admissible learning algorithm for KLRP to construct an MDP model.
We proceed by specifying the transition model for each of the (inﬁnitely many) state-action pairs.
Given a ﬁxed state-action pair (s, a), we need to estimate the next-state distribution of the MDP from
past experience, which consists of input state-action pairs (transformed by the nonlinear function φ)
and output next states. For each state component i ∈ {1, . . . , nS}, we have a separate learning
problem that can be solved by any instance Ai of an admissible KLRP algorithm.3 If each instance
makes a valid prediction (not ∅), then we simply construct an approximate next-state distribution
whose ith component is normally distributed with variance σ2 and whose mean is given by the
prediction of Ai (this procedure is equivalent to constructing an approximate transition matrix ˆM
whose ith row is equal to the transpose of the approximate parameter vector ˆθ learned by Ai).
If any instance of our KLRP algorithm predicts ∅ for state-action pair (s, a), then we cannot estimate
the next-state distribution. Instead, we make s highly rewarding in the MDP model to encourage
exploration, as done in the R-MAX algorithm (Brafman & Tennenholtz, 2002). Following the ter-
minology introduced by Kearns and Singh (2002), we call such a state (state-action) an “unknown”
state (state-action) and we ensure that the value function of our model assigns vmax (maximum pos-
sible) to state s. The standard way to satisfy this condition for ﬁnite MDPs is to make the transition
function for action a from state s a self-loop with reward 1 (yielding a value of vmax = 1/(1− γ) for
state s). We can affect the exact same result in a continuous MDP by adding a component to each
state vector s and to each vector φ(s, a) for every state-action pair (s, a). If (s, a) is “unknown” we
set the value of the additional components (of φ(s, a) and s) to 1, otherwise we set it to 0. We add an
additional row and column to M that preserves this extra component (during the transformation from
φ(s, a) to the next state s0) and otherwise doesn’t change the next-state distribution. Finally, we give
a reward of 1 to any unknown state, leaving rewards for the known states unchanged. Pseudocode
for the resulting KWIK-RMAX algorithm is provided in Algorithm 2.

Theorem 2 For any  and δ, the KWIK-RMAX algorithm executes an -optimal policy on at most
a polynomial (in n, nS, 1/, 1/δ, and 1/(1 − γ)) number of steps, with probability at least 1 − δ.
2The algorithm can be modiﬁed to deal with bounds (on the norms of the rows of M) that are larger than

one.

3One minor technical detail is that our KLRP setting requires bounded outputs (see Deﬁnition 1) while our
application to MBRL requires dealing with normal, and hence unbounded outputs. This is easily dealt with by
ignoring any extremely large (or small) outputs and showing that the resulting norm of the truncated normal
distribution learned by the each instance Ai is very close to the norm of the untruncated distribution.

6

Algorithm 2 KWIK-RMAX Algorithm
0: Inputs: nS, nA, n, R, φ(·,·), σ, γ, , δ, and admissible learning algorithm ModelLearn.
1: for all state components i ∈ {1, . . . , nS} do
2:

Initialize a new instantiation of ModelLearn, denoted Ai, with inputs C (1−γ)2
and δ/nS,
2√n
for inputs  and δ, respectively, in Deﬁnition 2, and where C is some constant determined by
the analysis.

γ and transition function speciﬁed by Ai for i ∈ {1, . . . , nS} as described above.

3: end for
4: Initialize an MDP Model with state space S, action space A, reward function R, discount factor
5: for t = 1, 2, 3,··· do
6:
7:
8:
9:
10:
11:
12:
13: end for

Let s denote the state at time t.
Choose action a := ˆπ∗(s) where ˆπ∗ is the optimal policy of the MDP Model.
Let s0 be the next state after executing action a.
for all factors i ∈ {1, . . . , n} do
end for
Update MDP Model.

Present input-output pair (φ(s, a), s0(i)) to Ai,a.

2.3 Analysis

Proof sketch: (of Theorem 2)
It can be shown that, with high probability, policy ˆπ∗ is either an -optimal policy (V ˆπ∗
(s) ≥
V ∗(s) − ) or it is very likely to lead to an unknown state. However, the number of times the latter
event can occur is bounded by the maximum number of times the instances Ai can predict ∅, which
is polynomial in the relevant parameters. 2

2.4 The Planning Assumption

We have shown that the KWIK-RMAX Algorithm acts near-optimally on all but a small (poly-
nomial) number of timesteps, with high probability. Unfortunately, to do so, the algorithm must
solve its internal MDP model completely and exactly. It is easy to extend the analysis to allow -
approximate solution. However, it is not clear whether even this approximate computation can be
done efﬁciently. In any case, discretization of the state space can be used, which yields computa-
tional complexity that is exponential in the number of (state and action) dimensions of the problem,
similar to the work of Chow and Tsitsiklis (1991). Alternatively, sparse sampling can be used, whose
complexity has no dependence on the size of the state space but depends exponentially on the time
horizon (≈ 1/(1 − γ)) (Kearns et al., 1999). Practically, there are many promising techniques that
make use of value-function approximation for fast and efﬁcient solution (planning) of MDPs (Sutton
& Barto, 1998). Nevertheless, it remains future work to fully analyze the complexity of planning.

2.5 Related Work

The general exploration problem in continuous state spaces was considered by Kakade et al. (2003),
and at a high level our approach to exploration is similar in spirit. However, a direct application
of Kakade et al.’s (2003) algorithm to linearly-parameterized MDPs results in an algorithm whose
sample complexity scales exponentially, rather than polynomially, with the state-space dimension.
That is because the analysis uses a factor of the size of the “cover” of the metric space. Reinforce-
ment learning in continuous MDPs with linear dynamics was studied by Fiechter (1997). However,
an exact linear relationship between the current state and next state is required for this analysis to
go through, while we allow the current state to be transformed (for instance, adding non-linear state
features) through non-linear function φ. Furthermore, Fiechter’s algorithm relied on the existence
of a “reset” action and a speciﬁc form of reward function. These assumptions admit a solution
that follows a ﬁxed policy and doesn’t depend on the actual history of the agent or the underlying
MDP. The model that we consider, linearly parameterized MDPs, is taken directly from the work by
Abbeel and Ng (2005), where it was justiﬁed in part by an application to robotic helicopter ﬂight. In

7

that work, a provably efﬁcient algorithm was developed in the apprenticeship RL setting. In this set-
ting, the algorithm is given limited access (polynomial number of calls) to a ﬁxed policy (called the
teacher’s policy). With high probably, a policy is learned that is nearly as good as the teacher’s pol-
icy. Although this framework is interesting and perhaps more useful for certain applications (such as
helicopter ﬂying), it requires a priori expert knowledge (to construct the teacher) and alleviates the
problem of exploration altogether. In addition, Abbeel and Ng’s (2005) algorithm also relies heavily
on a reset assumption, while ours does not.

Conclusion

We have provided a provably efﬁcient RL algorithm that learns a very rich and important class of
MDPs with continuous state and action spaces. Yet, many real-world MDPs do not satisfy the lin-
earity assumption, a concern we now address. Our RL algorithm utilized a speciﬁc online linear
regression algorithm. We have identiﬁed certain interesting and general properties (see Deﬁnition 2)
of this particular algorithm that support online exploration. These properties are meaningful without
the linearity assumption and should be useful for development of new algorithms for different mod-
eling assumptions. Our real goal of the paper is to work towards developing a general technique for
applying regression algorithms (as black boxes) to model-based reinforcement-learning algorithms
in a robust and formally justiﬁed way. We believe the approach used with linear regression can be
repeated for other important classes, but we leave the details as interesting future work.

Acknowledgements

We thank NSF and DARPA IPTO for support.

References

Abbeel, P., & Ng, A. Y. (2005). Exploration and apprenticeship learning in reinforcement learning. ICML ’05:
Proceedings of the 22nd international conference on Machine learning (pp. 1–8). New York, NY, USA:
ACM Press.

Auer, P. (2002). Using conﬁdence bounds for exploitation-exploration trade-offs. Journal of Machine Learning

Research, 3, 397–422.

Brafman, R. I., & Tennenholtz, M. (2002). R-MAX—a general polynomial time algorithm for near-optimal

reinforcement learning. Journal of Machine Learning Research, 3, 213–231.

Chow, C.-S., & Tsitsiklis, J. N. (1991). An optimal one-way multigrid algorithmfor discrete time stochastic

control. IEEE Transactions on Automatic Control, 36, 898–914.

Fiechter, C.-N. (1997). PAC adaptive control of linear systems. Tenth Annual Conference on Computational

Learning Theory (COLT) (pp. 72–80).

Golub, G. H., & Van Loan, C. F. (1996). Matrix computations. Baltimore, Maryland: The Johns Hopkins

University Press. 3rd edition.

Kakade, S. M. (2003). On the sample complexity of reinforcement learning. Doctoral dissertation, Gatsby

Computational Neuroscience Unit, University College London.

Kakade, S. M. K., Kearns, M. J., & Langford, J. C. (2003). Exploration in metric state spaces. Proceedings of

the 20th International Conference on Machine Learning (ICML-03).

Kearns, M., Mansour, Y., & Ng, A. Y. (1999). A sparse sampling algorithm for near-optimal planning in
large Markov decision processes. Proceedings of the Sixteenth International Joint Conference on Artiﬁcial
Intelligence (IJCAI-99) (pp. 1324–1331).

Kearns, M. J., & Singh, S. P. (2002). Near-optimal reinforcement learning in polynomial time. Machine

Learning, 49, 209–232.

Ng, A. Y., Kim, H. J., Jordan, M. I., & Sastry, S. (2003). Autonomous helicopter ﬂight via reinforcement

learning. Advances in Neural Information Processing Systems 16 (NIPS-03).

Seber, G. A. F., & Lee, A. J. (2003). Linear regression analysis. Wiley-Interscience.
Sutton, R. S., & Barto, A. G. (1998). Reinforcement learning: An introduction. The MIT Press.
Tesauro, G. (1994). TD-Gammon, a self-teaching backgammon program, achieves master-level play. Neural

Computation, 6, 215–219.

8

"
524,2007,A Randomized Algorithm for Large Scale Support Vector Learning,"We propose a randomized algorithm for large scale SVM learning which solves the problem by iterating over random subsets of the data. Crucial to the algorithm for scalability is the size of the subsets chosen. In the context of text classification we show that, by using ideas from random projections, a sample size of O(log n) can be used to obtain a solution which is close to the optimal with a high probability. Experiments done on synthetic and real life data sets demonstrate that the algorithm scales up SVM learners, without loss in accuracy.","A Randomized Algorithm for Large Scale Support

Vector Learning

Department of Computer Science and Automation, Indian Institute of Science, Bangalore-12

Krishnan S.

krishi@csa.iisc.ernet.in

Department of Computer Science and Automation, Indian Institute of Science, Bangalore-12

Chiranjib Bhattacharyya

chiru@csa.iisc.ernet.in

Ramesh Hariharan

Strand Genomics, Bangalore-80

ramesh@strandls.com

Abstract

This paper investigates the application of randomized algorithms for large scale
SVM learning. The key contribution of the paper is to show that, by using ideas
random projections, the minimal number of support vectors required to solve al-
most separable classiﬁcation problems, such that the solution obtained is near
optimal with a very high probability, is given by O(log n); if on removal of prop-
erly chosen O(log n) points the data becomes linearly separable then it is called
almost separable. The second contribution is a sampling based algorithm, moti-
vated from randomized algorithms, which solves a SVM problem by considering
subsets of the dataset which are greater in size than the number of support vectors
for the problem. These two ideas are combined to obtain an algorithm for SVM
classiﬁcation problems which performs the learning by considering only O(log n)
points at a time. Experiments done on synthetic and real life datasets show that the
algorithm does scale up state of the art SVM solvers in terms of memory required
and execution time without loss in accuracy. It is to be noted that the algorithm
presented here nicely complements existing large scale SVM learning approaches
as it can be used to scale up any SVM solver.

1 Introduction

Consider a training dataset D = f(xi; yi)g; i = 1 : : : n and yi = f+1;(cid:0)1g, where xi 2 Rd are data
points and yi specify the class labels. the problem of learning the classiﬁer, y = sign(wT x + b),
can be narrowed down to computing fw; bg such that it has good generalization ability. The SVM
formulation for classiﬁcation, which will be called C (cid:0) SV M, for determining fw; bg is given by
[1]
C-SVM-1:

M inimize(w;b;(cid:24))

1
2jjwjj2 + C

(cid:24)i

nX

i=1

At optimality w is given by w = X

Subject to : yi(w (cid:1) xi + b) (cid:21) 1 (cid:0) (cid:24)i; ; (cid:24)i (cid:21) 0; i = 1 : : : n

i:(cid:11)i>0

(cid:11)iyixi; 0 (cid:20) (cid:11)i (cid:20) C

1

Consider the set S = fxij(cid:11)i > 0g; the elements of this set are called the Support vectors. Note
that S completely determines the solution of C (cid:0) SV M.The set S may not be unique, though w is.
Deﬁne a parameter (cid:1) to be the minimum cardinality over all S. See that (cid:1) does not change with
number of examples, n, and is often much less than n.
More generally, the C (cid:0) SV M problem can be seen as an instance of Abstract optimization prob-
lem(AOP) [2, 3, 4]. An AOP is deﬁned as follows:
An AOP is a triple (H; <; (cid:8)) where H is a ﬁnite set, < a total ordering on 2H, and (cid:8) an oracle
that, for a given F (cid:18) G (cid:18) H, either reports F = min<F 0jF 0 (cid:18) G or returns a set F 0 (cid:18) G with
F 0 < F .
Many SVM learning problems are AOP problems; algorithms developed for AOP problems can be
used for solving SVM problems. Every AOP has a combinatorial dimension associated with it; the
combinatorial dimension captures the notion of number of free variables for that AOP. An AOP can
be solved by a randomized algorithm by selecting subsets of size greater than the combinatorial
dimension of the problem [2].

For SVM, (cid:1) is the combinatorial dimension of the problem; by iterating over subsets of size greater
than (cid:1), the subsets chosen using random sampling, the problem can be solved efﬁciently [3, 4]; this
algorithm was called RandSVM by the authors. Apriori the value of (cid:1) is not known, but for linearly
separable classiﬁcation problems the following holds: 2 (cid:20) (cid:1) (cid:20) d + 1. This follows from the fact
that the dual problem is the minimum distance between 2 non-overlapping convex hulls[5]. When
the problem is not linearly separable, the authors use the reduced convex hull formulation [5] to
come up with an estimate of the combinatorial dimension; this estimate is not very clear and much
higher than d1. The algorithm RandSVM2 iterates over subsets of size proportional to (cid:1)2.
RandSVM is not practical because of the following reasons: the sample size is too large in case of
high dimensional datasets, the dimension of feature space is usually unknown when using kernels,
and the reduced convex hull method used to calculate the combinatorial dimension, when the data is
not separable in the feature space, isn’t really useful as the number obtained is very large.

This work overcomes the above problems using ideas from random projections[6, 7] and random-
ized algorithms[8, 9, 2, 10],. As mentioned by the authors of RandSVM, the biggest bottleneck
in their algorithm is the value of (cid:1) as it is too large. The main contribution is, using ideas from
random projections, the conjecture that if RandSVM is solved using (cid:1) equal to O(log n), then the
solution obtained is close to optimal with high probability(Theorem 3), in particular for almost
separable datasets. Almost separable datasets are those which become linearly separable when a
small number of properly chosen data points are deleted from them. The second contribution is an
algorithm which, using ideas from randomized algorithms for Linear Programming(LP), solves the
SVM problem by using samples of size linear in (cid:1). This work also shows that the theory can be
applied to non-linear kernels.

2 A NEW RANDOMIZED ALGORITHM FOR CLASSIFICATION

This section uses results from random projections, and randomized algorithms for linear program-
ming, to develop a new algorithm for learning large scale SVM problems. In Section 2.1, we discuss
the case of linearly separable data and estimate the number of support vectors required such that the
margin is preserved with high probability, and show that this number is much smaller than the data
dimension d, using ideas from random projections. In Section 2.2, we look how the analysis applies
to almost separable data and present the main result of the paper(Theorem 2.2). The section ends
with a discussion on the application of the theory to non-linear kernels. In Section 2.3, we present
shows the randomized algorithm from SVM learning.

2.1 Linearly separable data

We start with determining the dimension k of the target space such that on performing a random pro-
jection to the space, the Euclidean distances and dot products are preserved. The appendix contains
a few results from random projections which will be used in this section.

1Details of this calculation are present in the supplementary material
2Presented in supplementary material

2

For a linearly separable dataset D = f(xi; yi); i = 1; : : : ; ng; xi 2 Rd; yi 2 f+1;(cid:0)1g, the C-SVM
formulation is the same as C-SVM-1 with (cid:24)i = 0; i = 1 : : : n. By dividing all the constraints by
jjwjj, the problem can be reformulated as follows:
C-SVM-2a:

M aximize( ^w;b;l)l; Subject to : yi( ^w (cid:1) xi + ^b) (cid:21) l; i = 1 : : : n; jj ^wjj = 1

, and l = 1
jjwjj

, ^b = b
where ^w = w
. l is the margin induced by the separating hyperplane,
jjwjj
jjwjj
that is, it is the distance between the 2 supporting hyperplanes, h1 : yi(w (cid:1) xi + b) = 1 and
h2 : yi(w (cid:1) xi + b) = (cid:0)1.
The determination of k proceeds as follows. First, for any given value of k, we show the change in
the margin as a function of k, if the data points are projected onto the k dimensional subspace and
the problem solved. From this, we determine the value k(k << d) which will preserve margin with
a very high probability. In a k dimensional subspace, there are at the most k + 1 support vectors.
Using the idea of orthogonal extensions(deﬁnition appears later in this section), we prove that when
the problem is solved in the original space, using an estimate of k + 1 on the number of support
vectors, the margin is preserved with a very high probability.
Let w0 and x0i; i = 1; : : : ; n be the projection of ^w and xi; i = 1; : : : ; n respectively onto a k
dimensional subspace (as in Lemma 2, Appendix A). The classiﬁcation problem in the projected
space with the dataset being D0 = f(x0i; yi); i = 1; : : : ; ng; x0i 2 Rk; yi 2 f+1;(cid:0)1g can be written
as follows:
C-SVM-2b:

M aximize(w0;^b;l0)l0; Subject to : yi(w0 (cid:1) x0i + ^b) (cid:21) l0; i = 1 : : : n; jjw0jj (cid:20) 1

where l0 = l(1 (cid:0) (cid:13)), (cid:13) is the distortion and 0 < (cid:13) < 1. The following lemma predicts, for a given
value of (cid:13), the k such that the margin is preserved with a high probability upon projection. be solved
with the optimal margin obtained close to the optimal margin for the original problem is given by
the following lemma.
Theorem 1. Let L = maxjjxijj and (w(cid:3); b(cid:3); l(cid:3)) be the optimal solution for C-SVM-2a. Let R be
a random d (cid:2) k matrix as given in Lemma 2(Appendix A). Let ew = RT w(cid:3)
; i =
pk
1; : : : ; n and k (cid:21) 8
(cid:14) ; 0 < (cid:13) < 1; 0 < (cid:14) < 1, then the following bound holds
on the optimal margin lP obtained by solving the problem C-SVM-2b:

and x0i = RT xipk

(cid:13)2 (1 + (1+L2)

)2 log 4n

2l(cid:3)

P (lP (cid:21) l(cid:3)(1 (cid:0) (cid:13))) (cid:21) 1 (cid:0) (cid:14)
Proof. From Corollary 1 of Lemma 2(Appendix A), we have

w(cid:3) (cid:1) xi (cid:0)

(cid:15)
2

(1 + L2) (cid:20) ew (cid:1) x0i (cid:20) w(cid:3) (cid:1) xi +

(cid:15)
2

(1 + L2)

8 , for some (cid:15) > 0. Consider some example xi with

which holds with probability at least 1 (cid:0) 4e(cid:0)(cid:15)2 k
yi = 1. Then the following holds with probability at least 1 (cid:0) 2e(cid:0)(cid:15)2 k
(cid:15)
(1 + L2) + b(cid:3) (cid:21) l(cid:3) (cid:0)
2
2 (1+L2)
jj ewjj

ew (cid:1) x0i + b(cid:3) (cid:21) w(cid:3) (cid:1) xi (cid:0)

(cid:21) l(cid:3)(cid:0) (cid:15)

(cid:15)
2

8

i+b(cid:3)
jj ewjj

at
Hence we have

Dividing the above by jjewjj, we have ew(cid:1)x0
1(Appendix A), we have (1 (cid:0) (cid:15))jjw(cid:3)jj (cid:20) jjewjj (cid:20) (1 + (cid:15))jjw(cid:3)jj, with probability
Since jjw(cid:3)jj = 1, we have p1 (cid:0) (cid:15) (cid:20) jjewjj (cid:20) p1 + (cid:15).
l(cid:3) (cid:0) (cid:15)
2 (1 + L2)
p1 + (cid:15)
(1 + L2))(p1 (cid:0) (cid:15)) = l(cid:3)(1 (cid:0)
(cid:15)
2

least 1 (cid:0) 2e(cid:0)(cid:15)2 k
8 .
ew (cid:1) x0i + b(cid:3)

(1 + L2)(p1 (cid:0) (cid:15)))

. Note that from Lemma

jjewjj

(1 + L2)

(cid:21)
(cid:21) (l(cid:3) (cid:0)
(cid:21) l(cid:3)(p1 (cid:0) (cid:15) (cid:0)

(cid:15)
2l(cid:3)
(1 + L2)) = l(cid:3)(1 (cid:0) (cid:15)(1 +

(cid:15)
2l(cid:3)

1 + L2

2l(cid:3)

))

3

This holds with probability at least 1 (cid:0) 4e(cid:0)(cid:15)2 k
8 . A similar result can be derived for a point xj for
which yj = (cid:0)1. The above analysis guarantees that by projecting onto a k dimensional space, there
exists at least one hyperplane ( ew
jj ewjj

; b(cid:3)
jj ewjj

), which guarantees a margin of l(cid:3)(1 (cid:0) (cid:13)) where
(cid:13) (cid:20) (cid:15)(1 +

1 + L2

)

2l(cid:3)

(1)

with probability at least 1 (cid:0) n4e(cid:0)(cid:15)2 k
can only be better than this. So the value of k is given by:

8 . The margin obtained by solving the problem C-SVM-2b, lP

(cid:0)

(1+ 1+L2

(cid:13)2
2l(cid:3) )2

k
8

n4e

(cid:20) (cid:14) ) k (cid:21)

8(1 + (1+L2)

2l(cid:3)

)2

(cid:13)2

log

4n
(cid:14)

(2)

As seen above, by randomly projecting the points onto a k dimensional subspace, the margin is
preserved with a high probability. This result is similar to the results obtained in work on random
projections[7]. But there are fundamental differences between the method proposed in this paper
and the previous methods: No random projection is actually done here, and no black box access
to the data distribution is required. We use Theorem 1 to determine an estimate on the number of
support vectors such that margin is preserved with a high probability, when the problem is solved
in the original space. This is given in Theorem 2 and is the main contribution of this section. The
theorem is based on the following fact: in a k dimensional space, the number of support vectors
is upper bounded by k + 1. We show that this k + 1 can be used as an estimate of the number of
support vectors in the original space such that the solution obtained preserves the margin with a high
probability. We start with the following deﬁnition.
Deﬁnition An orthogonal extension of a k (cid:0) 1-dimensional ﬂat( a k (cid:0) 1 dimensional ﬂat
is a k (cid:0) 1-dimensional afﬁne space) hp = (wp; b), where wp = (w1; : : : ; wk), in a subspace Sk
of dimension k to a d (cid:0) 1-dimensional hyperplane h = (ew; b) in d-dimensional space, is deﬁned
as follows. Let R 2 Rd(cid:2)d be a random projection matrix as in Lemma 2((Appendix A)). Let
^R 2 Rd(cid:2)k be a another random projection matrix which consists of only the the ﬁrst k columns of
R. Let ^xi = RT xi and x0i = ^RT
xi as follows: Let wp = (w1; : : : ; wk) be the optimal hyperplane
pk
classiﬁer with margin lP for the points x01; : : : ; x0n in the k dimensional subspace. Now deﬁne ew
to be all 0’s in the last d (cid:0) k coordinates and identical to wp in the ﬁrst k coordinates, that is,
ew = (w1; : : : ; wk; 0; : : : ; 0). Orthogonal extensions have the following key property. If (wp; b) is a
separator with margin lp for the projected points, then its orthogonal extension (ew; b) is a separator
with margin lp for the original points,that is,
if, yi(wp (cid:1) x0i + b) (cid:21) l; i = 1; : : : ; n then yi(ew (cid:1) ^xi + b) (cid:21) l; i = 1; : : : ; n

An important point to note, which will be required when extending orthogonal extensions to non-
linear kernels, is that dot products between the points are preserved upon doing orthogonal projec-
tions, that is, x0T
Let L; l(cid:3); (cid:13); (cid:14) and n be as deﬁned in Theorem 1. The following is the main result of this section.
Theorem 2. Given k (cid:21) 8
(cid:14) and n training points with maximum norm L in d
dimensional space and separable by a hyperplane with margin l(cid:3), there exists a subset of k0 training
points x10 : : : xk0 where k0 (cid:20) k and a hyperplane h satisfying the following conditions:

(cid:13)2 (1 + (1+L2)

i x0j = ^xi

)2 log 4n

T ^xj.

2l(cid:3)

1. h has margin at least l(cid:3)(1 (cid:0) (cid:13)) with probability at least 1 (cid:0) (cid:14)
2. x10 : : : xk0 are the only training points which lie either on h1 or on h2

Proof. Let w(cid:3); b(cid:3) denote the normal to a separating hyperplane with margin l(cid:3), that is, yi(w(cid:3) (cid:1) xi +
b(cid:3)) (cid:21) l(cid:3) for all xi and jjw(cid:3)jj = 1. Consider a random projection of x1; : : : ; xn to a k dimensional
space and let w0; z1; : : : ; zn be the projections of w(cid:3); x1; : : : ; xn, respectively, scaled by 1=pk. By
Theorem 1, yi(w0 (cid:1) zi + b(cid:3)) (cid:21) l(cid:3)(1 (cid:0) (cid:13)) holds for all zi with probability at least 1 (cid:0) (cid:14). Let h be the
orthogonal extension of w0; b(cid:3) to the full d dimensional space. Then h has margin at least l(cid:3)(1(cid:0) (cid:13)),
as required. This shows the ﬁrst part of the claim.
To prove the second part, consider the projected training points which lie on w0; b(cid:3) (that is, they lie
on either of the two sandwiching hyperplanes). Barring degeneracies, there are at the most k such
points. Clearly, these will be the only points which lie on the orthogonal extension h, by deﬁnition.(cid:3)

4

From the above analysis, it is seen that if k << d, then we can estimate that the number of support
vectors is k + 1, and the algorithm RandSVM would take on average O(k log n) iterations to solve
the problem [3, 4].

2.2 Almost separable data

In this section, we look at how the above analysis can be applied to almost separable datasets. We
call a dataset almost separable if by removing a fraction (cid:20) = O( log n
n ) of the points, the dataset
becomes linearly separable.

The C-SVM formulation when the data is not linearly separable(and almost separable) was given in
C-SVM-1. This problem can be reformulated as follows:
nX

M inimize(w;b;(cid:24))

(cid:24)i

Subject to : yi(w (cid:1) xi + b) (cid:21) l (cid:0) (cid:24)i; (cid:24)i (cid:21) 0; i = 1 : : : n;jjwjj (cid:20)

1
l

i=1

This formulation is known as the Generalized Optimal Hyperplane formulation. Here l depends on
the value of C in the C-formulation. At optimality, the margin l(cid:3) = l. The following theorem proves
a result for almost separable data similar to the one proved in Claim 1 for separable data.
Theorem 3. Given k (cid:21) 8
(cid:14) + (cid:20)n, l(cid:3) being the margin at optimality, l the
lower bound on l(cid:3) as in the Generalized Optimal Hyperplane formulation and (cid:20) = O( log n
n ), there
exists a subset of k0 training points x10 : : : xk0, k0 (cid:20) k and a hyperplane h satisfying the following
conditions:

(cid:13)2 (1 + (1+L2)

)2 log 4n

2l(cid:3)

1. h has margin at least l(1 (cid:0) (cid:13)) with probability at least 1 (cid:0) (cid:14)
2. At the most 8(1+ (1+L2)

(cid:14) points lie on the planes h1 or on h2

log 4n

)2

2l(cid:3)

(cid:13)2

3. x10 ; : : : ; xk0 are the only points which deﬁne the hyperplane h, that is, they are the support

vectors of h.

i:(cid:11)i>0

jjw(cid:3)jj

(cid:11)iyixi, and l(cid:3) = 1

Proof. Let the optimal solution for the generalized optimal hyperplane formulation be (w(cid:3); b(cid:3); (cid:24)(cid:3)).
w(cid:3) = X
as mentioned before. The set of support vectors can be split
into to 2 disjoint sets,SV1 = fxi : (cid:11)i > 0 and (cid:24)(cid:3)i = 0g(unbounded SVs), and SV2 = fxi : (cid:11)i >
0 and (cid:24)(cid:3)i > 0g(bounded SVs).
Now, consider removing the points in SV2 from the dataset. Then the dataset becomes linearly
separable with margin l(cid:3). Using an analysis similar to Theorem 1, and the fact that l(cid:3) (cid:21) l, we have
the proof for the ﬁrst 2 conditions.
When all the points in SV2 are added back, at most all these points are added to the set of support
vectors and the margin does not change. The margin not changing is guaranteed by the fact that for
proving the conditions 1 and 2, we have assumed the worst possible margin, and any value lower
than this would violate the constraints of the problem. This proves condition 3. (cid:3)

Hence the number of support vectors, such that the margin is preserved with high probability, can
be upper bounded by

k + 1 =

8
(cid:13)2 (1 +

(1 + L2)

2l(cid:3)

)2 log

4n
(cid:14)

+ (cid:20)n + 1 =

8
(cid:13)2 (1 +

(1 + L2)

2l(cid:3)

)2 log

4n
(cid:14)

+ O(log n)

(3)

Using a non-linear kernel Consider a mapping function (cid:8) : Rd ! Rd0
; d0 > d, which projects
a point xi 2 Rd to a point zi 2 Rd0, where Rd0 is a Euclidean space. Let the points be projected
onto a random k dimensional subspace as before. Then, as in the case of linear kernels, the lemmata
in the appendix are applicable to these random projections[11]. The orthogonal extensions can be

5

considered as a projection from the k dimensional space to the (cid:8)-space, such that the kernel function
values are preserved. Then it can be shown that Theorem 3 applies when using non-linear kernels
also.

2.3 A Randomized Algorithm

The reduction in the sample size from 6d2 to 6k2 is not enough to make RandSVM useful
in practice as 6k2 is still a large number. This section presents another randomized algorithm
which only requires that the sample size be greater than the number of support vectors. Hence
a sample size linear in k can be used in the algorithm. This algorithm was ﬁrst proposed to
solve large scale LP problems[10]; it has been adapted for solving large scale SVM problems.

Algorithm 1 RandSVM-1(D,k,r)
Require: D - The dataset.
Require: k - The estimate of the number of support vectors.
Require: r - Sample size = ck; c > 0.
1: S = randomsubset(D; r); // Pick a random subset, S, of size r from the dataset D
2: SV = svmlearn((cid:8); S); // SV - set of support vectors obtained by solving the problem S
3: V = fx 2 D(cid:0)Sjviolates(x; SV )g //violator - nonsampled point not satisfying KKT conditions
4: while jV j > 0 and jSV j < k do
5: R = randomsubset(V , r (cid:0) jSV j); //Pick a random subset from the set of violators
6:
7:
8: end while
9: return SV

SV = svmlearn(SV; R); //SV - set of support vectors obtained by solving the problem SV [ R
V = fx 2 D (cid:0) (SV [ R)jviolates(x; SV )g; //Determine violators from nonsampled set

Proof of Convergence: Let SV be the current set of support vectors. Condition jSV j < k comes
from Theorem 3. Hence if the condition is violated, then the algorithm terminates solution which
is near optimal with a very high probability.
Now consider the case where jSV j < k and jV j > 0. Let xi be a violator(xi is a non-sampled
point such that yi(wT xi + b) < 1). Solving the problem with the set of constraints as SV [ xi will
only result, since SVM is an instance of AOP, in the increase(decrease) of the objective function
of the primal(dual). As there are only ﬁnite number of basis for an AOP, the algorithm is bound to
terminate; also if termination happens with the number of violators equal to zero, then the solution
obtained is optimal.

Determination of k The value of k depends on the l which is not available in case of C-SVM and
nu-SVM. This can be handled only be solving for k as a function of (cid:15) where (cid:15) is the maximum al-
lowed distortion in the L2 norms of the vectors upon projection. If all the data points are normalized
to length 1, that is, L = 1, then Equation 1 becomes (cid:15) (cid:21) (cid:13)=(1 + 1+L2
2l(cid:3) ). Combining this with the
result from Theorem 2, the value of k can be determined in terms of (cid:15) as follows:

8
(cid:13)2 (1 +

k (cid:21)

(1 + L2)

2l(cid:3)

)2 log

4n
(cid:14)

+ O(log n) (cid:21)

16
(cid:13)2 (1 +

(1 + L2)

2l(cid:3)

)2 log

4n
(cid:14)

) (cid:21)

16
(cid:15)2 log

4n
(cid:14)

(4)

3 Experiments

This section discusses the performance of RandSVM in practice. The experiments were performed
on 3 synthetic and 1 real world dataset. RandSVM was used with LibSVM as the solver when using
a non-linear kernel; with SVMLight for a linear kernel. This choice was made because it was ob-
served that SVMLight is much faster than LibSVM when using a linear kernel, and vice-versa when
using non-linear kernels. RandSVM has been compared with state of the art SVM solvers: LibSVM
for non-linear kernels, and SVMPerf and SVMLin for linear kernels.
Synthetic datasets
The twonorm dataset is a 2 class problem where each class is drawn from a multivariate nor-
mal distribution with unit variance. Each vector is a 20 dimensional vector. One class has mean

(a; a; : : : ; a), and the other class has mean ((cid:0)a;(cid:0)a; : : : ;(cid:0)a), where a = 2=p(20).

The ringnorm dataset is a 2 class problem with each vector consisting of 20 dimensions. Each class

6

Category
twonorm1
twonorm2
ringnorm1
ringnorm2

Kernel
Gaussian
Gaussian
Gaussian
Gaussian
checkerboard1 Gaussian
checkerboard2 Gaussian
Linear
Linear

CCAT(cid:3)
C11(cid:3)

RandSVM

300 (94.98%)
437 (94.71%)
2637 (70.66%)
4982 (65.74%)
406 (93.70%)
814 (94.10%)
345 (94.37%)
449 (96.57%)

LibSVM

SVMPerf

SVMLin

8542 (96.48%)

256 (70.31%)
85124 (65.34%)
1568.93 (96.90%)

-

-
X
X

X
X
X
X
X
X

148 (94.38%)
120 (97.53%)

X
X
X
X
X
X

429(95.1913%)
295 (97.71%)

Table 1: The table gives the execution time(in seconds) and the classiﬁcation accuracy(in brackets).
The subscripts 1 and 2 indicate that the corresponding training set sizes are 105 and 106 respectively.
A ’-’ indicates that the solver did not ﬁnish execution even after a running for a day. A ’X’ indicates
that the experiment is not applicable for the corresponding solver. The ’(cid:3)’ indicates that the solver
used with RandSVM was SVMLight; otherwise it was LibSVM.

is drawn from a multivariate normal distribution. One class has mean 1, and covariance 4 times the
identity. The other class has mean (a; a; : : : ; a), and unit covariance where a = 2=p(20).
The checkerboard dataset consists of vectors in a 2 dimensional space. The points are generated in
a 4 (cid:2) 4 grid. Both the classes are generated from a multivariate uniform distribution; each point is
(x1 = U (0; 4); x2 = U (0; 4)). The points are labelled as follows - if(dx1e%2 6= dx2e%2), then the
point is labelled negative, else the point is labelled positive.
For each of the synthetic datasets, a training set of 10,00,000 points and a test set of 10,000 points
was generated. A smaller subset of 1,00,000 points was chosen from training set for parameter tun-
ing. From now on, the smaller training set will have a subscript of 1 and the larger training set will
have a subscript of 2, for example, ringnorm1 and ringnorm2.
Real world dataset
The RCV1 dataset consists of 804,414 documents, with each document consisting of 47,236 fea-
tures. Experiments were performed using 2 categories of the dataset - CCAT and C11. The dataset
was split into a training set of 7,00,000 documents and a test set of 104,414 documents.
Table 1 shows the kernels which were used for each of the datasets. The parameters used for the
gaussian kernels, (cid:27) and C, were obtained using grid search based tuning. The parameter for the
linear kernel, C, for CCAT and C11 were obtained from previous work done[12].
Selection of k for RandSVM: The values of (cid:15) and (cid:14) were ﬁxed to 0:2 and 0:9 respectively, for all
the datasets. For linearly separable datasets, k was set to (16 log(4n=(cid:14)))=(cid:15)2. For the others, k was
set to (32 log(4n=(cid:14)))=(cid:15)2.
Discussion of results: Table 1, which has the timing and classiﬁcation accuracy comparisons, shows
that RandSVM can scale up SVM solvers for very large datasets. Using just a small wrapper around
the solvers, RandSVM has scaled up SVMLight so that its performance is comparable to that of
state of the art solvers such as SVMPerf and SVMLin. Similarly LibSVM has been made capable of
quickly solving problems which it could not do before, even after executing for a day. Furthermore,
it is clear, from the experiments on the synthetic datasets, that the execution times taken for training
with 105 examples and 106 examples are not too far apart; this is a clear indication that the execution
time does not increase rapidly with the increase in the dataset size.
All the runs of RandSVM terminated with the condition jSV j < k being violated. Since the clas-
siﬁcation accuracies obtained by using RandSVM and the baseline solvers are very close, it is clear
that Theorem 2 holds in practice.

4 Further Research

It is clear from the experimental evaluations that randomized algorithms can be used to scale up
SVM solvers to large scale classiﬁcation problems. If an estimate of the number of support vectors
is obtained then algorithm RandSVM-1 can be used for other SVM learning problems also, as they
are usually instances of an AOP. The future work would be to apply the work done here to such
problems.

7

A Some Results from Random Projections

Here we review a few lemmas from random projections [7]. The following lemma discusses how
the L2 norm of a vector is preserved when it is projected on a random subspace.
Lemma 1. Let R = (rij) be a random d (cid:2) k matrix, such that each entry (rij) is chosen indepen-
dently according to N (0; 1). For any ﬁxed vector u 2 Rd, and any (cid:15) > 0, let u0 = RT upk
. Then
E[jju0jj2] = jjujj2 and the following bound holds:

P ((1 (cid:0) (cid:15))jjujj2 (cid:20) jju0jj2 (cid:20) (1 + (cid:15))jjujj2) (cid:21) 1 (cid:0) 2e(cid:0)((cid:15)2(cid:0)(cid:15)3) k

4

The following theorem and its corollary show the change in the Euclidean distance between 2 points
and the dot products when they are projected onto a lower dimensional space [7].
Lemma 2. Let u; v 2 Rd. Let u0 = RT upk
be the projections of u and v to Rk via a
random matrix R whose entries are chosen independently from N (0; 1) or U ((cid:0)1; 1). Then for any
(cid:15) > 0, the following bounds hold

and v0 = RT upk

P ((1 (cid:0) (cid:15))ku (cid:0) vk2 (cid:20) ku0 (cid:0) v0k2) (cid:21) 1 (cid:0) e(cid:0)((cid:15)2(cid:0)(cid:15)3) k
P (ku0 (cid:0) v0k2 (cid:20) (1 + (cid:15))ku (cid:0) vk2) (cid:21) 1 (cid:0) e(cid:0)((cid:15)2(cid:0)(cid:15)3) k

4

4 ; and

A corollary of the above theorem shows how well the dot products are preserved upon projec-
tion(This is a slight modiﬁcation of the corollary given in [7]).
Corollary 1. Let u; v be vectors in Rd s.t. kuk (cid:20) L1;kvk (cid:20) L2. Let R be a random matrix whose
entries are chosen independently from either N (0; 1) or U ((cid:0)1; 1). Deﬁne u0 = RT upk
and v0 = RT vpk
.
Then for any (cid:15) > 0, the following holds with probability at least 1 (cid:0) 4e(cid:0)(cid:15)2 k
1 + L2
2)

1 + L2

(L2

(L2

8

u (cid:1) v (cid:0)

(cid:15)
2

2) (cid:20) u0 (cid:1) v0 (cid:20) u (cid:1) v +

(cid:15)
2

References

[1] V. Vapnik. The Nature of Statistical Learning Theory. Springer, New York, 1995.
[2] Bernd Gartner. A subexponential algorithm for abstract optimization problems. In Proceedings

33rd Symposium on Foundations of Computer Science, IEEE CS Press, 1992.

[3] Jose L. Balcazar, Yang Dai, and Osamu Watanabe. A random sampling technique for training

support vector machines. In ALT. Springer, 2001.

[4] Jose L. Balcazar, Yang Dai, and Osamu Watanabe. Provably fast training algorithms for sup-

port vector machines. In ICDM, pages 43–50, 2001.

[5] K. P. Bennett and E. J. Bredensteiner. Duality and geometry in SVM classiﬁers. In P. Langley,

editor, ICML, pages 57–64, San Francisco, California, 2000.

[6] W. Johnson and J. Lindenstauss. Extensions of lipschitz maps into a hilbert space. Contempo-

rary Mathematics, 1984.

[7] R. I. Arriaga and S. Vempala. An algorithmic theory of learning: Random concepts and random

projections. In Proceedings of the 40th Foundations of Computer Science, 1999.

[8] Kenneth L. Clarkson. Las vegas algorithms for linear and integer programming when the

dimension is small. Journal of the ACM, 42(2):488–499, 1995.

[9] B. Gartner and E. Welzl. A simple sampling lemma: analysis and application in geometric
optimization. In Proceedings of the 16th annual ACM symposium on Computational Geometry,
2000.

[10] M. Pellegrini. Randomizing combinatorial algorithms for linear programming when the di-

mension is moderately high. In SODA ’01, pages 101–108, Philadelphia, PA, USA, 2001.

[11] Maria-Florina Balcan, Avrim Blum, and Santosh Vempala. On kernels, margins and low-

dimensional mappings. In Proc. of the 15th Conf. Algorithmic Learning Theory, 2004.

[12] T. Joachims. Training linear svms in linear time. In Proceedings of the ACM Conference on

Knowledge Discovery and Data Mining (KDD), 2006.

8

"
232,2007,Direct Importance Estimation with Model Selection and Its Application to Covariate Shift Adaptation,"When training and test samples follow different input distributions (i.e., the situation called \emph{covariate shift}), the maximum likelihood estimator is known to lose its consistency. For regaining consistency, the log-likelihood terms need to be weighted according to the \emph{importance} (i.e., the ratio of test and training input densities). Thus, accurately estimating the importance is one of the key tasks in covariate shift adaptation. A naive approach is to first estimate training and test input densities and then estimate the importance by the ratio of the density estimates. However, since density estimation is a hard problem, this approach tends to perform poorly especially in high dimensional cases. In this paper, we propose a direct importance estimation method that does not require the input density estimates. Our method is equipped with a natural model selection procedure so tuning parameters such as the kernel width can be objectively optimized. This is an advantage over a recently developed method of direct importance estimation. Simulations illustrate the usefulness of our approach.","Direct Importance Estimation with Model Selection
and Its Application to Covariate Shift Adaptation

Masashi Sugiyama

Tokyo Institute of Technology
sugi@cs.titech.ac.jp

Shinichi Nakajima
Nikon Corporation

nakajima.s@nikon.co.jp

Hisashi Kashima

IBM Research

Paul von B¨unau

Technical University Berlin

Motoaki Kawanabe
Fraunhofer FIRST

hkashima@jp.ibm.com

buenau@cs.tu-berlin.de

nabe@first.fhg.de

Abstract

A situation where training and test samples follow different input distributions is
called covariate shift. Under covariate shift, standard learning methods such as
maximum likelihood estimation are no longer consistent—weighted variants ac-
cording to the ratio of test and training input densities are consistent. Therefore,
accurately estimating the density ratio, called the importance, is one of the key is-
sues in covariate shift adaptation. A naive approach to this task is to ﬁrst estimate
training and test input densities separately and then estimate the importance by
taking the ratio of the estimated densities. However, this naive approach tends to
perform poorly since density estimation is a hard task particularly in high dimen-
sional cases. In this paper, we propose a direct importance estimation method that
does not involve density estimation. Our method is equipped with a natural cross
validation procedure and hence tuning parameters such as the kernel width can be
objectively optimized. Simulations illustrate the usefulness of our approach.

1 Introduction

A common assumption in supervised learning is that training and test samples follow the same
distribution. However, this basic assumption is often violated in practice and then standard machine
learning methods do not work as desired. A situation where the input distribution P (x) is different
in the training and test phases but the conditional distribution of output values, P (yjx), remains
unchanged is called covariate shift [8]. In many real-world applications such as robot control [10],
bioinformatics [1], spam ﬁltering [3], brain-computer interfacing [9], or econometrics [5], covariate
shift is conceivable and thus learning under covariate shift is gathering a lot of attention these days.

The inﬂuence of covariate shift could be alleviated by weighting the log likelihood terms according
to the importance [8]: w(x) = pte(x)=ptr(x), where pte(x) and ptr(x) are test and training input
densities. Since the importance is usually unknown, the key issue of covariate shift adaptation is
how to accurately estimate the importance.

A naive approach to importance estimation would be to ﬁrst estimate the training and test densities
separately from training and test input samples, and then estimate the importance by taking the ratio
of the estimated densities. However, density estimation is known to be a hard problem particularly
in high-dimensional cases. Therefore, this naive approach may not be effective—directly estimating
the importance without estimating the densities would be more promising.

Following this spirit, the kernel mean matching (KMM) method has been proposed recently [6],
which directly gives importance estimates without going through density estimation. KMM is shown

1

to work well, given that tuning parameters such as the kernel width are chosen appropriately. In-
tuitively, model selection of importance estimation algorithms (such as KMM) is straightforward
by cross validation (CV) over the performance of subsequent learning algorithms. However, this is
highly unreliable since the ordinary CV score is heavily biased under covariate shift—for unbiased
estimation of the prediction performance of subsequent learning algorithms, the CV procedure itself
needs to be importance-weighted [9]. Since the importance weight has to have been ﬁxed when
model selection is carried out by importance weighted CV, it can not be used for model selection of
importance estimation algorithms.

The above fact implies that model selection of importance estimation algorithms should be per-
formed within the importance estimation step in an unsupervised manner. However, since KMM
can only estimate the values of the importance at training input points, it can not be directly applied
in the CV framework; an out-of-sample extension is needed, but this seems to be an open research
issue currently.

In this paper, we propose a new importance estimation method which can overcome the above
problems, i.e., the proposed method directly estimates the importance without density estimation
and is equipped with a natural model selection procedure. Our basic idea is to ﬁnd an importance

estimate bw(x) such that the Kullback-Leibler divergence from the true test input density pte(x)
to its estimate bpte(x) = bw(x)ptr(x) is minimized. We propose an algorithm that can carry out

this minimization without explicitly modeling ptr(x) and pte(x). We call the proposed method the
Kullback-Leibler Importance Estimation Procedure (KLIEP). The optimization problem involved in
KLIEP is convex, so the unique global solution can be obtained. Furthermore, the solution tends to
be sparse, which contributes to reducing the computational cost in the test phase.

Since KLIEP is based on the minimization of the Kullback-Leibler divergence, its model selection
can be naturally carried out through a variant of likelihood CV, which is a standard model selection
technique in density estimation. A key advantage of our CV procedure is that, not the training
samples, but the test input samples are cross-validated. This highly contributes to improving the
model selection accuracy since the number of training samples is typically limited while test input
samples are abundantly available.

The simulation studies show that KLIEP tends to outperform existing approaches in importance
estimation including the logistic regression based method [2], and it contributes to improving the
prediction performance in covariate shift scenarios.

2 New Importance Estimation Method

In this section, we propose a new importance estimation method.

2.1 Formulation and Notation

i gntr
Let D (cid:26) (Rd) be the input domain and suppose we are given i.i.d. training input samples fxtr
i=1
j=1 from a
from a training input distribution with density ptr(x) and i.i.d. test input samples fxte
test input distribution with density pte(x). We assume that ptr(x) > 0 for all x 2 D. Typically,
the number ntr of training samples is rather small, while the number nte of test input samples is
very large. The goal of this paper is to develop a method of estimating the importance w(x) from
fxtr

i=1 and fxte

j gnte
j=1:

j gnte

i gntr

w(x) =

pte(x)
ptr(x)

:

Our key restriction is that we avoid estimating densities pte(x) and ptr(x) when estimating the
importance w(x).

2.2 Kullback-Leibler Importance Estimation Procedure (KLIEP)

Let us model the importance w(x) by the following linear model:

(cid:11)‘’‘(x);

(1)

bw(x) =

bX‘=1

2

where f(cid:11)‘gb
such that

‘=1 are parameters to be learned from data samples and f’‘(x)gb

‘=1 are basis functions

i=1 and fxte
j=1, i.e., kernel
‘=1 are chosen in Section 2.3.

j gnte

‘=1 in the model (1) so that the Kullback-Leibler divergence from

’‘(x) (cid:21) 0 for all x 2 D and for ‘ = 1; 2; : : : ; b:
i gntr
‘=1 could be dependent on the samples fxtr
Note that b and f’‘(x)gb
models are also allowed—we explain how the basis functions f’‘(x)gb

Using the model bw(x), we can estimate the test input density pte(x) by
pte(x) tobpte(x) is minimized:

bpte(x) = bw(x)ptr(x):

We determine the parameters f(cid:11)‘gb

pte(x) log

pte(x)

dx

KL[pte(x)kbpte(x)] =ZD
=ZD

pte(x) log

bw(x)ptr(x)
dx (cid:0)ZD

pte(x)
ptr(x)

Since the ﬁrst term in the last equation is independent of f(cid:11)‘gb
second term. We denote it by J:

J =ZD

1
nte

(cid:25)

pte(x) logbw(x)dx
nteXj=1
logbw(xte

j ) =

1
nte

log bX‘=1

nteXj=1

(cid:11)‘’‘(xte

j )! ;
j gnte

pte(x) logbw(x)dx:

‘=1, we ignore it and focus on the

(2)

j gnte

j=1 is used from the ﬁrst
where the empirical approximation based on the test input samples fxte
line to the second line above. This is our objective function to be maximized with respect to the
parameters f(cid:11)‘gb
‘=1, which is concave [4]. Note that the above objective function only involves the
test input samples fxte
i=1 yet. As shown
i gntr
below, fxtr

j=1, i.e., we did not use the training input samples fxtr

i=1 will be used in the constraint.

bw(x) is an estimate of the importance w(x) which is non-negative by deﬁnition. Therefore, it is
natural to impose bw(x) (cid:21) 0 for all x 2 D, which can be achieved by restricting
In addition to the non-negativity, bw(x) should be properly normalized sincebpte(x) (= bw(x)ptr(x))

(cid:11)‘ (cid:21) 0 for ‘ = 1; 2; : : : ; b:

is a probability density function:

i gntr

(3)

1 =ZDbpte(x)dx =ZD bw(x)ptr(x)dx
bX‘=1

ntrXi=1

ntrXi=1 bw(xtr

1
ntr

1
ntr

i ) =

(cid:25)

(cid:11)‘’‘(xtr

i );

where the empirical approximation based on the training input samples fxtr
ﬁrst line to the second line above.

i gntr

i=1 is used from the

Now our optimization criterion is summarized as follows.

(cid:11)‘’‘(xte

j )!35

nteXj=1

maximize
f(cid:11)‘gb

‘=1 24

log bX‘=1
ntrXi=1
bX‘=1
Figure 1-(a). Note that the solution fb(cid:11)‘gb

subject to

This is a convex optimization problem and the global solution can be obtained, e.g., by simply
performing gradient ascent and feasibility satisfaction iteratively. A pseudo code is described in
‘=1 tends to be sparse [4], which contributes to reducing the
computational cost in the test phase. We refer to the above method as Kullback-Leibler Importance
Estimation Procedure (KLIEP).

(cid:11)‘’‘(xtr

i ) = ntr and (cid:11)1; (cid:11)2; : : : ; (cid:11)b (cid:21) 0:

3

i=1, and fxte

j gnte
j=1

i gntr

‘=1, fxtr

Input: m = f’‘(x)gb
Output: bw(x)
Aj;‘ (cid:0) ’‘(xte
j );
Pntr
i=1 ’‘(xtr
b‘ (cid:0) 1
ntr
Initialize (cid:11) (> 0) and "" (0 < "" (cid:28) 1);
Repeat until convergence

i );

(cid:11) (cid:0) (cid:11) + ""A>(1:=A(cid:11));
(cid:11) (cid:0) (cid:11) + (1 (cid:0) b>(cid:11))b=(b>b);
(cid:11) (cid:0) max(0; (cid:11));
(cid:11) (cid:0) (cid:11)=(b>(cid:11));

end

bw(x) (cid:0) Pb

‘=1 (cid:11)‘’‘(x);

(x)gb(k)

‘=1 g,

Input: M = fmk j mk = f’(k)
j gnte
j=1

i=1, and fxte

‘

i gntr
fxtr
Output: bw(x)
j gnte
Split fxte
for each model m 2 M

j=1 into R disjoint subsets fX te

r gR

r=1;

j gj6=r);

i gntr
i=1; fX te
log bwr(x);

for each split r = 1; : : : ; R

bwr(x) (cid:0) KLIEP(m; fxtr
r j Px2X te
bJr(m) (cid:0) 1
R PR

r=1 bJr(m);

end
bJ(m) (cid:0) 1

jX te

r

end
bm (cid:0) argmaxm2M bJ(m);
bw(x) (cid:0) KLIEP(bm; fxtr

i gntr

i=1; fxte

j gnte

j=1);

(a) KLIEP main code

(b) KLIEP with model selection

Figure 1: KLIEP algorithm in pseudo code. ‘./’ indicates the element-wise division and > denotes
the transpose. Inequalities and the ‘max’ operation for a vector are applied element-wise.

2.3 Model Selection by Likelihood Cross Validation

The performance of KLIEP depends on the choice of basis functions f’‘(x)gb
how they can be appropriately chosen from data samples.

‘=1. Here we explain

r

bJr =

j gnte

Since KLIEP is based on the maximization of the score J (see Eq.(2)), it would be natural to select
the model such that J is maximized. The expectation over pte(x) involved in J can be numer-
ically approximated by likelihood cross validation (LCV) as follows: First, divide the test sam-
ples fxte
fX te

j gj6=r and approximate the score J using X te

j=1 into R disjoint subsets fX te

r=1. Then obtain an importance estimate bwr(x) from
r j Xx2X te

1
jX te

r gR

r as

1
R

bJ =

We repeat this procedure for r = 1; 2; : : : ; R, compute the average of bJr over all r, and use the
average bJ as an estimate of J:
For model selection, we compute bJ for all model candidates (the basis functions f’‘(x)gb
the current setting) and choose the one that minimizes bJ. A pseudo code of the LCV procedure is

One of the potential limitations of CV in general is that it is not reliable in small sample cases
since data splitting by CV further reduces the sample size. On the other hand, in our CV procedure,
the data splitting is performed over the test input samples, not over the training samples. Since we
typically have a large number of test input samples, our CV procedure does not suffer from the small
sample problem.

summarized in Figure 1-(b)

‘=1 in

(4)

logbwr(x):
RXr=1 bJr:

A good model may be chosen by the above CV procedure, given that a set of promising model
candidates is prepared. As model candidates, we propose using a Gaussian kernel model centered at
the test input points fxte

j=1, i.e.,

j gnte

where K(cid:27)(x; x0) is the Gaussian kernel with kernel width (cid:27):

nteX‘=1

‘ );

(cid:11)‘K(cid:27)(x; xte

bw(x) =
K(cid:27)(x; x0) = exp(cid:26)(cid:0)kx (cid:0) x0k2

2(cid:27)2

4

(cid:27) :

(5)

j gnte

i gntr

j=1 as the Gaussian centers, not the training
The reason why we chose the test input points fxte
input points fxtr
i=1, is as follows. By deﬁnition, the importance w(x) tends to take large values
if the training input density ptr(x) is small and the test input density pte(x) is large; conversely,
w(x) tends to be small (i.e., close to zero) if ptr(x) is large and pte(x) is small. When a function
is approximated by a Gaussian kernel model, many kernels may be needed in the region where the
output of the target function is large; on the other hand, only a small number of kernels would be
enough in the region where the output of the target function is close to zero. Following this heuristic,
we decided to allocate many kernels at high test input density regions, which can be achieved by
setting the Gaussian centers at the test input points fxte
Alternatively, we may locate (ntr +nte) Gaussian kernels at both fxtr
j=1. However,
in our preliminary experiments, this did not further improve the performance, but slightly increased
j gnte
the computational cost. Since nte is typically very large, just using all the test input points fxte
j=1
as Gaussian centers is already computationally rather demanding. To ease this problem, we practi-
j=1 as Gaussian centers for computational efﬁciency, i.e.,
cally propose using a subset of fxte

i=1 and fxte

j gnte
j=1.

i gntr

j gnte

j gnte
bw(x) =

bX‘=1

(cid:11)‘K(cid:27)(x; c‘);

(6)

j gnte
where c‘ is a template point randomly chosen from fxte
In the rest of this paper, we ﬁx the number of template points at

j=1 and b ((cid:20) nte) is a preﬁxed number.

and optimize the kernel width (cid:27) by the above CV procedure.

b = min(100; nte);

3 Experiments

In this section, we compare the experimental performance of KLIEP and existing approaches.

3.1

Importance Estimation for Artiﬁcial Data Sets

Let ptr(x) be the d-dimensional Gaussian density with mean (0; 0; : : : ; 0)> and covariance identity
and pte(x) be the d-dimensional Gaussian density with mean (1; 0; : : : ; 0)> and covariance identity.
The task is to estimate the importance at training input points:

wi = w(xtr

i ) =

pte(xtr
i )
ptr(xtr
i )

for i = 1; 2; : : : ; ntr:

We compare the following methods:

KLIEP((cid:27)): fwigntr

i=1 are estimated by KLIEP with the Gaussian kernel model (6). Since the per-
formance of KLIEP is dependent on the kernel width (cid:27), we test several different values of
(cid:27).

KLIEP(CV): The kernel width (cid:27) in KLIEP is chosen based on 5-fold LCV (see Section 2.3).
KDE(CV): fwigntr

i=1 are estimated through the kernel density estimator (KDE) with the Gaussian
kernel. The kernel widths for the training and test densities are chosen separately based on
5-fold likelihood cross-validation.

KMM((cid:27)): fwigntr

i=1 are estimated by kernel mean matching (KMM) [6]. The performance of KMM
is dependent on tuning parameters such as B, (cid:15), and (cid:27). We set B = 1000 and (cid:15) = (pntr (cid:0)
1)=pntr following the paper [6], and test several different values of (cid:27). We used the CPLEX
software for solving quadratic programs in the experiments.

LogReg((cid:27)): Importance weights are estimated by logistic regression (LogReg) [2]. The Gaussian
kernels are used as basis functions. Since the performance of LogReg is dependent on the
kernel width (cid:27), we test several different values of (cid:27). We used the LIBLINEAR implemen-
tation of logistic regression for the experiments [7].

LogReg(CV): The kernel width (cid:27) in LogReg is chosen based on 5-fold CV.

5

of the importance estimates fbwigntr

NMSE =

1
ntr

ntrXi=1(cid:18)

bwiPntr
i0=1 bwi0 (cid:0)

i0=1 wi0(cid:19)2
wiPntr

:

l

)
e
a
c
S
g
o
L

 

 

l

n
i
(
 
s
a
i
r
T
0
0
1

 

 

 
r
e
v
o
E
S
M
N
e
g
a
r
e
v
A

 

10−3

10−4

10−5

10−6

2

4

6

10

8
d (Input Dimension)

12

KLIEP(0.5)
KLIEP(2)
KLIEP(7)
KLIEP(CV)
KDE(CV)
KMM(0.1)
KMM(1)
KMM(10)
LogReg(0.5)
LogReg(2)
LogReg(7)
LogReg(CV)

14

16

18

20

l

)
e
a
c
S
g
o
L

 

 

l

n
i
(
 
s
a
i
r
T
0
0
1

 

 

 
r
e
v
o
E
S
M
N
e
g
a
r
e
v
A

 

KLIEP(0.5)
KLIEP(2)
KLIEP(7)
KLIEP(CV)
KDE(CV)
KMM(0.1)
KMM(1)
KMM(10)
LogReg(0.5)
LogReg(2)
LogReg(7)
LogReg(CV)

10−3

10−4

10−5

10−6

50

100

n
 (Number of Training Samples)
tr

150

(a) When input dimension is changed

(b) When training sample size is changed

Figure 2: NMSEs averaged over 100 trials in log scale.

We ﬁxed the number of test input points at nte = 1000 and consider the following two settings for
the number ntr of training samples and the input dimension d:

(a) ntr = 100 and d = 1; 2; : : : ; 20,
(b) d = 10 and ntr = 50; 60; : : : ; 150.

We run the experiments 100 times for each d, each ntr, and each method, and evaluate the quality

i=1 by the normalized mean squared error (NMSE):

NMSEs averaged over 100 trials are plotted in log scale in Figure 2. Figure 2(a) shows that the error
of KDE(CV) sharply increases as the input dimension grows, while KLIEP, KMM, and LogReg
with appropriate kernel widths tend to give smaller errors than KDE(CV). This would be the fruit
of directly estimating the importance without going through density estimation. The graph also
show that the performance of KLIEP, KMM, and LogReg is dependent on the kernel width (cid:27)—the
results of KLIEP(CV) and LogReg(CV) show that model selection is carried out reasonably well
and KLIEP(CV) works signiﬁcantly better than LogReg(CV).

Figure 2(b) shows that the errors of all methods tend to decrease as the number of training samples
grows. Again, KLIEP, KMM, and LogReg with appropriate kernel widths tend to give smaller
errors than KDE(CV). Model selection in KLIEP(CV) and LogReg(CV) works reasonably well and
KLIEP(CV) tends to give signiﬁcantly smaller errors than LogReg(CV).

Overall, KLIEP(CV) is shown to be a useful method in importance estimation.

3.2 Covariate Shift Adaptation with Regression and Classiﬁcation Benchmark Data Sets

k=1 into [0; 1]d and choose the test samples f(xte

Here we employ importance estimation methods for covariate shift adaptation in regression and
classiﬁcation benchmark problems (see Table 1).
k=1. We normalize all the input samples
Each data set consists of input/output samples f(xk; yk)gn
j )gnte
k=1 as
j ; yte
fxkgn
follows. We randomly choose one sample (xk; yk) from the pool and accept this with probabil-
ity min(1; 4(x(c)
is the c-th element of xk and c is randomly determined and ﬁxed
in each trial of experiments; then we remove xk from the pool regardless of its rejection or ac-
ceptance, and repeat this procedure until we accept nte samples. We choose the training samples
i=1 uniformly from the rest. Intuitively, in this experiment, the test input density tends
f(xtr

j=1 from the pool f(xk; yk)gn

i )gntr

k )2), where x(c)

k

i ; ytr

6

to be lower than the training input density when x(c)
ntr = 100 and nte = 500 for all data sets. Note that we only use f(xtr
for training regressors or classiﬁers; the test output values fyte
generalization performance.

j gnte
We use the following kernel model for regression or classiﬁcation:

is small. We set the number of samples at
j gnte
j=1
j=1 are used only for evaluating the

i=1 and fxte

i )gntr

i ; ytr

k

where Kh(x; x0) is the Gaussian kernel (5) and m‘ is a template point randomly chosen from
j=1. We set the number of kernels at t = 50. We learn the parameter (cid:18) by importance-
fxte
weighted regularized least squares (IWRLS) [9]:

j gnte

tX‘=1

(cid:18)‘Kh(x; m‘);

bf (x; (cid:18)) =
"" ntrXi=1 bw(xtr
The solutionb(cid:18)IW RLS is analytically given by
b(cid:18) = (K >cW K + (cid:21)I)(cid:0)1K>cW y;

b(cid:18)IW RLS (cid:17) argmin

i )(cid:16)bf (xtr

where I is the identity matrix and

i (cid:17)2
i ; (cid:18)) (cid:0) ytr

(cid:18)

+ (cid:21)k(cid:18)k2# :

(7)

The results are summarized in Table 1, where ‘Uniform’ denotes uniform weights, i.e., no impor-
tance weight is used. The table shows that KLIEP(CV) compares favorably with Uniform, implying
that the importance weighted methods combined with KLIEP(CV) are useful for improving the pre-
diction performance under covariate shift. KLIEP(CV) works much better than KDE(CV); actually
KDE(CV) tends to be worse than Uniform, which may be due to high dimensionality. We tested
10 different values of the kernel width (cid:27) for KMM and described three representative results in the
table. KLIEP(CV) is slightly better than KMM with the best kernel width. Finally, LogReg(CV)
works reasonably well, but it sometimes performs poorly.

Overall, we conclude that the proposed KLIEP(CV) is a promising method for covariate shift adap-
tation.

4 Conclusions

In this paper, we addressed the problem of estimating the importance for covariate shift adaptation.
The proposed method, called KLIEP, does not involve density estimation so it is more advantageous
than a naive KDE-based approach particularly in high-dimensional problems. Compared with KMM

7

The kernel width h and the regularization parameter (cid:21) in IWRLS (7) are chosen by 5-fold importance
weighted CV (IWCV) [9]. We compute the IWCV score by

y = (y1; y2; : : : ; yntr )>;

Ki;‘ = Kh(xtr

i ; m‘);

1
jZ tr

cW = diag (bw1;bw2; : : : ;bwntr ) :
r bw(x)L(cid:16)bfr(x); y(cid:17) ;
r j X(x;y)2Z tr
L (by; y) =(cid:26)(by (cid:0) y)2
2 (1 (cid:0) signfbyyg)
L(cid:16)bf (xte
nteXj=1

j (cid:17) :

j ); yte

1
nte

1

(Regression),
(Classiﬁcation).

where

We run the experiments 100 times for each data set and evaluate the mean test error:

Table 1: Mean test error averaged over 100 trials. The numbers in the brackets are the standard devi-
ation. All the error values are normalized so that the mean error by ‘Uniform’ (uniform weighting,
or equivalently no importance weighting) is one. For each data set, the best method and comparable
ones based on the Wilcoxon signed rank test at the signiﬁcance level 5% are described in bold face.
The upper half are regression data sets taken from DELVE and the lower half are classiﬁcation data
sets taken from IDA. ‘KMM((cid:27))’ denotes KMM with kernel width (cid:27).

Data

Dim Uniform

KLIEP
(CV)

KDE
(CV)

KMM
(0.01)

KMM
(0.3)

KMM

(1)

LogReg

(CV)

kin-8fh
1:00(0:34) 0:95(0:31) 1:22(0:52) 1:00(0:34) 1:12(0:37) 1:59(0:53) 1:30(0:40)
8
kin-8fm 8
1:00(0:39) 0:86(0:35) 1:12(0:57) 1:00(0:39) 0:98(0:46) 1:95(1:24) 1:29(0:58)
kin-8nh
8 1:00(0:26) 0:99(0:22) 1:09(0:20) 1:00(0:27) 1:04(0:17) 1:16(0:25) 1:06(0:17)
kin-8nm 8 1:00(0:30) 0:97(0:25) 1:14(0:26) 1:00(0:30) 1:09(0:23) 1:20(0:22) 1:13(0:25)
abalone
7 1:00(0:50) 0:94(0:67) 1:02(0:41) 1:01(0:51) 0:96(0:70) 0:93(0:39) 0:92(0:41)
image
18 1:00(0:51) 0:94(0:44) 0:98(0:45) 0:97(0:50) 0:97(0:45) 1:09(0:54) 0:99(0:48)
ringnorm 20
1:00(0:04) 0:99(0:06) 0:87(0:04) 1:00(0:04) 0:87(0:05) 0:87(0:05) 0:95(0:08)
twonorm 20
1:00(0:58) 0:91(0:52) 1:16(0:71) 0:99(0:50) 0:86(0:55) 0:99(0:70) 0:94(0:59)
waveform 21 1:00(0:45) 0:93(0:34) 1:05(0:47) 1:00(0:44) 0:93(0:32) 0:98(0:31) 0:95(0:34)
Average
1.06(0.37)

0.98(0.37)

0.94(0.35)

1.20(0.47)

1.00(0.38)

1.07(0.40)

1.00(0.36)

which also directly gives importance estimates, KLIEP is practically more useful since it is equipped
with a model selection procedure. Our experiments highlighted these advantages and therefore
KLIEP is shown to be a promising method for covariate shift adaptation.

In KLIEP, we modeled the importance function by a linear (or kernel) model, which resulted in a
convex optimization problem with a sparse solution. However, our framework allows the use of any
models. An interesting future direction to pursue would be to search for a class of models which has
additional advantages.

Finally, the range of application of importance weights is not limited to covariate shift adaptation.
For example, the density ratio could be used for novelty detection. Exploring possible application
areas will be important future directions.

Acknowledgments

This work was supported by MEXT (17700142 and 18300057), the Okawa Foundation, the Mi-
crosoft CORE3 Project, and the IBM Faculty Award.

References
[1] P. Baldi and S. Brunak. Bioinformatics: The Machine Learning Approach. MIT Press, Cambridge, 1998.
[2] S. Bickel, M. Br¨uckner, and T. Scheffer. Discriminative learning for differing training and test distribu-

tions. In Proceedings of the 24th International Conference on Machine Learning, 2007.

[3] S. Bickel and T. Scheffer. Dirichlet-enhanced spam ﬁltering based on biased samples. In B. Sch¨olkopf,
J. Platt, and T. Hoffman, editors, Advances in Neural Information Processing Systems 19. MIT Press,
Cambridge, MA, 2007.

[4] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, Cambridge, 2004.
[5] J. J. Heckman. Sample selection bias as a speciﬁcation error. Econometrica, 47(1):153–162, 1979.
[6] J. Huang, A. Smola, A. Gretton, K. M. Borgwardt, and B. Sch¨olkopf. Correcting sample selection bias
by unlabeled data. In B. Sch¨olkopf, J. Platt, and T. Hoffman, editors, Advances in Neural Information
Processing Systems 19, pages 601–608. MIT Press, Cambridge, MA, 2007.

[7] C.-J. Lin, R. C. Weng, and S. S. Keerthi. Trust region Newton method for large-scale logistic regression.

Technical report, Department of Computer Science, National Taiwan University, 2007.

[8] H. Shimodaira.

Improving predictive inference under covariate shift by weighting the log-likelihood

function. Journal of Statistical Planning and Inference, 90(2):227–244, 2000.

[9] M. Sugiyama, M. Krauledat, and K.-R. M¨uller. Covariate shift adaptation by importance weighted cross

validation. Journal of Machine Learning Research, 8:985–1005, May 2007.

[10] R. S. Sutton and G. A. Barto. Reinforcement Learning: An Introduction. MIT Press, Cambridge, MA,

1998.

8

"
50,2007,Support Vector Machine Classification with Indefinite Kernels,"In this paper, we propose a method for support vector machine classification using indefinite kernels. Instead of directly minimizing or stabilizing a nonconvex loss function, our method simultaneously finds the support vectors and a proxy kernel matrix used in computing the loss. This can be interpreted as a robust classification problem where the indefinite kernel matrix is treated as a noisy observation of the true positive semidefinite kernel. Our formulation keeps the problem convex and relatively large problems can be solved efficiently using the analytic center cutting plane method. We compare the performance of our technique with other methods on several data sets.","Support Vector Machine Classiﬁcation

with Indeﬁnite Kernels

Ronny Luss

ORFE, Princeton University

Princeton, NJ 08544

Alexandre d’Aspremont
ORFE, Princeton University

Princeton, NJ 08544

rluss@princeton.edu

aspremon@princeton.edu

Abstract

In this paper, we propose a method for support vector machine classiﬁcation using
indeﬁnite kernels. Instead of directly minimizing or stabilizing a nonconvex loss
function, our method simultaneously ﬁnds the support vectors and a proxy kernel
matrix used in computing the loss. This can be interpreted as a robust classiﬁcation
problem where the indeﬁnite kernel matrix is treated as a noisy observation of the
true positive semideﬁnite kernel. Our formulation keeps the problem convex and
relatively large problems can be solved efﬁciently using the analytic center cutting
plane method. We compare the performance of our technique with other methods
on several data sets.

1 Introduction

Here, we present an algorithm for support vector machine (SVM) classiﬁcation using indeﬁnite ker-
nels. Our interest in indeﬁnite kernels is motivated by several observations. First, certain similarity
measures take advantage of application-speciﬁc structure in the data and often display excellent
empirical classiﬁcation performance. Unlike popular kernels used in support vector machine clas-
siﬁcation, these similarity matrices are often indeﬁnite and so do not necessarily correspond to a
reproducing kernel Hilbert space (see [1] for a discussion).

An application of classiﬁcation with indeﬁnite kernels to image classiﬁcation using Earth Mover’s
Distance was discussed in [2]. Similarity measures for protein sequences such as the Smith-
Waterman and BLAST scores are indeﬁnite yet have provided hints for constructing useful positive
semideﬁnite kernels such as those decribed in [3] or have been transformed into positive semideﬁnite
kernels (see [4] for example). Here instead, our objective is to directly use these indeﬁnite similarity
measures for classiﬁcation.

Our work also closely follows recent results on kernel learning (see [5] or [6]), where the kernel
matrix is learned as a linear combination of given kernels, and the resulting kernel is explicitly
constrained to be positive semideﬁnite (the authors of [7] have adapted the SMO algorithm to solve
the case where the kernel is written as a positively weighted combination of other kernels). In our
case however, we never explicitly optimize the kernel matrix because this part of the problem can be
solved explicitly, which means that the complexity of our method is substantially lower than that of
classical kernel learning methods and closer in spirit to the algorithm used in [8], who formulate the
multiple kernel learning problem of [7] as a semi-inﬁnite linear program and solve it with a column
generation technique similar to the analytic center cutting plane method we use here.

Finally, it is sometimes impossible to prove that some kernels satisfy Mercer’s condition or the
numerical complexity of evaluating the exact positive semideﬁnite kernel is too high and a proxy
(and not necessarily positive semideﬁnite) kernel has to be used instead (see [9] for example). In
both cases, our method allows us to bypass these limitations.

1

1.1 Current results

Several methods have been proposed for dealing with indeﬁnite kernels in SVMs. A ﬁrst direction
embeds data in a pseudo-Euclidean (pE) space: [10] for example, formulates the classiﬁcation prob-
lem with an indeﬁnite kernel as that of minimizing the distance between convex hulls formed from
the two categories of data embedded in the pE space. The nonseparable case is handled in the same
manner using reduced convex hulls (see [11] for a discussion of SVM geometric interpretations).

Another direction applies direct spectral transformations to indeﬁnite kernels: ﬂipping the nega-
tive eigenvalues or shifting the kernel’s eigenvalues and reconstructing the kernel with the original
eigenvectors in order to produce a positive semideﬁnite kernel (see [12] and [2]).

Yet another option is to reformulate either the maximum margin problem or its dual in order to
use the indeﬁnite kernel in a convex optimization problem (see [13]). An equivalent formulation
of SVM with the same objective but where the kernel appears in the constraints can be modiﬁed
to a convex problem by eliminating the kernel from the objective. Directly solving the nonconvex
problem sometimes gives good results as well (see [14] and [10]).

1.2 Contribution

Here, instead of directly transforming the indeﬁnite kernel, we simultaneously learn the support vec-
tor weights and a proxy positive semideﬁnite kernel matrix, while penalizing the distance between
this proxy kernel and the original, indeﬁnite one. Our main result is that the kernel learning part of
that problem can be solved explicitly, meaning that the classiﬁcation problem with indeﬁnite kernels
can simply be formulated as a perturbation of the positive semideﬁnite case.

Our formulation can also be interpreted as a worst-case robust classiﬁcation problem with uncer-
tainty on the kernel matrix. In that sense, indeﬁnite similarity matrices are seen as noisy observa-
tions of an unknown positive semideﬁnite kernel. From a complexity standpoint, while the original
SVM classiﬁcation problem with indeﬁnite kernel is nonconvex, the robustiﬁcation we detail here is
a convex problem, and hence can be solved efﬁciently with guaranteed complexity bounds.

The paper is organized as follows. In Section 2 we formulate our main classiﬁcation problem and
detail its interpretation as a robust SVM. In Section 3 we describe an algorithm for solving this
problem. Finally, in Section 4, we test the numerical performance of these methods on various
applications.

2 SVM with indeﬁnite kernels

Here, we introduce our robustiﬁcation of the SVM classiﬁcation problem with indeﬁnite kernels.

2.1 Robust classiﬁcation

Let K ∈ Sn be a given kernel matrix and y ∈ Rn be the vector of labels, with Y = diag(y) the
matrix with diagonal y, where Sn is the set of symmetric matrices of size n and Rn is the set of
n-vectors of real numbers. We can write the dual of the SVM classiﬁcation problem with hinge loss
and quadratic penalty as:

maximize αT e − Tr(K(Y α)(Y α)T )/2
subject to αT y = 0

0 ≤ α ≤ C

(1)

in the variable α ∈ Rn and where e is an n-vector of ones. When K is positive semideﬁnite, this
problem is a convex quadratic program. Suppose now that we are given an indeﬁnite kernel matrix
K0 ∈ Sn. We formulate a robust version of problem (1) by restricting K to be a positive semideﬁnite
kernel matrix in some given neighborhood of the original (indeﬁnite) kernel matrix K0:

max

{αT y=0, 0≤α≤C}

min

{K(cid:23)0, kK−K0k2

F ≤β}

αT e −

1
2

Tr(K(Y α)(Y α)T )

(2)

in the variables K ∈ Sn and α ∈ Rn, where the parameter β > 0 controls the distance between
the original matrix K0 and the proxy kernel K. This can be interpreted as a worst-case robust

2

classiﬁcation problem with bounded uncertainty on the kernel matrix K. The above problem is
infeasible for some values of β so we replace here the hard constraint on K by a penalty on the
distance between the proxy positive semideﬁnite kernel and the given indeﬁnite matrix. The problem
we solve is now:

max

{αT y=0,0≤α≤C}

min
{K(cid:23)0}

αT e −

1
2

Tr(K(Y α)(Y α)T ) + ρkK − K0k2
F

(3)

in the variables K ∈ Sn and α ∈ Rn, where the parameter ρ > 0 controls the magnitude of the
penalty on the distance between K and K0. The inner minimization problem is a convex conic
program on K. Also, as the pointwise minimum of a family of concave quadratic functions of α, the
solution to the inner problem is a concave function of α, and hence the outer optimization problem
is also convex (see [15] for further details). Thus, (3) is a concave maximization problem subject to
linear constraints and is therefore a convex problem in α.
Our key result here is that the inner kernel learning optimization problem can be solved in closed
form. For a ﬁxed α, the inner minimization problem is equivalent to the following problem:

minimize
subject to K (cid:23) 0

kK − (K0 + 1

4ρ (Y α)(Y α)T )k2

F

in the variable K ∈ Sn. This is the projection of the K0 + (1/4ρ)(Y α)(Y α)T on the cone of
positive semideﬁnite matrices. The optimal solution to this problem is then given by:

K ∗ = (K0 + (1/4ρ)(Y α)(Y α)T )+

(4)
i where λi and xi are

where X+ is the positive part of the matrix X, i.e. X+ = Pi max(0, λi)xixT

the ith eigenvalue and eigenvector of the matrix X. Plugging this solution into (3), we get:

max

{αT y=0, 0≤α≤C}

αT e −

1
2

Tr(K ∗(Y α)(Y α)T ) + ρkK ∗ − K0k2
F

in the variable α ∈ Rn, where (Y α)(Y α)T is the rank one matrix with coefﬁcients yiαiαj yj,
i, j = 1, . . . , n. We can rewrite this as an eigenvalue optimization problem by using the eigenvalue
representation of X+. Letting the eigenvalue decomposition of K0+(1/4ρ)(Y α)(Y α)T be V DV T ,
we get K ∗ = V D+V T and, with vi the ith column of V , we can write:

Tr(K ∗(Y α)(Y α)T ) = (Y α)T V D+V T (Y α)

=

n

Xi=1

max(cid:18)0, λi(cid:18)K0 +

1
4ρ

(Y α)(Y α)T(cid:19)(cid:19) (αT Y vi)2

where λi (X) is the ith eigenvalue of the quantity X. Using the same technique, we can also rewrite
the term kK ∗ − K0|2
F using this eigenvalue decomposition. Our original optimization problem (3)
ﬁnally becomes:

maximize αT e − 1

2 Pi max(0, λi(K0 + (Y α)(Y α)T /4ρ))(αT Y vi)2

+ρPi (max(0, λi(K0 + (Y α)(Y α)T /4ρ)))2
−2ρPi Tr((vivT

i )K0)max(0, λi(K0 + (Y α)(Y α)T /4ρ)) + ρ Tr(K0K0)

(5)

subject to αT y = 0, 0 ≤ α ≤ C

in the variable α ∈ Rn.

2.2 Dual problem

Because problem (3) is convex with at least one compact feasible set, we can formulate the dual
problem to (5) by simply switching the max and the min. The inner maximization is a quadratic
program in α, and hence has a quadratic program as its dual. We then get the dual by plugging this
inner dual quadratic program into the outer minimization, to get the following problem:

minimize Tr(K −1(Y (e − λ + µ + yν))(Y (e − λ + µ + yν))T )/2 + CµT e + ρkK − K0k2
F
subject to K (cid:23) 0, λ, µ ≥ 0

(6)

3

in the variables K ∈ Sn, λ, µ ∈ Rn and ν ∈ R. This dual problem is a quadratic program in the
variables λ and µ which correspond to the primal constraints 0 ≤ α ≤ C and ν which is the dual
variable for the constraint αT y = 0. As we have seen earlier, any feasible solution to the primal
problem produces a corresponding kernel in (4), and plugging this kernel into the dual problem in
(6) allows us to calculate a dual feasible point by solving a quadratic program which gives a dual
objective value, i.e. an upper bound on the optimum of (5). This bound can then be used to compute
a duality gap and track convergence.

2.3 Interpretation

We noted that our problem can be viewed as a worst-case robust classiﬁcation problem with uncer-
tainty on the kernel matrix. Our explicit solution of the optimal worst-case kernel given in (4) is the
projection of a penalized rank-one update to the indeﬁnite kernel on the cone of positive semideﬁnite
matrices. As ρ tends to inﬁnity, the rank-one update has less effect and in the limit, the optimal ker-
nel is the kernel given by zeroing out the negative eigenvalues of the indeﬁnite kernel. This means
that if the indeﬁnite kernel contains a very small amount of noise, the best positive semideﬁnite
kernel to use with SVM in our framework is the positive part of the indeﬁnite kernel.

This limit as ρ tends to inﬁnity also motivates a heuristic for the transformation of the kernel on
the testing set. Since the negative eigenvalues of the training kernel are thresholded to zero in the
limit, the same transformation should occur for the test kernel. Hence, we update the entries of the
full kernel corresponding to training instances by the rank-one update resulting from the optimal
solution to (3) and threshold the negative eigenvalues of the full kernel matrix to zero. We then use
the test kernel values from the resulting positive semideﬁnite matrix.

3 Algorithms

We now detail two algorithms that can be used to solve Problem (5). The optimization problem is
the maximization of a nondifferentiable concave function subject to convex constraints. An optimal
point always exists since the feasibility set is bounded and nonempty. For numerical stability, in both
algorithms, we quadratically smooth our objective to calculate a gradient instead. We ﬁrst describe
a simple projected gradient method which has numerically cheap iterations but has no convergence
bound. We then show how to apply the much more efﬁcient analytic center cutting plane method
whose iterations are slightly more complex but which converges linearly.

Smoothing Our objective contains terms of the form max{0, f (x)} for some function f (x), which
are not differentiable (described in the section below). These functions are easily smoothed out by
a regularization technique (see [16] for example). We replace them by a continuously differentiable
2 -approximation as follows:
ǫ

ϕǫ(f (x)) = max
0≤u≤1

(uf (x) −

ǫ
2

u2).

and the gradient is given by ∇ϕǫ(f (x)) = u∗(x)∇f (x) where u∗(x) = argmax ϕǫ(f (x)).

Gradient Calculating the gradient of our objective requires a full eigenvalue decomposition to
compute the gradient of each eigenvalue. Given a matrix X(α), the derivative of the ith eigenvalue
with respect to α is given by:

∂λi(X(α))

∂α

= vT
i

∂X(α)

∂α

vi

(7)

where vi is the ith eigenvector of X(α). We can then combine this expression with the smooth
approximation above to get the gradient.

We note that eigenvalues of symmetric matrices are not differentiable when some of them have mul-
tiplicities greater than one (see [17] for a discussion). In practice however, most tested kernels were
of full rank with distinct eigenvalues so we ignore this issue here. One may also consider projected
subgradient methods, which are much slower, or use subgradients for analytic center cutting plane
methods (which does not affect complexity).

4

3.1 Projected gradient method

The projected gradient method takes a steepest descent, then projects the new point back onto the
feasible region (see [18] for example). In order to use these methods the objective function must be
differentiable and the method is only efﬁcient if the projection step is numerically cheap. We choose
an initial point α0 ∈ Rn and the algorithm proceeds as follows:

Projected gradient method

1. Compute αi+1 = αi + t∇f (αi).
2. Set αi+1 = pA(αi+1).
3. If gap ≤ ǫ stop, otherwise go back to step 1.

The complexity of each iteration breaks down as follows.

Step 1. This requires an eigenvalue decomposition and costs O(n3). We note that a line search would
be costly because it would require multiple eigenvalue decompositions to recalculate the objective
multiple times.

Step 2. This is a projection onto the region A = {αT y = 0, 0 ≤ α ≤ C} and can be solved
explicitly by sorting the vector of entries, with cost O(n log n).
Stopping Criterion. We can compute a duality gap using the results of §2.2: let Ki = (K0 +
(Y αi)(Y αi)T /4ρ)+ (the kernel at iteration i), then solving problem (1) which is just an SVM with
a convex kernel Ki produces an upper bound on (5), and hence a bound on the suboptimality of the
current solution.

Complexity. The number of iterations required by this method to reach a target precision of ǫ is
typically in O(1/ǫ2).

3.2 Analytic center cutting plane method

The analytic center cutting plane method (ACCPM) reduces the feasible region on each iteration
using a new cut of the feasible region computed by evaluating a subgradient of the objective function
at the analytic center of the current set, until the volume of the reduced region converges to the target
precision. This method does not require differentiability. We set A0 = {αT y = 0, 0 ≤ α ≤ C}
which we can write as {A0 ≤ b0} to be our ﬁrst localization set for the optimal solution. The
method then works as follows (see [18] for a more complete reference on cutting plane methods):

Analytic center cutting plane method

1. Compute αi as the analytic center of Ai by solving:

αi+1 = argmin
y∈Rn

−

m

Xi=1

log(bi − aT

i y)

where aT

i represents the ith row of coefﬁcients from the left-hand side of {A0 ≤ b0}.

2. Compute ∇f (x) at the center αi+1 and update the (polyhedral) localization set:

Ai+1 = Ai ∪ {∇f (αi+1)(α − αi+1) ≥ 0}

3. If gap ≤ ǫ stop, otherwise go back to step 1.

The complexity of each iteration breaks down as follows.

Step 1. This step computes the analytic center of a polyhedron and can be solved in O(n3) operations
using interior point methods for example.

5

Step 2. This simply updates the polyhedral description.

Stopping Criterion. An upper bound is computed by maximizing a ﬁrst order Taylor approximation
of f (α) at αi over all points in an ellipsoid that covers Ai, which can be done explicitly.
Complexity. ACCPM is provably convergent in O(n log(1/ǫ)2) iterations when using cut elimina-
tion which keeps the complexity of the localization set bounded. Other schemes are available with
slightly different complexities: an O(n2/ǫ2) is achieved in [19] using (cheaper) approximate centers
for example.

4 Experiments

In this section we compare the generalization performance of our technique to other methods of
applying SVM classiﬁcation given an indeﬁnite similarity measure. We also test SVM classiﬁcation
performance on positive semideﬁnite kernels using the LIBSVM library. We ﬁnish with experiments
showing convergence of our algorithms. Our algorithms were implemented in Matlab.

4.1 Generalization

We compare our method for SVM classiﬁcation with indeﬁnite kernels to several of the kernel pre-
processing techniques discussed earlier. The ﬁrst three techniques perform spectral transformations
on the indeﬁnite kernel. The ﬁrst, denoted denoise, thresholds the negative eigenvalues to zero. The
second transformation, called ﬂip, takes the absolute value of all eigenvalues. The last transforma-
tion, shift, adds a constant to each eigenvalue making them all positive. See [12] for further details.
We ﬁnally also compare with using SVM on the original indeﬁnite kernel (SVM converges but the
solution is only a stationary point and is not guaranteed to be optimal).

We experiment on data from the USPS handwritten digits database (described in [20]) using the
indeﬁnite Simpson score (SS) to compare two digits and on two data sets from the UCI repository
(see [21]) using the indeﬁnite Epanechnikov (EP) kernel. The data is randomly divided into training
and testing data. We apply 5-fold cross validation and use an accuracy measure (described below)
to determine the optimal parameters C and ρ. We then train a model with the full training set and
optimal parameters and test on the independent test set.

Table 1: Statistics for various data sets.

Data Set

# Train

USPS-3-5-SS
USPS-4-6-SS
Diabetes-EP

Liver-EP

767
829
614
276

# Test
773
857
154
69

λmin

-34.76
-37.30
-0.27

-1.38e-15

λmax

453.58
413.17
18.17
3.74

Table 1 provides statistics including the minimum and maximum eigenvalues of the training kernels.
The main observation is that the USPS data uses highly indeﬁnite kernels while the UCI data use
kernels that are nearly positive semideﬁnite. Table 2 displays performance by comparing accuracy
and recall. Accuracy is deﬁned as the percentage of total instances predicted correctly. Recall is the
percentage of true positives that were correctly predicted positive.

Our method is referred to as Indeﬁnite SVM. We see that our method performs favorably among
the USPS data. Both measures of performance are quite high for all methods. Our method does
not perform as well on the UCI data sets but is still favorable on one of the measures in each
experiment. Notice though that recall is not good in the liver data set overall which could be the
result of overﬁtting one of the classiﬁcation categories. The liver data set uses a kernel that is almost
positive semideﬁnite - this is an example where the input is almost a true kernel and Indeﬁnite
SVM ﬁnds one slightly better. We postulate that our method will perform better in situations where
the similarity measure is highly indeﬁnite as in the USPS dataset, while measures that are almost
positive semideﬁnite maybe be seen as having a small amount of noise.

6

Table 2: Performance Measures for various data sets.

Data Set

Measure
Accuracy

USPS-3-5-SS

Recall
USPS-4-6-SS Accuracy
Recall

Diabetes-EP

Liver-EP

Accuracy

Recall

Accuracy

Recall

4.2 Algorithm Convergence

Denoise
95.47
94.50
97.78
98.42
75.32
90.00
63.77
22.58

Flip
95.73
95.45
97.90
98.65
74.68
90.00
63.77
22.58

Shift
90.43
92.11
94.28
93.68
68.83
92.00
57.97
25.81

SVM Indeﬁnite SVM
74.90
72.73
90.08
88.49
75.32
90.00
63.77
22.58

96.25
96.65
97.90
98.87
68.83
95.00
65.22
22.58

We ran our two algorithms on data sets created by randomly perturbing the four USPS data sets used
above. The average results with one standard deviation above and below the mean are displayed in
Figure 1 with the duality gap in log scale (note that the codes were not stopped here and that the
target gap improvement is usually much smaller than 10−8). As expected, ACCPM converges much
faster (in fact linearly) to a higher precision while each iteration requires solving a linear program
of size n. The gradient projection method converges faster in the beginning but stalls at a higher
precision, however each iteration only requires sorting the current point.

p
a
G
y
t
i
l
a
u
D

104

103

102

101

100

10−1

10−2

10−3

10−4

0

101

100

10−1

10−2

p
a
G
y
t
i
l
a
u
D

50

100

150

200

10−3

0

200

Iteration

400
600
Iteration

800

1000

Figure 1: Convergence plots for ACCPM (left) & projected gradient method (right) on randomly perturbed
USPS data sets (average gap versus iteration number, dashed lines at plus and minus one standard deviation).

5 Conclusion

We have proposed a technique for incorporating indeﬁnite kernels into the SVM framework with-
out any explicit transformations. We have shown that if we view the indeﬁnite kernel as a noisy
instance of a true kernel, we can learn an explicit solution for the optimal kernel with a tractable
convex optimization problem. We give two convergent algorithms for solving this problem on rel-
atively large data sets. Our initial experiments show that our method can at least fare comparably
with other methods handling indeﬁnite kernels in the SVM framework but provides a much clearer
interpretation for these heuristics.

7

References

[1] C. S. Ong, X. Mary, S. Canu, and A. J. Smola. Learning with non-positive kernels. Proceedings of the

21st International Conference on Machine Learning, 2004.

[2] A. Zamolotskikh and P. Cunningham. An assessment of alternative strategies for constructing emd-based

kernel functions for use in an svm for image classiﬁcation. Technical Report UCD-CSI-2007-3, 2004.

[3] H. Saigo, J. P. Vert, N. Ueda, and T. Akutsu. Protein homology detection using string alignment kernels.

Bioinformatics, 20(11):1682–1689, 2004.

[4] G. R. G. Lanckriet, N. Cristianini, M. I. Jordan, and W. S. Noble. Kernel-based integration of genomic

data using semideﬁnite programming. 2003. citeseer.ist.psu.edu/648978.html.

[5] G. R. G. Lanckriet, N. Cristianini, P. Bartlett, L. El Ghaoui, and M. I. Jordan. Learning the kernel matrix

with semideﬁnite programming. Journal of Machine Learning Research, 5:27–72, 2004.

[6] C. S. Ong, A. J. Smola, and R. C. Williamson. Learning the kernel with hyperkernels. Journal of Machine

Learning Research, 6:1043–1071, 2005.

[7] F. R. Bach, G. R. G. Lanckriet, and M. I. Jordan. Multiple kernel learning, conic duality, and the smo

algorithm. Proceedings of the 21st International Conference on Machine Learning, 2004.

[8] S. Sonnenberg, G. R¨atsch, C. Sch¨afer, and B. Sch¨olkopf. Large scale multiple kernel learning. Journal of

Machine Learning Research, 7:1531–1565, 2006.

[9] Marco Cuturi. Permanents, transport polytopes and positive deﬁnite kernels on histograms. Proceedings

of the Twentieth International Joint Conference on Artiﬁcial Intelligence, 2007.

[10] B. Haasdonk. Feature space interpretation of svms with indeﬁnite kernels. IEEE Transactions on Pattern

Analysis and Machine Intelligence, 27(4), 2005.

[11] K. P. Bennet and E. J. Bredensteiner. Duality and geometry in svm classiﬁers. Proceedings of the 17th

International conference on Machine Learning, pages 57–64, 2000.

[12] G. Wu, E. Y. Chang, and Z. Zhang. An analysis of transformation on non-positive semideﬁnite similarity
matrix for kernel machines. Proceedings of the 22nd International Conference on Machine Learning,
2005.

[13] H.-T. Lin and C.-J. Lin. A study on sigmoid kernel for svm and the training of non-psd kernels by

smo-type methods. 2003.

[14] A. Wo´znica, A. Kalousis, and M. Hilario. Distances and (indeﬁnite) kernels for set of objects. Proceedings

of the 6th International Conference on Data Mining, pages 1151–1156, 2006.

[15] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004.
[16] C. Gigola and S. Gomez. A regularization method for solving the ﬁnite convex min-max problem. SIAM

Journal on Numerical Analysis, 27(6):1621–1634, 1990.

[17] M. Overton. Large-scale optimization of eigenvalues. SIAM Journal on Optimization, 2(1):88–120, 1992.
[18] D. Bertsekas. Nonlinear Programming, 2nd Edition. Athena Scientiﬁc, 1999.
[19] J.-L. Gofﬁn and J.-P. Vial. Convex nondifferentiable optimization: A survey focused on the analytic center

cutting plane method. Optimization Methods and Software, 17(5):805–867, 2002.

[20] J. J. Hull. A database for handwritten text recognition research. IEEE Transactions on Pattern Analysis

and Machine Intelligence, 16(5), 1994.
and D.J. Newman.

[21] A. Asuncion

of California,

sity
http://www.ics.uci.edu/∼mlearn/MLRepository.html.

School

Irvine,

UCI Machine Learning Repository.
of

and Computer

Information

Sciences,

Univer-
2007.

8

"
892,2007,Automatic Generation of Social Tags for Music Recommendation,"Social tags are user-generated keywords associated with some resource on the Web. In the case of music, social tags have become an important component of Web2.0"" recommender systems, allowing users to generate playlists based on use-dependent terms such as ""chill"" or ""jogging"" that have been applied to particular songs. In this paper, we propose a method for predicting these social tags directly from MP3 files. Using a set of boosted classifiers, we map audio features onto social tags collected from the Web. The resulting automatic tags (or ""autotags"") furnish information about music that is otherwise untagged or poorly tagged, allowing for insertion of previously unheard music into a social recommender. This avoids the ''cold-start problem'' common in such systems. Autotags can also be used to smooth the tag space from which similarities and recommendations are made by providing a set of comparable baseline tags for all tracks in a recommender system.""","Automatic Generation of Social Tags for Music

Recommendation

Douglas Eck∗

Sun Labs, Sun Microsystems

Burlington, Mass, USA

douglas.eck@umontreal.ca

Paul Lamere

Sun Labs, Sun Microsystems

Burlington, Mass, USA

paul.lamere@sun.com

Thierry Bertin-Mahieux
Sun Labs, Sun Microsystems

Burlington, Mass, USA

bertinmt@iro.umontreal.ca

Stephen Green

Sun Labs, Sun Microsystems

Burlington, Mass, USA

stephen.green@sun.com

Abstract

Social tags are user-generated keywords associated with some resource on the
Web. In the case of music, social tags have become an important component of
“Web2.0” recommender systems, allowing users to generate playlists based on
use-dependent terms such as chill or jogging that have been applied to particular
songs. In this paper, we propose a method for predicting these social tags directly
from MP3 ﬁles. Using a set of boosted classiﬁers, we map audio features onto
social tags collected from the Web. The resulting automatic tags (or autotags)
furnish information about music that is otherwise untagged or poorly tagged, al-
lowing for insertion of previously unheard music into a social recommender. This
avoids the ”cold-start problem” common in such systems. Autotags can also be
used to smooth the tag space from which similarities and recommendations are
made by providing a set of comparable baseline tags for all tracks in a recom-
mender system.

1 Introduction

Social tags are a key part of “Web 2.0” technologies and have become an important source of in-
formation for recommendation. In the domain of music, Web sites such as Last.fm use social tags
as a basis for recommending music to listeners. In this paper we propose a method for predicting
social tags using audio feature extraction and supervised learning. These automatically-generated
tags (or “autotags”) can furnish information about music for which good, descriptive social tags
are lacking. Using traditional information retrieval techniques a music recommender can use these
autotags (combined with any available listener-applied tags) to predict artist or song similarity. The
tags can also serve to smooth the tag space from which similarities and recommendations are made
by providing a set of comparable baseline tags for all artists or songs in a recommender.
This is not the ﬁrst attempt to predict something about textual data using music audio as input.
Whitman & Rifkin [10], for example, provide an audio-driven model for predicting words found
near artists in web queries . One main contribution of the work in this paper lies in the scale of our
experiments. As is described in Section 4 we work with a social tag database of millions of tags
applied to ∼ 100, 000 artists and an audio database of ∼ 90, 000 songs spanning many of the more
popular of these artists. This compares favorably with previous attempts which by and large treat
only very small datasets (e.g. [10] used 255 songs drawn from 51 artists.)

∗Eck and Bertin-Mahieux currently at Dept. of Computer Science, Univ. of Montreal, Montreal , Canada

1

This paper is organized as follows: in Section 2 we describe social tags in more depth, including
a description of how social tags can be used to avoid problems found in traditional collaborative
ﬁltering systems, as well as a description of the tag set we built for these experiments. In Section 3
we present an algorithm for autotagging songs based on labeled data collected from the Internet.
In Section 4 we present experimental results and also discuss the ability to use model results for
visualization. Finally, in Section 5 we describe our conclusions and future work.

2 Using social tags for recommendation

As the amount of online music grows, automatic music recommendation becomes an increasingly
important tool for music listeners to ﬁnd music that they will like. Automatic music recommenders
commonly use collaborative ﬁltering (CF) techniques to recommend music based on the listening
behaviors of other music listeners. These CF recommenders (CFRs) harness the “wisdom of the
crowds” to recommend music. Even though CFRs generate good recommendations there are still
some problems with this approach. A signiﬁcant issue for CFRs recommenders is the cold-start
problem. A recommender needs a signiﬁcant amount of data before it can generate good recom-
mendations. For new music, music by an unknown artist with few listeners, a CFR cannot generate
good recommendations. Another issue is the lack of transparency in recommendations [7]. A CFR
cannot tell a listener why an artist was recommended beyond the description: “people who listen to
X also listen to Y”. Also, a CFR is relatively insensitive to multimodal uses of the same album or
song. For example songs from an album (a single purchase in a standard CFR system) may be used
in the context of dining, jogging and working. In each context, the reason the song was selected
changes.
An alternative style of recommendation that addresses many of the shortcomings of a CFR is to
recommend music based upon the similarity of “social tags” that have been applied to the music.
Social tags are free text labels that music listeners apply to songs, albums or artists. Typically, users
are motivated to tag as a way to organize their own personal music collection. The real strength
of a tagging system is seen when the tags of many users are aggregated. When the tags created by
thousands of different listeners are combined, a rich and complex view of the song or artist emerges.
Table 1 show the top 21 tags and frequencies of tags applied to the band “The Shins”. Users have
applied tags associated with the genre (Indie, Pop, etc.), with the mood (mellow, chill), opinion
(favorite, love), style (singer-songwriter) and context (Garden State). From these tags and their
frequencies we learn much more about “The Shins” than we would from a traditional single genre
assignment of “Indie Rock”.
In this paper, we investigate the automatic generation of tags with properties similar to those gener-
ated by social taggers. Speciﬁcally, we introduce a machine learning algorithm that takes as input
acoustic features and predicts social tags mined from the web (in our case, Last.fm). The model
can then be used to tag new or otherwise untagged music, thus providing a partial solution to the
cold-start problem.
For this research, we extracted tags and tag frequencies for nearly 100,000 artists from the social
music website Last.fm using the Audioscrobbler web service [1]. The majority of tags describe
audio content. Genre, mood and instrumentation account for 77% of the tags. See “extra material”
for a breakdown of tag types.
Overcoming the cold-start problem is the primary motivation for this area of research. For new music
or sparsely tagged music, we predict social tags directly from the audio and apply these automati-
cally generated tags (called autotags) in lieu of traditionally applied social tags. By automatically
tagging new music in this fashion, we can reduce or eliminate much of the cold-start problem.

3 An autotagging algorithm

We now describe a machine learning model which uses the meta-learning algorithm AdaBoost [5]
to predict tags from acoustic features. This model is an extension of a previous model [3] which won
the Genre Prediction Contest and was the 2nd place performer in the Artist Identiﬁcation Contest at
MIREX 2005 (ISMIR conference, London, 2005). The model has two principal advantages. First
it selects features based on a feature’s ability to minimize empirical error. We can therefore use the

2

Tag Freq
The Shins
2375
Indie
Favorites
1138
Indie rock
Emo
841
Indie pop
Mellow
Alternative
653
512
Rock
Folk
298 Alternative rock
Seen Live
Pop
231
Acoustic

Tag Freq
190
138
113
85
85
83
54

Tag Freq
49
Punk
45
Chill
41
Singer-songwriter
Garden State
39
37
Favorite
36
Electronic
Love
35

Table 1: Top 21 tags applied to The Shins

Figure 1: Overview of our model

model to eliminate useless feature sets by looking at the order in which those features are selected.
We used this property of the model to discard many candidate features such as chromagrams (which
map spectral energy onto the 12 notes of the Western musical scale) because the weak learners
associated with those features were selected very late by AdaBoost. Second, though AdaBoost may
need relatively more weak learners to achieve the same performance on a large dataset than a small
one, the computation time for a single weak learner scales linearly with the number of training
examples. Thus AdaBoost has the potential to scale well to very large datasets. Both of these
properties are general to AdaBoost and are not explored further in this short paper. See [5, 9] for
more.

3.1 Acoustic feature extraction

The features we use include 20 Mel-Frequency Cepstral Coefﬁcients, 176 autocorrelation coefﬁ-
cients computed for lags spanning from 250msec to 2000msec at 10ms intervals, and 85 spectro-
gram coefﬁcients sampled by constant-Q (or log-scaled) frequency (see [6] for descriptions of these
standard acoustic features.)
The audio features described above are calculated over short windows of audio ( 100ms with 25ms
overlap). This yields too many features per song for our purposes. To address this, we create “aggre-
gate” features by computing individual means and standard deviations (i.e., independent Gaussians)
of these features over 5s windows of feature data. When ﬁxing hyperparameters for these experi-
ments, we also tried a combination of 5s and 10s features, but saw no real improvement in results.
For reasons of computational efﬁciency we used random sampling to retain a maximum of 12 ag-
gregate features per song, corresponding to 1 minute of audio data.

3.2 Labels as a classiﬁcation problem

Intuitively, automatic labeling would be a regression task where a learner would try to predict tag
frequencies for artists or songs. However, because tags are sparse (many artist are not tagged at all;
others like Radiohead are heavily tagged) this proves to be too difﬁcult using our current Last.fm

3

Artist A80s rockcoolSong 180s rockcoolSONG TAGGINGLEARNING ’80s’ TAGSong 1audio features target: ’80s’none/some/a lot’80s’ boostertrainingPREDICTIONSET OF BOOSTERSnew songpredicted tagsdataset. Instead, we chose to treat the task as a classiﬁcation one. Speciﬁcally, for each tag we try to
predict if a particular artist has “none”, “some” or “a lot” of a particular tag relative to other tags.
We normalize the tag frequencies for each artist so that artists having many tags can be compared to
artists having few tags. Then for each tag, an individual artist is placed into a single class “none”,
“some” or “a lot” depending on the proportion of times the tag was assigned to that artist relative
to other tags assigned to that artist. Thus if an artist received only 50 rock tags and nothing else, it
would be treated as having “a lot” of rock. Conversely, if an artist received 5000 rock tags but 10,000
jazz tags it would be treated as having “some” rock and “a lot” of jazz. The speciﬁc boundaries
between “none”, “some” and “a lot” were decided by summing the normalized tag counts or all
artists, generating a 100-bin histogram for each tag and moving the category boundaries such that
an equal number of artists fall into each of the categories. In Figure 2 the histogram for “rock” is
shown (with only 30 bins to make the plot easier to read). Note that most artists fall into the lowest
bin (no or very few instances of the “rock” tag) and that otherwise most of the mass is in high bins.
This was the trend for most tags and one of our motivations for using only 3 bins. As described in
the paper we do not directly use the predictions of the “some” bin. Rather it serves as a class for
holding those artists for which we cannot conﬁdently say “none” or “a lot”. See Figure 2 for an
example.

Figure 2: A 30-bin histogram of the proportion of “rock” tags to other tags for all songs in the dataset.

3.3 Tag prediction with AdaBoost

AdaBoost [5] is a meta-learning method that constructs a strong classiﬁer from a set of simpler
classiﬁers, called weak learners in an iterative way. Originally intended for binary classiﬁcation,
there exist several ways to extend it to multiclass classiﬁcation. We use AdaBoost.MH [9] which
treats multiclass classiﬁcation as a set of one-versus-all binary classiﬁcation problems.
In each
iteration t, the algorithm selects the best classiﬁer, called h(t) from a pool of weak learners, based
on its performance on the training set, and assigns it a coefﬁcient α(t). The input to the weak
learner is a d-dimensional observation vector x ∈ <d containing audio features for one segment of
aggregated data (5 seconds in our experiments). The output of h(t) is a binary vector y ∈ {−1, 1}k
l = 1 means a vote for class l by a weak learner while h(t), −1 is a vote
over the k classes. h(t)
against. After T iterations, the algorithm output is a vector-valued discriminant function:

TX

g(x) =

α(t)h(y)(x)

(1)

As weak learners we used single stumps, e.g. a binary threshold on one of the features. In previous
work we also tried decision trees without any signiﬁcant improvement. Usually we obtain a single
label by taking the class with the most votes i.e f(x) = arg maxl gl(x), but in our model, we use
the output value for each class rather than the argmax.

t=1

3.4 Generating autotags

For each aggregate segment, a booster yields a prediction over the classes “none”, “some”, and “a
lot”. A booster’s raw output for a single segment might be (none:−3.56) (some:0.14) (a lot:2.6).

4

These segment predictions can then be combined to yield artist-level predictions. This can be
achieved in two ways: a winning class can be chosen for each segment (in this example the class “a
lot” would win with 2.6) and the mean over winners can be tallied for all segments belonging to an
artist. Alternately we can skip choosing a winner and simply take the mean of the raw outputs for an
artist’s segments. Because we wanted to estimate tag frequencies using booster magnitude we used
the latter strategy.
The next step is to transform these class for our individual social tag boosters into a bag of words to
be associated with an artist. The most naive way to obtain a single value for rock is to look solely
at the prediction for the “a lot” class. However this discards valuable information such as when a
booster votes strongly “none”. A better way to obtain a measure for rock-ness is to take the center
of mass of the three values. However, because the values are not scaled well with respect to one
another, we ended up with poorly scaled results. Another intuitive idea is simply to subtract the
value of the “none” bin from the value of the “a lot” bin, the reasoning being that “none” is truly
the opposite of “a lot”. In our example, this would yield a rock strength of 7.16. In experiments
for setting hyperparameters, this was shown to work better than other methods. Thus to generate
our ﬁnal measure of rock-ness, we ignore the middle bin (“some”). However this should not be
taken to mean that the middle “some” bin is useless: the booster needed to learn to predict “some”
during training thus forcing it to be more selective in predicting “none” and “a lot”. As a large-
margin classiﬁer, AdaBoost tries to separate the classes as much as possible, so the magnitude of the
values for each bin are not easily comparable. To remedy this, we normalize by taking the minimum
and maximum prediction for each booster, which seems to work for ﬁnding similar artists. This
normalization would not be necessary if we had good tagging data for all artists and could perform
regression on the frequency of tag occurrence across artists.

4 Experiments

To test our model we selected the 60 most popular tags from the Last.fm crawl data described in
Section 2. These tags included genres such as “Rock”, “Electronica”, and “Post Punk”, mood-
related terms such as “Chillout”. The full list of tags and frequencies are available in the “extra
materials”. We collected MP3s for a subset of the artists obtained in our Audioscrobbler crawl.
From those MP3s we extracted several popular acoustic features. In total our training and testing
data included 89924 songs for 1277 artists and yielded more than 1 million 5s aggregate features.

4.1 Booster Errors

As described above, a classiﬁer was trained to map audio features onto aggregate feature segments
for each of the 60 tags. A third of the data was withheld for testing. Because each of the 60
boosters needed roughly 1 day to process, we did not perform cross-validation. However each
booster was trained on a large amount of data relative to the number of decision stumps learned,
making overﬁtting a remote possibility. Classiﬁcation errors are shown in Table 2. These errors are
broken down by tag in the annex for this paper. Using 3 bins and balanced classes, the random error
is about 67%.

Segment
Song

Mean Median Min Max
49.6
40.93
37.61
46.6

43.1
39.69

21.3
17.8

Table 2: Summary of test error (%) on predicting bins for songs and segments.

4.2 Evaluation measures

We use three measures to evaluate the performance of the model. The ﬁrst TopN compares two
ranked lists, a target “ground truth” list A and our predicted list B. This measure is introduced in
[2], and is intended to place emphasis on how well our list predicts the top few items of the target
list. Let kj be the position in list B of the jth element from list A. αr = 0.51/3, and αc = 0.52/3,

5

as in [2]. The result is a value between 0 (dissimilar) and 1 (identical top N),

PN
PN
rαkj
j=1 αj
l=1(αr ∗ αc)l

c

si =

(2)

For the results produced below, we look at the top N = 10 elements in the lists.
Our second measure is Kendall’s T au, a classic measure in collaborative ﬁltering which measures
the number of discordant pairs in 2 lists. Let RA(i) be the rank of the element i in list A, if i is not
explicitly present, RA(i) = length(A) + 1. Let C be the number of concordant pairs of elements
(i, j), e.g. RA(i) > RA(j) and RB(i) < RB(j). In a similar way, D is the number of discordant
pairs. We use τ’s approximation in [8]. We also deﬁne TA and TB the number of ties in list A and
B. In our case, it’s the number of pairs of artists that are in A but not in B, because they end up
having the same position RB = length(B) + 1, and reciprocally. Kendall’s tau value is deﬁned as:

τ =

sqrt((C + D + TA)(C + D + TB))

C − D

(3)

Unless otherwise noted, we analyzed the top 50 predicted values for the target and predicted lists.
Finally, we compute what we call the TopBucket, which is simply the percentage of common ele-
ments in the top N of 2 ranked lists. Here as in Kendall we compare the top 50 predicted values
unless otherwise noted.

4.3 Constructing ground truth

As has long been acknowledged [4] one of the biggest challenges in addressing this task is to ﬁnd a
reasonable “ground truth” against which to compare our results. We seek a similarity matrix among
artists which is not overly biased by current popularity, and which is not built directly from the
social tags we are using for learning targets. Furthermore we want to derive our measure using
data that is freely available data on the web, thus ruling out commercial services such as AllMusic
(www.allmusic.com). Our solution is to construct our ground truth similarity matrix using correla-
tions from the listening habits of Last.fm users. If a signiﬁcant number of users listen to artists A
and B (regardless of the tags they may assign to that artist) we consider those two artists similar.
One challenge, of course, is that some users listen to more music than others and that some artists
are more popular than others. Text search engines must deal with a similar problem: they want
to ensure that frequently used words (e.g., system) do not outweigh infrequently used words (e.g.,
prestidigitation) and that long documents do not always outweigh short documents. Search engines
assign a weight to each word in a document. The weight is meant to represent how important that
word is for that document. Although many such weighting schemes have been described (see [11]
for a comprehensive review), the most popular is the term frequency-inverse document frequency
(or TF×IDF) weighting scheme. TF×IDF assigns high weights to words that occur frequently in a
given document and infrequently in the rest of the collection. The fundamental idea is that words
that are assigned high weights for a given document are good discriminators for that document from
the rest of the collection. Typically, the weights associated with a document are treated as a vector
that has its length normalized to one.
In the case of LastFM, we can consider an artist to be a “document”, where the “words” of the
document are the users that have listened to that artist. The TF×IDF weight for a given user for a
given artist takes into account the global popularity of a given artist and ensures that users who have
listened to more artists do not automatically dominate users who have listened to fewer artists. The
resulting similarity measure seems to us to do a reasonable enough job of capturing artist similarity.
Furthermore it does not seem to be overly biased towards popular bands. See “extra material” for
some examples.

4.4 Similarity Results

One intuitive way to compare autotags and social tags is to look at how well the autotags reproduce
the rank order of the social tags. We used the measures in Section 4.2 to measure this on 100 artists
not used for training (Table 3). The results were well above random. For example, the top 5 autotags
were in agreement with the top 5 social tags 61% of the time.

6

TopN 10 Kendall (N=5) TopBucket (N=5)

autotags
random

0.636
0.111

-0.099
-0.645

61.0%
8.1%

Table 3: Results for all three measures on tag order for 100 out-of-sample artists.

A more realistic way to compare autotags and social tags is via their artist similarity predictions.
We construct similarity matrices from our autotag results and from the Last.fm social tags used for
training and testing. The similarity measure we used wascosine similarity scos(A1, A2) = A1 ∗
A2/(||A1|| ||A2||) where A1 and A2 are tag magnitudes for an artist. In keeping with our interest in
developing a commercial system, we used all available data for generating the similarity matrices,
including data used for training. (The chance of overﬁtting aside, it would be unwise to remove The
Beatles from your recommender simply because you trained on some of their songs). The similarity
matrix is then used to generate a ranked list of similar artists for each artist in the matrix. These lists
are used to compute the measures describe in Section 4.2. Results are found at the top in Table 4.
One potential ﬂaw in this experiment is that the ground truth comes from the same data source as
the training data. Though the ground truth is based on user listening counts and our learning data
comes from aggregate tagging counts, there is still a clear chance of contamination. To investigate
this, we selected the autotags and social tags for 95 of the artists from the USPOP database [2]. We
constructed a ground truth matrix based on the 2002 MusicSeer web survey eliciting similarity rank-
ings between artists from appro 1000 listeners [2]. These results show much closer correspondence
between our autotag results and the social tags from Last.fm than the previous test. See bottom,
Table 4.

Groundtruth Model
Last.FM

TopN 10 Kendall 50 TopBucket 20

MusicSeer

social tags
autotags
random
social tags
autotags
random

0.26
0.118
0.005
0.237
0.184
0.051

-0.23
-0.406
-0.635
-0.182
-0.161
-0.224

34.6%
22.5%
3.9%
29.7%
28.2%
21.5%

Table 4: Performance against Last.Fm (top) and MusicSeer (bottom) ground truth.

It is clear from these previous two experiments that our autotag results do not outperform the social
tags on which they were trained. Thus we asked whether combining the predictions of the autotags
with the social tags would yield better performance than either of them alone. To test this we blended
the autotag similarity matrix Sa with the social tag matrix Ss using αSa + (1 − α)Ss. The results
shown in Figure 3 show a consistent performance increase when blending the two similarity sources.
It seems clear from these results that the autotags are of value. Though they do not outperform the
social tags on which they were trained, they do yield improved performance when combined with
social tags. At the same time they are driven entirely by audio and so can be applied to new, untagged
music. With only 60 tags the model makes some reasonable predictions. When more boosters are
trained, it is safe to assume that the model will perform better.

5 Conclusion and future work

The work presented here is preliminary, but we believe that a supervised learning approach to au-
totagging has substantial merit. Our next step is to compare the performance of our boosted model
to other approaches such as SVMs and neural networks. The dataset used for these experiments
is already larger than those used for published results for genre and artist classiﬁcation. However,
a dataset another order of magnitude larger is necessary to approximate even a small commercial
database of music. A further next step is comparing the performance of our audio features with other
sets of audio features.

7

Figure 3: Similarity performance results when autotag similarities are blended with social tag simi-
larities. The horizontal line is the performance of the social tags against ground truth.

We plan to extend our system to predict many more tags than the current set of 60 tags. We expect
the accuracy of our system to improve as we extend our tag set, especially as we add tags such as
Classical and Folk that are associated with whole genres of music. We will also continue exploring
ways in which the autotag results can drive music visualization. See “extra examples” for some
preliminary work.
Our current method of evaluating our system is biased to favor popular artists. In the future, we
plan to extend our evaluation to include comparisons with music similarity derived from human
analysis of music. This type of evaluation should be free of popularity bias. Most importantly, the
machine-generated autotags need to be tested in a social recommender. It is only in such a context
that we can explore whether autotags, when blended with real social tags, will in fact yield improved
recommendations.

References
[1] Audioscrobbler. Web Services described at http://www.audioscrobbler.net/data/webservices/.
[2] A. Berenzweig, B. Logan, D. Ellis, and B. Whitman. A large-scale evaluation of acoustic and subjective
In Proceedings of the 4th International Conference on Music Information

music similarity measures.
Retrieval (ISMIR 2003), 2003.

[3] J. Bergstra, N. Casagrande, D. Erhan, D. Eck, and B. K´egl. Aggregate features and AdaBoost for music

classiﬁcation. Machine Learning, 65(2-3):473–484, 2006.

[4] D. Ellis, B. Whitman, A. Berenzweig, and S. Lawrence. The quest for ground truth in musical artist
similarity. In Proceedings of the 3th International Conference on Music Information Retrieval (ISMIR
2002), 2002.

[5] Y. Freund and R.E. Shapire. Experiments with a new boosting algorithm. In Machine Learning: Pro-

ceedings of the Thirteenth International Conference, pages 148–156, 1996.

[6] B. Gold and N. Morgan. Speech and Audio Signal Processing: Processing and Perception of Speech and

Music. Wiley, Berkeley, California., 2000.

[7] Jonathan L. Herlocker, Joseph A. Konstan, and John Riedl. Explaining collaborative ﬁltering recommen-

dations. In Computer Supported Cooperative Work, pages 241–250, 2000.

[8] Jonathan L. Herlocker, Joseph A. Konstan, Loren G. Terveen, and John T. Riedl. Evaluating collaborative

ﬁltering recommender systems. ACM Trans. Inf. Syst., 22(1):5–53, 2004.

[9] R. E. Schapire and Y. Singer. Improved boosting algorithms using conﬁdence-rated predictions. Machine

Learning, 37(3):297–336, 1999.

[10] Brian Whitman and Ryan M. Rifkin. Musical query-by-description as a multiclass learning problem. In
IEEE Workshop on Multimedia Signal Processing, pages 153–156. IEEE Signal Processing Society, 2002.

[11] Justin Zobel and Alistair Moffat. Exploring the similarity space. SIGIR Forum, 32(1):18–34, 1998.

8

"
534,2007,Efficient Convex Relaxation for Transductive Support Vector Machine,"We consider the problem of Support Vector Machine transduction, which involves a combinatorial problem with exponential computational complexity in the number of unlabeled examples. Although several studies are devoted to Transductive SVM, they suffer either from the high computation complexity or from the solutions of local optimum. To address this problem, we propose solving Transductive SVM via a convex relaxation, which converts the NP-hard problem to a semi-definite programming. Compared with the other SDP relaxation for Transductive SVM, the proposed algorithm is computationally more efficient with the number of free parameters reduced from O(n2) to O(n) where n is the number of examples. Empirical study with several benchmark data sets shows the promising performance of the proposed algorithm in comparison with other state-of-the-art implementations of Transductive SVM.","Efﬁcient Convex Relaxation for

Transductive Support Vector Machine

Zenglin Xu

Dept. of Computer Science & Engineering

The Chinese University of Hong Kong

Shatin, N.T., Hong Kong

Rong Jin

Dept. of Computer Science & Engineering

Michigan State University
East Lansing, MI, 48824

zlxu@cse.cuhk.edu.hk

rongjin@cse.msu.edu

Jianke Zhu

Irwin King

Michael R. Lyu

Dept. of Computer Science & Engineering

The Chinese University of Hong Kong

Shatin, N.T., Hong Kong

fjkzhu,king,lyug@cse.cuhk.edu.hk

Abstract

We consider the problem of Support Vector Machine transduction, which involves
a combinatorial problem with exponential computational complexity in the num-
ber of unlabeled examples. Although several studies are devoted to Transductive
SVM, they suffer either from the high computation complexity or from the so-
lutions of local optimum. To address this problem, we propose solving Trans-
ductive SVM via a convex relaxation, which converts the NP-hard problem to a
semi-deﬁnite programming. Compared with the other SDP relaxation for Trans-
ductive SVM, the proposed algorithm is computationally more efﬁcient with the
number of free parameters reduced from O(n2) to O(n) where n is the number of
examples. Empirical study with several benchmark data sets shows the promising
performance of the proposed algorithm in comparison with other state-of-the-art
implementations of Transductive SVM.

1 Introduction

Semi-supervised learning has attracted an increasing amount of research interest recently [3, 15]. An
important semi-supervised learning paradigm is the Transductive Support Vector Machine (TSVM),
which maximizes the margin in the presence of unlabeled data and keeps the boundary traversing
through low density regions, while respecting labels in the input space.

Since TSVM requires solving a combinatorial optimization problem, extensive research efforts have
been devoted to efﬁciently ﬁnding the approximate solution to TSVM. The popular version of TSVM
proposed in [8] uses a label-switching-retraining procedure to speed up the computation. In [5], the
hinge loss in TSVM is replaced by a smooth loss function, and a gradient descent method is used
to ﬁnd the decision boundary in a region of low density. Chapelle et al. [2] employ an iterative
approach for TSVM. It begins with minimizing an easy convex object function, and then gradu-
ally approximates the objective of TSVM with more complicated functions. The solution of the
simple function is used as the initialization for the solution to the complicated function. Other it-
erative methods, such as deterministic annealing [11] and the concave-convex procedure (CCCP)
method [6], are also employed to solve the optimization problem related to TSVM. The main draw-
back of the approximation methods listed above is that they are susceptible to local optima, and
therefore are sensitive to the initialization of solutions. To address this problem, in [4], a branch-

Time Comparison

 

CTSVM
RTSVM

2000

1800

1600

1400

1200

1000

800

600

400

200

)
s
d
n
o
c
e
s
(
 

e
m
T

i

0

 

50

100

150

200

250

300

Number of Samples

Figure 1: Computation time of the proposed convex relaxation approach for TSVM (i.e., CTSVM)
and the semi-deﬁnite relaxation approach for TSVM (i.e., RTSVM) versus the number of unlabeled
examples. The Course data set is used, and the number of labeled examples is 20.

and-bound search method is developed to ﬁnd the exact solution. In [14], the authors approximate
TSVM by a semi-deﬁnite programming problem, which leads to a relaxation solution to TSVM
(noted as RTSVM), to avoid the solution of local optimum. However, both approaches suffer from
the high computational cost and can only be applied to small sized data sets.
To this end, we present the convex relaxation for Transductive SVM (CTSVM). The key idea of our
method is to approximate the non-convex optimization problem of TSVM by its dual problem. The
advantage of doing so is twofold:

† Unlike the semi-deﬁnite relaxation [14] that approximates TSVM by dropping the rank
constraint, the proposed approach approximates TSVM by its dual problem. As the basic
result of convex analysis, the conjugate of conjugate of any function f (x) is the convex en-
velope of f (x), and therefore provides a tighter convex relaxation for f (x) [7]. Hence, the
proposed approach provides a better convex relaxation than that in [14] for the optimization
problem in TSVM.

† Compared to the semi-deﬁnite relaxation TSVM, the proposed algorithm involves fewer
free parameters and therefore signiﬁcantly improves the efﬁciency by reducing the worst-
case computational complexity from O(n6:5) to O(n4:5). Figure 1 shows the running time
of both the semi-deﬁnite relaxation of TSVM in [14] and the proposed convex relaxation for
TSVM versus increasing number of unlabeled examples. The data set used in this example
is the Course data set (see the experiment section), and the number of labeled examples
is 20. We clearly see that the proposed convex relaxation approach is considerably more
efﬁcient than the semi-deﬁnition approach.

The rest of this paper is organized as follows. Section 2 reviews the related work on the semi-
deﬁnite relaxation for TSVM. Section 3 presents the convex relaxation approach for Transductive
SVM. Section 4 presents the empirical studies that verify the effectiveness of the proposed relaxation
for TSVM. Section 5 sets out the conclusion.

2 Related Work

In this section, we review the key formulae for Transductive SVM, followed by the semi-deﬁnite
programming relaxation for TSVM.
Let X = (x1; : : : ; xn) denote the entire data set, including both the labeled examples and the
unlabeled ones. We assume that the ﬁrst l examples within X are labeled by y‘ = (y‘
2; : : : ; y‘
l )
where y‘
i 2 f¡1; +1g represents the binary class label assigned to xi. We further denote by y =
(y1; y2; : : : ; yn) 2 f¡1; +1gn the binary class labels predicted for all the data points in X . The goal
of TSVM is to estimate y by using both the labeled examples and the unlabeled ones.

1; y‘

Following the framework of maximum margin, TSVM aims to identify the classiﬁcation model that
will result in the maximum classiﬁcation margin for both labeled and unlabeled examples, which
amounts to solve the following optimization problem:

n

min

w;b;y2f¡1;+1gn;""

kwk2

2 + C

""i

Xi=1

s. t.

yi(w>xi ¡ b) ‚ 1 ¡ ""i; ""i ‚ 0; i = 1; 2; : : : ; n
yi = y‘

i ; i = 1; 2; : : : ; l;

where C ‚ 0 is the trade-off parameter between the complexity of function w and the margin errors.
The prediction function can be formulated as f (x) = w>x ¡ b.
Evidently, the above problem is a non-convex optimization problem due to the product term yiwj in
the constraint. In order to approximate the above problem into a convex programming problem, we
ﬁrst rewrite the above problem into the following form using the Lagrange Theorem:

min

”;y2f¡1;+1gn;–;‚

1
2

(e + ” ¡ – + ‚y)>D(y)K¡1D(y)(e + ” ¡ – + ‚y) + C–>e

(1)

s. t.

” ‚ 0;

– ‚ 0;

yi = y‘

i ; i = 1; 2; : : : ; l;

where ”, – and ‚ are the dual variables. e is the n-dimensional column vector of all ones and K is
the kernel matrix. D(y) represents a diagonal matrix whose diagonal elements form the vector y.
Detailed derivation can be found in [9, 13]. Using the Schur complement, the above formulation can
be further formulated as follows:

min

y2f¡1;+1gn;t;”;–;‚

t

(2)

s. t.

(cid:181)

yy> – K

(e + ” ¡ – + ‚y)>
” ‚ 0; – ‚ 0; yi = y‘

e + ” ¡ – + ‚y

t ¡ 2C–>e ¶ ” 0

i ; i = 1; 2; : : : ; l;

where the operator – represents the element wise product.
To convert the above problem into a convex optimization problem, the key idea is to replace the
quadratic term yy> by a linear variable. Based on the result that the set Sa = fM = yy>jy 2
f¡1; +1gng is equivalent to the set Sb = fMjMi;i = 1; rank(M) = 1g, we can approximate the
problem in (2) as follows:

min

M;t;”;–;‚

t

(3)

s. t.

(cid:181) M – K
(e + ” ¡ –)> t ¡ 2C–>e ¶ ” 0

e + ” ¡ –

” ‚ 0; – ‚ 0;
M ” 0; Mi;i = 1; i = 1; 2; : : : ; n;

where Mij = y‘

i y‘

j for 1 • i; j • l.

Note that the key differences between (2) and (3) are (a) the rank constraint rank(M) = 1 is re-
moved, and (b) the variable ‚ is set to be zero, which is equivalent to setting b = 0. The above
approximation is often referred to as the Semi-Deﬁnite Programming (SDP) relaxation. As re-
vealed by the previous studies [14, 1], the SDP programming problem resulting from the approx-
imation is computationally expensive. More speciﬁcally, there are O(n2) parameters in the SDP
cone and O(n) linear inequality constraints, which implies a worst-case computational complexity
of O(n6:5). To avoid the high computational complexity, we present a different approach for relax-
ing TSVM into a convex problem. Compared to the SDP relaxation approach, it is advantageous
in that (1) it produces the best convex approximation for TSVM, and (2) it is computationally more
efﬁcient than the previous SDP relaxation.

3 Relaxed Transductive Support Vector Machine

In this section, we follow the work of generalized maximum margin clustering [13] by ﬁrst studying
the case of hard margin, and then extending it to the case of soft margin.

3.1 Hard Margin TSVM

In the hard margin case, SVM does not penalize the classiﬁcation error, which corresponds to – = 0
in (1). The resulting formulism of TSVM becomes

min
”;y;‚
s: t:

(e + ” + ‚y)>D(y)K¡1D(y)(e + ” + ‚y)

1
2
” ‚ 0;
yi = y‘
y2
i = 1; i = l + 1; l + 2; : : : ; n:

i ; i = 1; 2; : : : ; l;

(4)

Instead of employing the SDP relaxation as in [14], we follow the work in [13] and introduce a
variable z = D(y)(e + ”) = y – (e + ”). Given that ” ‚ 0, the constraints in (4) can be written
as y‘
i ‚ 1 for all the unlabeled examples. Hence, z can be
used as the prediction function, i.e., f ⁄ = z. Using this new notation, the optimization problem in
(4) can be rewritten as follows:

i zi ‚ 1 for the labeled examples, and z2

min
z;‚
s. t.

(z + ‚e)>K¡1(z + ‚e)

1
2
y‘
i zi ‚ 1; i = 1; 2; : : : ; l;
z2
i ‚ 1; i = l + 1; l + 2; : : : ; n:

(5)

One problem with Transductive SVMs is that it is possible to classify all the unlabeled data to one of
the classes with a very large margin due to the high dimension and few labeled data. This will lead
to poor generalization ability. To solve this problem, we introduce the following balance constraint
to ensure that no class takes all the unlabeled examples:

¡† •

1
l

zi ¡

1

n ¡ l

l

Xi=1

n

Xi=l+1

zi • †;

(6)

where † ‚ 0 is a constant. Through the above constraint, we aim to ensure that the difference
between the labeled data and the unlabeled data in their class assignment is small.

To simplify the expression, we further deﬁne w = (z; ‚) 2 Rn+1 and P = (In; e) 2 Rn£(n+1).
Then, the problem in (5) becomes:

min

w
s. t.

w>P>K¡1Pw

y‘
i wi ‚ 1; i = 1; 2; : : : ; l;
w2
i ‚ 1; i = l + 1; l + 2; : : : ; n;

(7)

¡† •

1
l

wi ¡

1

n ¡ l

l

Xi=1

n

Xi=l+1

wi • †:

When this problem is solved, the label vector y can be directly determined by the sign of the pre-
diction function, i.e., sign(w). This is because wi = (1 + ”)yi for i = l + 1; : : : ; n and ” ‚ 0.
The following theorem shows that the problem in (7) can be relaxed to a semi-deﬁnite programming.
Theorem 1. Given a sample X = fx1; : : : ; xng and a partial set of the labels y‘ = (y‘
2; : : : ; y‘
l )
where 1 • l • n, the variable w that optimizes (7) can be calculated by

1; y‘

w =

1
2

[A ¡ D((cid:176) – b)]¡1 ((cid:176) – a ¡ (ﬁ ¡ ﬂ)c);

(8)

where a = (yl; 0n¡l; 0) 2 Rn+1, b = (0l; 1n¡l; 0) 2 Rn+1, c = ( 1
A = P>K¡1P, and (cid:176) is determined by the following semi-deﬁnite programming:

l 1l; ¡ 1

u 1n¡l; 0) 2 Rn+1,

(cid:176)i ¡ †(ﬁ + ﬂ)

(9)

max
(cid:176);t;ﬁ;ﬂ

s: t:

¡

1
4

t +

n

Xi=1

(cid:181)

A ¡ D((cid:176) – b)

(cid:176) – a ¡ (ﬁ ¡ ﬂ)c;

((cid:176) – a ¡ (ﬁ ¡ ﬂ)c)>

t

ﬁ ‚ 0; ﬂ ‚ 0; (cid:176)i ‚ 0; i = 1; 2; : : : ; n:

¶ ” 0

Proof Sketch. We deﬁne the Lagrangian of the minimization problem (7) as follows:

min

w

max

(cid:176)

F(w; (cid:176)) = w>P>K¡1Pw +

(cid:176)i(1 ¡ y‘

i wi) +

l

Xi=1

(cid:176)i(1 ¡ w2
i )

n

Xi=l+1

+ﬁ(c>w ¡ †) + ﬂ(¡c>w ¡ †);

where (cid:176)i ‚ 0 for i = 1; : : : ; n. It can be derived from the duality that minw max(cid:176) F(w; (cid:176)) =
max(cid:176) minw F(w; (cid:176)):
At the optimum, the derivatives of F with respect to the variable w are derived as below:

@F
@w

= 2 [A ¡ D((cid:176) – b)] w ¡ (cid:176) – a + (ﬁ ¡ ﬂ)c = 0;

where A = P>K¡1P. The inverse of A¡D((cid:176)–b) can be computed through adding a regularization
parameter. Therefore, w is able to be calculated by:

w =

1
2

[A ¡ D((cid:176) – b)]¡1 ((cid:176) – a ¡ (ﬁ ¡ ﬂ)c):

Thus, the dual form of the problem becomes:

max

(cid:176)

L((cid:176)) = ¡

1
4

((cid:176) – a ¡ (ﬁ ¡ ﬂ)c)> [A ¡ D(b – (cid:176))]¡1 ((cid:176) – a ¡ (ﬁ ¡ ﬂ)c) +

We import a variable t, so that

(cid:176)i ¡ †(ﬁ + ﬂ);

n

Xi=1

¡

1
4

((cid:176) – a ¡ (ﬁ ¡ ﬂ)c)>[A ¡ D(b – (cid:176))]¡1((cid:176) – a ¡ (ﬁ ¡ ﬂ)c) ‚ ¡t:

According to the Schur Complement, we obtain a semi-deﬁnite programming cone, from which the
optimization problem (9) can be formulated. ¥
Remark I. The problem in (9) is a convex optimization problem, more speciﬁcally, a semi-deﬁnite
programming problem, and can be efﬁciently solved by the interior-point method [10] implemented
in some optimization packages, such as SeDuMi [12]. Besides, our relaxation algorithm has O(n)
parameters in the SDP cone and O(n) linear equality constraints, which involves a worst-case com-
putational complexity of O(n4:5). However, in the previous relaxation algorithms [1, 14], there
are approximately O(n2) parameters in the SDP cone, which involve a worst-case computational
complexity in the scale of O(n6:5). Therefore, our proposed convex relaxation algorithm is more
efﬁcient. In addition, as analyzed in Section 2, the approximation in [1, 14] drops the rank constraint
of the matrix y>y, which does not lead to a tight approximation. On the other hand, our prediction
function f ⁄ implements the conjugate of conjugate of the prediction function f (x), which is the
convex envelope of f (x) [7]. Thus, our proposed convex approximation method provides a tighter
approximation than the previous method.
Remark II. It is interesting to discuss the connection between the solution of the proposed algorithm
and that of harmonic functions. We consider a special case of (8), where ‚ = 0 (which implies no
bias term in the primal SVM), and there is no balance constraint. Then the solution of (9) can be
expressed as follows:

(10)

(11)

It can be further derived as follows:

1

z =

2£K¡1 ¡ D((cid:176) – (0l; 1n¡l))⁄¡1
n!¡1ˆ l
z =ˆIn ¡
Xi=1

Xi=l+1

(cid:176)iKIi

n

((cid:176) – (yl; 0n¡l)):

(cid:176)iy‘

i K(xi; ¢)! ;

where Ii
n is deﬁned as an n £ n matrix with all elements being zero except the i-th diagonal el-
ement which is 1, and K(xi; ¢) is the i-th column of K. Similar to the solution of the harmonic
function, we ﬁrst propagate the class labels from the labeled examples to the unlabeled one by term

Pl

i=1 (cid:176)iy‘

i K(xi; ¢), and then adjust the prediction labels by the factor ¡In ¡Pn

The key difference in our solution is that (1) different weights (i.e., (cid:176)i) are assigned to the labeled
examples, and (2) the adjustment factor is different to that in the harmonic function [16].

i=l+1 (cid:176)iKIi

n¢¡1.

3.2 Soft Margin TSVM

We extend TSVM to the case of soft margin by considering the following problem:

min
”;y;–;‚

s: t:

1
2

(e + ” ¡ – + ‚y)>D(y)K¡1D(y)(e + ” ¡ – + ‚y) + C‘

” ‚ 0; – ‚ 0;
yi = y‘
y2
i = 1; l + 1 • i • n;

i ; 1 • i • l;

–2
i + Cu

l

Xi=1

–2
i

n

Xi=l+1

where –i is related to the margin error. Note that we distinguish the labeled examples from the
unlabeled examples by introducing different penalty constants for margin errors, C‘ for labeled
examples and Cu for unlabeled examples.
Similarly, we introduce the slack variable z, and then derive the following dual problem:

(cid:176)i ¡ †(ﬁ + ﬂ)

(12)

max
(cid:176);t;ﬁ;ﬂ

s: t:

¡

1
4

t +

n

Xi=1

(cid:181)

A ¡ D((cid:176) – b)

(cid:176) – a ¡ (ﬁ ¡ ﬂ)c

((cid:176) – a ¡ (ﬁ ¡ ﬂ)c)>

t

¶ ” 0;

0 • (cid:176)i • C‘; i = 1; 2; : : : ; l;
0 • (cid:176)i • Cu; i = l + 1; l + 2; : : : ; n;
ﬁ ‚ 0; ﬂ ‚ 0;

which is also a semi-deﬁnite programming problem and can be solved similarly.

4 Experiments

In this section, we report empirical study of the proposed method on several benchmark data sets.

4.1 Data Sets Description

To make evaluations comprehensive, we have collected four UCI data sets and three text data sets
as our experimental testbeds. The UCI data sets include Iono, sonar, Banana, and Breast, which are
widely used in data classiﬁcation. The WinMac data set consists of the classes, mswindows and
mac, of the Newsgroup20 data set. The IBM data set contains the classes, IBM and non-IBM, of the
Newsgroup20 data set. The course data set is made of the course pages and non-course pages of the
WebKb corpus. For each text data set, we randomly sample the data with the sample size of 60, 300
and 1000, respectively. Each resulted sample is noted by the sufﬁx, “-s”, “-m”, or “-l” depending on
whether the sample size is small, medium or large. Table 1 describes the information of these data
sets, where d represents the data dimensionality, l means the number of labeled data points, and n
denotes the total number of examples.

Table 1: Data sets used in the experiments, where d represents the data dimensionality, l means the
number of labeled data points, and n denotes the total number of examples.
l
20
20
20
50
50
50

n
351 WinMac-m 7511
11960
208
IBM-m
400
1800
Course-m
7511
300 WinMac-l
11960
60
60
1800

Data set
Iono
Sonar
Banana
Breast
IBM-s
Course-s

n
300
300
300
1000
1000
1000

l
20
20
20
20
10
10

Data set

d

IBM-l
Course-l

d
34
60
4
9

11960
1800

4.2 Experimental Protocol

To evaluate the effectiveness of the proposed CTSVM method, we choose the conventional SVM
as our baseline method. In our experiments, we also make comparisons with three state-of-the-art

methods: the SVM-light algorithm [8], the Gradient Decent TSVM (rTSVM) algorithm [5], and
the Concave Convex Procedure (CCCP) [6]. Since the SDP approximation TSVM [14] has very
high time complexity O(n6:5), which is difﬁcult to process data sets with hundreds of examples.
Thus, it is only evaluated on the smaller data sets, i.e., “IBM-s” and “Course-s”.

The experiment setup is described as follows. For each data set, we conduct 10 trials. In each trial,
the training set contains each class of data, and the remaining data are then used as the unlabeled
(test) data. Moreover, the RBF kernel is used for “Iono”, “Sonar” and “Banana”, and the linear
kernel is used for the other data sets. This is because the linear kernel performs better than the RBF
kernel on these data sets. The kernel width of RBF kernel is chosen by 5-cross validation on the
labeled data. The margin parameter C‘ is tuned by using the labeled data in all algorithms. Due to
the small number of labeled examples, for CTSVM and CCCP, the margin parameter for unlabeled
data, Cu, is set equal to C‘. Other parameters in these algorithms are set to the default values
according to the relevant literatures.

4.3 Experimental Results

SVM

SVM-light
78.25§0.36
55.26§5.88

-

Table 2: The classiﬁcation performance of Transductive SVMs on benchmark data sets.
Data Set
CTSVM
78.55§4.83
80.09§2.63
Iono
51.76§5.05
67.39§6.26
Sonar
79.51§3.02
58.45§7.15
Banana
97.79§0.23
96.46§1.18
Breast
75.25§7.49
52.75§15.01
IBM-s
79.75§8.45
Course-s
63.52§5.82
84.82§2.12
WinMac-m 57.64§9.58
73.17§0.89
IBM-m
53.00§6.83
92.92§2.28
80.18§1.27
Course-m
91.25§2.67
60.86§10.10
WinMac-l
73.42§3.23
61.82§7.26
IBM-l
94.62§0.97
Course-l
83.56§3.10

rTSVM
81.72§4.50
69.36§4.69
71.54§7.28
97.17§0.35
65.80§6.56
75.80§12.87
81.03§8.23
64.65§13.38
90.35§3.59
90.19§2.65
73.11§1.99
93.58§2.68

95.68§1.82
67.60§9.29
76.82§4.78
79.42§4.60
67.55§6.74
93.89§1.49
89.81§2.10
75.40§2.26
92.35§3.02

CCCP

82.11§3.83
56.01§6.70
79.33§4.22
96.89§0.67
65.62§14.83
74.20§11.50
84.28§8.84
69.62§11.03
88.78§2.87
91.00§2.42
74.80§1.87
91.32§4.08

Table 2 summarizes the classiﬁcation accuracy and the standard deviations of the proposed algo-
rithm, the baseline method and the state-of-the-art methods. It can be observed that our proposed
algorithm performs signiﬁcantly better than the standard SVM across all the data sets. Moreover, on
the small-size data sets, i.e., “IBM-s” and “Course-s”, the results of the SDP-relaxation method are
68.57§22.73 and 64.03§7.65, which are worse than the proposed CTSVM method. In addition, the
proposed CTSVM algorithm performs much better than other TSVM methods over “WinMac-m”
and “Course-l”. As shown in Table 2, the SVM-light algorithm achieves the best results on “Course-
m” and “IBM-l”, however, it fails to converge on “Banana”. On the remaining data sets, comparable
results have been obtained for our proposed algorithm. From above, the empirical evaluations in-
dicate that our proposed CTSVM method achieves promising classiﬁcation results comparing with
the state-of-the-art methods.

5 Conclusion and Future Work

This paper presents a novel method for Transductive SVM by relaxing the unknown labels to the
continuous variables. In contrast to the previous relaxation method which involves O(n2) free pa-
rameters in the semi-deﬁnite matrix, our method takes the advantages of reducing the number of
free parameters to O(n), and can solve the optimization problem more efﬁciently. In addition, the
proposed approach provides a tighter convex relaxation for the optimization problem in TSVM. Em-
pirical studies on benchmark data sets demonstrate that the proposed method is more efﬁcient than
the previous semi-deﬁnite relaxation method and achieves promising classiﬁcation results compar-
ing to the state-of-the-art methods.

As the current model is only designed for a binary-classiﬁcation, we plan to develop a multi-class
Transductive SVM model in the future. Moreover, it is desirable to extend the current model to
classify the new incoming data.

Acknowledgments

The work described in this paper is supported by a CUHK Internal Grant (No. 2050346) and a grant
from the Research Grants Council of the Hong Kong Special Administrative Region, China (Project
No. CUHK4150/07E).

References

[1] T. D. Bie and N. Cristianini. Convex methods for transduction.

In S. Thrun, L. Saul, and
B. Sch¨olkopf, editors, Advances in Neural Information Processing Systems 16. MIT Press,
Cambridge, MA, 2004.

[2] O. Chapelle, M. Chi, and A. Zien. A continuation method for semi-supervised SVMs. In ICML
’06: Proceedings of the 23rd international conference on Machine learning, pages 185–192,
New York, NY, USA, 2006. ACM Press.

[3] O. Chapelle, B. Sch¨olkopf, and A. Zien. Semi-Supervised Learning. MIT Press, Cambridge,

MA, 2006.

[4] O. Chapelle, V. Sindhwani, and S. Keerthi. Branch and bound for semi-supervised support
vector machines. In B. Sch¨olkopf, J. Platt, and T. Hoffman, editors, Advances in Neural Infor-
mation Processing Systems 19. MIT Press, Cambridge, MA, 2007.

[5] O. Chapelle and A. Zien. Semi-supervised classiﬁcation by low density separation. In Pro-
ceedings of the Tenth International Workshop on Artiﬁcial Intelligence and Statistics, pages
57–64, 2005.

[6] R. Collobert, F. Sinz, J. Weston, and L. Bottou. Large scale transductive SVMs. Journal of

Machine Learning Reseaerch, 7:1687–1712, 2006.

[7] J.-B. Hiriart-Urruty and C. Lemarechal. Convex analysis and minimization algorithms II:

advanced theory and bundle methods. (2nd part edition). Springer-Verlag, New York, 1993.

[8] T. Joachims. Transductive inference for text classiﬁcation using support vector machines. In
ICML ’99: Proceedings of the Sixteenth International Conference on Machine Learning, pages
200–209, San Francisco, CA, USA, 1999. Morgan Kaufmann Publishers Inc.

[9] G. R. G. Lanckriet, N. Cristianini, P. Bartlett, L. E. Ghaoui, and M. I. Jordan. Learning the
kernel matrix with semideﬁnite programming. Journal of Machine Learning Research, 5:27–
72, 2004.

[10] Y. Nesterov and A. Nemirovsky. Interior point polynomial methods in convex programming:

Theory and applications. Studies in Applied Mathematics. Philadelphia, 1994.

[11] V. Sindhwani, S. S. Keerthi, and O. Chapelle. Deterministic annealing for semi-supervised
kernel machines. In ICML ’06: Proceedings of the 23rd international conference on Machine
learning, pages 841–848, New York, NY, USA, 2006. ACM Press.

[12] J. F. Sturm. Using SeDuMi 1.02, a MATLAB toolbox for optimization over symmetric cones.

Optimization Methods and Software, 11:625–653, 1999.

[13] H. Valizadegan and R. Jin. Generalized maximum margin clustering and unsupervised kernel
learning. In B. Sch¨olkopf, J. Platt, and T. Hoffman, editors, Advances in Neural Information
Processing Systems 19. MIT Press, Cambridge, MA, 2007.

[14] L. Xu and D. Schuurmans. Unsupervised and semi-supervised multi-class support vector ma-

chines. In AAAI, pages 904–910, 2005.

[15] X. Zhu. Semi-supervised learning literature survey. Technical report, Computer Sciences,

University of Wisconsin-Madison, 2005.

[16] X. Zhu, Z. Ghahramani, and J. D. Lafferty. Semi-supervised learning using gaussian ﬁelds
In Proceedings of Twentith International Conference on Machine

and harmonic functions.
Learning (ICML-2003), pages 912–919, 2003.

"
1118,2007,Sparse Feature Learning for Deep Belief Networks,"Unsupervised learning algorithms aim to discover the structure hidden in the data, and to learn representations that are more suitable as input to a supervised machine than the raw input. Many unsupervised methods are based on reconstructing the input from the representation, while constraining the representation to have certain desirable properties (e.g. low dimension, sparsity, etc). Others are based on approximating density by stochastically reconstructing the input from the representation. We describe a novel and efficient algorithm to learn sparse representations, and compare it theoretically and experimentally with a similar machines trained probabilistically, namely a Restricted Boltzmann Machine. We propose a simple criterion to compare and select different unsupervised machines based on the trade-off between the reconstruction error and the information content of the representation. We demonstrate this method by extracting features from a dataset of handwritten numerals, and from a dataset of natural image patches. We show that by stacking multiple levels of such machines and by training sequentially, high-order dependencies between the input variables can be captured.","Sparse Feature Learning for Deep Belief Networks

Marc’Aurelio Ranzato1

Y-Lan Boureau2,1

Yann LeCun1

1 Courant Institute of Mathematical Sciences, New York University

2 INRIA Rocquencourt

{ranzato,ylan,yann@courant.nyu.edu}

Abstract

Unsupervised learning algorithms aim to discover the structure hidden in the data,
and to learn representations that are more suitable as input to a supervised machine
than the raw input. Many unsupervised methods are based on reconstructing the
input from the representation, while constraining the representation to have cer-
tain desirable properties (e.g. low dimension, sparsity, etc). Others are based on
approximating density by stochastically reconstructing the input from the repre-
sentation. We describe a novel and efﬁcient algorithm to learn sparse represen-
tations, and compare it theoretically and experimentally with a similar machine
trained probabilistically, namely a Restricted Boltzmann Machine. We propose a
simple criterion to compare and select different unsupervised machines based on
the trade-off between the reconstruction error and the information content of the
representation. We demonstrate this method by extracting features from a dataset
of handwritten numerals, and from a dataset of natural image patches. We show
that by stacking multiple levels of such machines and by training sequentially,
high-order dependencies between the input observed variables can be captured.

1 Introduction

One of the main purposes of unsupervised learning is to produce good representations for data, that
can be used for detection, recognition, prediction, or visualization. Good representations eliminate
irrelevant variabilities of the input data, while preserving the information that is useful for the ul-
timate task. One cause for the recent resurgence of interest in unsupervised learning is the ability
to produce deep feature hierarchies by stacking unsupervised modules on top of each other, as pro-
posed by Hinton et al. [1], Bengio et al. [2] and our group [3, 4]. The unsupervised module at one
level in the hierarchy is fed with the representation vectors produced by the level below. Higher-
level representations capture high-level dependencies between input variables, thereby improving
the ability of the system to capture underlying regularities in the data. The output of the last layer in
the hierarchy can be fed to a conventional supervised classiﬁer.

A natural way to design stackable unsupervised learning systems is the encoder-decoder
paradigm [5]. An encoder transforms the input into the representation (also known as the code
or the feature vector), and a decoder reconstructs the input (perhaps stochastically) from the repre-
sentation. PCA, Auto-encoder neural nets, Restricted Boltzmann Machines (RBMs), our previous
sparse energy-based model [3], and the model proposed in [6] for noisy overcomplete channels are
just examples of this kind of architecture. The encoder/decoder architecture is attractive for two rea-
sons: 1. after training, computing the code is a very fast process that merely consists in running the
input through the encoder; 2. reconstructing the input with the decoder provides a way to check that
the code has captured the relevant information in the data. Some learning algorithms [7] do not have
a decoder and must resort to computationally expensive Markov Chain Monte Carlo (MCMC) sam-
pling methods in order to provide reconstructions. Other learning algorithms [8, 9] lack an encoder,
which makes it necessary to run an expensive optimization algorithm to ﬁnd the code associated
with each new input sample. In this paper we will focus only on encoder-decoder architectures.

1

In general terms, we can view an unsupervised model as deﬁning a distribution over input vectors
Y through an energy function E(Y, Z, W ):

P (Y |W ) = Zz

P (Y, z|W ) = Rz e−βE(Y,z,W )
Ry,z e−βE(y,z,W )

(1)

where Z is the code vector, W the trainable parameters of encoder and decoder, and β is an arbitrary
positive constant. The energy function includes the reconstruction error, and perhaps other terms
as well. For convenience, we will omit W from the notation in the following. Training the machine
to model the input distribution is performed by ﬁnding the encoder and decoder parameters that
minimize a loss function equal to the negative log likelihood of the training data under the model.
For a single training sample Y , the loss function is

L(W, Y ) = −

1
β

logZz

e−βE(Y,z) +

1
β

logZy,z

e−βE(y,z)

(2)

The ﬁrst term is the free energy Fβ(Y ). Assuming that the distribution over Z is rather peaked, it
can be simpler to approximate this distribution over Z by its mode, which turns the marginalization
over Z into a minimization:

L∗(W, Y ) = E(Y, Z ∗(Y )) +

1
β

logZy

e−βE(y,Z ∗(y))

(3)

where Z ∗(Y ) is the maximum likelihood value Z ∗(Y ) = argminzE(Y, z), also known as the
optimal code. We can then deﬁne an energy for each input point, that measures how well it is
reconstructed by the model:

F∞(Y ) = E(Y, Z ∗(Y )) = lim
β→∞

−

1
β

logZz

e−βE(Y,z)

(4)

The second term in equation 2 and 3 is called the log partition function, and can be viewed as a
penalty term for low energies. It ensures that the system produces low energy only for input vectors
that have high probability in the (true) data distribution, and produces higher energies for all other
input vectors [5]. The overall loss is the average of the above over the training set.

Regardless of whether only Z ∗ or the whole distribution over Z is considered, the main difﬁculty
with this framework is that it can be very hard to compute the gradient of the log partition function
in equation 2 or 3 with respect to the parameters W . Efﬁcient methods shortcut the computation by
drastically and cleverly reducing the integration domain. For instance, Restricted Boltzmann Ma-
chines (RBM) [10] approximate the gradient of the log partition function in equation 2 by sampling
values of Y whose energy will be pulled up using an MCMC technique. By running the MCMC for
a short time, those samples are chosen in the vicinity of the training samples, thereby ensuring that
the energy surface forms a ravine around the manifold of the training samples. This is the basis of
the Contrastive Divergence method [10].

The role of the log partition function is merely to ensure that the energy surface is lower around
training samples than anywhere else. The method proposed here eliminates the log partition function
from the loss, and replaces it by a term that limits the volume of the input space over which the energy
surface can take a low value. This is performed by adding a penalty term on the code rather than on
the input. While this class of methods does not directly maximize the likelihood of the data, it can be
seen as a crude approximation of it. To understand the method, we ﬁrst note that if for each vector
Y , there exists a corresponding optimal code Z ∗(Y ) that makes the reconstruction error (or energy)
F∞(Y ) zero (or near zero), the model can perfectly reconstruct any input vector. This makes the
energy surface ﬂat and indiscriminate. On the other hand, if Z can only take a small number of
different values (low entropy code), then the energy F∞(Y ) can only be low in a limited number of
places (the Y ’s that are reconstructed from this small number of Z values), and the energy cannot
be ﬂat.

More generally, a convenient method through which ﬂat energy surfaces can be avoided is to limit
the maximum information content of the code. Hence, minimizing the energy F∞(Y ) together with
the information content of the code is a good substitute for minimizing the log partition function.

2

A popular way to minimize the information content in the code is to make the code sparse or low-
dimensional [5]. This technique is used in a number of unsupervised learning methods, including
PCA, auto-encoders neural network, and sparse coding methods [6, 3, 8, 9]. In sparse methods,
the code is forced to have only a few non-zero units while most code units are zero most of the
time. Sparse-overcomplete representations have a number of theoretical and practical advantages,
as demonstrated in a number of recent studies [6, 8, 3]. In particular, they have good robustness to
noise, and provide a good tiling of the joint space of location and frequency. In addition, they are
advantageous for classiﬁers because classiﬁcation is more likely to be easier in higher dimensional
spaces. This may explain why biology seems to like sparse representations [11]. In our context, the
main advantage of sparsity constraints is to allow us to replace a marginalization by a minimization,
and to free ourselves from the need to minimize the log partition function explicitly.

In this paper we propose a new unsupervised learning algorithm called Sparse Encoding Symmetric
Machine (SESM), which is based on the encoder-decoder paradigm, and which is able to produce
sparse overcomplete representations efﬁciently without any need for ﬁlter normalization [8, 12] or
code saturation [3]. As described in more details in sec. 2 and 3, we consider a loss function which
is a weighted sum of the reconstruction error and a sparsity penalty, as in many other unsupervised
learning algorithms [13, 14, 8]. Encoder and decoder are constrained to be symmetric, and share
a set of linear ﬁlters. Although we only consider linear ﬁlters in this paper, the method allows
the use of any differentiable function for encoder and decoder. We propose an iterative on-line
learning algorithm which is closely related to those proposed by Olshausen and Field [8] and by us
previously [3]. The ﬁrst step computes the optimal code by minimizing the energy for the given
input. The second step updates the parameters of the machine so as to minimize the energy.

In sec. 4, we compare SESM with RBM and PCA. Following [15], we evaluate these methods by
measuring the reconstruction error for a given entropy of the code. In another set of experiments,
we train a classiﬁer on the features extracted by the various methods, and measure the classiﬁcation
error on the MNIST dataset of handwritten numerals. Interestingly, the machine achieving the best
recognition performance is the one with the best trade-off between RMSE and entropy. In sec. 5, we
compare the ﬁlters learned by SESM and RBM for handwritten numerals and natural image patches.
In sec.5.1.1, we describe a simple way to produce a deep belief net by stacking multiple levels of
SESM modules. The representational power of this hierarchical non-linear feature extraction is
demonstrated through the unsupervised discovery of the numeral class labels in the high-level code.

2 Architecture

In this section we describe a Sparse Encoding Symmetric Machine (SESM) having a set of linear ﬁl-
ters in both encoder and decoder. However, everything can be easily extended to any other choice of
parameterized functions as long as these are differentiable and maintain symmetry between encoder
and decoder. Let us denote with Y the input deﬁned in RN , and with Z the code deﬁned in RM ,
where M is in general greater than N (for overcomplete representations). Let the ﬁlters in encoder
and decoder be the columns of matrix W ∈ RN ×M , and let the biases in the encoder and decoder
be denoted by benc ∈ RM and bdec ∈ RN , respectively. Then, encoder and decoder compute:

fenc(Y ) = W T Y + benc,

fdec(Z) = W l(Z) + bdec

(5)

where the function l is a point-wise logistic non-linearity of the form:

(6)
with g ﬁxed gain. The system is characterized by an energy measuring the compatibility between
pairs of input Y and latent code Z, E(Y, Z) [16]. The lower the energy, the more compatible (or
likely) is the pair. We deﬁne the energy as:

l(x) = 1/(1 + exp(−gx)),

E(Y, Z) = αekZ − fenc(Y )k2

2 + kY − fdec(Z)k2
2

(7)

During training we minimize the following loss:

L(W, Y ) = E(Y, Z) + αsh(Z) + αrkW k1

(8)
The ﬁrst term tries to make the output of the encoder as similar as possible to the code Z. The second
term is the mean-squared error between the input Y and the reconstruction provided by the decoder.

2 + αsh(Z) + αrkW k1

= αekZ − fenc(Y )k2

2 + kY − fdec(Z)k2

3

acts independently on each code unit and it is deﬁned as h(Z) = PM

The third term ensures the sparsity of the code by penalizing non zero values of code units; this term
i=1 log(1+l2(zi)), (correspond-
ing to a factorized Student-t prior distribution on the non linearly transformed code units [8] through
the logistic of equation 6). The last term is an L1 regularization on the ﬁlters to suppress noise and
favor more localized ﬁlters. The loss formulated in equation 8 combines terms that characterize
also other methods. For instance, the ﬁrst two terms appear in our previous model [3], but in that
work, the weights of encoder and decoder were not tied and the parameters in the logistic were up-
dated using running averages. The second and third terms are present in the “decoder-only” model
proposed in [8]. The third term was used in the “encoder-only” model of [7]. Besides the already-
mentioned advantages of using an encoder-decoder architecture, we point out another good feature
of this algorithm due to its symmetry. A common idiosyncrasy for sparse-overcomplete methods
using both a reconstruction and a sparsity penalty in the objective function (second and third term in
equation 8), is the need to normalize the basis functions in the decoder during learning [8, 12] with
somewhat ad-hoc technique, otherwise some of the basis functions collapse to zero, and some blow
up to inﬁnity. Because of the sparsity penalty and the linear reconstruction, code units become tiny
and are compensated by the ﬁlters in the decoder that grow without bound. Even though the overall
loss decreases, training is unsuccessful. Unfortunately, simply normalizing the ﬁlters makes less
clear which objective function is minimized. Some authors have proposed quite expensive meth-
ods to solve this issue: by making better approximations of the posterior distribution [15], or by
using sampling techniques [17]. In this work, we propose to enforce symmetry between encoder
and decoder (through weight sharing) so as to have automatic scaling of ﬁlters. Their norm cannot
possibly be large because code units, produced by the encoder weights, would have large values as
well, producing bad reconstructions and increasing the energy (the second term in equation 7 and
8).

3 Learning Algorithm

Learning consists of determining the parameters in W , benc, and bdec that minimize the loss in
equation 8. As indicated in the introduction, the energy augmented with the sparsity constraint is
minimized with respect to the code to ﬁnd the optimal code. No marginalization over code distribu-
tion is performed. This is akin to using the loss function in equation 3. However, the log partition
function term is dropped. Instead, we rely on the code sparsity constraints to ensure that the energy
surface is not ﬂat.
Since the second term in equation 8 couples both Z and W and bdec, it is not straightforward to
minimize this energy with respect to both. On the other hand, once Z is given, the minimization
with respect to W is a convex quadratic problem. Vice versa, if the parameters W are ﬁxed, the
optimal code Z ∗ that minimizes L can be computed easily through gradient descent. This suggests
the following iterative on-line coordinate descent learning algorithm:
1. for a given sample Y and parameter setting, minimize the loss in equation 8 with respect to Z by
gradient descent to obtain the optimal code Z ∗
2. clamping both the input Y and the optimal code Z ∗ found at the previous step, do one step of
gradient descent to update the parameters.
Unlike other methods [8, 12], no column normalization of W is required. Also, all the parameters
are updated by gradient descent unlike in our previous work [3] where some parameters are updated
using a moving average.

After training, the system converges to a state where the decoder produces good reconstructions
from a sparse code, and the optimal code is predicted by a simple feed-forward propagation through
the encoder.

4 Comparative Coding Analysis

In the following sections, we mainly compare SESM with RBM in order to better understand their
differences in terms of maximum likelihood approximation, and in terms of coding efﬁciency and
robustness.
RBM
As explained in the introduction, RBMs minimize an approximation of the negative log
likelihood of the data under the model. An RBM is a binary stochastic symmetric machine deﬁned

4

decY . Although this is not
by an energy function of the form: E(Y, Z) = −Z T W T Y − bT
obvious at ﬁrst glance, this energy can be seen as a special case of the encoder-decoder architecture
that pertains to binary data vectors and code vectors [5]. Training an RBM minimizes an approxima-
tion of the negative log likelihood loss function 2, averaged over the training set, through a gradient
descent procedure. Instead of estimating the gradient of the log partition function, RBM training
uses contrastive divergence [10], which takes random samples drawn over a limited region Ω around
the training samples. The loss becomes:

encZ − bT

L(W, Y ) = −

1
β

logXz

e−βE(Y,z) +

1
β

log Xy∈ΩXz

e−βE(y,z)

(9)

Because of the RBM architecture, given a Y , the components of Z are independent, hence the sum
over conﬁgurations of Z can be done independently for each component of Z. Sampling y in the
neighborhood Ω is performed with one, or a few alternated MCMC steps over Y , and Z. This means
that only the energy of points around training samples is pulled up. Hence, the likelihood function
takes the right shape around the training samples, but not necessarily everywhere. However, the
code vector in an RBM is binary and noisy, and one may wonder whether this does not have the
effect of surreptitiously limiting the information content of the code, thereby further minimizing the
log partition function as a bonus.
SESM
RBM and SESM have almost the same architecture because they both have a symmetric
encoder and decoder, and a logistic non-linearity on the top of the encoder. However, RBM is trained
using (approximate) maximum likelihood, while SESM is trained by simply minimizing the average
energy F∞(Y ) of equation 4 with an additional code sparsity term. SESM relies on the sparsity
term to prevent ﬂat energy surfaces, while RBM relies on an explicit contrastive term in the loss, an
approximation of the log partition function. Also, the coding strategy is very different because code
units are “noisy” and binary in RBM, while they are quasi-binary and sparse in SESM. Features
extracted by SESM look like object parts (see next section), while features produced by RBM lack
an intuitive interpretation because they aim at modeling the input distribution and they are used in a
distributed representation.

4.1 Experimental Comparison

σq 1

P N kY − fdec( ¯Z)k2

In the ﬁrst experiment we have trained SESM, RBM, and PCA on the ﬁrst 20000 digits in the
MNIST training dataset [18] in order to produce codes with 200 components. Similarly to [15] we
have collected test image codes after the logistic non linearity (except for PCA which is linear), and
we have measured the root mean square error (RMSE) and the entropy. SESM was run for different
values of the sparsity coefﬁcient αs in equation 8 (while all other parameters are left unchanged, see
2, where ¯Z is the uniformly
next section for details). The RMSE is deﬁned as 1
quantized code produced by the encoder, P is the number of test samples, and σ is the estimated
variance of units in the input Y . Assuming to encode the (quantized) code units independently and
with the same distribution, the lower bound on the number of bits required to encode each of them
P M , where ci is the number of counts in the i-th bin, and Q
is the number of quantization levels. The number of bits per pixel is then equal to: M
N Hc.u.. Unlike
in [15, 12], the reconstruction is done taking the quantized code in order to measure the robustness
of the code to the quantization noise. As shown in ﬁg. 1-C, RBM is very robust to noise in the
code because it is trained by sampling. The opposite is true for PCA which achieves the lowest
RMSE when using high precision codes, but the highest RMSE when using a coarse quantization.
SESM seems to give the best trade-off between RMSE and entropy. Fig. 1-D/F compare the features
learned by SESM and RBM. Despite the similarities in the architecture, ﬁlters look quite different
in general, revealing two different coding strategies: distributed for RBM, and sparse for SESM.

is given by: Hc.u. = −PQ

ci
P M log2

i=1

ci

In the second experiment, we have compared these methods by means of a supervised task in order to
assess which method produces the most discriminative representation. Since we have available also
the labels in the MNIST, we have used the codes (produced by these machines trained unsupervised)
as input to the same linear classiﬁer. This is run for 100 epochs to minimize the squared error
between outputs and targets, and has a mild ridge regularizer. Fig. 1-A/B show the result of these
experiments in addition to what can be achieved by a linear classiﬁer trained on the raw pixel data.
Note that: 1) training on features instead of raw data improves the recognition (except for PCA

5

10 samples

100 samples

1000 samples

10 samples

100 samples

1000 samples

 

%
E
T
A
R
R
O
R
R
E

 

45

40

35

30

25

20

15

10

5

0
0

18

16

14

12

10

8

6

4

2

10

9

8

7

6

5

4

 

%
E
T
A
R
R
O
R
R
E

 

 

%
E
T
A
R
R
O
R
R
E

 

0.2

RMSE

0.4

0
0

0.4

3
0

0.2

RMSE

0.2

RMSE

0.4

45

40

35

30

25

20

15

10

5

 

%
E
T
A
R
R
O
R
R
E

 

18

16

14

12

10

8

6

4

2

10

 

RAW: train
RAW: test
PCA: train
PCA: test
RBM: train
RBM: test
SESM: train
SESM: test

9

8

7

6

5

4

 

%
E
T
A
R
R
O
R
R
E

 

 

%
E
T
A
R
R
O
R
R
E

 

0
0
2
ENTROPY (bits/pixel)

1

0
0
2
ENTROPY (bits/pixel)

1

3
 
0
2
ENTROPY (bits/pixel)

1

(A)

(B)

0.45

0.4

0.35

0.3

E
S
M
R

0.25

0.2

0.15

0.1

0.05
 
0

(C)

(E)

(G)

Symmetric Sparse Coding − RBM − PCA

 

PCA: quantization in 5 bins
PCA: quantization in 256 bins
RBM: quantization in 5 bins
RBM: quantization in 256 bins
Sparse Coding: quantization in 5 bins
Sparse Coding: quantization in 256 bins

0.5

1

Entropy (bits/pixel)

1.5

2

(D)

(F)

(H)

Figure 1: (A)-(B) Error rate on MNIST training (with 10, 100 and 1000 samples per class) and
test set produced by a linear classiﬁer trained on the codes produced by SESM, RBM, and PCA.
The entropy and RMSE refers to a quantization into 256 bins. The comparison has been extended
also to the same classiﬁer trained on raw pixel data (showing the advantage of extracting features).
The error bars refer to 1 std. dev. of the error rate for 10 random choices of training datasets
(same splits for all methods). The parameter αs in eq. 8 takes values: 1, 0.5, 0.2, 0.1, 0.05. (C)
Comparison between SESM, RBM, and PCA when quantizing the code into 5 and 256 bins. (D)
Random selection from the 200 linear ﬁlters that were learned by SESM (αs = 0.2). (E) Some pairs
of original and reconstructed digit from the code produced by the encoder in SESM (feed-forward
propagation through encoder and decoder). (F) Random selection of ﬁlters learned by RBM. (G)
Back-projection in image space of the ﬁlters learned in the second stage of the hierarchical feature
extractor. The second stage was trained on the non linearly transformed codes produced by the ﬁrst
stage machine. The back-projection has been performed by using a 1-of-10 code in the second stage
machine, and propagating this through the second stage decoder and ﬁrst stage decoder. The ﬁlters
at the second stage discover the class-prototypes (manually ordered for visual convenience) even
though no class label was ever used during training. (H) Feature extraction from 8x8 natural image
patches: some ﬁlters that were learned.

6

when the number of training samples is small), 2) RBM performance is competitive overall when
few training samples are available, 3) the best performance is achieved by SESM for a sparsity level
which trades off RMSE for entropy (overall for large training sets), 4) the method with the best
RMSE is not the one with lowest error rate, 5) compared to a SESM having the same error rate
RBM is more costly in terms of entropy.

5 Experiments

This section describes some experiments we have done with SESM. The coefﬁcient αe in equation 8
has always been set equal to 1, and the gain in the logistic have been set equal to 7 in order to achieve
a quasi-binary coding. The parameter αs has to be set by cross-validation to a value which depends
on the level of sparsity required by the speciﬁc application.

5.1 Handwritten Digits

Fig. 1-B/E shows the result of training a SESM with αs is equal to 0.2. Training was performed on
20000 digits scaled between 0 and 1, by setting αr to 0.0004 (in equation 8) with a learning rate
equal to 0.025 (decreased exponentially). Filters detect the strokes that can be combined to form a
digit. Even if the code unit activation has a very sparse distribution, reconstructions are very good
(no minimization in code space was performed).

5.1.1 Hierarchical Features

A hierarchical feature extractor can be trained layer-by-layer similarly to what has been proposed
in [19, 1] for training deep belief nets (DBNs). We have trained a second (higher) stage machine
on the non linearly transformed codes produced by the ﬁrst (lower) stage machine described in the
previous example. We used just 20000 codes to produce a higher level representation with just 10
components. Since we aimed to ﬁnd a 1-of-10 code we increased the sparsity level (in the second
stage machine) by setting αs to 1. Despite the completely unsupervised training procedure, the
feature detectors in the second stage machine look like digit prototypes as can be seen in ﬁg. 1-G.
The hierarchical unsupervised feature extractor is able to capture higher order correlations among
the input pixel intensities, and to discover the highly non-linear mapping from raw pixel data to the
class labels. Changing the random initialization can sometimes lead to the discover of two different
shapes of “9” without a unit encoding the “4”, for instance. Nevertheless, results are qualitatively
very similar to this one. For comparison, when training a DBN, prototypes are not recovered because
the learned code is distributed among units.

5.2 Natural Image Patches

A SESM with about the same set up was trained on a dataset of 30000 8x8 natural image patches
randomly extracted from the Berkeley segmentation dataset [20]. The input images were simply
scaled down to the range [0, 1.7], without even subtracting the mean. We have considered a 2
times overcomplete code with 128 units. The parameters αs, αr and the learning rate were set to
0.4, 0.025, and 0.001 respectively. Some ﬁlters are localized Gabor-like edge detectors in different
positions and orientations, other are more global, and some encode the mean value (see ﬁg. 1-H).

6 Conclusions

There are two strategies to train unsupervised machines: 1) having a contrastive term in the loss
function minimized during training, 2) constraining the internal representation in such a way that
training samples can be better reconstructed than other points in input space. We have shown that
RBM, which falls in the ﬁrst class of methods, is particularly robust to channel noise, it achieves very
low RMSE and good recognition rate. We have also proposed a novel symmetric sparse encoding
method following the second strategy which: is particularly efﬁcient to train, has fast inference,
works without requiring any withening or even mean removal from the input, can provide the best
recognition performance and trade-off between entropy/RMSE, and can be easily extended to a
hierarchy discovering hidden structure in the data. We have proposed an evaluation protocol to
compare different machines which is based on RMSE, entropy and, eventually, error rate when also

7

labels are available. Interestingly, the machine achieving the best performance in classiﬁcation is the
one with the best trade-off between reconstruction error and entropy. A future avenue of work is to
understand the reasons for this “coincidence”, and deeper connections between these two strategies.
Acknowledgments
We wish to thank Jonathan Goodman, Geoffrey Hinton, and Yoshua Bengio for helpful discussions. This work
was supported in part by NSF grant IIS-0535166 “toward category-level object recognition”, NSF ITR-0325463
“new directions in predictive learning”, and ONR grant N00014-07-1-0535 “integration and representation of
high dimensional data”.

References

[1] G.E. Hinton and R. R Salakhutdinov. Reducing the dimensionality of data with neural networks. Science,

313(5786):504–507, 2006.

[2] Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle. Greedy layer-wise training of deep networks. In

NIPS, 2006.

[3] M. Ranzato, C. Poultney, S. Chopra, and Y. LeCun. Efﬁcient learning of sparse representations with an

energy-based model. In NIPS 2006. MIT Press, 2006.

[4] Y. Bengio and Y. LeCun. Scaling learning algorithms towars ai. In D. DeCoste L. Bottou, O. Chapelle

and J. Weston, editors, Large-Scale Kernel Machines. MIT Press, 2007.

[5] M. Ranzato, Y. Boureau, S. Chopra, and Y. LeCun. A uniﬁed energy-based framework for unsupervised

learning. In Proc. Conference on AI and Statistics (AI-Stats), 2007.

[6] E. Doi, D. C. Balcan, and M. S. Lewicki. A theoretical analysis of robust coding over noisy overcomplete

channels. In NIPS. MIT Press, 2006.

[7] Y. W. Teh, M. Welling, S. Osindero, and G. E. Hinton. Energy-based models for sparse overcomplete

representations. Journal of Machine Learning Research, 4:1235–1260, 2003.

[8] B. A. Olshausen and D. J. Field. Sparse coding with an overcomplete basis set: a strategy employed by

v1? Vision Research, 37:3311–3325, 1997.

[9] D. D. Lee and H. S. Seung. Learning the parts of objects by non-negative matrix factorization. Nature,

401:788–791, 1999.

[10] G.E. Hinton. Training products of experts by minimizing contrastive divergence. Neural Computation,

14:1771–1800, 2002.

[11] P. Lennie. The cost of cortical computation. Current biology, 13:493–497, 2003.
[12] J.F. Murray and K. Kreutz-Delgado. Learning sparse overcomplete codes for images. The Journal of

VLSI Signal Processing, 45:97–110, 2008.

[13] G.E. Hinton and R.S. Zemel. Autoencoders, minimum description length, and helmholtz free energy. In

NIPS, 1994.

[14] G.E. Hinton, P. Dayan, and M. Revow. Modeling the manifolds of images of handwritten digits. IEEE

Transactions on Neural Networks, 8:65–74, 1997.

[15] M.S. Lewicki and T.J. Sejnowski. Learning overcomplete representations. Neural Computation, 12:337–

365, 2000.

[16] Y. LeCun, S. Chopra, R. Hadsell, M. Ranzato, and F.J. Huang. A tutorial on energy-based learning. In

G. Bakir and al.., editors, Predicting Structured Data. MIT Press, 2006.

[17] P. Sallee and B.A. Olshausen. Learning sparse multiscale image representations. In NIPS. MIT Press,

2002.

[18] http://yann.lecun.com/exdb/mnist/.
[19] G.E. Hinton, S. Osindero, and Y.-W. Teh. A fast learning algorithm for deep belief nets. Neural Compu-

tation, 18:1527–1554, 2006.

[20] http://www.cs.berkeley.edu/projects/vision/grouping/segbench/.

8

"
1101,2007,Receding Horizon Differential Dynamic Programming,"The control of high-dimensional, continuous, non-linear systems is a key problem in reinforcement learning and control. Local, trajectory-based methods, using techniques such as Differential Dynamic Programming (DDP) are not directly subject to the curse of dimensionality, but generate only local controllers. In this paper, we introduce Receding Horizon DDP (RH-DDP), an extension to the classic DDP algorithm, which allows us to construct stable and robust controllers based on a library of local-control trajectories. We demonstrate the effectiveness of our approach on a series of high-dimensional control problems using a simulated multi-link swimming robot. These experiments show that our approach effectively circumvents dimensionality issues, and is capable of dealing effectively with problems with (at least) 34 state and 14 action dimensions.","Receding Horizon

Differential Dynamic Programming

Yuval Tassa ∗

Tom Erez & Bill Smart †

Abstract

The control of high-dimensional, continuous, non-linear dynamical systems is a
key problem in reinforcement learning and control. Local, trajectory-based meth-
ods, using techniques such as Differential Dynamic Programming (DDP), are not
directly subject to the curse of dimensionality, but generate only local controllers.
In this paper,we introduce Receding Horizon DDP (RH-DDP), an extension to the
classic DDP algorithm, which allows us to construct stable and robust controllers
based on a library of local-control trajectories. We demonstrate the effective-
ness of our approach on a series of high-dimensional problems using a simulated
multi-link swimming robot. These experiments show that our approach effectively
circumvents dimensionality issues, and is capable of dealing with problems of (at
least) 24 state and 9 action dimensions.

1 Introduction

We are interested in learning controllers for high-dimensional, highly non-linear dynamical systems,
continuous in state, action, and time. Local, trajectory-based methods, using techniques such as Dif-
ferential Dynamic Programming (DDP), are an active ﬁeld of research in the Reinforcement Learn-
ing and Control communities. Local methods do not model the value function or policy over the
entire state space by focusing computational effort along likely trajectories. Featuring algorithmic
complexity polynomial in the dimension, local methods are not directly affected by dimensionality
issues as space-ﬁlling methods.
In this paper, we introduce Receding Horizon DDP (RH-DDP), a set of modiﬁcations to the classic
DDP algorithm, which allows us to construct stable and robust controllers based on local-control
trajectories in highly non-linear, high-dimensional domains. Our new algorithm is reminiscent of
Model Predictive Control, and enables us to form a time-independent value function approximation
along a trajectory. We aggregate several such trajectories into a library of locally-optimal linear
controllers which we then select from, using a nearest-neighbor rule.
Although we present several algorithmic contributions, a main aspect of this paper is a conceptual
one. Unlike much of recent related work (below), we are not interested in learning to follow a
pre-supplied reference trajectory. We deﬁne a reward function which represents a global measure
of performance relative to a high level objective, such as swimming towards a target. Rather than
a reward based on distance from a given desired conﬁguration, a notion which has its roots in the
control community’s deﬁnition of the problem, this global reward dispenses with a “path planning”
component and requires the controller to solve the entire problem.
We demonstrate the utility of our approach by learning controllers for a high-dimensional simulation
of a planar, multi-link swimming robot. The swimmer is a model of an actuated chain of links
in a viscous medium, with two location and velocity coordinate pairs, and an angle and angular
velocity for each link. The controller must determine the applied torque, one action dimension for

∗Y. Tassa is with the Hebrew University, Jerusalem, Israel.
†T. Erez and W.D. Smart are with the Washington University in St. Louis, MO, USA.

1

each articulated joint. We reward controllers that cause the swimmer to swim to a target, brake on
approach and come to a stop over it.
We synthesize controllers for several swimmers, with state dimensions ranging from 10 to 24 dimen-
sions. The controllers are shown to exhibit complex locomotive behaviour in response to real-time
simulated interaction with a user-controlled target.

1.1 Related work

Optimal control of continuous non-linear dynamical systems is a central research goal of the RL
community. Even when important ingredients such as stochasticity and on-line learning are re-
moved, the exponential dependence of computational complexity on the dimensionality of the do-
main remains a major computational obstacle. Methods designed to alleviate the curse of dimen-
sionality include adaptive discretizations of the state space [1], and various domain-speciﬁc manip-
ulations [2] which reduce the effective dimensionality.
Local trajectory-based methods such as DDP were introduced to the NIPS community in [3], where
a local-global hybrid method is employed. Although DDP is used there, it is considered an aid to the
global approximator, and the local controllers are constant rather than locally-linear. In this decade
DDP was reintroduced by several authors.
In [4] the idea of using the second order local DDP
models to make locally-linear controllers is introduced. In [5] DDP was applied to the challenging
high-dimensional domain of autonomous helicopter control, using a reference trajectory.
In [6]
a minimax variant of DDP is used to learn a controller for bipedal walking, again by designing
a reference trajectory and rewarding the walker for tracking it. In [7], trajectory-based methods
including DDP are examined as possible models for biological nervous systems. Local methods
have also been used for purely policy-based algorithms [8, 9, 10], without explicit representation of
the value function.
The best known work regarding the swimming domain is that by Ijspeert and colleagues (e.g. [11])
using Central Pattern Generators. While the inherently stable domain of swimming allows for such
open-loop control schemes, articulated complex behaviours such as turning and tracking necessitate
full feedback control which CPGs do not provide.

2 Methods

2.1 Deﬁnition of the problem
We consider the discrete-time dynamics xk+1 = F (xk, uk) with states x ∈ Rn and actions u ∈ Rm.
In this context we assume F (xk, uk) = xk +
0 f(x(t), uk)dt for a continuous f and a small ∆t,
approximating the continuous problem and identifying with it in the ∆t → 0 limit. Given some
scalar reward function r(x, u) and a ﬁxed initial state x1 (superscripts indicating the time index), we
wish to ﬁnd the policy which maximizes the total reward1 acquired over a ﬁnite temporal horizon:

(cid:82) ∆t

π∗(xk, k) = argmax
π(·,·)

[

r(xi, π(xi, i))].

N(cid:88)

i=k

The quantity maximized on the RHS is the value function, which solves Bellman’s equation:

V (x, k) = max

[r(x, u) + V (F (x, u), k+1)].

(1)

u

Each of the functions in the sequence {V (x, k)}N
k=1 describes the optimal reward-to-go of the opti-
mization subproblem from k to N. This is a manifestation of the dynamic programming principle. If
N = ∞, essentially eliminating the distinction between different time-steps, the sequence collapses
to a global, time-independent value function V (x).

2.2 DDP

Differential Dynamic Programming [12, 13] is an iterative improvement scheme which ﬁnds a
locally-optimal trajectory emanating from a ﬁxed starting point x1. At every iteration, an approx-

1We (arbitrarily) choose to use phrasing in terms of reward-maximization, rather than cost-minimization.

2

imation to the time-dependent value function is constructed along the current trajectory {xk}N
which is formed by iterative application of F using the current control sequence {uk}N
iteration is comprised of two sweeps of the trajectory: a backward and a forward sweep.
In the backward sweep, we proceed backwards in time to generate local models of V in the following
manner. Given quadratic models of V (xk+1, k + 1), F (xk, uk) and r(xk, uk), we can approximate
the unmaximised value function, or Q-function,

k=1,
k=1. Every

Q(xk, uk) = r(xk, uk) + V k+1(F (xk, uk))

as a quadratic model around the present state-action pair (xk, uk):

Q(xk + δx, uk + δu) ≈ Q0 + Qxδx + Quδu +

1
2

[δxT δuT ]

Qxx Qxu
Qux Quu

(cid:34)

(cid:35)(cid:104)

(cid:105)

δx
δu

(2)

(3)

Where the coefﬁcients Q(cid:63)(cid:63) are computed by equating coefﬁcients of similar powers in the second-
order expansion of (2)

Qx = rx + V k+1
Qu = ru + V k+1

xx F k
xx F k
xx F k
Once the local model of Q is obtained, the maximizing δu is solved for

Qxx = rxx + F k
Quu = ruu + F k
Qxu = rxu + F k

x V k+1
u V k+1
x V k+1

x F k
x
x F k
u

x + V k+1
u + V k+1
u + V k+1

x F k
xx
x F k
uu
x F k
xu.

∗ = argmax

δu

δu

[Q(xk + δx, uk + δu)] = −Q−1

uu (Qu + Quxδx)

and plugged back into (3) to obtain a quadratic approximation of V k:

(4)

(5)

0 = V k+1
V k
x = Qk+1
V k
xx = Qk+1
V k

0 − Qu(Quu)−1 Qu
x − Qu(Quu)−1 Qux
xx − Qxu(Quu)−1Qux.

(6a)
(6b)
(6c)
This quadratic model can now serve to propagate the approximation to V k−1. Thus, equations (4),
(5) and (6) iterate in the backward sweep, computing a local model of the Value function along
with a modiﬁcation to the policy in the form of an open-loop term −Q−1
uu Qu and a feedback term
−Q−1
uu Quxδx, essentially solving a local linear-quadratic problem in each step. In some senses, DDP
can be viewed as dual to the Extended Kalman Filter (though employing a higher order expansion
of F ).
In the forward sweep of the DDP iteration, both the open-loop and feedback terms are combined to
create a new control sequence (ˆuk)N

k=1 which results in a new nominal trajectory (ˆxk)N

k=1.

ˆx1 = x1
ˆuk = uk − Q−1
ˆxk+1 = F (ˆxk, ˆuk)

(7a)
(7b)
(7c)
We note that in practice the inversion in (5) must be conditioned. We use a Levenberg Marquardt-
like scheme similar to the ones proposed in [14]. Similarly, the u-update in (7b) is performed with
an adaptive line search scheme similar to the ones described in [15].

uu Qux(ˆxk − xk)

uu Qu − Q−1

2.2.1 Complexity and convergence

The leading complexity term of one iteration of DDP itself, assuming the model of F as required for
(4) is given, is O(N mγ1) for computing (6) N times, with 2 < γ1 < 3, the complexity-exponent of
inverting Quu. In practice, the greater part of the computational effort is devoted to the measurement
of the dynamical quantities in (4) or in the propagation of collocation vectors as described below.
DDP is a second order algorithm with convergence properties similar to, or better than Newton’s
method performed on the full vectorial uk with an exact N m × N m Hessian [16]. In practice,
convergence can be expected after 10-100 iterations, with the stopping criterion easily determined
as the size of the policy update plummets near the minimum.

3

2.2.2 Collocation Vectors

We use a new method of obtaining the quadratic model of Q (Eq. (2)), inspired by [17]2. Instead
of using (4), we ﬁt this quadratic model to samples of the value function at a cloud of collocation
vectors {xk
i }i=1..p, spanning the neighborhood of every state-action pair along the trajectory.
We can directly measure r(xk
i ) for each point in the cloud, and by using the
approximated value function at the next time step, we can estimate the value of (2) at every point:

i ) and F (xk

i , uk

i , uk

i , uk

q(xk

i , uk

i ) = r(xk

i , uk

i ) + V k+1(F (xk

i , uk

i ))

i , uk

i , uk

i ) and (xk

Then, we can insert the values of q(xk
i ) on the LHS and RHS of (3) respectively,
and solve this set of p linear equations for the Q(cid:63)(cid:63) terms. If p > (3(n + m) + (m + n)2)/2, and
the cloud is in general conﬁguration, the equations are non-singular and can be easily solved by a
generic linear algebra package.
There are several advantages to using such a scheme. The full nonlinear model of F is used to
construct Q, rather than only a second-order approximation. Fxx, which is an n× n× n tensor need
not be stored. The addition of more vectors can allow the modeling of noise, as suggested in [17].
In addition, this method allows us to more easily apply general coordinate transformations in order
to represent V in some internal space, perhaps of lower dimension.
The main drawback of this scheme is the additional complexity of an O(N pγ2) term for solving the
p-equation linear system. Because we can choose {xk
i } in way which makes the linear system
sparse, we can enjoy the γ2 < γ1 of sparse methods and, at least for the experiments performed
here, increase the running time only by a small factor.
In the same manner that DDP is dually reminiscent of the Extended Kalman Filter, this method bears
a resemblance to the test vectors propagated in the Unscented Kalman Filter [18], although we use
a quadratic, rather than linear number of collocation vectors.

i , uk

2.3 Receding Horizon DDP

When seeking to synthesize a global controller from many local controllers, it is essential that the
different local components operate synergistically. In our context this means that local models of
the value function must all model the same function, which is not the case for the standard DDP
solution. The local quadratic models which DDP computes around the trajectory are approximations
to V (x, k), the time-dependent value function. The standard method in RL for creating a global
value function is to use an exponentially discounted horizon. Here we propose a ﬁxed-length non-
discounted Receding Horizon scheme in the spirit of Model Predictive Control [19].
Having computed a DDP solution to some problem starting from many different starting points
x1, we can discard all the models computed for points xk>1 and save only the ones around the
x1’s. Although in this way we could accumulate a time-independent approximation to V (x, N)
only, starting each run of N-step DDP from scratch would be prohibitively expensive. We therefore
propose the following: After obtaining the solution starting from x1, we save the local model at
k = 1 and proceed to solve a new N-step problem starting at x2, this time initialized with the
policy obtained on the previous run, shifted by one time-step, and appended with the last control
unew = [u2, u3...uN uN ]. Because this control sequence is very close to the optimal solution, the
second-order convergence of DDP is in full effect and the algorithm converges in 1 or 2 sweeps.
Again saving the model at the ﬁrst time step, we iterate. We stress the that without the fast and exact
convergence properties of DDP near the maximum, this algorithm would be far less effective.

2.4 Nearest Neighbor control with Trajectory Library

A run of DDP computes a locally quadratic model of V and a locally linear model of u, expressed by
the gain term −Q−1
uu Qux. This term generalizes the open-loop policy to a tube around the trajectory,
inside of which a basin-of-attraction is formed. Having lost the dependency on the time k with
the receding-horizon scheme, we need some space-based method of determining which local gain
model we select at a given state. The simplest choice, which we use here, is to select the nearest
Euclidian neighbor.

2Our method is a speciﬁc instantiation of a more general algorithm described therein.

4

Outside of the basin-of-attraction of a single trajectory, we can expect the policy to perform very
poorly and lead to numerical divergence if no constraint on the size of u is enforced. A possible
solution to this problem is to ﬁll some volume of the state space with a library of local-control
trajectories [20], and consider all of them when selecting the nearest linear gain model.

3 Experiments

3.1 The swimmer dynamical system

(cid:161) cos(θ)

(cid:162)

(cid:88)

i

(cid:80)

(cid:162)

(cid:161) − sin(θ)

We describe a variation of the d-link swimmer dynamical system [21]. A stick or link of length
l, lying in a plane at an angle θ to some direction, parallel to ˆt =
and perpendicular to
, moving with velocity ˙x in a viscous ﬂuid, is postulated to admit a normal frictional
ˆn =
force −knlˆn( ˙x · ˆn) and a tangential frictional force −ktlˆt( ˙x · ˆt), with kn > kt > 0. The swimmer
is modeled as a chain of d such links of lengths li and masses mi, its conﬁguration described by
the generalized coordinates q = ( xcm
θ ), of two center-of-mass coordinates and d angles. Letting
¯xi = xi − xcm be the positions of the link centers WRT the center of mass , the Lagrangian is

cos(θ)

sin(θ)

(cid:88)

(cid:88)

L = 1

2 ˙x2

cm

mi + 1
2

i

i

mi ˙¯x2

i + 1

2

˙θ2

i

Ii

12 mil2

with Ii = 1
and angles of the links is given by the d − 1 equations ¯xi+1 − ¯xi = 1
express the joining of successive links, and by the equation

i the moments-of-inertia. The relationship between the relative position vectors
2 liˆti, which
i mi¯xi = 0 which comes from the

2 li+1ˆti+1 + 1

(a) Time course of two angular velocities.

(b) State projection.

(a) three snapshots of the receding horizon trajectory (dotted)
Figure 1: RH-DDP trajectories.
with the current ﬁnite-horizon optimal trajectory (solid) appended, for two state dimensions. (b)
Projections of the same receding-horizon trajectories onto the largest three eigenvectors of the full
state covariance matrix. As described in Section 3.3, the linear regime of the reward, here applied
to a 3-swimmer, compels the RH trajectories to a steady swimming gait – a limit cycle.

5

deﬁnition of the ¯xi’s relative to the center-of-mass. The function
i ] − 1
˙θ2
2 kt

[li( ˙xi · ˆni)2 + 1

F = − 1

12 l3

2 kn

i

(cid:88)

i

(cid:88)

i

li( ˙xi · ˆti)2

known as the dissipation function, is that function whose derivatives WRT the ˙qi’s provide the postu-
lated frictional forces. With these in place, we can obtain ¨q from the 2+d Euler-Lagrange equations:

d

dt( ∂
∂qi

L) = ∂
∂ ˙qi

F + u

with u being the external forces and torques applied to the system. By applying d − 1 torques τj
in action-reaction pairs at the joints ui = τi − τi−1, the isolated nature of the dynamical system
is preserved. Performing the differentiations, solving for ¨q, and letting x =
be the 4 + 2d-
dimensional state variable, ﬁnally gives the dynamics ˙x = ( ˙q

˙q

¨q ) = f(x, u).

(cid:161) q

(cid:162)

3.2

Internal coordinates

The two coordinates specifying the position of the center-of-mass and the d angles are deﬁned
relative to an external coordinate system, which the controller should not have access to. We make
a coordinate transformation into internal coordinates, where only the d−1 relative angles {ˆθj =
θj+1 − θj}d−1
j=1 are given, and the location of the target is given relative to coordinate system ﬁxed
on one of the links. This makes the learning isotropic and independent of a speciﬁc location on the
plane. The collocation method allows us to perform this transformation directly on the vector cloud
without having to explicitly differentiate it, as we would have had to using classical DDP. Note also
that this transformation reduces the dimension of the state (one angle less), suggesting the possibility
of further dimensionality reduction.

3.3 The reward function

The reward function we used was

r(x, u) = −cx

(cid:112)||xnose||2 + 1

||xnose||2

− cu||u||2

(8)

Where xnose = [x1x2]T is the 2-vector from some designated point on the swimmer’s body to the
target (the origin in internal space), and cx and cu are positive constants. This reward is maximized
when the nose is brought to rest on the target under a quadratic action-cost penalty. It should not be
confused with the desired state reward of classical optimal control since values are speciﬁed only
for 2 out of the 2d + 4 coordinates. The functional form of the target-reward term is designed to
be linear in ||xnose|| when far from the target and quadratic when close to it (Figure 2(b)). Because

(a) Swimmer

Figure 2: (a) A 5-swimmer with the “nose” point at its tip and a ring-shaped target. (b) The func-
tional form of the planar reward component r(xnose) = −||xnose||2/
translates into a steady swimming gait at large distances with a smooth braking and stopping at the
goal.

(b) Reward

(cid:112)||xnose||2 + 1. This form

6

of the differentiation in Eq. (5), the solution is independent of V0, the constant part of the value.
Therefore, in the linear regime of the reward function, the solution is independent of the distance
from the target, and all the trajectories are quickly compelled to converge to a one-dimensional
manifold in state-space which describes steady-state swimming (Figure 1(b)). Upon nearing the
target, the swimmer must initiate a braking maneuver, and bring the nose to a standstill over the
target. For targets that are near the swimmer, the behaviour must also include various turns and
jerks, quite different from steady-state swimming, which maneuver the nose into contact with the
target. Our experience during interaction with the controller, as detailed below, leads us to believe
that the behavioral variety that would be exhibited by a hypothetical exact optimal controller for this
system to be extremely large.

4 Results

(cid:82) t+∆t

In order to asses the controllers we constructed a real-time interaction package3. By dragging the
target with a cursor, a user can interact with controlled swimmers of 3 to 10 links with a state di-
mension varying from 10 to 24, respectively. Even with controllers composed of a single trajectory,
the swimmers perform quite well, turning, tracking and braking on approach to the target.
All of the controllers in the package control swimmers with unit link lengths and unit masses. The
normal-to-tangential drag coefﬁcient ratio was kn/kt = 25. The function F computes a single 4th-
order Runge-Kutta integration step of the continuous dynamics F (xk, uk) = xk+
f(xk, uk)dt
t
with ∆t=0.05s. The receding horizon window was of 40 time-steps, or 2 seconds.
When the state doesn’t gravitate to one of the basins of attraction around the trajectories, numerical
divergence can occur. This effect can be initiated by the user by quickly moving the target to a
“surprising” location. Because nonlinear viscosity effects are not modeled and the local controllers
are also linear, exponentially diverging torques and angular velocities can be produced. When adding
as few as 20 additional trajectories, divergence is almost completely avoided.
Another claim which may be made is that there is no guarantee that the solutions obtained, even on
the trajectories, are in fact optimal. Because DDP is a local optimization method, it is bound to stop
in a local minimum. An extension of this claim is that even if the solutions are optimal, this has to
do with the swimmer domain itself, which might be inherently convex in some sense and therefore
an “easy” problem.
While both divergence and local minima are serious issues, they can both be addressed by appealing
to our panoramic motivation in the biology. Real organisms cannot apply unbounded torque. By
hard-limiting the torque to large but ﬁnite values, non-divergence can be guaranteed4. Similarly,
local minima exist even in the motor behaviour of the most complex organisms, famously evidenced
by Fosbury’s reinvention of the high jump.
Regarding the easiness or difﬁculty of the swimmer problem – we made the documented code avail-
able and hope that it might serve as a useful benchmark for other algorithms.

5 Conclusions

The signiﬁcance of this work lies at its outlining of a new kind of tradeoff in nonlinear motor control
design. If biological realism is an accepted design goal, and physical and biological constraints taken
into account, then the expectations we have from our controllers can be more relaxed than those of
the control engineer. The unavoidable eventual failure of any speciﬁc biological organism makes
the design of truly robust controllers a futile endeavor, in effect putting more weight on the mode,
rather than the tail of the behavioral distribution. In return for this forfeiture of global guarantees,
we gain very high performance in a small but very dense sub-manifold of the state-space.

3Available at http://alice.nc.huji.ac.il/∼tassa/
4We actually constrain angular velocities since limiting torque would require a stiffer integrator, but theo-
retical non-divergence is fully guaranteed by the viscous dissipation which enforces a Lyapunov function on
the entire system, once torques are limited.

7

Since we make use of biologically grounded arguments, we brieﬂy outline the possible implications
of this work to biological nervous systems. It is commonly acknowledged, due both to theoretical
arguments and empirical ﬁndings, that some form of dimensionality reduction must be at work in
neural control mechanisms. A common object in models which attempt to describe this reduction
is the motor primitive, a hypothesized atomic motor program which is combined with other such
programs in a small “alphabet”, to produce complex behaviors in a given context. Our controllers
imply a different reduction: a set of complex prototypical motor programs, each of which is near-
optimal only in a small volume of the state-space, yet in that space describes the entire complexity of
the solution. Giving the simplest building blocks of the model such a high degree of task speciﬁcity
or context, would imply a very large number of these motor prototypes in a real nervous system, an
order of magnitude analogous, in our linguistic metaphor, to that of words and concepts.

References
[1] Remi Munos and Andrew W. Moore. Variable Resolution Discretization for High-Accuracy Solutions of
Optimal Control Problems. In International Joint Conference on Artiﬁcial Intelligence, pages 1348–1355,
1999.

[2] M. Stilman, C. G. Atkeson, J. J. Kuffner, and G. Zeglin. Dynamic programming in reduced dimensional
spaces: Dynamic planning for robust biped locomotion. In Proceedings of the 2005 IEEE International
Conference on Robotics and Automation (ICRA 2005), pages 2399–2404, 2005.

[3] Christopher G. Atkeson. Using local trajectory optimizers to speed up global optimization in dynamic

programming. In NIPS, pages 663–670, 1993.

[4] C. G. Atkeson and J. Morimoto. Non-parametric representation of a policies and value functions: A

trajectory based approach. In Advances in Neural Information Processing Systems 15, 2003.

[5] P. Abbeel, A. Coates, M. Quigley, and A. Y. Ng. An application of reinforcement learning to aerobatic

helicopter ﬂight. In Advances in Neural Information Processing Systems 19, 2007.

[6] J. Morimoto and C. G. Atkeson. Minimax differential dynamic programming: An application to robust

bipedwalking. In Advances in Neural Information Processing Systems 14, 2002.

[7] Emanuel Todorov and Wei-Wei Li. Optimal control methods suitable for biomechanical systems. In 25th

Annual Int. Conf. IEE Engineering in Medicine and Biology Society, 2003.

[8] R. Munos. Policy gradient in continuous time. Journal of Machine Learning Research, 7:771–791, 2006.
[9] J. Peters and S. Schaal. Reinforcement learning for parameterized motor primitives. In Proceedings of

the IEEE International Joint Conference on Neural Networks (IJCNN 2006), 2006.

[10] Tom Erez and William D. Smart. Bipedal walking on rough terrain using manifold control. In IEEE/RSJ

International Conference on Robots and Systems (IROS), 2007.

[11] A. Crespi and A. Ijspeert. AmphiBot II: An amphibious snake robot that crawls and swims using a central
pattern generator. In Proceedings of the 9th International Conference on Climbing and Walking Robots
(CLAWAR 2006), pages 19–27, 2006.

[12] D. Q. Mayne. A second order gradient method for determining optimal trajectories for non-linear discrete-

time systems. International Journal of Control, 3:85–95, 1966.

[13] D. H. Jacobson and D. Q. Mayne. Differential Dynamic Programming. Elsevier, 1970.
[14] L.-Z. Liao and C. A. Shoemaker. Convergence in unconstrained discrete-time differential dynamic pro-

gramming. IEEE Transactions on Automatic Control, 36(6):692–706, 1991.

[15] S. Yakowitz. Algorithms and computational techniques in differential dynamic programming. Control

and Dynamic Systems: Advances in Theory and Applications, 31:75–91, 1989.

[16] L.-Z. Liao and C. A. Shoemaker. Advantages of differential dynamic programming over newton’s method

[17] E. Todorov.

for discrete-time optimal control problems. Technical Report 92-097, Cornell Theory Center, 1992.
www.cogsci.ucsd.edu/∼todorov/papers/ildp.pdf, 2007.

Iterative local dynamic programming. Manuscript under

review, available at

[18] S. J. Julier and J. K. Uhlmann. A new extension of the kalman ﬁlter to nonlinear systems. In Proceedings

of AeroSense: The 11th Int. Symp. on Aerospace/Defence Sensing, Simulation and Controls, 1997.

[19] C. E. Garcia, D. M. Prett, and M. Morari. Model predictive control: theory and practice. Automatica, 25:

335–348, 1989.

[20] M. Stolle and C. G. Atkeson. Policies based on trajectory libraries. In Proceedings of the International

Conference on Robotics and Automation (ICRA 2006), 2006.

[21] R. Coulom. Reinforcement Learning Using Neural Networks, with Applications to Motor Control. PhD

thesis, Institut National Polytechnique de Grenoble, 2002.

8

"
581,2007,A Risk Minimization Principle for a Class of Parzen Estimators,"This paper explores the use of a Maximal Average Margin (MAM) optimality principle for the design of learning algorithms. It is shown that the application of this risk minimization principle results in a class of (computationally) simple learning machines similar to the classical Parzen window classifier. A direct relation with the Rademacher complexities is established, as such facilitating analysis and providing a notion of certainty of prediction. This analysis is related to Support Vector Machines by means of a margin transformation. The power of the MAM principle is illustrated further by application to ordinal regression tasks, resulting in an $O(n)$ algorithm able to process large datasets in reasonable time.","A Risk Minimization Principle
for a Class of Parzen Estimators

Kristiaan Pelckmans, Johan A.K. Suykens, Bart De Moor
Department of Electrical Engineering (ESAT) - SCD/SISTA

K.U.Leuven University

Kasteelpark Arenberg 10, Leuven, Belgium

Kristiaan.Pelckmans@esat.kuleuven.be

Abstract

This paper1 explores the use of a Maximal Average Margin (MAM) optimality
principle for the design of learning algorithms. It is shown that the application
of this risk minimization principle results in a class of (computationally) simple
learning machines similar to the classical Parzen window classiﬁer. A direct rela-
tion with the Rademacher complexities is established, as such facilitating analysis
and providing a notion of certainty of prediction. This analysis is related to Sup-
port Vector Machines by means of a margin transformation. The power of the
MAM principle is illustrated further by application to ordinal regression tasks,
resulting in an O(n) algorithm able to process large datasets in reasonable time.

1 Introduction

The quest for efﬁcient machine learning techniques which (a) have favorable generalization capac-
ities, (b) are ﬂexible for adaptation to a speciﬁc task, and (c) are cheap to implement is a pervasive
theme in literature, see e.g. [14] and references therein. This paper introduces a novel concept for
designing a learning algorithm, namely the Maximal Average Margin (MAM) principle. It closely
resembles the classical notion of maximal margin as lying on the basis of perceptrons, Support Vec-
tor Machines (SVMs) and boosting algorithms, see a.o. [14, 11]. It however optimizes the average
margin of points to the (hypothesis) hyperplane, instead of the worst case margin as traditional. The
full margin distribution was studied earlier in e.g. [13], and theoretical results were extended and
incorporated in a learning algorithm in [5].

The contribution of this paper is twofold. On a methodological level, we relate (i) results in structural
risk minimization, (ii) data-dependent (but dimension-independent) Rademacher complexities [8, 1,
14] and a new concept of ’certainty of prediction’, (iii) the notion of margin (as central is most
state-of-the-art learning machines), and (iv) statistical estimators as Parzen windows and Nadaraya-
Watson kernel estimators.
In [10], the principle was already shown to underlie the approach of
mincuts for transductive inference over a weighted undirected graph. Further, consider the model-
class consisting of all models with bounded average margin (or classes with a ﬁxed Rademacher
complexity as we will indicate lateron). The set of such classes is clearly nested, enabling structural
risk minimization [8].

On a practical level, we show how the optimality principle can be used for designing a computation-
ally fast approach to (large-scale) classiﬁcation and ordinal regression tasks, much along the same

1Acknowledgements - K. Pelckmans is supported by an FWO PDM. J.A.K. Suykens and B. De Moor are a
(full) professor at the Katholieke Universiteit Leuven, Belgium. Research supported by Research Council KUL:
GOA AMBioRICS, CoE EF/05/006 OPTEC, IOF-SCORES4CHEM, several PhD/postdoc & fellow grants;
Flemish Government: FWO: PhD/postdoc grants, projects G.0452.04, G.0499.04, G.0211.05, G.0226.06,
G.0321.06, G.0302.07, (ICCoS, ANMMM, MLDM); IWT: PhD Grants, McKnow-E, Eureka-Flite+ Belgian
Federal Science Policy Ofﬁce: IUAP P6/04, EU: ERNSI;

1

lines as Parzen classiﬁers and Nadaraya-Watson estimators. It becomes clear that this result enables
researchers on Parzen windows to beneﬁt directly from recent advances in kernel machines, two
ﬁelds which have evolved mostly separately. It must be emphasized that the resulting learning rules
were already studied in different forms and motivated by asymptotic and geometric arguments, as
e.g. the Parzen window classiﬁer [4], the ’simple classiﬁer’ as in [12] chap. 1, probabilistic neural
networks [15], while in this paper we show how an (empirical) risk based optimality criterion un-
derlies this approach. A number of experiments conﬁrm the use of the resulting cheap learning rules
for providing a reasonable (baseline) performance in a small time-window.

The following notational conventions are used throughout the paper. Let the random vector
(X, Y ) ∈ Rd × {−1, 1} obey a (ﬁxed but unknown) joint distribution PXY from a probability
space (Rd×{−1, 1},P). Let Dn = {(Xi, Yi)}n
i=1 be sampled i.i.d. according to PXY . Let y ∈ Rn
be deﬁned as y = (Y1, . . . , Yn)T ∈ {−1, 1}n and X = (X1, . . . , Xn)T ∈ Rn×d. This paper
is organized as follows. The next section illustrates the principle of maximal average margin for
classiﬁcation problems. Section 3 investigates the close relationship with Rademacher complexi-
ties, Section 4 develops the maximal average margin principle for ordinal regression, and Section
5 reports experimental results of application of the MAM to classiﬁcation and ordinal regression
tasks.

2 Maximal Average Margin for Classiﬁers

2.1 The Linear Case

Let the class of hypotheses be deﬁned as

H =nf (·) : Rd → R, w ∈ Rd(cid:12)(cid:12)(cid:12)∀x ∈ Rd : f (x) = wT x, kwk2 = 1o .

Consequently, the signed distance of a sample (X, Y ) to the hyper-plane wT x = 0, or the margin
M (w) ∈ R, can be deﬁned as

(1)

M (w) =

.

(2)

SVMs maximize the worst-case margin. We instead focus on the ﬁrst moment of the margin distri-
bution. Maximizing the expected (average) margin follows from solving

Y (wT X)
kwk2

M∗ = max

w

E(cid:20) Y (wT X)

kwk2 (cid:21) = max

f∈H

E [Y f (X)] .

(3)

Remark that the non-separable case does not require the need for slack-variables. The empirical
counterpart becomes

ˆM = max

w

1
n

n

Xi=1

Yi(wT Xi)

kwk2

,

(4)

nPn
which can be written as a constrained convex problem as minw − 1
i=1 Yi(wT Xi) s.t. kwk2 ≤
nPn
1. The Lagrangian with multiplier λ ≥ 0 becomes L(w, λ) = − 1
2 (wT w − 1).
i=1 Yi(wT Xi) + λ
By switching the minimax problem to a maximin problem (application of Slater’s condition), the
ﬁrst order condition for optimality ∂L(w,λ)

∂w = 0 gives

wn =

1
λn

YiXi =

1
λn

XT y,

(5)

n

Xi=1

where wn ∈ Rd denotes the optimum to (4). The corresponding parameter λ can be found by
n kPn
npyT XXT y since the
substituting (5) in the constraint wT w = 1, or λ = 1
optimum is obviously taking place when wT w = 1. It becomes clear that the above derivations
remain valid as n → ∞, resulting in the following theorem.
Theorem 1 (Explicit Actual Optimum for the MAMC) The function f (x) = wT x in H maxi-
mizing the expected margin satisﬁes

i=1 YiXik2 = 1

kwk2 (cid:21) =
1
λ
where λ is a normalization constant such that kw∗k2 = 1.

E(cid:20) Y (wT X)

arg max

w

E[XY ] , w∗,

(6)

2

2.2 Kernel-based Classiﬁer and Parzen Window

It becomes straightforward to recast the resulting classiﬁer as a kernel classiﬁer by mapping the
input data-samples X in a feature space ϕ : Rd → Rdϕ where dϕ is possibly inﬁnite. In particular,
we do not have to resort to Lagrange duality in a context of convex optimization (see e.g. [14, 9] for
an overview) or functional analysis in a Reproducing Kernel Hilbert Space. Speciﬁcally,

wT

n ϕ(X) =

1
λn

n

Xi=1

YiK(Xi, X),

(7)

where K : Rd × Rd → R is deﬁned as the inner product such that ϕ(X)T ϕ(X′) = K(X, X′)
for any X, X′. Conversely, any function K corresponds with the inner product of a valid map ϕ
npyT Ωy with
if the function K is positive deﬁnite. As previously, the term λ becomes λ = 1
kernel matrix Ω ∈ Rn×n where Ωij = K(Xi, Xj) for all i, j = 1, . . . , n. Now the class of
positive deﬁnite Mercer kernels can be used as they induce a proper mapping ϕ. A classical choice
is the use of a linear kernel (or K(X, X′) = X T X′), a polynomial kernel of degree p ∈ N0 (or
2/σ)), or a dedicated
K(X, X′) = (X T X′ + b)p), an RBF kernel (or K(X, X′) = exp(−kX − X′k2
kernel for a speciﬁc application (e.g. a string kernel, a Fisher kernel, see e.g. [14] and references
therein). Figure 1.a depicts an example of a nonlinear classiﬁer based on the well-known Ripley
dataset, and the contourlines score the ’certainty of prediction’ as explained in the next section.

The expression (7) is similar (proportional) to the classical Parzen window for classiﬁcation, but
differs in the use of a positive deﬁnite (Mercer) kernel K instead of the pdf κ( X−·h ) with bandwidth
h > 0, and in the form of the denominator. The classical motivation of statistical kernel estimators is
based on asymptotic theory in low dimensions (i.e d = O(1)), see e.g. [4], chap. 10 and references.
The functional form of the optimal rule (7) is similar to the ’simple classiﬁer’ described in [12],
chap. 1. Thirdly, this estimator was also termed and empirically validated as a probabilistic neural
network by [15]. The novel element from above result is the derivation of a clear (both theoretical
and empirical) optimality principle of the rule, as opposed to the asymptotic results of [4] and the
geometric motivations in [12, 15]. As a direct byproduct, it becomes straightforward to extend
the Parzen window classiﬁer easily with an additional intercept term or other parametric parts, or
towards additive (structured) models as in [9].

3 Analysis and Rademacher Complexities

The quantity of interest in the analysis of the generalization performance is the probability of pre-
dicting a mistake (the risk R(w; PXY )), or

where I(z) equals one if z is true, and zero otherwise.

R(w; PXY ) = PXY (cid:0)Y (wT ϕ(X)) ≤ 0(cid:1) = E(cid:2)I(Y (wT ϕ(X)) ≤ 0)(cid:3) ,

(8)

3.1 Rademacher Complexity

Let {σi}n
−1) = 1

i=1 taken from the set {−1, 1}n be Bernoulli random variables with P (σ = 1) = P (σ =
2 . The empirical Rademacher complexity is then deﬁned [8, 1] as

ˆRn(H) , Eσ""sup

f∈H

n

Xi=1

2

n(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

σif (Xi)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)

X1, . . . , Xn# ,

(9)

where the expectation is taken over the choice of the binary vector σ = (σ1, . . . , σn)T ∈ {−1, 1}n.
It is observed that the empirical Rademacher complexity deﬁnes a natural complexity measure to
study the maximal average margin classiﬁer, as both the deﬁnitions of the empirical Rademacher
complexity and the maximal average margin resemble closely (see also [8]). The following result
was given in [1], Lemma 22, but we give an alternative proof by exploiting the structure of the
optimal estimate explicitly.
Lemma 1 (Trace bound for the Empirical Rademacher Complexity for H) Let Ω ∈ Rn×n be
deﬁned as Ωij = K(Xi, Xj) for all i, j = 1, . . . , n, then

ˆRn(H) ≤

2

nptr(Ω).

3

(10)

i=1 σi(wT ϕ(Xi)) = σT Ωσ√σT Ωσ

Proof: The proof goes along the same lines as the classical bound on the empirical Rademacher
complexity for kernel machines outlined in [1], Lemma 22. Speciﬁcally, once a vector σ ∈ {−1, 1}n
i=1 σif (Xi) equals the solution as in (7) or
is ﬁxed, it is immediately seen that the maxf∈H
= √σT Ωσ. Now, application of the expectation operator E
maxwPn
ˆRn(H) = E(cid:20) 2

over the choice of the Rademacher variables gives

√σT Ωσ(cid:21) ≤

nPn

2 =

n

2

2

1

1

1

n(cid:0)E(cid:2)σT Ωσ(cid:3)(cid:1)

2

n
Xi,j
n n
Xi=1

E [σiσj] K(Xi, Xj)

K(Xi, Xi)!
nptr(Ω),

=

2

2

2

1

=

(11)

where the inequality is based on application of Jensen’s inequality. This proves the Lemma. (cid:3)

in the case of the RBF kernel

Remark that in the case of a kernel with constant trace (as e.g.

gives as in [8, 1].

whereptr(Ω) = √n), it follows from this result that also the (expected) Rademacher complexity
E[ ˆRn(H)] ≤ptr(Ω). In general, one has that E[K(X, X)] equals the trace of the integral operator
TK deﬁned on L2(PX ) deﬁned as TK(f ) = R K(X, Y )f (X)dPX (X) as in [1]. Application of
McDiarmid’s inequality on the variable Z = supf∈H(cid:0)E[Y (wT ϕ(X))] − 1
i=1 Yi(wT ϕ(Xi))(cid:1)
Lemma 2 (Deviation Inequality) Let 0 < Bϕ < ∞ be a ﬁxed constant such that supz kϕ(z)k2
= supzpK(z, z) ≤ Bϕ such that |wT ϕ(z)| ≤ Bφ, and let δ ∈ R+
0 be ﬁxed. Then with probability
exceeding 1 − δ, one has for any w ∈ Rd that

nPn

E[Y (wT ϕ(X))] ≥

Yi(wT ϕ(Xi)) − ˆRn(H) − 3Bϕs 2 ln(cid:0) 2
δ(cid:1)n

.

1
n

n

Xi=1

(12)

Therefore it follows that one maximizes the expected margin by maximizing the empirical average
margin, while controlling the empirical Rademacher complexity by choice of the model class (ker-
nel). In the case of RBF kernels, Bϕ = 1, resulting in a reasonable tight bound. It is now illustrated
how one can obtain a practical upper-bound to the ’certainty of prediction’ using f (x) = wT

n x.

Theorem 2 (Occurrence of Mistakes) Given an i.i.d. sample Dn = {(Xi, Yi)}n
B ∈ R such that supzpK(z, z) ≤ Bϕ, and a ﬁxed δ ∈ R+
1 − δ, one has for all w ∈ Rd that
pyT Ωy
P (cid:0)Y (wT ϕ(X)) ≤ 0(cid:1) ≤

Bϕ − E[Y (wT ϕ(X))]

≤ 1 −

ˆRn(H)
Bϕ

nBϕ

Bϕ

+

i=1, a constant
0 . Then, with probability exceeding

δ(cid:1)n 
+ 3s 2 ln(cid:0) 2
 .

(13)

Proof: The proof follows directly from application of Markov’s inequality on the positive random
variable Bϕ − Y (wT ϕ(X)), with expectation Bϕ − E[Y (wT ϕ(X))], estimated accurately by the
sample average as in the previous theorem. (cid:3)
More generally, one obtains that with probability exceeding 1 − δ that for any w ∈ Rd and for any
ρ such that −Bϕ < ρ < Bϕ that
δ(cid:1)n 
Bϕ + ρs 2 ln(cid:0) 2
P (cid:0)Y (wT ϕ(X)) ≤ −ρ(cid:1) ≤
 ,
with probability exceeding 1 − δ < 1. This results in a practical assessment of the ’certainty’ of a
prediction as follows. At ﬁrst, note that the random variable Y (wT
n ϕ(x)) for a ﬁxed X = x can take
two values: either −|wT
n ϕ(x)|. Therefore P (Y (wT
n ϕ(x)) ≤ 0) = P (Y (wT
n ϕ(x)) =

 pyT Ωy

Bϕ + ρ −

n ϕ(x)| or |wT

ˆRn(H)
Bϕ + ρ

n(Bϕ + ρ)

3Bϕ

(14)

Bϕ

+

+

4

1

0.8

0.6

0.4

0.2

0

2

X

 

Class prediction
class 1
class 2

1

0.8

0.6

0.4

0.2

0

2

X

 

−0.2

 

−1.2

−1

−0.8 −0.6 −0.4 −0.2
X1
(a)

0

0.2

0.4

0.6

0.8

−0.2

 

−1.2

−1

0

0.2

0.4

0.6

0.8

−0.8 −0.6 −0.4 −0.2
X1
(b)

Figure 1: Example of (a) the MAM classiﬁer and (b) the SVM on the Ripley dataset. The contourlines
representtheestimateofcertaintyofprediction(’scores’)asderivedinTheorem2fortheMAMclassiﬁerfor
(a),andasinCorollary1forthecaseofSVMswith g(z) = min(1, max(−1, z)) where |z| < 1 corresponds
with the inner part of the margin of the SVM (b). While the contours in (a) give an overall score of the
predictions,thescoresgivenin(b)focustowardsthemarginoftheSVM.

n ϕ(x)) ≤ −|wT

n ϕ(x)|) ≤ P (Y (wT

n ϕ(x)|) as Y can only take the two values −1 or 1. Thus
−|wT
the event ’Y 6= sign(wT x∗)’ for samples X = x∗ occurs with probability lower than the rhs. of
(13) with ρ = |wT x∗|. When asserting this for a number nv ∈ N of samples X ∼ PX with
nv → ∞, a misprediction would occur less than δnv times. In this sense, one can use the latent
variable wT ϕ(x∗) as an indication of how ’certain’ the prediction is. Figure 1.a gives an example
of the MAM classiﬁer, together with the level plots indicating the certainty of prediction. Remark
however that the described ’certainty of prediction’ statement differs from a conditional statement
of the risk given as P (Y (wT ϕ(X)) < 0 | X = x∗). The essential difference with the probabilistic
estimates based on the density estimates resulting from the Parzen window estimator is that results
become independent of the data dimension, as one avoids estimating the joint distribution.

3.2 Transforming the Margin Distribution

Consider the case where the assumption of a reasonable constant B such that P (kXk2 < B) = 1 is
unrealistic. Then, a transformation of the random variable Y (wT X) can be fruitful using a monotone
increasing function g : R → R with a constant B′ϕ ≪ B such that |g(z)| ≤ B′ϕ, and g(0) = 0. In
the choice of a proper transformation, two counteracting effects should be traded properly. At ﬁrst,
a small choice of B improves the bound as e.g. described in Lemma 2. On the other hand, such a
transformation would make the expected value E[g(Y (wT ϕ(X)))] smaller than E[Y (wT ϕ(X))].
Modifying Theorem 2 gives

Corollary 1 (Occurrence of Mistakes, bis) Given i.i.d. samples Dn = {(Xi, Yi)}n
i=1, and a ﬁxed
δ ∈ R+
0 . Let g : R → R be a monotonically increasing function with Lipschitz constant 0 < Lg <
∞, let B′ϕ ∈ R such that |g(z)| ≤ B′ϕ for all z, and g(0) = 0. Then with probability exceeding
1 − δ, one has for any ρ such that −B′ϕ ≤ ρ ≤ B′ϕ and w ∈ Rd that

B′ϕ
B′ϕ + ρ−

1

nPn

i=1 g(Yi(wT

n ϕ(Xi))) − Lg ˆRn(H) − 3B′ϕq 2 log( 2

n

δ )

.

n ϕ(X))) ≤ −ρ(cid:1) ≤

P (cid:0)g(Y (wT
This result follows straightforwardly from Theorem 2 using the property that ˆRn(g ◦ H) ≤
n ϕ(X))) ≤ 0(cid:1) ≤ 1−E[Y g(wT ϕ(X))]
Lg ˆRn(H), see e.g.
.
Similar as in the previous section, corollary 1 can be used to score the certainty of prediction by
considering for each X = x∗ the value of g(wT x∗) and g(−wT x∗). Figure 1.b gives an example by
considering the clipping transformation g(z) = min(1, max(−1, z)) ∈ [−1, 1] such that B′ϕ = 1.

[1]. When ρ = 0, one has P (cid:0)g(Y (wT

B′ϕ + ρ

(15)

1

5

Note that this a-priori choice of the function g is not dependent on the (empirical) optimality criterion
at hand.

3.3 Soft-margin SVMs and MAM classiﬁers

Except the margin-based mechanisms, the MAM classiﬁer shares other properties with the soft-
margin maximal margin classiﬁer (SVM) as well. Consider the following saturation function g(z) =
(1 − z)+, where (·)+ is deﬁned as (z)+ = z if z ≥ 0, and zero otherwise (g(0) = 0). Application
of this function to the MAM formulation of (4), one obtains for a C > 0

n

w −
max

Xi=1(cid:0)1 − Yi(wT ϕ(Xi))(cid:1)+

which is similar to the support vector machine (see e.g.
explicit, consider the following formulation of (16)

s.t. wT w = C,

(16)

[14]). To make this equivalence more

min
w,ξ

ξi

s.t. wT w ≤ C and Yi(wT ϕ(Xi)) ≥ 1 − ξi, ξi ≥ 0 ∀i = 1, . . . , n,

(17)

which is similar to the SVM. Consider the following modiﬁcation

n

Xi=1

n

Xi=1

with rY denoting the ranks of all Yi in y. This expression simpliﬁes expression for wn as wn =
1
λn Xdy. It is seen that using kernels as before, the resulting estimator of the order of the responses
corresponding to x and x′ becomes

ˆfK(x, x′) = sign (m(x) − m(x′)) , where m(x) =

1
λn

n

Xi=1

K(Xi, x) rY (i).

(22)

6

min
w,ξ

ξi

s.t. wT w ≤ C and Yi(wT ϕ(Xi)) ≥ 1 − ξi

∀i = 1, . . . , n,

(18)

which is equivalent to (4) as in the optimum, Yi(wT ϕ(Xi)) = (1 − ξi) for all i. Thus, omission of
the slack constraints ξi ≥ 0 in the SVM formulation results in the Parzen window classiﬁer.
4 Maximal Average Margin for Ordinal Regression

Along the same lines as [6], the maximal average margin principle can be applied to ordinal re-
gression tasks. Let (X, Y ) ∈ Rd × {1, . . . , m} with distribution PXY . The w ∈ Rd maximizing
P (I(wT (ϕ(X) − ϕ(X)′)(Y − Y ′) > 0)) can be found by solving for the maximal average margin
between pairs as follows

M∗ = max

w

E(cid:20) sign(Y − Y ′)wT (ϕ(X) − ϕ(X)′)

kwk2

(cid:21) .

Given n i.i.d. samples {(Xi, Yi)}n

i=1, empirical risk minimization is obtained by solving

w −
min

1
n

n

Xi,j=1

sign(Yj − Yi)wT (ϕ(Xj) − ϕ(Xi)) s.t. kwk2 ≤ 1.

The Lagrangian with multiplier λ ≥ 0 becomes L(w, λ) = − 1
nPi,j wT sign(Yj − Yi)(ϕ(Xj) −
2 (wT w−1). Let there be n′ couples (i, j). Let Dy ∈ {−1, 0, 1}n′×n such that Dy,ki = 1
ϕ(Xi))+ λ
and Dy,kj = −1 if the kth couple equals (i, j). Then, by switching the minimax problem to a
maximin problem, the ﬁrst order condition for optimality ∂L(w,λ)
∂w = 0 gives the expression. wn =
1
λn XDy1n′. Now the parameter λ can be found by substituting
λ′nPYi<Yj
y XT X Dy1n′. Now the key element is the

(ϕ(Xj) − ϕ(Xi)) = 1

(5) in the constraint wT w = 1, or λ = 1
computation of dy = Dy1n′. Note that

(19)

(20)

(21)

n′ DT

nq1T
Xj=1
sign(Yj − Yi) , ry(i),

n

dy(i) =

 

oMAM
LS−SVM
oSVM
oGP

120

100

80

60

40

20

y
c
n
e
u
q
e
r
F

 
0
0.5

0.55

0.6

0.65

0.7

0.75

τ

0.8

0.85

0.9

0.95

1

(a)

Data (train/test)
Bank(1) (100/8.092)
Bank(1) (500/7.629)
Bank(1) (5.000/3.192)
Bank(1) (7.500/692)
Bank(2) (100/8.092)
Bank(2) (500/7.629)
Bank(2) (5.000/3.192)
Bank(2) (7.500/692)
Cpu(1) (100/20.540)
Cpu(1) (500/20.140)
Cpu(1) (5.000/15.640)
Cpu(1) (7.500/13.140)
Cpu(1) (15.000/5.640)
(b)

0.37
0.49
0.56
0.57
0.81
0.83
0.86
0.88
0.44
0.50
0.57
0.60
0.69

oMAM LS-SVM oSVM oGP
0.41
0.50

0.46
0.55

0.43
0.51
0.56

-

-
-

-
-

0.84
0.86
0.88

-

0.62
0.66
0.68

-
-

0.87
0.87

-
-

0.64
0.66

-
-
-

0.80
0.81

-
-

0.63
0.65

-
-
-

Figure 2: ResultsonordinalregressiontasksusingoMAM(22)ofO(n),aregressionontherank-transformed
responsesusingLS-SVMs[16]of O(n2) − O(n3),ordinalSVMsandordinalGaussianProcessesforprefer-
entiallearningof O(n4) − O(n6). TheresultsareexpressedasKendall’s τ (with −1 ≤ τ ≤ 1)computedon
thevalidationdatasets. Figure(a)reportsthenumericalresultsoftheartiﬁciallygenerateddata,Table(b)gives
theresultonanumberoflargescaleddatasetsdescribedin[2],ifthecomputationtooklessthan5minutes.

Remark that the estimator m : Rd → R equals (except for the normalization term) the Nadaraya-
Watson kernel based on the rank-transform rY of the responses. This observation suggest the appli-
cation of standard regression tools based on the rank-transformed responses as in [7]. Experiments
conﬁrm the use of the proposed ranking estimator, and also motivate the use of a more involved
function approximation tools as e.g. LS-SVMs [16] based on the rank-transformed responses.

5 Illustrative Example

i=1 ⊂ R5 × R with n = 100 and a validation set {(X v

Table 2.b provides numerical results on the 13 classiﬁcation (including 100 randomizations) bench-
mark datasets as described in [11]. The choice of an appropriate kernel parameter was obtained by
cross-validation over a range of bandwidths from σ = 1e − 2 to σ = 1e15. The results illustrate
that the Parzen window classiﬁer performs in general slightly (but not signiﬁcantly so) worse than
the other methods, but obviously reduces the required amount of memory and computation time
(i.e. O(n) versus O(n2) − O(n3)). Hence, it is advised to use the Parzen classiﬁer as a cheap
base-line method, or to use it in a context where time- or memory requirements are stringent. The
ﬁrst artiﬁcial dataset for testing the ordinal regression scheme is constructed as follows. The train-
i )}nv
ing set {(Xi, Yi)}n
i=1 ⊂ R5 × R
i , Y v
with nv = 250 is constructed such that Zi = (wT
i with
i )3 + ev
X v
i = (wT
∗
∗
w∗ ∈ N (0, 1), X, X v ∼ N (0, I5), and e, ev ∼ N (0, 0.25). Now Y (and Y v) are generated pre-
serving the order implied by {Zi}100
i=1) with the intervals χ2-distributed with 5 degrees
of freedom. Figure 2.a shows the results of a Monte Carlo experiment relating both the O(n) pro-
posed estimator (22), a LS-SVM regressor of O(n2) − O(n3) on the rank-transformed responses
{(Xi, rY (i))}, the O(n4) − O(n6) SVM approach as proposed in [3] and the Gaussian Process
approach of O(n4) − O(n6) given in [2]. The performance of the different algorithms is expressed
in terms of Kendall’s τ computed on the validation data. Table 2.b reports the results on some large
scale datasets as described in [2], imposing a maximal computation time of 5 minutes. Both tests
suggest the competitive nature of the proposed O(n) procedure, while clearly showing the beneﬁt
of using function estimation (as e.g. LS-SVMs) based on the rank-transformed responses.

i=1 (and {Z v

Xi)3 + ei and Z v

i }250

7

6 Conclusion

This paper discussed the use of the MAM risk optimality principle for designing a learning ma-
chine for classiﬁcation and ordinal regression. The relation with classical methods including Parzen
windows and Nadaraya-Watson estimators is established, while the relation with the empirical
Rademacher complexity is used to provide a measure of ’certainty of prediction’. Empirical exper-
iments show the applicability of the O(n) algorithms on real world problems, trading performance
somewhat for computational efﬁciency with respect to state-of-the art learning algorithms.

References

[1] P.L. Bartlett and S. Mendelson. Rademacher and gaussian complexities: Risk bounds and structural

results. Journal of Machine Learning Research, 3:463–482, 2002.

[2] W. Chu and Z. Ghahramani. Gaussian processes for ordinal regression. Journal of Machine Learning

Research, 6:1019–1041, 2006.

[3] W. Chu and S. S. Keerthi. New approaches to support vector ordinal regression. In in Proc. of Interna-

tional Conference on Machine Learning, pages 145–152. 2005.

[4] L. Devroye, L. Gy¨orﬁ, and G. Lugosi. A Probabilistic Theory of Pattern Recognition. Springer-Verlag,

1996.

[5] A. Garg and D. Roth. Margin distribution and learning algorithms.

In Proceedings of the Fifteenth
International Conference on Machine Learning (ICML), pages 210–217. Morgan Kaufmann Publishers,
2003.

[6] R. Herbrich, T. Graepel, and K. Obermayer. Large margin rank boundaries for ordinal regression. Ad-

vances in Large Margin Classiﬁers, pages 115–132, 2000. MIT Press, Cambridge, MA.

[7] R.L. Iman and W.J. Conover. The use of the rank transform in regression. Technometrics, 21(4):499–509,

1979.

[8] V. Koltchinski. Rademacher penalties and structural risk minimization. IEEE Transactions on Information

Theory, 47(5):1902–1914, 1999.

[9] K. Pelckmans. Primal-Dual kernel Machines. PhD thesis, Faculty of Engineering, K.U.Leuven, May.

2005. 280 p., TR 05-95.

[10] K. Pelckmans, J. Shawe-Taylor, J.A.K. Suykens, and B. De Moor. Margin based transductive graph
In Proceedings of the Eleventh International Conference on Artiﬁcial

cuts using linear programming.
Intelligence and Statistics, (AISTATS 2007), pp. 360-367, San Juan, Puerto Rico, 2007.

[11] G. R¨atsch, T. Onoda, and K.-R. M¨uller. Soft margins for adaboost. Machine Learning, 42(3):287 – 320,

2001.

[12] B. Sch¨olkopf and A. Smola. Learning with Kernels. MIT Press, Cambridge, MA, 2002.
[13] J. Shawe-Taylor and N. Cristianini. Further results on the margin distribution.

In Proceedings of the
twelfth annual conference on Computational learning theory (COLT), pages 278–285. ACM Press, 1999.
[14] J. Shawe-Taylor and N. Cristianini. Kernel Methods for Pattern Analysis. Cambridge University Press,

2004.

[15] D.F. Specht. Probabilistic neural networks. Neural Networks, 3:110–118, 1990.
[16] J.A.K. Suykens, T. van Gestel, J. De Brabanter, B. De Moor, and J. Vandewalle. Least Squares Support

Vector Machines. World Scientiﬁc, Singapore, 2002.

8

"
922,2007,Discriminative Batch Mode Active Learning,"Active learning sequentially selects unlabeled instances to label with the goal of reducing the effort needed to learn a good classifier. Most previous studies in active learning have focused on selecting one unlabeled instance at one time while retraining in each iteration. However, single instance selection systems are unable to exploit a parallelized labeler when one is available. Recently a few batch mode active learning approaches have been proposed that select a set of most informative unlabeled instances in each iteration, guided by some heuristic scores. In this paper, we propose a discriminative batch mode active learning approach that formulates the instance selection task as a continuous optimization problem over auxiliary instance selection variables. The optimization is formuated to maximize the discriminative classification performance of the target classifier, while also taking the unlabeled data into account. Although the objective is not convex, we can manipulate a quasi-Newton method to obtain a good local solution. Our empirical studies on UCI datasets show that the proposed active learning is more effective than current state-of-the art batch mode active learning algorithms.","Discriminative Batch Mode Active Learning

Yuhong Guo and Dale Schuurmans

Department of Computing Science

University of Alberta

fyuhong, daleg@cs.ualberta.ca

Abstract

Active learning sequentially selects unlabeled instances to label with the goal of
reducing the effort needed to learn a good classi(cid:2)er. Most previous studies in ac-
tive learning have focused on selecting one unlabeled instance to label at a time
while retraining in each iteration. Recently a few batch mode active learning
approaches have been proposed that select a set of most informative unlabeled
instances in each iteration under the guidance of heuristic scores. In this paper,
we propose a discriminative batch mode active learning approach that formulates
the instance selection task as a continuous optimization problem over auxiliary
instance selection variables. The optimization is formulated to maximize the dis-
criminative classi(cid:2)cation performance of the target classi(cid:2)er, while also taking
the unlabeled data into account. Although the objective is not convex, we can
manipulate a quasi-Newton method to obtain a good local solution. Our empirical
studies on UCI datasets show that the proposed active learning is more effective
than current state-of-the art batch mode active learning algorithms.

1 Introduction

Learning a good classi(cid:2)er requires a suf(cid:2)cient number of labeled training instances. In many cir-
cumstances, unlabeled instances are easy to obtain, while labeling is expensive or time consuming.
For example, it is easy to download a large number of webpages, however, it typically requires man-
ual effort to produce classi(cid:2)cation labels for these pages. Randomly selecting unlabeled instances
for labeling is inef(cid:2)cient in many situations, since non-informative or redundant instances might be
selected. Hence, active learning (i.e., selective sampling) methods have been adopted to control the
labeling process in many areas of machine learning, with the goal of reducing the overall labeling
effort.
Given a large pool of unlabeled instances, active learning provides a way to iteratively select the
most informative unlabeled instances(cid:151)the queries(cid:151)to label. This is the typical setting of pool-
based active learning. Most active learning approaches, however, have focused on selecting only one
unlabeled instance at one time, while retraining the classi(cid:2)er on each iteration. When the training
process is hard or time consuming, this repeated retraining is inef(cid:2)cient. Furthermore, if a parallel
labeling system is available, a single instance selection system can make wasteful use of the re-
source. Thus, a batch mode active learning strategy that selects multiple instances each time is more
appropriate under these circumstances. Note that simply using a single instance selection strategy to
select more than one unlabeled instance in each iteration does not work well, since it fails to take the
information overlap between the multiple instances into account. Principles for batch mode active
learning need to be developed to address the multi-instance selection speci(cid:2)cally. In fact, a few
batch mode active learning approaches have been proposed recently [2, 8, 9, 17, 19]. However, most
extend existing single instance selection strategies into multi-instance selection simply by using a
heuristic score or greedy procedure to ensure both the instance diversity and informativeness.

In this paper, we propose a new discriminative batch mode active learning strategy that exploits
information from an unlabeled set to attempt to learn a good classi(cid:2)er directly. We de(cid:2)ne a good
classi(cid:2)er to be one that obtains high likelihood on the labeled training instances and low uncertainty
on labels of the unlabeled instances. We therefore formulate the instance selection problem as an
optimization problem with respect to auxiliary instance selection variables, taking a combination
of discriminative classi(cid:2)cation performance and label uncertainty as the objective function. Un-
fortunately, this optimization problem is NP-hard, thus seeking the optimal solution is intractable.
However, we can approximate it locally using a second order Taylor expansion and obtain a subop-
timal solution using a quasi-Newton local optimization technique.
The instance selection variables we introduce can be interpreted as indicating self-supervised, op-
timistic guesses for the labels of the selected unlabeled instances. A concern about the instance
selection process, therefore, is that some information in the unlabeled data that is inconsistent with
the true classi(cid:2)cation partition might mislead instance selection. Fortunately, the active learning
method can immediately tell whether it has been misled, by comparing the true labels with its opti-
mized guesses. Therefore, one can then adjust the active selection strategy to avoid such over-(cid:2)tting
in the next iteration, whenever a mismatch between the labeled and unlabeled data has been detected.
An empirical study on UCI datasets shows that the proposed batch mode active learning method is
more effective than some current state-of-the-art batch mode active learning algorithms.

2 Related Work

Many researchers have addressed the active learning problem in a variety of ways. Most have
focused on selecting a single most informative unlabeled instance to label at a time. Many such
approaches therefore make myopic decisions based solely on the current learned classi(cid:2)er, and select
the unlabeled instance for which there is the greatest uncertainty. [10] chooses the unlabeled instance
with conditional probability closest to 0.5 as the most uncertain instance. [5] takes the instance on
which a committee of classi(cid:2)ers disagree the most. [3, 18] suggest choosing the instance closest
to the classi(cid:2)cation boundary, where [18] analyzes this active learning strategy as a version space
reduction process. Approaches that exploit unlabeled data to provide complementary information
for active learning have also been proposed. [4, 20] exploit unlabeled data by using the prior density
p(x) as uncertainty weights. [16] selects the instance that optimizes the expected generalization error
over the unlabeled data. [11] uses an EM approach to integrate information from unlabeled data. [13,
22] consider combining active learning with semi-supervised learning. [14] presents a mathematical
model that explicitly combines clustering and active learning. [7] presents a discriminative approach
that implicitly exploits the clustering information contained in the unlabeled data by considering
optimistic labelings.
Since single instance selection strategies require tedious retraining with each instance labeled (and,
moreover, since they cannot take advantage of parallel labeling systems), many batch mode active
learning methods have recently been proposed. [2, 17, 19] extend single instance selection strategies
that use support vector machines. [2] takes the diversity of the selected instances into account, in
addition to individual informativeness. [19] proposes a representative sampling approach that selects
the cluster centers of the instances lying within the margin of a support vector machine.
[8, 9]
choose multiple instances that ef(cid:2)ciently reduce the Fisher information. Overall, these approaches
use a variety of heuristics to guide the instance selection process, where the selected batch should
be informative about the classi(cid:2)cation model while being diverse enough so that their information
overlap is minimized.
Instead of using heuristic measures, in this paper, we formulate batch mode active learning as an
optimization problem that aims to learn a good classi(cid:2)er directly. Our optimization selects the best
set of unlabeled instances and their labels to produce a classi(cid:2)er that attains maximum likelihood
on labels of the labeled instances while attaining minimum uncertainty on labels of the unlabeled
instances. It is intractable to conduct an exhaustive search for the optimal solution; our optimization
problem is NP-hard. Nevertheless we can exploit a second-order Taylor approximation and use
a quasi-Newton optimization method to quickly reach a local solution. Our proposed approach
provides an example of exploiting optimization techniques in batch model active learning research,
much like other areas of machine learning where optimization techniques have been widely applied
[1].

3 Logistic Regression

In this paper, we use binary logistic regression as the base classi(cid:2)cation algorithm. Logistic re-
gression is a well-known and mature statistical model for probabilistic classi(cid:2)cation that has been
actively studied and applied in machine learning. Given a test instance x, binary logistic regression
models the conditional probability of the class label y 2 f+1; (cid:0)1g by

p(yjx; w) =

1

1 + exp((cid:0)yw>x)

where w is the model parameter. Here the bias term is omitted for simplicity of notation. The model
parameters can be trained by maximizing the likelihood of the labeled training data, i.e., minimizing
the logloss of the training instances

w X
min

i2L

log(1 + exp((cid:0)yiw>xi)) +

w>w

(cid:21)
2

(1)

2 w>w is a regularization term introduced to avoid
where L indexes the training instances, and (cid:21)
over-(cid:2)tting problems. Logistic regression is a robust classi(cid:2)er that can be trained ef(cid:2)ciently using
various convex optimization techniques [12]. Although it is a linear classi(cid:2)er, it is easy to obtain
nonlinear classi(cid:2)cations by simply introducing kernels [21].

4 Discriminative Batch Mode Active Learning

For active learning, one typically encounters a small number of labeled instances and a large number
of unlabeled instances. Instance selection strategies based only on the labeled data therefore ignore
potentially useful information embodied in the unlabeled instances.
In this section, we present
a new discriminative batch mode active learning algorithm for binary classi(cid:2)cation that exploits
information in the unlabeled instances. The proposed approach is discriminative in the sense that
(1) it selects a batch of instances by optimizing a discriminative classi(cid:2)cation model; and (2) it
selects instances by considering the best discriminative con(cid:2)guration of their labels leading to the
best classi(cid:2)er. Unlike other batch mode active learning methods, which identify the most informative
batch of instances using heuristic measures, our approach aims to identify the batch of instances that
directly optimizes classi(cid:2)cation performance.

4.1 Optimization Problem

An optimal active learning strategy selects a set of instances to label that leads to learning the best
classi(cid:2)er. We assume the learner selects a set of a (cid:2)xed size m, which is chosen as a parameter. Su-
pervised learning methods typically maximize the likelihood of training instances. With unlabeled
data being available, semi-supervised learning methods have been proposed that train by simultane-
ously maximizing the likelihood of labeled instances and minimizing the uncertainty of the labels
for unlabeled instances [6]. That is, to achieve a classi(cid:2)er with better generalization performance,
one can maximizing the expected log likelihood of the labeled data and minimize the entropy of the
missing labels on the unlabeled data, according to
X

log P (yijxi; w) + (cid:11) X

P (yjxj; w) log P (yjxj; w)

(2)

X

i2L

j2U

y=(cid:6)1

where (cid:11) is a tradeoff parameter used to adjust the relative in(cid:3)uence of the labeled and unlabeled data,
w speci(cid:2)es the conditional model, L indexes the labeled instances, and U indexes the unlabeled
instances.
The new active learning approach we propose is motivated by this semi-supervised learning princi-
ple. We propose to select a batch of m unlabeled instances, S, to label in each iteration from the
total unlabeled set U , with the goal of maximizing the objective (2). Speci(cid:2)cally, we de(cid:2)ne the
score function for a set of selected instances S in iteration t + 1 as follows

f (S) = X

i2Lt[S

log P (yijxi; wt+1) (cid:0) (cid:11) X

j2U tnS

H(yjxj ; wt+1)

(3)

where wt+1 is the parameter set for the conditional classi(cid:2)cation model trained on the new la-
beled set Lt+1 = Lt [ S, and H(yjxj ; wt+1) denotes the entropy of the conditional distribution
P (yjxj; wt+1), such that

H(yjxj ; wt+1) = (cid:0) X

y=(cid:6)1

P (yjxj; wt+1) log P (yjxj; wt+1)

The proposed active learning strategy is to select the batch of instances that has the highest score.
In practice, however it is problematic to use the f (S) score directly to guide instance selection: the
labels for instances S are not known when the selection is conducted. One typical solution for this
problem is to use the expected f (S) score computed under the current conditional model speci(cid:2)ed
by wt

E[f (S)] = X

yS

P (ySjxS; wt)f (S)

However, using P (ySjxS; wt) as weights, this expectation might aggravate any ambiguity that al-
ready exists in the current classi(cid:2)cation model wt, since it has been trained on a very small labeled
set Lt. Instead, we propose an optimistic strategy: use the best f (S) score that the batch of unla-
beled instances S can achieve over all possible label con(cid:2)gurations. This optimistic scoring function
can be written as

f (S) = max

yS X

i2Lt[S

log P (yijxi; wt+1) (cid:0) (cid:11) X

j2U tnS

H(yjxj ; wt+1)

(4)

Thus the problem becomes how to select a set of instances S that achieves the best optimistic f (S)
score de(cid:2)ned in (4). Although this problem can be solved using an exhaustive search on all size
m subsets, S, of the unlabeled set U, it is intractable to do so in practice since the search space is
exponentially large. Explicit heuristic search approaches seeking a local optima do not exist either,
since it is hard to de(cid:2)ne an ef(cid:2)cient set of operators that can transfer from one position to another
one within the search space while guaranteeing improvements to the optimistic score.
Instead, in this paper we propose to approach the problem by formulating optimistic batch mode
active learning as an explicit mathematical optimization. Given the labeled set Lt and unlabeled set
U t after iteration t, the task in iteration t + 1 is to select a size m subset S from U t that achieves the
best score de(cid:2)ned in (4). To do so, we (cid:2)rst introduce a set of f0; 1g-valued instance selection vari-
ables (cid:22). In particular, (cid:22) is a jU tj (cid:2) 2 sized indicator matrix, where each row vector (cid:22)j corresponds
to the two possible labels f+1; (cid:0)1g of the jth instance in U t. Then the optimistic instance selection
for iteration t + 1 can be formulated as the following optimization problem

(cid:22) X
max

log P (yijxi; wt+1) + (cid:12) X

j2U t

s:t:

i2Lt
(cid:22) 2 f0; 1gjU tj(cid:2)2
(cid:22) (cid:15) E = m
(cid:22)j e (cid:20) 1; 8j
(cid:22) (cid:20) (cid:16) 1

1>

2

+ (cid:15)(cid:17)me>

vt+1

j (cid:22)

>

j (cid:0) (cid:11) X

j2U t

(1 (cid:0) (cid:22)j e)H(yjxj ; wt+1)

(5)

(6)
(7)
(8)

(9)

j

where vt+1
is a row vector [log P (y = 1jxj ; wt+1); log P (y = (cid:0)1jxj ; wt+1)]; e is a 2-entry
column vector with all 1s; 1 is a jU tj-entry column vector with all 1s; E is a U t (cid:2) 2 sized matrix
with all 1s; (cid:15) is matrix inner product; (cid:15) is a user-provided parameter that controls class balance
during instance selection; and (cid:12) is a parameter that we will use later to adjust our belief in the
guessed labels. Note that, the selection variables (cid:22) not only choose instances from U t, but also
select labels for the selected instances. Solving this optimization yields the optimal (cid:22) for instance
selection in iteration t + 1.
The optimization problem (5) is an integer programming problem that produces equivalent results
to using exhaustive search to optimize (4), except that we have additional class balance constraints
(9). Integer programming is an NP-hard problem. Thus, the (cid:2)rst step toward solving this problem
in practice is to relax it into a continuous optimization by replacing the integer constraints (6) with

continuous constraints 0 (cid:20) (cid:22) (cid:20) 1, yielding the relaxed formulation
j (cid:0) (cid:11) X

log P (yijxi; wt+1) + (cid:12) X

(cid:22) X
max

vt+1

j (cid:22)

>

j2U t

j2U t

s:t:

i2Lt
0 (cid:20) (cid:22) (cid:20) 1
(cid:22) (cid:15) E = m
(cid:22)j e (cid:20) 1; 8j
(cid:22) (cid:20) (cid:16) 1

1>

(1 (cid:0) (cid:22)j e)H(yjxj ; wt+1) (10)

(11)
(12)
(13)

2

+ (cid:15)(cid:17)me>

(14)
If we can solve this continuous optimization problem, a greedy strategy can then be used to recover
the integer solution by iteratively setting the largest non-integer (cid:22) value to 1 with respect to the
constraints. However, this relaxed optimization problem is still very complex: the objective function
(10) is not a concave function of (cid:22).1 Nevertheless, standard continuous optimization techniques can
be used to solve for a local maxima.

4.2 Quasi-Newton Method

To derive a local optimization technique, consider the objective function (10) as a function of the
instance selection variables (cid:22)

f ((cid:22)) = X

i2Lt

log P (yijxi; wt+1) + (cid:12) X

j2U t

vt+1

j (cid:22)

>

j (cid:0) (cid:11) X

j2U t

(1 (cid:0) (cid:22)j e)H(yjxj ; wt+1)

(15)

1
2

k vec((cid:22) (cid:0) (cid:22)(cid:22)(k)) +

~f ((cid:22)) = f ( (cid:22)(cid:22)(k)) + rf >

As noted, this function is non-concave, therefore convenient convex optimization techniques that
achieve global optimal solutions cannot be applied. Nevertheless, a local optimization approach
exploiting quasi-Newton methods can quickly determine a local optimal solution (cid:22)
(cid:3). Such a local
optimization approach iteratively updates (cid:22) to improve the objective (15), and stops when a local
maximum is reached. At each iteration, it makes a local move that allows it to achieve the largest
improvement in the objective function along the direction decided by cumulative information ob-
tained from the sequence of local gradients. Suppose (cid:22)(cid:22)(k) is the starting point for iteration k. We
(cid:2)rst derive a second-order Taylor approximation ~f ((cid:22)) for the objective function f ((cid:22)) at (cid:22)(cid:22)(k)
vec((cid:22) (cid:0) (cid:22)(cid:22)(k))> Hk vec((cid:22) (cid:0) (cid:22)(cid:22)(k))

(16)
where vec((cid:1)) is a function that transforms a matrix into a column vector, and rfk = rf ( (cid:22)(cid:22)(k)) and
Hk denote the gradient vector and Hessian matrix of f ((cid:22)) at point (cid:22)(cid:22)(k), respectively. Since our
original optimization function f ((cid:22)) is smooth, the quadratic function ~f ((cid:22)) can reasonably approx-
imate it in a small neighborhood of (cid:22)(cid:22)(k). Thus we can determine our update direction by solving
a quadratic programming with the objective (16) and linear constraints (11), (12), (13) and (14).
Suppose the optimal solution for this quadratic program is ~(cid:22)
(k). Then a reasonable update direction
(k) (cid:0) (cid:22)(cid:22)(k) can be obtained for iteration k. Given this direction, a backtrack line search can be
(cid:3)
dk = ~(cid:22)
used to guarantee improvement over the original objective (15). Note that for each different value of
(cid:22), wt+1 has to be retrained on Lt [ S to evaluate the new objective value, since S is determined by
(cid:22). In order to reduce the computational cost, we approximate the training of wt+1 in our empirical
study, by limiting it to a few Newton-steps with a starting point given by wt trained only on Lt.
The remaining issue is to compute the local gradient rf ( (cid:22)(cid:22)(k)) and the Hessian matrix Hk. We
assume wt+1 remains constant with small local updates on (cid:22)(cid:22). Thus the local gradient can be ap-
proximated as

(cid:3)

rf ( (cid:22)(cid:22)j(k)) = (cid:12) vt+1

j + (cid:11) [H(yjxj ; wt+1); H(yjxj ; wt+1)]

and therefore rf ( (cid:22)(cid:22)(k)) can be constructed from the individual rf ( (cid:22)(cid:22)j(k)). We then use BFGS
(Broyden-Fletcher-Goldfarb-Shanno) to compute the Hessian matrix, which starts as an identity
matrix for the (cid:2)rst iteration, and is updated in each iteration as follows [15]

Hk+1 = Hk (cid:0)

Hksks>

k Hk

s>
k Hksk

+

yky>
k
y>
k sk

1Note that w

t+1 is the classi(cid:2)cation model parameter set trained on Lt+1

= Lt

[ S, where S indexes the

unlabeled instances selected by (cid:22). Therefore w

t+1 is a function of (cid:22).

where yk = rfk+1 (cid:0) rfk, and sk = (cid:22)(cid:22)(k+1) (cid:0) (cid:22)(cid:22)(k). This Hessian matrix accumulates information
from the sequences of local gradients to help determine better update directions.

4.3 Adjustment Strategy

In the discriminative optimization problem formulated in Section 4.1, the (cid:22) variables are used to
optimistically select both instances and their labels, with the goal of achieving the best classi(cid:2)cation
model according to the objective (5). However, when the labeled set is small and the discriminative
partition (clustering) information contained in the large unlabeled set is inconsistent with the true
classi(cid:2)cation, the labels optimistically guessed for the selected instances through (cid:22) might not match
the underlying true labels. When this occurs, the instance selected will not be very useful for iden-
tifying the true classi(cid:2)cation model. Furthermore, the unlabeled data might continue to mislead the
next instance selection iteration.
Fortunately, we can immediately identify when the process has been misled once the true labels for
the selected instances have been obtained. If the true labels are different from the labels guessed by
the optimization, we need to make an adjustment for the next instance selection iteration. We have
tried a few adjustment strategies in our study, but report the most effective one in this paper. Note
that the being-misled problem is caused by the unlabeled data, which affects the target classi(cid:2)cation
j . Therefore, a simple way to (cid:2)x the problem is to adjust
model through the term (cid:12) Pj2U t vt+1
the parameter (cid:12). Speci(cid:2)cally, at the end of each iteration t, we obtain the true labels yS for the
selected instances S, and compare them with our guessed labels ^yS indicated by (cid:22)
(cid:3). If they are
consistent, we will set (cid:12) = 1, which means we trust the partition information from the unlabeled
data as same as the label information in the labeled data for building the classi(cid:2)cation model. If
yS 6= ^yS, apparently we should reduce the (cid:12) value, that is, reducing the in(cid:3)uence of the unlabeled
data for the next selection iteration t + 1. We use a simple heuristic procedure to determine the (cid:12)
value in this case. Starting from (cid:12) = 1, we then multiplicatively reduce its value by a small factor,
0:5, until a better objective value for (15) can be obtained when replacing the guessed indicator
variables (cid:22)
(cid:3) with the true label indicators. Note that, if we reduce (cid:12) to zero, our optimization
problem will be exactly equivalent to picking the most uncertain instance (when m = 1).

j (cid:22)

>

5 Experiments

To investigate the empirical performance of the proposed discriminative batch mode active learning
algorithm (Discriminative), we conducted a set of experiments on nine two-class UCI datasets, com-
paring with a baseline random instance selection algorithm (Random), a non-batch myopic active
learning method that selects the most uncertain instance each time (MostUncertain), and two batch
mode active learning methods proposed in the literature: svmD, an approach that incorporates diver-
sity in active learning with SVM [2]; and Fisher, an approach that uses Fisher information matrix for
instance selection [9]. The UCI datasets we used include (we show the name, followed by the num-
ber of instances and the number of attributes): Australian(690;14), Cleve(303;13), Corral(128;6),
Crx(690;15), Flare(1066;10), Glass2(163;9), Heart(270;13), Hepatitis(155;20) and Vote(435;15).
We consider a hard case of active learning, where only a few labeled instances are given at the
start. In each experiment, we start with four randomly selected labeled instances, two in each class.
We then randomly select 2/3 of the remaining instances as the unlabeled set, using the remaining
instances for testing. All the algorithms start with the same initial labeled set, unlabeled set and
testing set. For a (cid:2)xed batch size m, each algorithm repeatedly select m instances to label each time.
In this section, we report the experimental results with m = 5, averaged over 20 times repetitions.
Figure 1 shows the comparison results on the nine UCI datasets. These results suggest that although
the baseline random sampling method, Random, works surprisingly well in our experiments, the
proposed algorithm, Discriminative, always performs better or at least achieves a comparable per-
formance. Moreover, Discriminative also apparently outperforms the other two batch mode algo-
rithms, svmD and Fisher, on (cid:2)ve datasets(cid:151)Australian, Cleve, Flare, Heart and Hepatitis, and reaches
a tie on two datasets(cid:151)Crx and Vote. The myopic most uncertain selection method, MostUncertain,
shows an overall inferior performance to Discriminative on Australian, Cleve, Crx, Heart and Hep-
atitis, and achieves a tie on Flare and Vote. However, Discriminative demonstrates weak perfor-

australian

cleve

corral

 

0.85

 

 

y
c
a
r
u
c
c
A

y
c
a
r
u
c
c
A

y
c
a
r
u
c
c
A

0.85

0.8

0.75

0.7

0.65

0.6

0.55
 
0

0.85

0.8

0.75

0.7

0.65

0.6

0.55
 
0

0.85

0.8

0.75

0.7

0.65

0.6

0.55
 
0

y
c
a
r
u
c
c
A

y
c
a
r
u
c
c
A

0.8

0.75

0.7

0.65

0.6

0.55
 
0

0.8

0.75

0.7

0.65

0.6

0.55

0.5

0.45
 
0

y
c
a
r
u
c
c
A

0.85

0.8

0.75

0.7

0.65

0.6
 
0

5

Random
MostUncertain
svmD
Fisher
Discriminative

20
80
Number of Labeled Instances

40

60

crx

100

 

Random
MostUncertain
svmD
Fisher
Discriminative

20
80
Number of Labeled Instances

40

60

heart

100

 

Random
MostUncertain
svmD
Fisher
Discriminative

20
80
Number of Labeled Instances

60

40

100

y
c
a
r
u
c
c
A

0.9

0.85

0.8

0.75

0.7
 
0

10

Random
MostUncertain
svmD
Fisher
Discriminative

20
80
Number of Labeled Instances

40

60

100

flare

 

0.75

glass2

20

30

Number of Labeled Instances

40

50

60

Random
MostUncertain
svmD
Fisher
Discriminative

70

80

 

y
c
a
r
u
c
c
A

0.7

0.65

0.6

0.55

0.5
 
0

y
c
a
r
u
c
c
A

0.9

0.85

0.8
 
0

Random
MostUncertain
svmD
Fisher
Discriminative

20
80
Number of Labeled Instances

60

40

hepatitis

100

 

Random
MostUncertain
svmD
Fisher
Discriminative

15

10
35
Number of Labeled Instances

25

30

20

40

45

Random
MostUncertain
svmD
Fisher
Discriminative

20
80
Number of Labeled Instances

40

60

vote

100

 

Random
MostUncertain
svmD
Fisher
Discriminative

20
80
Number of Labeled Instances

60

40

100

Figure 1: Results on UCI Datasets

mance on two datasets(cid:151)Corral and Glass2, where the evaluation lines for most algorithms in the
(cid:2)gures are strangely very bumpy. The reason behind this remains to be investigated.
These empirical results suggest that selecting unlabeled instances through optimizing the classi(cid:2)-
cation model directly would obtain more relevant and informative instances, comparing with using
heuristic scores to guide the selection. Although the original optimization problem formulated is
NP-hard, a relaxed local optimization method that leads to a local optimal solution still works effec-
tively.

6 Conclusion

In this paper, we proposed a discriminative batch mode active learning approach that exploits in-
formation in unlabeled data and selects a batch of instances by optimizing the target classi(cid:2)cation
model. Although the proposed technique could be overly optimistic about the information presented
by the unlabeled set, and consequently be misled, this problem can be identi(cid:2)ed immediately after
obtaining the true labels. A simple adjustment strategy can then be used to rectify the problem in the
following iteration. Experimental results on UCI datasets show that this approach is generally more
effective comparing with other batch mode active learning methods, a random sampling method,
and a myopic non-batch mode active learning method. Our current work is focused on 2-class clas-
si(cid:2)cation problems, however, it is easy to be extended to multiclass classi(cid:2)cation problems.

References
[1] K. Bennett and E. Parrado-Hernandez. The interplay of optimization and machine learning

research. Journal of Machine Learning Research, 7, 2006.

[2] K. Brinker. Incorporating diversity in active learning with support vector machines. In Pro-

ceedings of the 20th International Conference on Machine learning, 2003.

[3] C. Campbell, N. Cristianini, and A. Smola. Query learning with large margin classi(cid:2)ers. In

Proceedings of the 17th International Conference on Machine Learning, 2000.

[4] D. Cohn, Z. Ghahramani, and M. Jordan. Active learning with statistical models. Journal of

Artiﬁcial Intelligence Research, 4, 1996.

[5] Y. Freund, H. S. Seung, E. Shamir, and N. Tishby. Selective sampling using the query by

committee algorithm. Machine Learning, 28, 1997.

[6] Y. Grandvalet and Y. Bengio. Semi-supervised learning by entropy minimization. In Advances

in Neural Information Processing Systems, 2005.

[7] Y. Guo and R. Greiner. Optimistic active learning using mutual information. In Proceedings

of the International Joint Conference on Artiﬁcial Intelligence, 2007.

[8] S. Hoi, R. Jin, and M. Lyu. Large-scale text categorization by batch mode active learning. In

Proceedings of the International World Wide Web Conference, 2006.

[9] S. Hoi, R. Jin, J. Zhu, and M. Lyu. Batch mode active learning and its application to med-
In Proceedings of the 23rd International Conference on Machine

ical image classi(cid:2)cation.
Learning, 2006.

[10] D. Lewis and W. Gale. A sequential algorithm for training text classi(cid:2)ers. In Proceedings of the
International ACM-SIGIR Conference on Research and Development in Information Retrieval,
1994.

[11] A. McCallum and K. Nigam. Employing EM in pool-based active learning for text classi(cid:2)ca-

tion. In Proceedings of the 15th International Conference on Machine Learning, 1998.

[12] T. Minka. A comparison of numerical optimizers for logistic regression. Technical report,

2003. http://research.microsoft.com/ minka/papers/logreg/.

[13] I. Muslea, S. Minton, and C. Knoblock. Active + semi-supervised learning = robust multi-view

learning. In Proceedings of the 19th International Conference on Machine Learning, 2002.

[14] H. Nguyen and A. Smeulders. Active learning using pre-clustering. In Proceedings of the 21st

International Conference on Machine Learning, 2004.

[15] J. Nocedal and S.J. Wright. Numerical Optimization. Springer, New York, 1999.
[16] N. Roy and A. McCallum. Toward optimal active learning through sampling estimation of
error reduction. In Proceedings of the 18th International Conference on Machine Learning,
2001.

[17] G. Schohn and D. Cohn. Less is more: Active learning with support vector machines.

Proceedings of the 17th International Conference on Machine Learning, 2000.

In

[18] S. Tong and D. Koller. Support vector machine active learning with applications to text classi-

(cid:2)cation. In Proceedings of the 17th International Conference on Machine Learning, 2000.

[19] Z. Xu, K. Yu, V. Tresp, X. Xu, and J. Wang. Representative sampling for text classi(cid:2)cation us-
ing support vector machines. In Proceedings of the 25th European Conference on Information
Retrieval Research, 2003.

[20] C. Zhang and T. Chen. An active learning framework for content-based information retrieval.

IEEE Trans on Multimedia, 4:260(cid:150)258, 2002.

[21] J. Zhu and T. Hastie. Kernel logistic regression and the import vector machine. Journal of

Computational and Graphical Statistics, 14, 2005.

[22] X. Zhu, J. Lafferty, and Z. Ghahramani. Combining active learning and semi-supervised learn-
ing using gaussian (cid:2)elds and harmonic functions. In ICML Workshop on The Continuum from
Labeled to Unlabeled Data in Machine Learning and Data Mining, 2003.

"
928,2007,Gaussian Process Models for Link Analysis and Transfer Learning,"In this paper we develop a Gaussian process (GP) framework to model a collection of reciprocal random variables defined on the \emph{edges} of a network. We show how to construct GP priors, i.e.,~covariance functions, on the edges of directed, undirected, and bipartite graphs. The model suggests an intimate connection between \emph{link prediction} and \emph{transfer learning}, which were traditionally considered two separate research topics. Though a straightforward GP inference has a very high complexity, we develop an efficient learning algorithm that can handle a large number of observations. The experimental results on several real-world data sets verify superior learning capacity.","Gaussian Process Models for

Link Analysis and Transfer Learning

Kai Yu

NEC Laboratories America

Cupertino, CA 95014

Wei Chu

Columbia University, CCLS

New York, NY 10115

Abstract

This paper aims to model relational data on edges of networks. We describe appro-
priate Gaussian Processes (GPs) for directed, undirected, and bipartite networks.
The inter-dependencies of edges can be effectively modeled by adapting the GP
hyper-parameters. The framework suggests an intimate connection between link
prediction and transfer learning, which were traditionally two separate research
topics. We develop an efﬁcient learning algorithm that can handle a large number
of observations. The experimental results on several real-world data sets verify
superior learning capacity.

1 Introduction

In many scenarios the data of interest consist of relational observations on the edges of networks.
Typically, a given ﬁnite collection of such relational data can be represented as an M × N matrix
Y = {yi,j}, which is often partially observed because many elements are missing. Sometimes
accompanying Y are attributes of nodes or edges. As an important nature of networks, {yi,j} are
highly inter-dependent even conditioned on known node or edge attributes. The phenomenon is
extremely common in real-world data, for example,

• Bipartite Graphs. The data represent relations between two different sets of objects or
measurements under a pair of heterogeneous conditions. One notable example is transfer
learning, also known as multi-task learning, which jointly learns multiple related but dif-
ferent predictive functions based on the M × N observed labels Y, namely, the results of
N functions acting on a set of M data examples. Collaborative ﬁltering is an important
application of transfer learning that learns many users’ interests on a large set of items.
• Undirected and Directed Graphs. The data are measurements of existences, strengths, and
types of links between a set of nodes in a graph, where a given collection of observations
are an M ×M (in this case N = M) matrix Y, which can be symmetric or asymmetric, de-
pending on whether the links are undirected or directed. Examples include protein-protein
interactions, social networks, citation networks, and hyperlinks on the WEB. Link predic-
tion aims to recover those missing measurements in Y, for example, predicting unknown
protein-protein interactions based on known interactions.

The goal of this paper is to design a Gaussian process (GP) [13] framework to model the depen-
dence structure of networks, and to contribute an efﬁcient algorithm to learn and predict large-scale
relational data. We explicitly construct a series of parametric models indexed by their dimension-
ality, and show that in the limit we obtain nonparametric GP priors consistent with the dependence
of edge-wise measurements. Since the kernel matrix is on a quadratic number of edges and the
computation cost is even cubic of the kernel size, we develop an efﬁcient algorithm to reduce the
computational complexity. We also demonstrate that transfer learning has an intimate connection to
link prediction. Our method generalizes several recent transfer learning algorithms by additionally
learning a task-speciﬁc kernel that directly expresses the dependence between tasks.

1

The application of GPs to learning on networks or graphs has been fairly recent. Most of the work
in this direction has focused on GPs over nodes of graphs and targeted at the classiﬁcation of nodes
[20, 6, 10]. In this paper, we regard the edges as the ﬁrst-class citizen and develop a general GP
framework for modeling the dependence of edge-wise observations on bipartite, undirected and
directed graphs. This work extends [19], which built GPs for only bipartite graphs and proposed
an algorithm scaling cubically to the number of nodes. In contrast, the work here is more general
and the algorithm scales linearly to the number of edges. Our study promises a careful treatment to
model the nature of edge-wise observations and offers a promising tool for link prediction.

2 Gaussian Processes for Network Data

2.1 Modeling Bipartite Graphs

We ﬁrst review the edge-wise GP for bipartite graphs [19], where each observation is a measurement
on a pair of objects of different types, or under a pair of heterogenous conditions. Formally, let U
and V be two index sets, then yi,j denotes a measurement on edge (i, j) with i ∈ U and j ∈ V.
In the context of transfer learning, the pair involves a data instance i and a task j, and yi,j denotes
the label of data i within task j. The probabilistic model assumes that yi,j are noisy outcomes of a
real-valued function f : U ×V → R, which follows a Gaussian process GP(b, K), characterized by
mean function b and covariance (kernel) function between edges

K ((i, j), (i(cid:48), j(cid:48))) = Σ(i, i(cid:48))Ω(j, j(cid:48))

(1)
where Σ and Ω are kernel functions on U and V, respectively. As a result, the realizations of f on a
ﬁnite set i = 1, . . . , M and j = 1, . . . , N form a matrix F, following a matrix-variate normal dis-
tribution NM×N (B, Σ, Ω), or equivalently a normal distribution N (b, K) with mean b = vec(B)
and covariance K = Ω⊗Σ, where ⊗ means Kronecker product. The dependence structure of edges
is decomposed into the dependence of nodes. Since a kernel is a notion of similarity, the model ex-
presses a prior belief – if node i is similar to node i(cid:48) and node j is similar node j(cid:48), then so are f(i, j)
and f(i(cid:48), j(cid:48)).
It is essential to learn the kernels Σ and Ω based on the partially observed Y, in order to capture the
dependence structure of the network. For transfer learning, this means to learn the kernel Σ between
data instances and the kernel Ω between tasks. Having Σ and Ω is it then possible to predict those
missing yi,j based on known observations by using GP inference.

Theorem 2.1 ([19]). Let f(i, j) = D−1/2(cid:80)D

iid∼ GP(0, Σ) and
iid∼ GP(0, Ω), then f ∼ GP(b, K) in the limit D → ∞, and the covariance between pairs is

k=1 gk(i)hk(j) + b(i, j), where gk

hk
K ((i, j), (i(cid:48), j(cid:48))) = Σ(i, i(cid:48))Ω(j, j(cid:48)).

Theorem (2.1) offers an alternative view to understand the model. The edge-wise function f can be
decomposed into a product of two sets of intermediate node-wise functions, {gk}∞
k=1 and {hk}∞
k=1,
which are i.i.d. samples from two GP priors GP(0, Σ) and GP(0, Ω). The theorem suggests that the
GP model for bipartite relational data is a generalization of a Bayesian low-rank matrix factorization
F = HG(cid:62) + B, under the prior H ∼ NM×D(0, Σ, I) and G ∼ NN×D(0, Ω, I). When D is ﬁnite,
the elements of F are not Gaussian random variables.

2.2 Modeling Directed and Undirected Graphs
In this section we model observations on pairs of nodes of the same set U. This case includes both
directed and undirected graphs. It turns out that the directed graph is relatively easy to handle while
deriving a GP prior for undirected graphs is slightly non-trivial. For the case of directed graphs, we
let the function f : U × U → R follow GP(b, K), where the covariance function between edges is
(2)
and C : U × U → R is a kernel function between nodes. Since a random function f drawn from the
GP is generally asymmetric (even if b is symmetric), namely f(i, j) (cid:54)= f(j, i), the direction of edges
can be modeled. The covariance function Eq. (2) can be derived from Theorem (2.1) by setting
that {gk} and {hk} are two independent sets of functions i.i.d. sampled from the same GP prior

K ((i, j), (i(cid:48), j(cid:48))) = C(i, i(cid:48))C(j, j(cid:48))

2

GP(0, C), modeling the situation that each node’s behavior as a sender is different but statistically
related to it’s behavior as a receiver. This is a reasonable modeling assumption. For example, if two
papers cite a common set of papers, their are also likely to be cited by a common set of other papers.

For the case of undirected graphs, we need to design a GP that ensures any sampled function to
be symmetric. Following the construction of GP in Theorem (2.1), it seems that f is symmetric if
gk ≡ hk for k = 1, . . . , D. However a calculation reveals that f is not bounded in the limit D → ∞.
Theorem (2.2) shows that the problem can be solved by subtracting a growing quantity D1/2C(i, j)
as D → ∞, and suggests the covariance function

K ((i, j), (i(cid:48), j(cid:48))) = C(i, i(cid:48))C(j, j(cid:48)) + C(i, j(cid:48))C(j, i(cid:48)).

(3)
With such covariance function , f is ensured to be symmetric because the covariance between f(i, j)
and f(j, i) equals the variance of either.

Theorem 2.2. Let f(i, j) = D−1/2(cid:80)D

k=1 tk(i)tk(j)+b(i, j)−D1/2C(i, j), where tk

iid∼ GP(0, C),
then f ∼ GP(b, K) in the limit D → ∞, and the covariance between pairs is K ((i, j), (i(cid:48), j(cid:48))) =
C(i, i(cid:48))C(j, j(cid:48)) + C(i, j(cid:48))C(j, i(cid:48)). If b(i, j) = b(j, i), then f(i, j) = f(j, i).
Proof. Without loss of generality, let b(i, j) ≡ 0. Based on the central limit theorem, for every (i, j),
(cid:80)D
f(i, j) converges to a zero-mean Gaussian random variable as D → ∞, because {tk(i)tk(j)}D
k=1
is a collection of random variables independently following the same distribution, and has the mean
k=1{E[tk(i)tk(j)tk(i(cid:48))tk(j(cid:48))] −
C(i, j). The covariance function is Cov(f(i, j), f(i(cid:48), j(cid:48))) = 1
C(i, j)E[tk(i(cid:48))tk(j(cid:48))] − C(i(cid:48), j(cid:48))E[tk(i)tk(j)] + C(i, j)C(i(cid:48), j(cid:48))} = C(i, i(cid:48))C(j, j(cid:48)) +
C(i, j(cid:48))C(j, i(cid:48)) + C(i, j)C(i(cid:48), j(cid:48)) − C(i, j)C(i(cid:48), j(cid:48)) = C(i, i(cid:48))C(j, j(cid:48)) + C(i, j(cid:48))C(j, i(cid:48)).

D

Interestingly, Theorem (2.2) recovers Theorem (2.1) and is thus more general. To see the connection,
let hk ∼ GP(0, Σ) and gk ∼ GP(0, Ω) be concatenated to form a function tk, then we have
tk ∼ GP(0, C) and the covariance is

Σ(i, j),

Ω(i, j),
0,

C(i, j) =

if i, j ∈ U,
if i, j ∈ V,
if i, j are in different sets.

D(cid:88)

(4)

(5)

(6)

For i, i(cid:48) ∈ U and j, j(cid:48) ∈ V, applying Theorem (2.2) leads to

D(cid:88)

f(i, j) = D−1/2
K ((i, j), (i(cid:48), j(cid:48))) = C(i, i(cid:48))C(j, j(cid:48)) + C(i, j(cid:48))C(j, i(cid:48)) = Σ(i, i(cid:48))Ω(j, j(cid:48)).

tk(i)tk(j) + b(i, j) − D1/2C(i, j) = D−1/2

k=1

k=1

hk(i)gk(j) + b(i, j),

Theorems (2.1) and (2.2) suggest a general GP framework to model directed or undirected relation-
ships connecting heterogeneous types of nodes. Basically, we learn node-wise covariance functions,
like Σ, Ω, and C, such that edge-wise covariances composed by Eq. (1), (2), or (3) can explain the
happening of observations yi,j on edges. The proposed framework can be extended to cope with
more complex network data, for example, networks containing both undirected links and directed
links. We will brieﬂy discuss some extensions in Sec. 6.

3 An Efﬁcient Learning Algorithm

We consider the regression case under a Gaussian noise model, and later brieﬂy discuss extensions
to the classiﬁcation case. Let y = [yi,j](i,j)∈O be the observational vector of length |O|, f be the
corresponding quantities of the latent function f, and K be the |O|×|O| matrix of K between edges
having observations, computed by Eq. (1)-(3). Then observations on edges are generated by

(7)
iid∼ N (0, β−1), and the mean has a parametric form bi,j = µi + νj. In the
where f ∼ N (0, K), i,j
directed/undirected graph case we let µi = νi for any i ∈ U. f can be analytically marginalized out,
the marginal distribution of observations is then

yi,j = f(i, j) + bi,j + i,j

p(y|θ) = N (y; b, K + β−1I),

(8)

3

L(θ) =

|O|
2

where θ = {β, b, K}. The parameters can be estimated by minimizing the penalized negative log-
likelihood L(θ) = − ln p(y|θ) + (cid:96)(θ) under a suitable regularization (cid:96)(θ). The objective function
has the form:

ln|C| +

1
2

1
2

tr

log 2π +

(9)
where C = K + β−1I, m = y − b and b = [bi,j], (i, j) ∈ O. (cid:96)(θ) will be conﬁgured in Sec. 3.1.
Gradient-based optimization packages can be applied to ﬁnd a local optimum of θ. However the
computation can be prohibitively high when the size |O| of measured edges is very big, because the
memory cost is O(|O|2), and the computational cost is O(|O|3). In our experiments |O| is about
tens of thousands or even millions. A slightly improved algorithm was introduced in [19], with
a complexity O(M 3 + N 3) cubic to the size of nodes. The algorithm employed a non-Gaussian
approximation based on Theorem (2.1) and is applicable to only bipartite graphs.

+ (cid:96)(θ),

(cid:163)
C−1mm(cid:62)(cid:164)

We reduce the memory and computational cost by exploring the special structure of K as discussed
in Sec. 2 and assume K to be composed by node-wise linear kernels Σ(i, i(cid:48)) = (cid:104)xi, xi(cid:48)(cid:105), Ω(i, i(cid:48)) =
(cid:104)zj, zj(cid:48)(cid:105), and C(i, j) = (cid:104)xi, xj(cid:105), with x ∈ RL1 and z ∈ RL2. The edge-wise covariance is then

• Bipartite Graphs: K ((i, j), (i(cid:48), j(cid:48))) = (cid:104)xi ⊗ zj, xi(cid:48) ⊗ zj(cid:48)(cid:105).
• Directed Graphs: K ((i, j), (i(cid:48), j(cid:48))) = (cid:104)xi ⊗ xj, xi(cid:48) ⊗ xj(cid:48)(cid:105).
• Undirected Graphs: K ((i, j), (i(cid:48), j(cid:48))) = (cid:104)xi ⊗ xj, xi(cid:48) ⊗ xj(cid:48)(cid:105) + (cid:104)xi ⊗ xj, xj(cid:48) ⊗ xi(cid:48)(cid:105)

(cid:163)

(cid:164)

We turn the problem of optimizing K into the problem of optimizing X = [x1, . . . , xM ](cid:62) and
Z = [z1, . . . , zN ](cid:62).
It is important to note that in all the cases the kernel matrix has the form
K = UU(cid:62), where U is an |O| × L matrix, L (cid:191) |O|, therefore applying the Woodbury identity
C−1 = β[I−U(U(cid:62)U+β−1I)−1U(cid:62)] can dramatically reduce the computational cost. For example,
in the bipartite graph case and the directed graph case, respectively there are

U(cid:62) =

xi ⊗ zj

and U(cid:62) =

xi ⊗ xj

(i,j)∈O,

(10)
where the rows of U are indexed by (i, j) ∈ O. For the undirected graph case, we ﬁrst rewrite the
kernel function
K ((i, j), (i(cid:48), j(cid:48))) = (cid:104)xi ⊗ xj, xi(cid:48) ⊗ xj(cid:48)(cid:105) + (cid:104)xi ⊗ xj, xj(cid:48) ⊗ xi(cid:48)(cid:105)
=

(cid:104)xi ⊗ xj, xi(cid:48) ⊗ xj(cid:48)(cid:105) + (cid:104)xj ⊗ xi, xj(cid:48) ⊗ xi(cid:48)(cid:105) + (cid:104)xi ⊗ xj, xj(cid:48) ⊗ xi(cid:48)(cid:105) + (cid:104)xj ⊗ xi, xi(cid:48) ⊗ xj(cid:48)(cid:105)
(xi ⊗ xj + xj ⊗ xi), (xi(cid:48) ⊗ xj(cid:48) + xj(cid:48) ⊗ xi(cid:48))

(cid:104)
(cid:104)(cid:173)

(i,j)∈O,

(cid:174)(cid:105)

(11)

(cid:105)

=

,

(cid:164)

1
2
1
2

(cid:163)

(cid:105)

(cid:104)

and then obtain a simple form for the undirected graph case
xi ⊗ xj + xj ⊗ xi

U(cid:62) =

(12)
The overall computational cost is at O(L3 + |O|L2). Empirically we found that the algorithm is
efﬁcient to handle L = 500 when |O| is about millions. The gradients with respect to U can be
found in [12]. Further calculation of gradients with respect to X and Z can be easily derived. Here
we omit the details for saving the space. Finally, in order to predict the missing measurements, we
only need to estimate a simple linear model f(i, j) = w(cid:62)ui,j + bi,j.

(i,j)∈O

1√
2

3.1

Incorporating Additional Attributes and Learning from Discrete Observations

There are different ways to incorporate node or edge attributes into our model. A common practice
is to let the kernel K, Σ, or Ω be some parametric function of attributes. One such choice is the RBF
function. However, node or edge attributes are typically local information while the network itself
is rather a global dependence structure, thus the network data often has a large part of patterns that
are independent of those known predictors. In the following, via the example of placing a Bayesian
prior on Σ : U×U → R, we describe a ﬂexible solution to incorporate additional knowledge. Let Σ0
Z exp(−τ E(Σ))
be the covariance that we wish Σ to be apriori close to. We apply the prior p(Σ) = 1
and use its negative log-likelihood as a regularization for Σ:
log |Σ + γ−1I| + tr

(Σ + γ−1I)−1Σ0

(cid:162)(cid:105)

(13)

(cid:104)

(cid:161)

(cid:96)(Σ) = τ E(Σ) = τ
2

4

where τ is a hyperparameter predetermined on validation data, and γ−1 is a small number to be
optimized. The energy function E(Σ) is related to the KL divergence DKL(GP(0, Σ0)||GP(0, Σ +
γ−1δ)), where δ(·,·) is the dirac kernel. If we let Σ0 be the linear kernel of attributes, normalized
by the dimensionality, then E(Σ) can be derived from a likelihood of Σ as if each dimension of the
attributes is a random sample from GP(0, Σ + γ−1δ). If the attributes are nonlinear predictors we
can conveniently set Σ0 by a nonlinear kernel. We set Σ0 = I if the corresponding attributes are
absent. (cid:96)(Ω), (cid:96)(C) and (cid:96)(K) can be set in the same way.
The observations can be discrete variables rather than real values. In this case, an appropriate like-
lihood function can be devised accordingly. For example, the probit function could be employed
as the likelihood function for binary labels, which relates f(i, j) to the target yi,j ∈ {−1, +1}, by
a cumulative normal Φ (yi,j(f(i, j) + bi,j)). To preserve computationally tractability, a family of
inference techniques, e.g. Laplace approximation, can be applied to ﬁnding a Gaussian distribution
that approximates the true likelihood. Then, the marginal likelihood (8) can be written as an explicit
expression and the gradient can be derived analytically as well.

4 Discussions on Related Work

Transfer Learning: As we have suggested before, the link prediction for bipartite graphs has a tight
connection to transfer learning. To make it clear, let fj(·) = f(·, j), then the edge-wise function
f : U × V → R consists of N node-wise functions fj : U → R for j = 1, . . . , N. If we ﬁx
Ω(j, j(cid:48)) ≡ δ(j, j(cid:48)), namely a Dirac delta function, then fj are assumed to be i.i.d. GP functions
from GP(0, Σ), where each function corresponds to one learning task. This is the hierarchical
Baysian model that assumes multiple tasks sharing the same GP prior [18]. In particular, the negative
logarithm of p

is

(cid:161){yi,j},{fj}|Σ
(cid:179)
(cid:180)

(cid:162)
N(cid:88)

L

{fj}, Σ

=

(cid:88)

(cid:161)

(cid:162)

 + N

2

l

yi,j, fj(i)

+

1
2

f jΣ−1f j

j=1

i∈Oj

log |Σ|,

(14)

where l(yi,j, fj(i)) = − log p(yi,j|fj(i)). The form is close to the recent convex multi-task learning
in a regularization framework [3], if the log-determinant term is replaced by a trace regularization
term λtr(Σ). It was proven in [3] that if l(·,·) is convex with fj, then the minimization of (14)
is convex with jointly {fj} and Σ. The GP approach differs from the regularization approach in
two aspects: (1) fj are treated as random variables which are marginalized out, thus we only need to
estimate Σ; (2) The regularization for Σ is a non-convex log-determinant term. Interestingly, because
log |Σ| ≤ tr(Σ)−M, the trace norm is the convex envelope for the log-determinant, and thus the two
minimization problems are somehow doing similar things. However, the framework introduced in
this paper goes beyond the two methods by introducing an informative kernel Ω between tasks. From
a probabilistic modeling point of view, the independence of {fj} conditioned on Σ is a restrictive
assumption and even incorrect when some task-speciﬁc attributes are given (which means that {fj}
are not exchangeable anymore). The task-speciﬁc kernel for transfer learning has been recently
introduced in [4], which however increased the computational complexity by a factor of N 2. One
contribution of this paper on transfer learning is an algorithm that can efﬁciently solve the learning
problem with both data kernel Σ and task kernel Ω.
Gaussian Process Latent-Variable Model (GPLVM): Our learning algorithm is also a generaliza-
tion of GPLVM. If we enforce Ω(j, j(cid:48)) = δ(j, j(cid:48)) in the model of bipartite graphs, then the evidence
Eq. (9) is equivalent to the form of GPLVM,

1
2

tr

ln|(Σ + β−1I)| +

(Σ + β−1I)−1YY(cid:62)

L(Σ, β) = M N
2

log 2π + N
2

(15)
where Y is a fully observed M × N matrix, the mean B = 0, and there is no further regularization
on Σ. GPLVM assumes that columns of Y are conditionally independent given Σ. In this paper we
consider a situation with complex dependence of edges in network graphs.
Other Related Work: Getoor et al. [7] introduced link uncertainty in the framework of probabilistic
relational models. Latent-class relational models [17, 11, 1] have been popular, aiming to ﬁnd
the block structure of links. Link prediction was casted as structured-output prediction in [15, 2].
Statistical models based on matrix factorization was studied by [8]. Our work is similar to [8] in the

,

(cid:105)

(cid:104)

5

Figure 1: The left-hand side: the subset of the UMist Faces data that contains 10 people at 10 different views.
The blank blocks indicate the ten knocked-off images as test cases; The right-hand side: the ten knocked-off
images (the ﬁrst row) along with predictive images. The second row is of our results, the third row is of the
MMMF results, and the fourth row is of the bilinear results.

sense that relations are modeled by multiplications of node-wise factors. Very recently, Hoff showed
in [9] that the multiplicative model generalizes the latent-class models [11, 1] and can encode the
transitivity of relations.

5 Numerical Experiments

We set the dimensionality of the model via validation on 10% of training data. In cases that the
additional attributes on nodes or edges are either unavailable or very weak, we compare our method
with max-margin matrix factorization (MMMF) [14] using a square loss, which is similar to singular
value decomposition (SVD) but can handle missing measurements.

5.1 A Demonstration on Face Reconstruction
A subset of the UMist Faces images of size 112 × 92 was selected to illustrate our algorithm, which
consists of 10 people at 10 different views. We manually knocked 10 images off as test cases, as
presented in Figure 1, and treated each image as a vector that leads to a 103040 × 10 matrix with
103040 missing values, where each column corresponds a view of faces. GP was trained by setting
L1 = L2 = 4 on this matrix to learn from the appearance relationships between person identity
and pose. The images recovered by GP for the test cases are presented as the second row of Figure
1-right (RMSE=0.2881). The results of MMMF are presented as the third row (RMSE=0.4351). We
also employed the bilinear models introduced by [16], which however does not handle missing data
of a matrix, and put the results at the bottom row for comparison. Quantitatively and perceptually
our model offers a better generalization to unseen views of known persons.

5.2 Collaborative Filtering

Collaborative ﬁltering is a typical case of bipartite graphs, where ratings are measurements on edges
of user-item pairs. We carried out a serial of experiments on the whole EachMovie data, which
includes 61265 users’ 2811718 distinct numeric ratings on 1623 movies. We randomly selected
80% of each user’s ratings for training and used the remaining 20% as test cases. The random
selection was carried out 20 times independently.
For comparison purpose, we also evaluated the predictive performance of four other approaches:
1) Movie Mean: the empirical mean of ratings per movie was used as the predictive value of all
users’ rating on the movie; 2) User Mean:
the empirical mean of ratings per user was used as
the predictive value of the users’ rating on all movies; 3) Pearson Score: the Pearson correlation
coefﬁcient corresponds to a dot product between normalized rating vectors. We computed the Gram
matrices of the Pearson score with mean imputation for movies and users respectively, and took
principal components as their individual attributes. We tried 20 or 50 principal components as
attributes in this experiment and carried out least square regression on observed entries. 4) MMMF.
The optimal rank was decided by validation.

6

Table 1: Test results on the EachMovie data. The number in bracket indicates the rank we applied. The
results are averaged over 20 trials, along with the standard deviation. To evaluate accuracy, we utilize root
mean squared error (RMSE), mean absolute error (MAE), and normalized mean squared error, i.e. ,the RMSE
normalized by the standard deviation of observations.

MAE

RMSE

NMSE

METHODS
MOVIE MEAN 1.3866±0.0013 1.1026±0.0010 0.7844±0.0012
USER MEAN 1.4251±0.0011 1.1405±0.0009 0.8285±0.0008
PEARSON(20) 1.3097±0.0012 1.0325±0.0013 0.6999±0.0011
PEARSON(50) 1.3034±0.0018 1.0277±0.0015 0.6931±0.0019
1.2245±0.0503 0.9392±0.0246 0.6127±0.0516
MMMF(3)
MMMF(15) 1.1696±0.0283 0.8918±0.0146 0.5585±0.0286
1.1557±0.0010 0.8781±0.0009 0.5449±0.0011
GP(3)

Table 2: Test results on the Cora data. The classiﬁcation accuracy rate is averaged over 5 trials, each with 4
folds for training and one fold for test.

DS

PL

HA

METHODS
CONTENT 53.70±0.50 67.50±1.70 68.30±1.60 56.40±0.70
48.90±1.70 65.80±1.40 60.70±1.10 58.20±0.70
LINK
PCA(50) 61.61±1.42 69.36±1.36 70.06±0.90 60.26±1.16
62.10±0.84 75.40±0.80 78.30±0.78 63.25±0.60
GP(50)

ML

The results of these approaches are reported in Table 1. The per-movie average yields much better
results than the per-user average, which is consistent with the ﬁndings previously reported by [5].
The improvement is noticeable by using more components of the Pearson score, but not signiﬁcant.
The generalization performance of our algorithm is better than that of others. T-test showed a signif-
icant difference with p-value 0.0387 of GP over MMMF (with 15 dimensions) in terms of RMSE.
It is well worth highlighting another attractiveness of our algorithm – the compact representation of
factors. On the EachMovie data, there are only three factors that well represent thousands of items
individually. We also trained MMMF with 3 factors as well. Although the three-factor solution GP
found is also accessible to other models, MMMF failed to achieve comparable performance on this
case (i.e., see results of MMMF(3)). In each trial, the number of training samples is around 2.25
million. Our program took about 865 seconds to accomplish 500 L-BFGS updates on all 251572
parameters using an AMD Opteron 2.6GHz processor.

5.3 Text Categorization based on Contents and Links

We used a part of Cora corpus including 751 papers on data structure (DS), 400 papers on hardware
and architecture (HA), 1617 on machine learning (ML) and 1575 on programming language (PL).
We treated the citation network as a directed graph and modeled the link existence as binary labels.
Our model applied the probit likelihood and learned a node-wise covariance function C, L = 50 ×
50, which composes an edge-wise covariance K by Eq. (2). We set the prior covariance C0 by the
linear kernel computed by bag-of-word content attributes. Thus the learned linear features encode
both link and content information, which were then used for document classiﬁcation. We compare
several other methods that provide linear features for one-against-all categorization using SVM: 1)
CONTENT: bag-of-words features; 2) LINK: each paper’s citation list; 3) PCA: 50 components
by PCA on the concatenation of bag-of-word features and citation list for each paper. We chose
the dimensionality 50 for both GP and PCA, because their performances both saturated when the
dimensionality exceeds 50. We reported results based on 5-fold cross validation in Table 2. GP
clearly outperformed other methods in 3 out of 4 categories. The main reason we believe is that our
approach models the in-bound and out-bound behaviors simultaneously for each paper .

6 Conclusion and Extensions

In this paper we proposed GPs for modeling data living on links of networks. We described solu-
tions to handle directed and undirected links, as well as links connecting heterogenous nodes. This
work paves a way for future extensions for learning more complex relational data. For example, we
can model a network containing both directed and undirected links. Let (i, j) be directed and (i(cid:48), j(cid:48))
√
be undirected. Based on the feature representations, Eq.(10)-right for directed links and Eq.(12)
for undirected links, the covaraince is K((i, j), (i(cid:48), j(cid:48))) = 1/
2[C(i, i(cid:48))C(j, j(cid:48)) + C(i, j(cid:48))C(j, i(cid:48))],

7

which indicates that dependence between a directed link and an undirected link is penalized com-
pared to dependence between two undirected links. Moreover, GPs can be employed to model
multiple networks involving multiple different types of nodes. For each type, we use one node-wise
covariance. Letting covariance between two different types of nodes be zero, we obtain a huge
block-diagonal node-wise covariance matrix, where each block corresponds to one type of nodes.
This big covariance matrix will induce the edge-wise covariance for links connecting nodes of the
same or different types. In the near future it is promising to apply the model to various link prediction
or network completion problems.

References

[1] E. M. Airoldi, D. M. Blei, S. E. Fienberg, and E. P. Xing, Mixed membership stochastic block
models for relational data with application to protein-protein interactions. Biometrics Society
Annual Meeting, 2006.

[2] S. Andrews and T. Jebara, Structured Network Learning. NIPS Workshop on Learning to

Compare Examples, 2006.

[3] A. Argyriou, T. Evgeniou, and M. Pontil. Convex multi-task feature learning. Machine Learn-

ing, 2007.

[4] E. V. Bonilla, F. V. Agakov, and C. K. I. Williams. Kernel multi-task learning using task-

speciﬁc features. International Conferences on Artiﬁcial Intelligence and Statistics, 2007.

[5] J. Canny. Collaborative ﬁltering with privacy via factor analysis. International ACM SIGIR

Conference , 2002.

[6] W. Chu, V. Sindhwani, Z. Ghahramani, and S. S. Keerthi. Relational learning with gaussian

processes. Neural Informaiton Processing Systems 19, 2007.

[7] L. Getoor, E. Segal, B. Taskar, and D. Koller. Probabilistic models of text and link structure

for hypertext classiﬁcation. ICJAI Workshop, 2001.

[8] P. Hoff. Multiplicative latent factor models for description and prediction of social networks.

to appear in Computational and Mathematical Organization Theory, 2007.

[9] P. Hoff. Modeling homophily and stochastic equivalence in symmetric relational data.

appear in Neural Informaiton Processing Systems 20, 2007.

to

[10] A. Kapoor, Y. Qi, H. Ahn, and R. W. Picard. Hyperparameter and kernel learning for graph

based semi-supervised classiﬁcation. Neural Informaiton Processing Systems 18, 2006.

[11] C. Kemp, J. B. Tenenbaum, T. L. Grifﬁths, T. Yamada, and N. Ueda. Learning systems of

concepts with an inﬁnite relational model. AAAI Conference on Artiﬁcial Intelligence, 2006.

[12] N. Lawrence. Gaussian process latent variable models. Journal of Machine Learning Research,

2005.

[13] C. E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning. The MIT

Press, 2006.

[14] J. D. M. Rennie and N. Srebro. Fast maximum margin matrix factorization for collaborative

prediction. International Conference on Machine Learning, 2005.

[15] B. Taskar, M. F. Wong, P. Abbeel, and D. Koller. Link prediction in relational data. Neural

Informaiton Processing Systems 16, 2004.

[16] J. B. Tenenbaum and W. T. Freeman. Separating style and content with bilinear models. Neural

Computation, 2000.

[17] Z. Xu, V. Tresp, K. Yu, and H.-P. Kriegel. Inﬁnite hidden relational models. International

Conference on Uncertainty in Artiﬁcial Intelligence, 2006.

[18] K. Yu, V. Tresp, and A. Schwaighofer. Learning Gaussian processes from multiple tasks.

International Conference on Machine Learning, 2005.

[19] K. Yu, W. Chu, S. Yu, V. Tresp, and Z. Xu. Stochastic relational models for discriminative link

prediction. Neural Informaiton Processing Systems 19, 2007.

[20] X. Zhu, J. Lafferty, and Z. Ghahramani. Semi-supervised learning: From gaussian ﬁelds to

gaussian processes. Technical Report CMU-CS-03-175, Carnegie Mellon University, 2003.

8

"
893,2007,Supervised Topic Models,"We introduce supervised latent Dirichlet allocation (sLDA), a statistical model of labelled documents. The model accommodates a variety of response types. We derive a maximum-likelihood procedure for parameter estimation, which relies on variational approximations to handle intractable posterior expectations. Prediction problems motivate this research: we use the fitted model to predict response values for new documents. We test sLDA on two real-world problems: movie ratings predicted from reviews, and web page popularity predicted from text descriptions. We illustrate the benefits of sLDA versus modern regularized regression, as well as versus an unsupervised LDA analysis followed by a separate regression.","Supervised topic models

David M. Blei

Department of Computer Science

Princeton University

Princeton, NJ

blei@cs.princeton.edu

Jon D. McAuliffe

Department of Statistics

University of Pennsylvania,

Wharton School
Philadelphia, PA

mcjon@wharton.upenn.edu

Abstract

We introduce supervised latent Dirichlet allocation (sLDA), a statistical model of
labelled documents. The model accommodates a variety of response types. We
derive a maximum-likelihood procedure for parameter estimation, which relies on
variational approximations to handle intractable posterior expectations. Prediction
problems motivate this research: we use the ﬁtted model to predict response values
for new documents. We test sLDA on two real-world problems: movie ratings
predicted from reviews, and web page popularity predicted from text descriptions.
We illustrate the beneﬁts of sLDA versus modern regularized regression, as well
as versus an unsupervised LDA analysis followed by a separate regression.

1 Introduction

There is a growing need to analyze large collections of electronic text. The complexity of document
corpora has led to considerable interest in applying hierarchical statistical models based on what are
called topics. Formally, a topic is a probability distribution over terms in a vocabulary. Informally,
a topic represents an underlying semantic theme; a document consisting of a large number of words
might be concisely modelled as deriving from a smaller number of topics. Such topic models provide
useful descriptive statistics for a collection, which facilitates tasks like browsing, searching, and
assessing document similarity.
Most topic models, such as latent Dirichlet allocation (LDA) [4], are unsupervised: only the words
in the documents are modelled. The goal is to infer topics that maximize the likelihood (or the pos-
terior probability) of the collection. In this work, we develop supervised topic models, where each
document is paired with a response. The goal is to infer latent topics predictive of the response.
Given an unlabeled document, we infer its topic structure using a ﬁtted model, then form its pre-
diction. Note that the response is not limited to text categories. Other kinds of document-response
corpora include essays with their grades, movie reviews with their numerical ratings, and web pages
with counts of how many online community members liked them.
Unsupervised LDA has previously been used to construct features for classiﬁcation. The hope was
that LDA topics would turn out to be useful for categorization, since they act to reduce data di-
mension [4]. However, when the goal is prediction, ﬁtting unsupervised topics may not be a good
choice. Consider predicting a movie rating from the words in its review. Intuitively, good predictive
topics will differentiate words like “excellent”, “terrible”, and “average,” without regard to genre.
But topics estimated from an unsupervised model may correspond to genres, if that is the dominant
structure in the corpus.
The distinction between unsupervised and supervised topic models is mirrored in existing
dimension-reduction techniques. For example, consider regression on unsupervised principal com-
ponents versus partial least squares and projection pursuit [7], which both search for covariate linear
combinations most predictive of a response variable. These linear supervised methods have non-

1

parametric analogs, such as an approach based on kernel ICA [6]. In text analysis, McCallum et al.
developed a joint topic model for words and categories [8], and Blei and Jordan developed an LDA
model to predict caption words from images [2]. In chemogenomic proﬁling, Flaherty et al. [5]
proposed “labelled LDA,” which is also a joint topic model, but for genes and protein function
categories. It differs fundamentally from the model proposed here.
This paper is organized as follows. We ﬁrst develop the supervised latent Dirichlet allocation model
(sLDA) for document-response pairs. We derive parameter estimation and prediction algorithms for
the real-valued response case. Then we extend these techniques to handle diverse response types,
using generalized linear models. We demonstrate our approach on two real-world problems. First,
we use sLDA to predict movie ratings based on the text of the reviews. Second, we use sLDA to
predict the number of “diggs” that a web page will receive in the www.digg.com community, a
forum for sharing web content of mutual interest. The digg count prediction for a page is based
on the page’s description in the forum. In both settings, we ﬁnd that sLDA provides much more
predictive power than regression on unsupervised LDA features. The sLDA approach also improves
on the lasso, a modern regularized regression technique.

2 Supervised latent Dirichlet allocation

In topic models, we treat the words of a document as arising from a set of latent topics, that is, a
set of unknown distributions over the vocabulary. Documents in a corpus share the same set of K
topics, but each document uses a mix of topics unique to itself. Thus, topic models are a relaxation
of classical document mixture models, which associate each document with a single unknown topic.
Here we build on latent Dirichlet allocation (LDA) [4], a topic model that serves as the basis for
many others. In LDA, we treat the topic proportions for a document as a draw from a Dirichlet
distribution. We obtain the words in the document by repeatedly choosing a topic assignment from
those proportions, then drawing a word from the corresponding topic.
In supervised latent Dirichlet allocation (sLDA), we add to LDA a response variable associated
with each document. As mentioned, this variable might be the number of stars given to a movie, a
count of the users in an on-line community who marked an article interesting, or the category of a
document. We jointly model the documents and the responses, in order to ﬁnd latent topics that will
best predict the response variables for future unlabeled documents.
We emphasize that sLDA accommodates various types of response: unconstrained real values, real
values constrained to be positive (e.g., failure times), ordered or unordered class labels, nonnegative
integers (e.g., count data), and other types. However, the machinery used to achieve this generality
complicates the presentation. So we ﬁrst give a complete derivation of sLDA for the special case
of an unconstrained real-valued response. Then, in Section 2.3, we present the general version of
sLDA, and explain how it handles diverse response types.
Focus now on the case y ∈ R. Fix for a moment the model parameters: the K topics β1:K (each
βk a vector of term probabilities), the Dirichlet parameter α, and the response parameters η and σ 2.
Under the sLDA model, each document and response arises from the following generative process:

1. Draw topic proportions θ | α ∼ Dir(α).
2. For each word

(a) Draw topic assignment zn | θ ∼ Mult(θ ).
3. Draw response variable y | z1:N , η, σ 2 ∼ N(cid:0)η>¯z, σ 2(cid:1).
(b) Draw word wn | zn, β1:K ∼ Mult(βzn
).

Here we deﬁne ¯z := (1/N )PN

n=1 zn. The family of probability distributions corresponding to this

generative process is depicted as a graphical model in Figure 1.
Notice the response comes from a normal linear model. The covariates in this model are the (un-
observed) empirical frequencies of the topics in the document. The regression coefﬁcients on those
frequencies constitute η. Note that a linear model usually includes an intercept term, which amounts
to adding a covariate that always equals one. Here, such a term is redundant, because the compo-
nents of ¯z always sum to one.

2

By regressing the response on the empirical topic frequencies, we treat the response as non-
exchangeable with the words. The document (i.e., words and their topic assignments) is generated
ﬁrst, under full word exchangeability; then, based on the document, the response variable is gen-
erated. In contrast, one could formulate a model in which y is regressed on the topic proportions
θ. This treats the response and all the words as jointly exchangeable. But as a practical matter,
our chosen formulation seems more sensible: the response depends on the topic frequencies which
actually occurred in the document, rather than on the mean of the distribution generating the topics.
Moreover, estimating a fully exchangeable model with enough topics allows some topics to be used
entirely to explain the response variables, and others to be used to explain the word occurrences.
This degrades predictive performance, as demonstrated in [2].
We treat α, β1:K , η, and σ 2 as unknown constants to be estimated, rather than random variables. We
carry out approximate maximum-likelihood estimation using a variational expectation-maximization
(EM) procedure, which is the approach taken in unsupervised LDA as well [4].

2.1 Variational E-step

Given a document and response, the posterior distribution of the latent variables is

(cid:16)QN
(cid:17)
p(θ, z1:N | w1:N , y, α, β1:K , η, σ 2) =
(cid:16)QN
(cid:17)
p(y | z1:N , η, σ 2)
n=1 p(zn | θ )p(wn | zn, β1:K )
R dθ p(θ | α)P
n=1 p(zn | θ )p(wn | zn, β1:K )

p(θ | α)

z1:N

p(y | z1:N , η, σ 2)

.

(1)

The normalizing value is the marginal probability of the observed data, i.e., the document w1:N and
response y. This normalizer is also known as the likelihood, or the evidence. As with LDA, it is not
efﬁciently computable. Thus, we appeal to variational methods to approximate the posterior.
Variational objective function. We maximize the evidence lower bound (ELBO) L(·), which for a
single document has the form

log p(cid:0)w1:N , y | α, β1:K , η, σ 2(cid:1) ≥ L(γ , φ1:N; α, β1:K , η, σ 2) = E[log p(θ | α)] +
NX

E[log p(wn | Zn, β1:K )] + E[log p(y | Z1:N , η, σ 2)] + H(q) .

E[log p(Zn | θ )] + NX

(2)

n=1

n=1

Here the expectation is taken with respect to a variational distribution q. We choose the fully factor-
ized distribution,

q(θ, z1:N | γ , φ1:N ) = q(θ | γ )QN

n=1 q(zn | φn),

(3)

3

Figure 1:  (Left) A graphical model representation of Supervised Latent Dirichlet allocation.  (Bottom) The topics of a 10-topic sLDA model ﬁt to the movie review data of Section 3. bothmotionsimpleperfectfascinatingpowercomplexhowevercinematographyscreenplayperformancespictureseffectivepicturehistheircharactermanywhileperformancebetween!30!20!1001020!!!!!!!!!!morehasthanﬁlmsdirectorwillcharactersonefromtherewhichwhomuchwhatawfulfeaturingroutinedryofferedcharlieparisnotaboutmovieallwouldtheyitshavelikeyouwasjustsomeoutbadguyswatchableitsnotonemovieleastproblemunfortunatelysupposedworseﬂatdullθdZd,nWd,nNDKβkαYdη,σ2(4)

(5)

2σ 2 .

(cid:17).

>(cid:3)η

y2 − 2yη>

where γ is a K-dimensional Dirichlet parameter vector and each φn parametrizes a categorical dis-
tribution over K elements. Notice E[Zn] = φn.
The ﬁrst three terms and the entropy of the variational distribution are identical to the corresponding
log(cid:0)2π σ 2(cid:1) −(cid:16)
terms in the ELBO for unsupervised LDA [4]. The fourth term is the expected log probability of the
response variable given the latent topic assignments,
E[log p(y | Z1:N , η, σ 2)] = = −1
2
The ﬁrst expectation is E(cid:2) ¯Z(cid:3) = ¯φ := (1/N )PN
(cid:16)PN
>(cid:3) = (1/N 2)
P
m6=n φnφ>
n=1
To see (5), notice that for m 6= n, E[Zn Z>
m ] = E[Zn]E[Zm]> = φnφ>
distribution is fully factorized. On the other hand, E[Zn Z>
is an indicator vector.
For a single document-response pair, we maximize (2) with respect to φ1:N and γ to obtain an
estimate of the posterior. We use block coordinate-ascent variational inference, maximizing with
respect to each variational parameter vector in turn.
Optimization with respect to γ . The terms that involve the variational Dirichlet γ are identical to
those in unsupervised LDA, i.e., they do not involve the response variable y. Thus, the coordinate
ascent update is as in [4],

m +PN
m because the variational
n ] = diag(E[Zn]) = diag(φn) because Zn

E(cid:2) ¯Z ¯Z
E(cid:2) ¯Z(cid:3) + η>
n=1 diag{φn}(cid:17)

n=1 φn, and the second expectation is
.

E(cid:2) ¯Z ¯Z

n=1 φn.
(6)
n6= j φn. Given j ∈ {1, . . . , N}. In [3], we
maximize the Lagrangian of the ELBO, which incorporates the constraint that the components of φ j
sum to one, and obtain the coordinate update

γ new ← α +PN
Optimization with respect to φ j. Deﬁne φ− j := P
(cid:2)2(cid:0)η>φ− j
(cid:26)
E[log θ | γ ] + E[log p(w j | β1:K )] +(cid:16) y
j ∝ exp
φnew
that E[log θi | γ ] = (γi ) − (P γ j ), where (·) is the digamma function.

Exponentiating a vector means forming the vector of exponentials. The proportionality symbol
means the components of φnew
are computed according to (7), then normalized to sum to one. Note

(cid:1)η + (η ◦ η)(cid:3)

2N 2σ 2

.

(7)

j

(cid:17)

η −

N σ 2

(cid:27)

The central difference between LDA and sLDA lies in this update. As in LDA, the jth word’s
variational distribution over topics depends on the word’s topic probabilities under the actual model
(determined by β1:K ). But w j ’s variational distribution, and those of all other words, affect the
probability of the response, through the expected residual sum of squares (RSS), which is the second
term in (4). The end result is that the update (7) also encourages φ j to decrease this expected RSS.
The update (7) depends on the variational parameters φ− j of all other words. Thus, unlike LDA, the
φ j cannot be updated in parallel. Distinct occurrences of the same term are treated separately.

2.2 M-step and prediction

The corpus-level ELBO lower bounds the joint log likelihood across documents, which is the sum of
the per-document log-likelihoods. In the E-step, we estimate the approximate posterior distribution
for each document-response pair using the variational inference algorithm described above. In the
M-step, we maximize the corpus-level ELBO with respect to the model parameters β1:K , η, and σ 2.
For our purposes, it sufﬁces simply to ﬁx α to 1/K times the ones vector. In this section, we add
document indexes to the previous section’s quantities, so y becomes yd and ¯Z becomes ¯Zd.
Estimating the topics. The M-step updates of the topics β1:K are the same as for unsupervised
LDA, where the probability of a word under a topic is proportional to the expected number of times
that it was assigned to that topic [4],
ˆβnew

NX

k,w ∝ DX

1(wd,n = w)φk
d,n.

(8)

d=1

n=1

4

Here again, proportionality means that each ˆβnew
Estimating the regression parameters. The only terms of the corpus-level ELBO involving η and
σ 2 come from the corpus-level analog of (4).
Deﬁne y = y1:D as the vector of response values across documents. Let A be the D × (K + 1)
matrix whose rows are the vectors ¯Z>

d . Then the corpus-level version of (4) is

is normalized to sum to one.

k

h

i
(y − Aη)>(y − Aη)

.

(9)

E[log p(y | A, η, σ 2)] = − D
2

log(2π σ 2) − 1

2σ 2 E

>

E(cid:2)A

Here the expectation is over the matrix A, using the variational distribution parameters chosen in
the previous E-step. Expanding the inner product, using linearity of expectation, and applying the
ﬁrst-order condition for η, we arrive at an expected-value version of the normal equations:

A(cid:3)η = E[A]
y
(10)
(cid:3), with each term having a ﬁxed value from the previous E-step
step. Also, E(cid:2)A> A(cid:3) =P
d E(cid:2) ¯Zd
Note that the dth row of E[A] is just ¯φd, and all these average vectors were ﬁxed in the previous E-
¯Z>
as well, given by (5). We caution again: formulas in the previous section, such as (5), suppress the
document indexes which appear here.
We now apply the ﬁrst-order condition for σ 2 to (9) and evaluate the solution at ˆηnew, obtaining:

ˆηnew ←(cid:16)

A(cid:3)(cid:17)−1

E(cid:2)A

E[A]

⇒

y .

>

>

>

d

new ← (1/D){y
ˆσ 2

>

y − y

>

E[A]

>

y} .

E[A]

(11)

(cid:16)

E(cid:2)A

>

A(cid:3)(cid:17)−1

Prediction. Our focus in applying sLDA is prediction. Speciﬁcally, we wish to compute the ex-
pected response value, given a new document w1:N and a ﬁtted model {α, β1:K , η, σ 2}:

E[Y | w1:N , α, β1:K , η, σ 2] = η>

E[ ¯Z | w1:N , α, β1:K ].

(12)
The identity follows easily from iterated expectation. We approximate the posterior mean of ¯Z using
the variational inference procedure of the previous section. But here, the terms depending on y are
removed from the φ j update in (7). Notice this is the same as variational inference for unsupervised
LDA: since we averaged the response variable out of the right-hand side in (12), what remains is the
standard unsupervised LDA model for Z1:N and θ.
Thus, given a new document, we ﬁrst compute Eq[Z1:N ], the variational posterior distribution of the
latent variables Zn. Then, we estimate the response with
E[Y | w1:N , α, β1:K , η, σ 2] ≈ η>
2.3 Diverse response types via generalized linear models

Eq[ ¯Z] = η> ¯φ.

(13)

Up to this point, we have conﬁned our attention to an unconstrained real-valued response variable.
In many applications, however, we need to predict a categorical label, or a non-negative integral
count, or a response with other kinds of constraints. Sometimes it is reasonable to apply a normal
linear model to a suitably transformed version of such a response. When no transformation results
in approximate normality, statisticians often make use of a generalized linear model, or GLM [9].
In this section, we describe sLDA in full generality, replacing the normal linear model of the earlier
exposition with a GLM formulation. As we shall see, the result is a generic framework which can be
specialized in a straightforward way to supervised topic models having a variety of response types.
There are two main ingredients in a GLM: the “random component” and the “systematic compo-
nent.” For the random component, one takes the distribution of the response to be an exponential
dispersion family with natural parameter ζ and dispersion parameter δ:

p(y | ζ, δ) = h(y, δ) exp

.

(14)

(cid:26) ζ y − A(ζ )

(cid:27)

δ

For each ﬁxed δ, (14) is an exponential family, with base measure h(y, δ), sufﬁcient statistic y,
and log-normalizer A(ζ ). The dispersion parameter provides additional ﬂexibility in modeling the
variance of y. Note that (14) need not be an exponential family jointly in (ζ, δ).

5

In the systematic component of the GLM, we relate the exponential-family parameter ζ of the ran-
dom component to a linear combination of covariates – the so-called linear predictor. For sLDA,
the linear predictor is η>¯z. In fact, we simply set ζ = η>¯z. Thus, in the general version of sLDA,
the previous speciﬁcation in step 3 of the generative process is replaced with

so that

p(y | z1:N , η, δ) = h(y, δ) exp

y | z1:N , η, δ ∼ GLM(¯z, η, δ) ,

(cid:26) η>(¯zy) − A(η>¯z)

(cid:27)

.

(15)

(16)

δ

δ

.

(17)

(cid:17)

n

(cid:19) ∂

η−(cid:18)1

E[log p(y | Z1:N , η, δ)] = log h(y, δ) + 1

The reader familiar with GLMs will recognize that our choice of systematic component means sLDA
uses only canonical link functions. In future work, we will relax this constraint.
We now have the ﬂexibility to model any type of response variable whose distribution can be written
in exponential dispersion form (14). As is well known, this includes many commonly used distribu-
tions: the normal; the binomial (for binary response); the Poisson and negative binomial (for count
data); the gamma, Weibull, and inverse Gaussian (for failure time data); and others. Each of these
√
distributions corresponds to a particular choice of h(y, δ) and A(ζ ). For example, it is easy to show
that the normal distribution corresponds to h(y, δ) = (1/
2π δ) exp{−y2/(2δ)} and A(ζ ) = ζ 2/2.
In this case, the usual parameters µ and σ 2 just equal ζ and δ, respectively.
η>(cid:0)E(cid:2) ¯Z(cid:3) y(cid:1) − E(cid:2)A(η> ¯Z )(cid:3)i
h
Variational E-step. The distribution of y appears only in the cross-entropy term (4). Its form under
the GLM is
= E[log θ | γ ]+E[log p(w j | β1:K )]−log φ j +1+(cid:16) y

This changes the coordinate ascent step for each φ j , but the variational optimization is otherwise
unaffected. In particular, the gradient of the ELBO with respect to φ j becomes
∂L
∂φ j
Thus, the key to variational inference in sLDA is obtaining the gradient of the expected GLM log-
normalizer. Sometimes there is an exact expression, such as the normal case of Section 2. As another
example, the Poisson GLM leads to an exact gradient, which we omit for brevity.
Other times, no exact gradient is available. In a longer paper [3], we study two methods for this
situation. First, we can replace −E[A(η> ¯Z )] with an adjustable lower bound whose gradient is
known exactly; then we maximize over the original variational parameters plus the parameter con-
trolling the bound. Alternatively, an application of the multivariate delta method for moments [1],
plus standard exponential family theory, shows

E(cid:2)A(η> ¯Z )(cid:3)o

E(cid:2)A(η> ¯Z )(cid:3) ≈ A(η> ¯φ) + VarGLM(Y | ζ = η> ¯φ) · η>

(19)
Here, VarGLM denotes the response variance under the GLM, given a speciﬁed value of the natu-
ral parameter—in all standard cases, this variance is a closed-form function of φ j . The variance-
covariance matrix of ¯Z under q is already known in closed from from E[ ¯Z] and (5). Thus, computing
∂/∂φ j of (19) exactly is mechanical. However, using this approximation gives up the usual guaran-
tee that the ELBO lower bounds the marginal likelihood. We forgo details and further examples due
to space constraints.
The GLM contribution to the gradient determines whether the φ j coordinate update itself has a
closed form, as it does in the normal case (7) and the Poisson case (omitted). If the update is not
closed-form, we use numerical optimization, supplying a gradient obtained from one of the methods
described in the previous paragraph.
Parameter estimation (M-step). The topic parameter estimates are given by (8), as before. For the
corpus-level ELBO, the gradient with respect to η becomes

Varq ( ¯Z )η .

. (18)

δ

∂φ j

N δ

(cid:18)1

(cid:19) DX

n

η> ¯φd yd − E(cid:2)A(η> ¯Zd )(cid:3)o =(cid:18)1

(cid:19)( DX

¯φd yd − DX

δ

d=1

(20)
The appearance of µ(·) = EGLM[Y | ζ = ·] follows from exponential family properties. This GLM
mean response is a known function of η> ¯Zd in all standard cases. However, Eq[µ(η> ¯Zd ) ¯Zd] has

d=1

d=1

Eq

δ

.

(cid:2)µ(η> ¯Zd ) ¯Zd

(cid:3))

∂
∂η

6

Figure 2: Predictive R2 and per-word likelihood for the movie and Digg data (see Section 3).

an exact solution only in some cases (e.g. normal, Poisson). In other cases, we approximate the
expectation with methods similar to those applied for the φ j coordinate update. Reference [3] has
details, including estimation of δ and prediction, where we encounter the same issues.
The derivative with respect to δ, evaluated at ˆηnew, is

¯Zd ) ¯Zd

new

.

(21)

( DX

d=1

∂h(yd , δ)/∂δ

h(yd , δ)

)

−(cid:18) 1

δ2

(cid:19)( DX

d=1

¯φd yd − DX

Eq

d=1

(cid:2)µ(ˆη>

(cid:3))

Given that the rightmost summation has been evaluated, exactly or approximately, during the η
optimization, (21) has a closed form. Depending on h(y, δ) and its partial with respect to δ, we
obtain ˆδnew either in closed form or via one-dimensional numerical optimization.
Prediction. We form predictions just as in Section 2.2. The difference is that we now approximate
the expected response value of a test document as

E[Y | w1:N , α, β1:K , η, δ] ≈ Eq[µ(η> ¯Z )].

(22)

Again, this follows from iterated expectation plus the variational approximation. When the varia-
tional expectation cannot be computed exactly, we apply the approximation methods we relied on
for the GLM E-step and M-step. We defer speciﬁcs to [3].

3 Empirical results

We evaluated sLDA on two prediction problems. First, we consider “sentiment analysis” of news-
paper movie reviews. We use the publicly available data introduced in [10], which contains movie
reviews paired with the number of stars given. While Pang and Lee treat this as a classiﬁcation
problem, we treat it as a regression problem. With a 5000-term vocabulary chosen by tf-idf, the
corpus contains 5006 documents and comprises 1.6M words.
Second, we introduce the problem of predicting web page popularity on Digg.com. Digg is a com-
munity of users who share links to pages by submitting them to the Digg homepage, with a short
description. Once submitted, other users “digg” the links they like. Links are sorted on the Digg
homepage by the number of diggs they have received. Our Digg data set contains a year of link
descriptions, paired with the number of diggs each received during its ﬁrst week on the homepage.
(This corpus will be made publicly available at publication.) We restrict our attention to links in the
technology category. After trimming the top ten outliers, and using a 4145-term vocabulary chosen
by tf-idf, the Digg corpus contains 4078 documents and comprises 94K words.
For both sets of response variables, we transformed to approximate normality by taking logs. This
makes the data amenable to the continuous-response model of Section 2; for these two problems,
generalized linear modeling turned out to be unnecessary. We initialized β1:K to uniform topics, σ 2
to the sample variance of the response, and η to a grid on [−1, 1] in increments of 2/K . We ran EM
until the relative change in the corpus-level likelihood bound was less than 0.01%. In the E-step,
we ran coordinate-ascent variational inference for each document until the relative change in the

7

●●●●●●●●●●●●●●241020300.000.020.040.060.080.100.12Number of topicsPredictive R2●●●●●●●●●●●●●●24102030−8.6−8.5−8.4−8.3−8.2−8.1−8.0Number of topicsPer−word held out log likelihood●●●●●●●●●●●●●●●●●●●●5101520253035404550−6.42−6.41−6.40−6.39−6.38−6.37Number of topicsPer−word held out log likelihood●●●●●●●●●●●●●●●●●●●●51015202530354045500.00.10.20.30.40.5Number of topicsPredictive R2sLDALDAMovie corpusDigg corpusper-document ELBO was less than 0.01%. For the movie review data set, we illustrate in Figure 1 a
matching of the top words from each topic to the corresponding coefﬁcient ηk.
captured by the out-of-fold predictions: pR2 := 1 − (P(y − ˆy)2)/(P(y − ¯y)2).
We assessed the quality of the predictions with “predictive R2.” In our 5-fold cross-validation (CV),
we deﬁned this quantity as the fraction of variability in the out-of-fold response values which is
We compared sLDA to linear regression on the ¯φd from unsupervised LDA. This is the regression
equivalent of using LDA topics as classiﬁcation features [4].Figure 2 (L) illustrates that sLDA pro-
vides improved predictions on both data sets. Moreover, this improvement does not come at the cost
of document model quality. The per-word hold-out likelihood comparison in Figure 2 (R) shows that
sLDA ﬁts the document data as well or better than LDA. Note that Digg prediction is signiﬁcantly
harder than the movie review sentiment prediction, and that the homogeneity of Digg technology
content leads the model to favor a small number of topics.
Finally, we compared sLDA to the lasso, which is L1-regularized least-squares regression. The
lasso is a widely used prediction method for high-dimensional problems. We used each document’s
empirical distribution over words as its lasso covariates, setting the lasso complexity parameter with
5-fold CV. On Digg data, the lasso’s optimal model complexity yielded a CV pR2 of 0.088. The best
sLDA pR2 was 0.095, an 8.0% relative improvement. On movie data, the best Lasso pR2 was 0.457
versus 0.500 for sLDA, a 9.4% relative improvement. Note moreover that the Lasso provides only a
prediction rule, whereas sLDA models latent structure useful for other purposes.

4 Discussion

We have developed sLDA, a statistical model of labelled documents. The model accommodates the
different types of response variable commonly encountered in practice. We presented a variational
procedure for approximate posterior inference, which we then incorporated in an EM algorithm
for maximum-likelihood parameter estimation. We studied the model’s predictive performance on
two real-world problems. In both cases, we found that sLDA moderately improved on the lasso,
a state-of-the-art regularized regression method. Moreover, the topic structure recovered by sLDA
had higher hold-out likelihood than LDA on one problem, and equivalent hold-out likelihood on the
other. These results illustrate the beneﬁts of supervised dimension reduction when prediction is the
ultimate goal.

Acknowledgments

David M. Blei is supported by grants from Google and the Microsoft Corporation.

References

[1] P. Bickel and K. Doksum. Mathematical Statistics. Prentice Hall, 2000.
[2] D. Blei and M. Jordan. Modeling annotated data. In SIGIR, pages 127–134. ACM Press, 2003.
[3] D. Blei and J. McAuliffe. Supervised topic models. In preparation, 2007.
[4] D. Blei, A. Ng, and M. Jordan. Latent Dirichlet allocation. JMLR, 3:993–1022, 2003.
[5] P. Flaherty, G. Giaever, J. Kumm, M. Jordan, and A. Arkin. A latent variable model for

chemogenomic proﬁling. Bioinformatics, 21(15):3286–3293, 2005.

[6] K. Fukumizu, F. Bach, and M. Jordan. Dimensionality reduction for supervised learning with

reproducing kernel Hilbert spaces. Journal of Machine Learning Research, 5:73–99, 2004.

[7] T. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical Learning. 2001.
[8] A. McCallum, C. Pal, G. Druck, and X. Wang. Multi-conditional learning: Genera-

tive/discriminative training for clustering and classiﬁcation. In AAAI, 2006.

[9] P. McCullagh and J. A. Nelder. Generalized Linear Models. Chapman & Hall, 1989.
[10] B. Pang and L. Lee. Seeing stars: Exploiting class relationships for sentiment categorization

with respect to rating scales. In Proceedings of the ACL, 2005.

8

"
703,2007,Discriminative Keyword Selection Using Support Vector Machines,"Many tasks in speech processing involve classification of long term characteristics of a speech segment such as language, speaker, dialect, or topic. A natural technique for determining these characteristics is to first convert the input speech into a sequence of tokens such as words, phones, etc. From these tokens, we can then look for distinctive phrases, keywords, that characterize the speech. In many applications, a set of distinctive keywords may not be known a priori. In this case, an automatic method of building up keywords from short context units such as phones is desirable. We propose a method for construction of keywords based upon Support Vector Machines. We cast the problem of keyword selection as a feature selection problem for n-grams of phones. We propose an alternating filter-wrapper method that builds successively longer keywords. Application of this method on a language recognition task shows that the technique produces interesting and significant qualitative and quantitative results.","Discriminative Keyword Selection Using Support

Vector Machines

W. M. Campbell, F. S. Richardson

MIT Lincoln Laboratory
Lexington, MA 02420

wcampbell,frichard@ll.mit.edu

Abstract

Many tasks in speech processing involve classiﬁcation of long term characteristics
of a speech segment such as language, speaker, dialect, or topic. A natural tech-
nique for determining these characteristics is to ﬁrst convert the input speech into
a sequence of tokens such as words, phones, etc. From these tokens, we can then
look for distinctive sequences, keywords, that characterize the speech. In many
applications, a set of distinctive keywords may not be known a priori. In this
case, an automatic method of building up keywords from short context units such
as phones is desirable. We propose a method for the construction of keywords
based upon Support Vector Machines. We cast the problem of keyword selection
as a feature selection problem for n-grams of phones. We propose an alternat-
ing ﬁlter-wrapper method that builds successively longer keywords. Application
of this method to language recognition and topic recognition tasks shows that the
technique produces interesting and signiﬁcant qualitative and quantitative results.

1 Introduction

A common problem in speech processing is to identify properties of a speech segment such as
the language, speaker, topic, or dialect. A typical solution to this problem is to apply a detection
paradigm. A set of classiﬁers is applied to a speech segment to produce a decision. For instance, for
language recognition, we might construct detectors for English, French, and Spanish. The maximum
scoring detector on a speech segment would be the predicted language.

Two basic categories of systems have been applied to the detection problem. A ﬁrst approach uses
short-term spectral characteristics of the speech and models these with Gaussian mixture models
(GMMs) or support vector machines (SVMs) directly producing a decision. Although quite accurate,
this type of system produces only a classiﬁcation decision with no qualitative interpretation. A
second approach uses high level features of the speech such as phones and words to detect the
properties. An advantage of this approach is that, in some instances, we can explain why we made a
decision. For example, a particular phone or word sequence might indicate the topic. We adopt this
latter approach for our paper.

SVMs have become a common method of extracting high-level properties of sequences of speech
tokens [1, 2, 3, 4]. Sequence kernels are constructed by viewing a speech segment as a document of
tokens. The SVM feature space in this case is a scaling of co-occurrence probabilities of tokens in
an utterance. This technique is analogous to methods for applying SVMs to text classiﬁcation [5].

SVMs have been applied at many linguistic levels of tokens as detectors. Our focus in this paper
is at the acoustic phone level. Our goal is to automatically derive long sequences of phones which

∗This work was sponsored by the Department of Homeland Security under Air Force Contract FA8721-
05-C-0002. Opinions, interpretations, conclusions, and recommendations are those of the authors and are not
necessarily endorsed by the United States Government.

1

we call keywords which are characteristic of a given class. Prior work, for example, in language
recognition [6], has shown that certain words are a signiﬁcant predictor of a language. For instance,
the presence of the phrase “you know” in a conversational speech segment is a strong indicator of
English. A difﬁculty in using words as the indicator of the language is that we may not have available
a speech-to-text (STT) system in all languages of interest. In this case, we’d like to automatically
construct keywords that are indicative of the language. Note that a similar problem can occur in
other property extraction problems. For instance, in topic recognition, proper names not in our STT
system dictionary may be a strong indicator of topic.

Our basic approach is to view keyword construction as a feature selection problem. Keywords are
composed of sequences of phones of length n, i.e. n-grams. We would like to ﬁnd the set of
n-grams that best discriminates between classes. Unfortunately, this problem is difﬁcult to solve
directly, since the number of unique n-grams grows exponentially with increasing n. To alleviate
this difﬁcultly, we propose a method that starts with lower order n-grams and successively builds
higher order n-grams.
The outline of the paper is as follows. In Section 2.1, we review the basic architecture that we use
for phone recognition and how it is applied to the problem. In Section 2.2, we review the application
of SVMs to determining properties. Section 3.1 describes a feature selection method for SVMs.
Section 3.2 presents our method for constructing long context units of phones to automatically cre-
ate keywords. We use a novel feature selection approach that attempts to ﬁnd longer strings that
discriminate well between classes. Finally, in Section 4, we show the application of our method
to language and topic recognition problems. We show qualitatively that the method produces in-
teresting keywords. Quantitatively, we show that the method produces keywords which are good
discriminators between classes.

2 Phonotactic Classiﬁcation

2.1 Phone Recognition

The high-level token extraction component of our system is a phone recognizer based upon the Brno
University (BUT) design [7]. The basic architecture of this system is a monophone HMM system
with a null grammar. Monophones are modeled by three states. This system uses two powerful
components to achieve high accuracy. First, split temporal context (STC) features provide contextual
cues for modeling monophones. Second, the BUT recognizer extensively uses discriminatively
trained feedforward artiﬁcial neural networks (ANNs) to model HMM state posterior probabilities.

We developed a phone recognizer for English units using the BUT architecture and automatically
generated STT transcripts on the Switchboard 2 Cell corpora [8]. Training data consisted of approx-
imately 10 hours of speech. ANN training was accomplished using the ICSI Quicknet package [9].
The resulting system has 49 monophones including silence.

The BUT recognizer is used along with the HTK HMM toolkit [10] to produce lattices. Lattices
encode multiple hypotheses with acoustic likelihoods. From a lattice, a 1-best (Viterbi) output can
be produced. Alternatively, we use the lattice to produce expected counts of tokens and n-grams of
tokens.

Expected counts of n-grams can be easily understood as an extension of standard counts. Suppose
we have a hypothesized string of tokens, W = w1,··· , wn. Then bigrams are created by group-
ing two tokens at a time to form, W2 = w1_w2, w2_w3,··· , wn−1_wn. Higher order n-grams
are formed from longer juxtapositions of tokens. The count function for a given bigram, di, is
count(di|W2) is the number of occurrences of di in the sequence W2. To extend counts to a lattice,
L, we ﬁnd the expected count over all all possible hypotheses in the lattice,

count(di|L) = EW [count(di|W )] = XW ∈L

p(W|L) count(di|W ).

(1)

The expected counts can be computed efﬁciently by a forward-backward algorithm; more details
can be found in Section 3.3 and [11].

2

Dj = min(cid:18)Cj, gj (cid:18)

1

p(dj|all)(cid:19)(cid:19)

(4)

A useful application of expected counts is to ﬁnd the probability of an n-gram in a lattice. For a
lattice, L, the joint probability of an n-gram, di, is

where the sum in (2) is performed over all unique n-grams in the utterance.

p(di|L) =

count(di|L)
Pj count(dj|L)

(2)

2.2 Discriminative Language Modeling: SVMs

We focus on token-based language recognition with SVMs using the approach from [1, 4]. Similar
to [1], a lattice of tokens, L, is modeled using a bag-of-n-grams approach. Joint probabilities of
the unique n-grams, dj, on a per conversation basis are calculated, p(dj|L), see (2). Then, the
probabilities are mapped to a sparse vector with entries
Djp(dj|W ).

The selection of the weighting, Dj, in (3) is critical for good performance. A typical choice is of the
form

(3)

where gj(·) is a function which squashes the dynamic range, and Cj is a constant. The probabil-
ity p(dj|all) in (4) is calculated from the observed probability across all classes. The squashing
function should monotonically map the interval [1,∞) to itself to suppress large inverse probabili-
ties. Typical choices for gj are gj(x) = √x and gj(x) = log(x) + 1. In both cases, the squashing
function gj normalizes out the typicality of a feature across all classes. The constant Cj limits the
effect of any one feature on the kernel inner product. If we set Cj = 1, then this makes Dj = 1 for
all j. For the experiments in this paper, we use gj(x) = √x, which is suited to high frequency token
streams.

The general weighting of probabilities is then combined to form a kernel between two lattices,
see [1] for more details. For two lattices, L1 and L2, the kernel is

K(L1,L2) = Xj

D2
j p(dj|L1)p(dj|L2).

(5)

Intuitively, the kernel in (5) says that if the same n-grams are present in two sequences and the
normalized frequencies are similar there will be a high degree of similarity (a large inner product).
If n-grams are not present, then this will reduce similarity since one of the probabilities in (5) will be
zero. The normalization Dj insures that n-grams with large probabilities do not dominate the kernel
function. The kernel can alternatively be viewed as a linearization of the log-likelihood ratio [1].

Incorporating the kernel (5) into an SVM system is straightforward. SVM training and scoring
require only a method of kernel evaluation between two objects that produces positive deﬁnite kernel
matrices (the Mercer condition). We use the package SVMTorch [12]. Training is performed with a
one-versus-all strategy. For each target class, we group all remaining class data and then train with
these two classes.

3 Discriminative Keyword Selection

3.1 SVM Feature Selection

A ﬁrst step towards an algorithm for automatic keyword generation using phones is to examine
feature selection methods. Ideally, we would like to select over all possible n-grams, where n is
varying, the most discriminative sequences for determining a property of a speech segment. The
number of features in this case is prohibitive, since it grows exponentially with n. Therefore, we
have to consider alternate methods.

As a ﬁrst step, we examine feature selection for ﬁxed n and look for keywords with n or less phones.
Suppose that we have a set of candidate keywords. Since we are already using an SVM, a natural
algorithm for discriminative feature selection in this case is to use a wrapper method [13].

3

Suppose that the optimized SVM solution is

and

f (X) = Xi

αiK(X, Xi) + c

w = Xi

αib(Xi)

(6)

(7)

where b(Xi) is the vector of weighted n-gram probabilities in (3). We note that the kernel presented
in (5) is linear. Also, the n-gram probabilities have been normalized in (3) by their probability across
the entire data set. Intuitively, because of this normalization and since f (X) = wtb(X) + c, large
magnitude entries in w correspond to signiﬁcant features.

A conﬁrmation of this intuitive idea is the algorithm of Guyon, et. al. [14]. Guyon proposes an
iterative wrapper method for feature selection for SVMs which has these basic steps:

• For a set of features, S, ﬁnd the SVM solution with model w.
• Rank the features by their corresponding model entries w2

in (7).

i . Here, wi is the ith entry of w

• Eliminate low ranking features using a threshold.

The algorithm may be iterated multiple times.

Guyon’s algorithm for feature selection can be used for picking signiﬁcant n-grams as keywords.
We can create a kernel which is the sum of kernels as in (5) up to the desired n. We then train an
SVM and rank n-grams according to the magnitude of the entries in the SVM model vector, w.
As an example, we have looked at this feature selection method for a language recognition task
with trigrams (to be described in Section 4). Figure 1 provides a motivation for the applicability of
Guyon’s feature selection method. The ﬁgure shows two functions. First, the cumulative density
function (CDF) of the SVM model values, |wi|, is shown. The CDF has an S-curve shape; i.e., only
a small set of models weights has large magnitudes. The second curve shows the equal error rate
(EER) of the task as a function of applying one iteration of the Guyon algorithm and retraining the
SVM. EER is deﬁned as the value where the miss and false alarm rates are equal. All features with
|wi| below the value on the x-axis are discarded in the ﬁrst iteration. From the ﬁgure, we see that
only a small fraction (< 5%) of the features are needed to obtain good error rates. This interesting
result provides motivation that a small subset of keywords are signiﬁcant to the task.

1

0.75

i

|

w

0.5

|
 

F
D
C

0.25

CDF

EER

0.2

0.15

0.1

0.05

e

t

a
R

 
r
o
r
r

E

 
l

a
u
q
E

0
10−4
10−4

10−3
10−3

10−2
10−2

Threshold

10−1
10−1

1000
100

Figure 1: Feature selection for a trigram language recognition task using Guyon’s method

4

3.2 Keywords via an alternating wrapper/ﬁlter method

The algorithm in Section 3.1 gives a method for n-gram selection for ﬁxed n. Now, suppose we
want to ﬁnd keywords for arbitrary n. One possible hypothesis for keyword selection is that since
higher order n-grams are discriminative, lower order n-grams in the keywords will also be discrim-
inative. Therefore, it makes sense to ﬁnding distinguishing lower order n-grams and then construct
longer units from these. On the basis of this idea, we propose the following algorithm for keyword
construction:
Keyword Building Algorithm

n, to all possible n-grams of phones

• Start with an initial value of n = ns. Initialize the set, S ′
• (Wrapper Step) General n. Apply the feature selection algorithm in Section 3.1 to produce
• (Filter Step) Construct a new set of (n + 1)-grams by juxtaposing elements from Sn with
n+1 =

including lower order grams. By default, let S1 be the set of all phones.
a subset of distinguishing n-grams, Sn ⊂ S ′
n.
phones. Nominally, we take this step to be juxtaposition on the right and left, S ′
{dp, qd|d ∈ Sn, p ∈ S1, q ∈ S1}.

• Iterate to the wrapper step.
• Output: Sn at some stopping n.

A few items should be noted about the proposed keyword building algorithm. First, we call the sec-
ond feature selection process a ﬁlter step, since induction has not been applied to the (n + 1)-gram
features. Second, note that the purpose of the ﬁlter step is to provide a candidate set of possible
(n + 1)-grams which can then be more systematically reduced. Third, several potential algorithms
exist for the ﬁlter step. In our experiments and in the algorithm description, we nominally append
one phone to the beginning and end of an n-gram. Another possibility is to try to combine over-
lapping n-grams. For instance, suppose the keyword is some_people which has phone transcript
s_ah_m_p_iy_p_l. Then, if we are looking at 4-grams, we might see as top features s_ah_m_p and
p_iy_p_l and combine these to produce a new keyword.

3.3 Keyword Implementation

The expected n-gram counts were computed from lattices using the forward-backward algo-
rithm. Equation (8) gives the posterior probability of a connected sequence of arcs in the lattice
where src_nd(a) and dst_nd(a) are the source and destination node of arc a, ℓ(a)is the likelihood
associated with arc a, α(n) and β(n) are the forward and backward probabilities of reaching node n
from the beginning or end of the lattice L respectively, and ℓ(L) is the total likelihood of the lattice
(the α(·) of the ﬁnal node or β(·) of the initial node of the lattice).

p(aj, ..., aj+n) =

α(src_nd(aj))ℓ(aj) . . . ℓ(aj+n)β(dst_nd(aj+n))

(8)

ℓ(L)

Now if we deﬁne the posterior probability of a node p(n) as p(n) = (α(n)β(n))/ℓ(L). Then
equation (8) becomes:

p(aj, ..., aj+n) =

p(aj) . . . p(aj+n)

p(src_nd(aj+1)) . . . p(src_nd(aj+n))

.

(9)

Equation (9) is attractive because it provides a way of computing the path posteriors locally using
only the individual arc and node posteriors along the path. We use this computation along with a
trie structure [15] to compute the posteriors of our keywords.

4 Experiments

4.1 Language Recognition Experimental Setup

The phone recognizer described in Section 2.1 was used to generate lattices across a train and an
evaluation data set. The training data set consists of more than 360 hours of telephone speech

5

spanning 13 different languages and coming from a variety of different sources including Callhome,
Callfriend and Fisher. The evaluation data set is the NIST 2005 Language Recognition Evaluation
data consisting of roughly 20,000 utterances (with duration of 30, 10 or 3 seconds depending on the
task) coming from three collection sources including Callfriend, Mixer and OHSU. We evaluated
our system for the 30 and 10 second task under the the NIST 2005 closed condition which limits
the evaluation data to 7 languages (English, Hindi, Japanese, Korean, Mandarin, Spanish and Tamil)
coming only from the OHSU data source.

The training and evaluation data was segmented using an automatic speech activity detector and
segments smaller than 0.5 seconds were thrown out. We also sub-segmented long audio ﬁles in the
training data to keep the duration of each utterance to around 5 minutes (a shorter duration would
have created too many training instances). Lattice arcs with posterior probabilities lower than 10−6
were removed and lattice expected counts smaller than 10−3 were ignored. The top and bottom
600 ranking keywords for each language were selected after each training iteration. The support
vector machine was trained using a kernel formulation which requires pre-computing all of the
kernel distances between the data points and using an alternate kernel which simply indexes into
the resulting distance matrix (this approach becomes difﬁcult when the number of data points is too
large).

4.2 Language Recognition Results (Qualitative and Quantitative)

To get a sense of how well our keyword building algorithm was working, we looked at the top
ranking keywords from the English model only (since our phone recognizer is trained using the
English phone set). Table 1 summarizes a few of the more compelling phone 5-grams, and a possible
keyword that corresponds to each one. Not suprisingly, we noticed that in the list of top-ranking
n-grams there were many variations or partial n-gram matches to the same keyword, as well as
n-grams that didn’t correspond to any apparent keyword.
The equal error rates for our system on the NIST 2005 language recognition evaluation are summa-
rized in Table 2. The 4-gram system gave a relative improvement of 12% on the 10 second task and
9% on the 30 second task, but despite the compelling keywords produced by the 5-gram system, the
performance actually degraded signiﬁcantly compared to the 3-gram and 4-gram systems.

Table 1: Top ranking keywords for 5-gram SVM for English language recognition model

phones

Rank

SIL_Y_UW_N_OW

!NULL_SIL_Y_EH_AX
!NULL_SIL_IY_M_TH

P_IY_P_AX_L
R_IY_L_IY_SIL

Y_UW_N_OW_OW

T_L_AY_K_SIL
L_AY_K_K_SIL

R_AY_T_SIL_!NULL

HH_AE_V_AX_N

!NULL_SIL_W_EH_L

1
3
4
6
7
8
17
23
27
29
37

keyword
you know
<s> yeah
<s> ???
people
really

you know (var)

? like

like (var)
right </s>
have an
<s> well

Table 2: %EER for 10 and 30 second NIST language recognition tasks

N

10sec
30sec

1

25.3
18.3

2

16.5
07.4

3

11.3
04.3

4

10.0
03.9

5

13.6
05.6

6

4.3 Topic Recognition Experimental Setup

Topic recognition was performed using a subset of the phase I Fisher corpus (English) from LDC.
This corpus consists of 5, 851 telephone conversations. Participants were given instructions to dis-
cuss a topic for 10 minutes from 40 different possible topics. Topics included “Education”, “Hob-
bies,” “Foreign Relations”, etc. Prompts were used to elicit discussion on the topics. An example
prompt is:

Movies: Do each of you enjoy going to the movies in a theater, or would you
rather rent a movie and stay home? What was the last movie that you saw? Was it
good or bad and why?

For our experiments, we used 2750 conversation sides for training. We also constructed development
and test sets of 1372 conversation sides each. The training set was used to ﬁnd keywords and models
for topic detection.

4.4 Topic Recognition Results

We ﬁrst looked at top ranking keywords for several topics; some results are shown in Table 3. We
can see that many keywords show a strong correspondence with the topic. Also, there are partial
keywords which correspond to what appears to be longer keywords, e.g. “eh_t_s_ih_k” corresponds
to get sick.

As in the language recognition task, we used EER as the performance measure. Results in Table 4
show the performance for several n-gram orders. Performance improves going from 3-grams to 4-
grams. But, as with the language recognition task, we see a degradation in performance for 5-grams.

5 Conclusions and future work

We presented a method for automatic construction of keywords given a discriminative speech classi-
ﬁcation task. Our method was based upon successively building longer span keywords from shorter
span keywords using phones as a fundamental unit. The problem was cast as a feature selection
problem, and an alternating ﬁlter and wrapper algorithm was proposed. Results showed that reason-
able keywords and improved performance could be achieved using this methodology.

Table 3: Top keyword for 5-gram SVM in Topic Recognition

Topic

Professional Sports on TV
Hypothetical: Time Travel

Afﬁrmative Action
US Public Schools

Movies
Hobbies

September 11

Issues in the Middle East

Illness

Hypothetical: One Million Dollars to leave the US

Phones

S_P_AO_R_T

Keyword

sport

G_OW_B_AE_K
AX_V_AE_K_CH [afﬁrmat]ive act[ion]

go back

S_K_UW_L_Z
IY_V_IY_D_IY
HH_OH_B_IY_Z
HH_AE_P_AX_N

IH_Z_R_IY_L
EH_T_S_IH_K
Y_UW_M_AY_Y

schools
DVD
hobbies
happen
Israel

[g]et sick
you may

Table 4: Performance of Topic Detection for Different n-gram orders

n-gram order

3

4

5

EER (%)

10.22

8.95

9.40

7

Numerous possibilities exist for future work on this task. First, extension and experimentation on
other tasks such as dialect and speaker recognition would be interesting. The method has the poten-
tial for discovery of new interesting characteristics. Second, comparison of this method with other
feature selection methods may be appropriate [16]. A third area for extension is various technical
improvements. For instance, we might want to consider more general keyword models where skips
are allowed (or more general ﬁnite state transducers [17]). Also, alternate methods for the ﬁlter for
constructing higher order n-grams is a good area for exploration.

References

[1] W. M. Campbell, J. P. Campbell, D. A. Reynolds, D. A. Jones, and T. R. Leek, “Phonetic
speaker recognition with support vector machines,” in Advances in Neural Information Pro-
cessing Systems 16, Sebastian Thrun, Lawrence Saul, and Bernhard Schölkopf, Eds. MIT
Press, Cambridge, MA, 2003.

[2] W. M. Campbell, T. Gleason, J. Navratil, D. Reynolds, W. Shen, E. Singer, and P. Torres-
Carrasquillo, “Advanced language recognition using cepstra and phonotactics: MITLL system
performance on the NIST 2005 language recognition evaluation,” in Proc. IEEE Odyssey,
2006.

[3] Bin Ma and Haizhou Li, “A phonotactic-semantic paradigm for automatic spoken document

classiﬁcation,” in The 28th Annual International ACM SIGIR Conference, Brazil, 2005.

[4] Lu-Feng Zhai, Man hung Siu, Xi Yang, and Herbert Gish, “Discriminatively trained language
models using support vector machines for language identiﬁcation,” in Proc. IEEE Odyssey:
The Speaker and Language Recognition Workshop, 2006.

[5] T. Joachims, Learning to Classify Text Using Support Vector Machines, Kluwer Academic

Publishers, 2002.

[6] W. M. Campbell, F. Richardson, and D. A. Reynolds, “Language recognition with word lattices

and support vector machines,” in Proceedings of ICASSP, 2007, pp. IV–989 – IV–992.

[7] Petr Schwarz, Matejka Pavel, and Jan Cernocky, “Hierarchical structures of neural networks

for phoneme recognition,” in Proceedings of ICASSP, 2006, pp. 325–328.

[8] Linguistic Data Consortium, “Switchboard-2 corpora,” http://www.ldc.upenn.edu.
[9] “ICSI QuickNet,” http://www.icsi.berkeley.edu/Speech/qn.html.
[10] S. Young, Gunnar Evermann, Thomas Hain, D. Kershaw, Gareth Moore, J. Odell, D. Ollason,

V. Valtchev, and P. Woodland, The HTK book, Entropic, Ltd., Cambridge, UK, 2002.

[11] L. Rabiner and B.-H. Juang, Fundamentals of Speech Recognition, Prentice-Hall, 1993.
[12] Ronan Collobert and Samy Bengio, “SVMTorch: Support vector machines for large-scale

regression problems,” Journal of Machine Learning Research, vol. 1, pp. 143–160, 2001.

[13] Avrim L. Blum and Pat Langley, “Selection of relevant features and examples in machine

learning,” Artiﬁcial Intelligence, vol. 97, no. 1-2, pp. 245–271, Dec. 1997.

[14] I. Guyon, J. Weston, S. Barnhill, and V. Vapnik, “Gene selection for cancer classiﬁcation using

support vector machines,” Machine Learning, vol. 46, no. 1-3, pp. 389–422, 2002.

[15] Konrad Rieck and Pavel Laskov, “Language models for detection of unknown attacks in net-

work trafﬁc,” Journal of Computer Virology, vol. 2, no. 4, pp. 243–256, 2007.

[16] Takaaki Hori, I. Lee Hetherington, Timothy J. Hazen, and James R. Glass, “Open-vocabulary

spoken utterance retrieval using confusion neworks,” in Proceedings of ICASSP, 2007.

[17] C. Cortes, P. Haffner, and M. Mohri, “Rational kernels,” in Advances in Neural Information
Processing Systems 15, S. Thrun S. Becker and K. Obermayer, Eds., Cambridge, MA, 2003,
pp. 601–608, MIT Press.

8

"
917,2007,Fitted Q-iteration in continuous action-space MDPs,"We consider continuous state, continuous action batch reinforcement learning where the goal is to learn a good policy from a sufficiently rich trajectory generated by another policy. We study a variant of fitted Q-iteration, where the greedy action selection is replaced by searching for a policy in a restricted set of candidate policies by maximizing the average action values. We provide a rigorous theoretical analysis of this algorithm, proving what we believe is the first finite-time bounds for value-function based algorithms for continuous state- and action-space problems.","Fitted Q-iteration in continuous action-space MDPs

Andr´as Antos

Computer and Automation Research Inst.
of the Hungarian Academy of Sciences
Kende u. 13-17, Budapest 1111, Hungary

antos@sztaki.hu

R´emi Munos

SequeL project-team, INRIA Lille
59650 Villeneuve d’Ascq, France

remi.munos@inria.fr

Csaba Szepesv´ari∗

Department of Computing Science

University of Alberta

Edmonton T6G 2E8, Canada

szepesva@cs.ualberta.ca

Abstract

We consider continuous state, continuous action batch reinforcement learning
where the goal is to learn a good policy from a sufﬁciently rich trajectory gen-
erated by some policy. We study a variant of ﬁtted Q-iteration, where the greedy
action selection is replaced by searching for a policy in a restricted set of can-
didate policies by maximizing the average action values. We provide a rigorous
analysis of this algorithm, proving what we believe is the ﬁrst ﬁnite-time bound
for value-function based algorithms for continuous state and action problems.

1 Preliminaries

We will build on the results from [1, 2, 3] and for this reason we use the same notation as these
papers. The unattributed results cited in this section can be found in the book [4].
A discounted MDP is deﬁned by a quintuple (X ,A, P, S, γ), where X is the (possible inﬁnite)
state space, A is the set of actions, P : X × A → M(X ) is the transition probability kernel with
P (·|x, a) deﬁning the next-state distribution upon taking action a from state x, S(·|x, a) gives the
corresponding distribution of immediate rewards, and γ ∈ (0, 1) is the discount factor. Here X is
a measurable space and M(X ) denotes the set of all probability measures over X . The Lebesgue-
measure shall be denoted by λ. We start with the following mild assumption on the MDP:
Assumption A1 (MDP Regularity) X is a compact subset of the dX -dimensional Euclidean space,
A is a compact subset of [−A∞, A∞]dA. The random immediate rewards are bounded by ˆRmax
rS(dr|x, a), is uniformly bounded
and that the expected immediate reward function, r(x, a) =
by Rmax: (cid:107)r(cid:107)∞ ≤ Rmax.
A policy determines the next action given the past observations. Here we shall deal with stationary
(cid:80)∞
(Markovian) policies which choose an action in a stochastic way based on the last observation only.
The value of a policy π when it is started from a state x is deﬁned as the total expected discounted
t=0 γtRt|X0 = x]. Here
reward that is encountered while the policy is executed: V π(x) = Eπ [
Rt ∼ S(·|Xt, At) is the reward received at time step t, the state, Xt, evolves according to Xt+1 ∼
∗Also with: Computer and Automation Research Inst. of the Hungarian Academy of Sciences Kende u.

(cid:82)

13-17, Budapest 1111, Hungary.

1

(cid:80)∞
t=0 γtRt|X0 = x, A0 = a].

P (·|Xt, At), where At is sampled from the distribution determined by π. We use Qπ : X ×A → R
to denote the action-value function of policy π: Qπ(x, a) = Eπ [
The goal is to ﬁnd a policy that attains the best possible values, V ∗(x) = supπ V π(x), at all states
x ∈ X . Here V ∗ is called the optimal value function and a policy π∗ that satisﬁes V π∗(x) =
V ∗(x) for all x ∈ X is called optimal. The optimal action-value function Q∗(x, a) is Q∗(x, a) =
supπ Qπ(x, a). We say that a (deterministic stationary) policy π is greedy w.r.t. an action-value
function Q ∈ B(X × A), and we write π = ˆπ(·; Q), if, for all x ∈ X , π(x) ∈ argmaxa∈A Q(x, a).
(cid:82)
Under mild technical assumptions, such a greedy policy always exists. Any greedy policy w.r.t. Q∗
is optimal. For π : X → A we deﬁne its evaluation operator, T π : B(X × A) → B(X × A), by
(cid:82)
X Q(y, π(y)) P (dy|x, a). It is known that Qπ = T πQπ. Further, if
(T πQ)(x, a) = r(x, a) + γ
we let the Bellman operator, T : B(X × A) → B(X × A), deﬁned by (T Q)(x, a) = r(x, a) +
X supb∈A Q(y, b) P (dy|x, a) then Q∗ = T Q∗. It is known that V π and Qπ are bounded by
γ
Rmax/(1 − γ), just like Q∗ and V ∗. For π : X → A, the operator Eπ : B(X × A) → B(X ) is
deﬁned by (EπQ)(x) = Q(x, π(x)), while E : B(X × A) → B(X ) is deﬁned by (EQ)(x) =
supa∈A Q(x, a).
(cid:82)
Throughout the paper F ⊂ {f : X × A → R} will denote a subset of real-valued functions over
(cid:82)
the state-action space X × A and Π ⊂ AX will be a set of policies. For ν ∈ M(X ) and f : X → R
X |f(x)|pν(dx). We simply write (cid:107)f(cid:107)ν for (cid:107)f(cid:107)2,ν.
measurable, we let (for p ≥ 1) (cid:107)f(cid:107)p
Further, we extend (cid:107)·(cid:107)ν to F by (cid:107)f(cid:107)2
X |f|2(x, a) dν(x) dλA(a), where λA is the uniform
distribution over A. We shall use the shorthand notation νf to denote the integral
f(x)ν(dx). We
denote the space of bounded measurable functions with domain X by B(X ). Further, the space of
measurable functions bounded by 0 < K < ∞ shall be denoted by B(X ; K). We let (cid:107)·(cid:107)∞ denote
the supremum norm.

p,ν =
ν =
A

(cid:82)

(cid:82)

2 Fitted Q-iteration with approximate policy maximization
We assume that we are given a ﬁnite trajectory, {(Xt, At, Rt)}1≤t≤N , generated by some stochastic
stationary policy πb, called the behavior policy: At ∼ πb(·|Xt), Xt+1 ∼ P (·|Xt, At), Rt ∼
S(·|Xt, At), where πb(·|x) is a density with π0
The generic recipe for ﬁtted Q-iteration (FQI) [5] is

def= inf (x,a)∈X×A πb(a|x) > 0.

(1)
where Regress is an appropriate regression procedure and Dk(Qk) is a dataset deﬁning a regression
problem in the form of a list of data-point pairs:

Qk+1 = Regress(Dk(Qk)),

(cid:105)

(cid:190)

(cid:189)(cid:104)

Dk(Qk) =

(Xt, At), Rt + γ max

b∈A Qk(Xt+1, b)

1≤t≤N

.1

(cid:82)

Fitted Q-iteration can be viewed as approximate value iteration applied to action-value func-
tions. To see this note that value iteration would assign the value (T Qk)(x, a) = r(x, a) +
maxb∈A Qk(y, b) P (dy|x, a) to Qk+1(x, a) [6]. Now, remember that the regression function for
γ
the jointly distributed random variables (Z, Y ) is deﬁned by the conditional expectation of Y given
Z: m(Z) = E [Y |Z]. Since for any ﬁxed function Q, E [Rt + γ maxb∈A Q(Xt+1, b)|Xt, At] =
(T Q)(Xt, At), the regression function corresponding to the data Dk(Q) is indeed T Q and hence if
FQI solved the regression problem deﬁned by Qk exactly, it would simulate value iteration exactly.
However, this argument itself does not directly lead to a rigorous analysis of FQI: Since Qk is
obtained based on the data, it is itself a random function. Hence, after the ﬁrst iteration, the “target”
function in FQI becomes random. Furthermore, this function depends on the same data that is used
to deﬁne the regression problem. Will FQI still work despite these issues? To illustrate the potential
difﬁculties consider a dataset where X1, . . . , XN is a sequence of independent random variables,
which are all distributed uniformly at random in [0, 1]. Further, let M be a random integer greater
than N which is independent of the dataset (Xt)N
t=1. Let U be another random variable, uniformly
distributed in [0, 1]. Now deﬁne the regression problem by Yt = fM,U (Xt), where fM,U (x) =
sgn(sin(2M 2π(x + U))). Then it is not hard to see that no matter how big N is, no procedure can

1Since the designer controls Qk, we may assume that it is continuous, hence the maximum exists.

2

estimate the regression function fM,U with a small error (in expectation, or with high probability),
even if the procedure could exploit the knowledge of the speciﬁc form of fM,U . On the other hand,
if we restricted M to a ﬁnite range then the estimation problem could be solved successfully. The
example shows that if the complexity of the random functions deﬁning the regression problem is
uncontrolled then successful estimation might be impossible.
Amongst the many regression methods in this paper we have chosen to work with least-squares
methods. In this case Equation (1) takes the form

(cid:181)

(cid:183)

(cid:184)(cid:182)2

Qk+1 = argmin

Q∈F

1

πb(At|Xt)

Q(Xt, At) −

Rt + γ max

b∈A Qk(Xt+1, b)

.

(2)

N(cid:88)

t=1

We call this method the least-squares ﬁtted Q-iteration (LSFQI) method. Here we introduced the
weighting 1/πb(At|Xt) since we do not want to give more weight to those actions that are preferred
by the behavior policy.
Besides this weighting, the only parameter of the method is the function set F. This function set
should be chosen carefully, to keep a balance between the representation power and the number of
samples. As a speciﬁc example for F consider neural networks with some ﬁxed architecture. In
this case the function set is generated by assigning weights in all possible ways to the neural net.
Then the above minimization becomes the problem of tuning the weights. Another example is to use
linearly parameterized function approximation methods with appropriately selected basis functions.
In this case the weight tuning problem would be less demanding. Yet another possibility is to let F
be an appropriate restriction of a Reproducing Kernel Hilbert Space (e.g., in a ball). In this case the
training procedure becomes similar to LS-SVM training [7].
As indicated above, the analysis of this algorithm is complicated by the fact that the new dataset
is deﬁned in terms of the previous iterate, which is already a function of the dataset. Another
complication is that the samples in a trajectory are in general correlated and that the bias introduced
by the imperfections of the approximation architecture may yield to an explosion of the error of the
procedure, as documented in a number of cases in, e.g., [8].
Nevertheless, at least for ﬁnite action sets, the tools developed in [1, 3, 2] look suitable to show
that under appropriate conditions these problems can be overcome if the function set is chosen in
a judicious way. However, the results of these works would become essentially useless in the case
of an inﬁnite number of actions since these previous bounds grow to inﬁnity with the number of
actions. Actually, we believe that this is not an artifact of the proof techniques of these works, as
suggested by the counterexample that involved random targets. The following result elaborates this
point further:
Proposition 2.1. Let F ⊂ B(X × A). Then even if the pseudo-dimension of F is ﬁnite, the fat-
shattering function of

(cid:190)

(cid:189)

F∨
max =

can be inﬁnite over (0, 1/2).2

VQ : VQ(·) = max

a∈A Q(·, a), Q ∈ F

Without going into further details, let us just note that the ﬁniteness of the fat-shattering function is a
sufﬁcient and necessary condition for learnability and the ﬁniteness of the fat-shattering function is
implied by the ﬁniteness of the pseudo-dimension [9].The above proposition thus shows that without
imposing further special conditions on F, the learning problem may become infeasible.
One possibility is of course to discretize the action space, e.g., by using a uniform grid. However, if
the action space has a really high dimensionality, this approach becomes unfeasible (even enumer-
ating 2dA points could be impossible when dA is large). Therefore we prefer alternate solutions.
Another possibility is to make the functions in F, e.g., uniformly Lipschitz in their state coordinates.
Then the same property will hold for functions in F∨
max and hence by a classical result we can bound
the capacity of this set (cf. pp. 353–357 of [10]). One potential problem with this approach is that
this way it might be difﬁcult to get a ﬁne control of the capacity of the resulting set.

2The proof of this and the other results are given in the appendix, available in the extended version of this

paper, downloadable from http://hal.inria.fr/inria-00185311/en/.

3

In the approach explored here we modify the ﬁtted Q-iteration algorithm by introducing a policy
set Π and a search over this set for an approximately greedy policy in a sense that will be made
precise in a minute. Our algorithm thus has four parameters: F, Π, K, Q0. Here F is as before, Π
is a user-chosen set of policies (mappings from X to A), K is the number of iterations and Q0 is an
initial value function (a typical choice is Q0 ≡ 0). The algorithm computes a sequence of iterates
(Qk, ˆπk), k = 0, . . . , K, deﬁned by the following equations:

N(cid:88)
N(cid:88)
N(cid:88)

t=1

t=1

t=1

ˆπ0 = argmax

Q0(Xt, π(Xt)),

π∈Π

Qk+1 = argmin

Q∈F

1

πb(At|Xt)

(cid:179)

Q(Xt, At) −(cid:163)

ˆπk+1 = argmax

Qk+1(Xt, π(Xt)).

π∈Π

Rt + γQk(Xt+1, ˆπk(Xt+1))

(cid:164)(cid:180)2

,

(3)

(4)

Thus, (3) is similar to (2), while (4) deﬁnes the policy search problem. The policy search will
generally be solved by a gradient procedure or some other appropriate method. The cost of this step
will be primarily determined by how well-behaving the iterates Qk+1 are in their action arguments.
For example, if they were quadratic and if π was linear then the problem would be a quadratic
optimization problem. However, except for special cases3 the action value functions will be more
complicated, in which case this step can be expensive. Still, this cost could be similar to that of
searching for the maximizing actions for each t = 1, . . . , N if the approximately maximizing actions
are similar across similar states.
This algorithm, which we could also call a ﬁtted actor-critic algorithm, will be shown to overcome
the above mentioned complexity control problem provided that the complexity of Π is controlled
appropriately. Indeed, in this case the set of possible regression problems is determined by the set

and the proof will rely on controlling the complexity of F∨

Π by selecting F and Π appropriately.

F∨
Π = { V : V (·) = Q(·, π(·)), Q ∈ F, π ∈ Π} ,

3 The main theoretical result

3.1 Outline of the analysis

In order to gain some insight into the behavior of the algorithm, we provide a brief summary of its
error analysis. The main result will be presented subsequently. For f,Q ∈ F and a policy π, we
deﬁne the tth TD-error as follows:

dt(f; Q, π) = Rt + γQ(Xt+1, π(Xt+1)) − f(Xt, At).

Further, we deﬁne the empirical loss function by

ˆLN (f; Q, π) =

1
N

N(cid:88)

t=1

t (f; Q, π)
d2

λ(A)πb(At|Xt) ,

where the normalization with λ(A) is introduced for mathematical convenience. Then (3) can be
written compactly as Qk+1 = argminf∈F ˆLN (f; Qk, ˆπk).
The algorithm can then be motivated by the observation that for any f,Q, and π, ˆLN (f; Q, π) is an
unbiased estimate of

(5)
where the ﬁrst term is the error we are interested in and the second term captures the variance of the
random samples:

L(f; Q, π) def= (cid:107)f − T πQ(cid:107)2

ν + L∗(Q, π),

(cid:90)

L∗(Q, π) =

E [Var [R1 + γQ(X2, π(X2))|X1, A1 = a]] dλA(a).

A

3Linear quadratic regulation is such a nice case. It is interesting to note that in this special case the obvious

choices for F and Π yield zero error in the limit, as can be proven based on the main result of this paper.

4

This result is stated formally by E

ˆLN (f; Q, π)

= L(f; Q, π).

(cid:104)

(cid:105)

is

(5)

the

variance

independent

=
term in
Since
argminf∈F (cid:107)f − T πQ(cid:107)2
ν. Thus, if ˆπk were greedy w.r.t. Qk then argminf∈F L(f; Qk, ˆπk) =
argminf∈F (cid:107)f − T Qk(cid:107)2
ν. Hence we can still think of the procedure as approximate value iteration
over the space of action-value functions, projecting T Qk using empirical risk minimization on the
space F w.r.t. (cid:107)·(cid:107)ν distances in an approximate manner. Since ˆπk is only approximately greedy, we
will have to deal with both the error coming from the approximate projection and the error coming
from the choice of ˆπk. To make this clear, we write the iteration in the form

argminf∈F L(f; Q, π)

of

f,

Qk+1 = T ˆπk Qk + ε(cid:48)

k = T Qk + ε(cid:48)

k + (T ˆπk Qk − T Qk) = T Qk + εk,

k is the error committed while computing T ˆπk Qk, ε(cid:48)(cid:48)

def= T ˆπk Qk − T Qk is the error commit-
where ε(cid:48)
ted because the greedy policy is computed approximately and εk = ε(cid:48)
k is the total error of step
k. Hence, in order to show that the procedure is well behaved, one needs to show that both errors are
controlled and that when the errors are propagated through these equations, the resulting error stays
controlled, too. Since we are ultimately interested in the performance of the policy obtained, we
will also need to show that small action-value approximation errors yield small performance losses.
For these we need a number of assumptions that concern either the training data, the MDP, or the
function sets used for learning.

k + ε(cid:48)(cid:48)

k

3.2 Assumptions

3.2.1 Assumptions on the training data

We shall assume that the data is rich, is in a steady state, and is fast-mixing, where, informally,
mixing means that future depends weakly on the past.
Assumption A2 (Sample Path Properties) Assume that {(Xt, At, Rt)}t=1,...,N is the sample path
of πb, a stochastic stationary policy. Further, assume that {Xt} is strictly stationary (Xt ∼ ν ∈
M(X )) and exponentially β-mixing with the actual rate given by the parameters (β, b, κ).4 We
further assume that the sampling policy πb satisﬁes π0 = inf (x,a)∈X×A πb(a|x) > 0.

The β-mixing property will be used to establish tail inequalities for certain empirical processes.5
Note that the mixing coefﬁcients do not need to be known. In the case when no mixing condition is
satisﬁed, learning might be impossible. To see this just consider the case when X1 = X2 = . . . =
XN . Thus, in this case the learner has many copies of the same random variable and successful
generalization is thus impossible. We believe that the assumption that the process is in a steady state
is not essential for our result, as when the process reaches its steady state quickly then (at the price
of a more involved proof) the result would still hold.

3.2.2 Assumptions on the MDP

In order to prevent the uncontrolled growth of the errors as they are propagated through the updates,
we shall need some assumptions on the MDP. A convenient assumption is the following one [11]:
Assumption A3 (Uniformly stochastic transitions) For all x ∈ X and a ∈ A, assume that
P (·|x, a) is absolutely continuous w.r.t. ν and the Radon-Nikodym derivative of P w.r.t. ν is bounded
uniformly with bound Cν: Cν
Note that by the deﬁnition of measure differentiation, Assumption A3 means that P (·|x, a) ≤
Cνν(·). This assumption essentially requires the transitions to be noisy. We will also prove (weaker)
results under the following, weaker assumption:

(cid:176)(cid:176)(cid:176)∞ < +∞.

(cid:176)(cid:176)(cid:176) dP (·|x,a)

def= supx∈X ,a∈A

dν

4For the deﬁnition of β-mixing, see e.g. [2].
5We say “empirical process” and “empirical measure”, but note that in this work these are based on depen-

dent (mixing) samples.

5

Assumption A4 (Discounted-average concentrability of future-state distributions) Given ρ,
ν, m ≥ 1 and an arbitrary sequence of stationary policies {πm}m≥1, assume that the future-
(cid:80)
state distribution ρP π1P π2 . . . P πm is absolutely continuous w.r.t. ν. Assume that c(m) def=
(cid:80)
m≥1 mγm−1c(m) < +∞. We shall call Cρ,ν
def=
supπ1,...,πm
satisﬁes
m≥1 mγm−1c(m), (1 − γ)
m≥1 γmc(m)
max
the discounted-average concentra-
bility coefﬁcient of the future-state distributions.

(cid:176)(cid:176)(cid:176) d(ρP π1 P π2 ...P πm )
(1 − γ)2(cid:80)

(cid:176)(cid:176)(cid:176)∞

(cid:169)

(cid:170)

dν

The number c(m) measures how much ρ can get ampliﬁed in m steps as compared to the reference
distribution ν. Hence, in general we expect c(m) to grow with m. In fact, the condition that Cρ,µ is
ﬁnite is a growth rate condition on c(m). Thanks to discounting, Cρ,µ is ﬁnite for a reasonably large
class of systems (see the discussion in [11]).
A related assumption is needed in the error analysis of the approximate greedy step of the algorithm:
Assumption A5 (The random policy “makes no peak-states”) Consider the distribution µ = (ν×
λA)P which is the distribution of a state that results from sampling an initial state according to ν and
then executing an action which is selected uniformly at random.6 Then Γν = (cid:107)dµ/dν(cid:107)∞ < +∞.
Note that under Assumption A3 we have Γν ≤ Cν. This (very mild) assumption means that after
one step, starting from ν and executing this random policy, the probability of the next state being in
a set is upper bounded by Γν-times the probability of the starting state being in the same set.
Besides, we assume that A has the following regularity property: Let Py(a, h, ρ)
(a(cid:48), v) ∈ RdA+1 : (cid:107)a − a(cid:48)(cid:107)1 ≤ ρ, 0 ≤ v/h ≤ 1 − (cid:107)a − a(cid:48)(cid:107)1 /ρ
h and base given by the (cid:96)1-ball B(a, ρ) def=

(cid:170)
a(cid:48) ∈ RdA : (cid:107)a − a(cid:48)(cid:107)1 ≤ ρ

def=
denote the pyramid with hight

centered at a.

(cid:169)

(cid:169)

(cid:170)

Assumption A6 (Regularity of the action space) We assume that there exists α > 0, such that for
all a ∈ A, for all ρ > 0,

λ(Py(a, 1, ρ) ∩ (A × R))

λ(Py(a, 1, ρ))

≥ min

λ(A)

λ(B(a, ρ))

α,

(cid:181)

(cid:182)

.

For example, if A is an (cid:96)1-ball itself, then this assumption will be satisﬁed with α = 2−dA.
Without assuming any smoothness of the MDP, learning in inﬁnite MDPs looks hard (see, e.g.,
[12, 13]). Here we employ the following extra condition:

Assumption A7 (Lipschitzness of the MDP in the actions) Assume that the transition probabilities
and rewards are Lipschitz w.r.t. their action variable, i.e., there exists LP , Lr > 0 such that for all
(x, a, a(cid:48)) ∈ X × A × A and measurable set B of X ,
|P (B|x, a) − P (B|x, a(cid:48))| ≤ LP (cid:107)a − a(cid:48)(cid:107)1 ,

|r(x, a) − r(x, a(cid:48))| ≤ Lr (cid:107)a − a(cid:48)(cid:107)1 .

Note that previously Lipschitzness w.r.t. the state variables was used, e.g., in [11] to construct con-
sistent planning algorithms.

3.2.3 Assumptions on the function sets used by the algorithm

These assumptions are less demanding since they are under the control of the user of the algorithm.
However, the choice of these function sets will greatly inﬂuence the performance of the algorithm,
as we shall see it from the bounds. The ﬁrst assumption concerns the class F:
Assumption A8 (Lipschitzness of candidate action-value functions) Assume F ⊂ B(X × A)
and that any elements of F is uniformly Lipschitz in its action-argument in the sense that |Q(x, a)−
Q(x, a(cid:48))| ≤ LA (cid:107)a − a(cid:48)(cid:107)1 holds for any x ∈ X , a,a(cid:48) ∈ A, and Q ∈ F.
6Remember that λA denotes the uniform distribution over the action set A.

6

We shall also need to control the capacity of our function sets. We assume that the reader is familiar
with the concept of VC-dimension.7 Here we use the pseudo-dimension of function sets that builds
upon the concept of VC-dimension:
Deﬁnition 3.1 (Pseudo-dimension). The pseudo-dimension VF + of F is deﬁned as the VC-
dimension of the subgraphs of functions in F (hence it is also called the VC-subgraph dimension of
F).
Since A is multidimensional, we deﬁne VΠ+ to be the sum of the pseudo-dimensions of the coordi-
nate projection spaces, Πk of Π:

VΠ+ =

, Πk = { πk : X → R : π = (π1, . . . , πk, . . . , πdA) ∈ Π} .

VΠ+

k

dA(cid:88)

k=1

Now we are ready to state our assumptions on our function sets:
Assumption A9 (Capacity of the function and policy sets) Assume that F ⊂ B(X × A; Qmax)
for Qmax > 0 and VF + < +∞. Also, A ⊂ [−A∞, A∞]dA and VΠ+ < +∞.
Besides their capacity, one shall also control the approximation power of the function sets involved.
Let us ﬁrst consider the policy set Π. Introduce
e∗(F, Π) = sup
Q∈F

ν(EQ − EπQ).

inf
π∈Π

inf π∈Π ν(EQ − EπQ) measures the quality of approximating νEQ by νEπQ. Hence,
Note that
e∗(F, Π) measures the worst-case approximation error of νEQ as Q is changed within F. This can
be made small by choosing Π large.
Another related quantity is the one-step Bellman-error of F w.r.t. Π. This is deﬁned as follows: For
a ﬁxed policy π, the one-step Bellman-error of F w.r.t. T π is deﬁned as
Q(cid:48)∈F (cid:107)Q(cid:48) − T πQ(cid:107)ν .
inf

E1(F; π) = sup
Q∈F

Taking again a pessimistic approach, the one-step Bellman-error of F is deﬁned as

E1(F, Π) = sup
π∈Π

E1(F; π).

Typically by increasing F, E1(F, Π) can be made smaller (this is discussed at some length in
[3]). However, it also holds for both Π and F that making them bigger will increase their capacity
(pseudo-dimensions) which leads to an increase of the estimation errors. Hence, F and Π must be
selected to balance the approximation and estimation errors, just like in supervised learning.

3.3 The main result
Theorem 3.2. Let πK be a greedy policy w.r.t. QK, i.e. πK(x) ∈ argmaxa∈A QK(x, a). Then
under Assumptions A1, A2, and A5–A9, for all δ > 0 we have with probability at least 1 − δ: given
Assumption A3 (respectively A4), (cid:107)V ∗ − V πK(cid:107)∞ (resp. (cid:107)V ∗ − V πK(cid:107)1,ρ), is bounded by


E1(F, Π) + e∗(F, Π) +

C

(log N + log(K/δ))

κ+1
4κ

N 1/4

 1

dA+1

+ γK

 ,

where C depends on dA, VF +, (VΠ+
Qmax, Rmax, ˆRmax, and A∞. In particular, C scales with V
plays the role of the “combined effective” dimension of F and Π.

k=1, γ, κ, b, β, Cν (resp. Cρ,ν), Γν, LA, LP ,Lr, α, λ(A), π0,
)dA
4κ(dA+1) , where V = 2VF + + VΠ+

κ+1

k

7Readers not familiar with VC-dimension are suggested to consult a book, such as the one by Anthony and

Bartlett [14].

7

4 Discussion

We have presented what we believe is the ﬁrst ﬁnite-time bounds for continuous-state and action-
space RL that uses value functions. Further, this is the ﬁrst analysis of ﬁtted Q-iteration, an algorithm
that has proved to be useful in a number of cases, even when used with non-averagers for which no
previous theoretical analysis existed (e.g., [15, 16]). In fact, our main motivation was to show that
there is a systematic way of making these algorithms work and to point at possible problem sources
the same time. We discussed why it can be difﬁcult to make these algorithms work in practice. We
suggested that either the set of action-value candidates has to be carefully controlled (e.g., assuming
uniform Lipschitzness w.r.t. the state variables), or a policy search step is needed, just like in actor-
critic algorithms. The bound in this paper is similar in many respects to a previous bound of a
Bellman-residual minimization algorithm [2].
It looks that the techniques developed here can be
used to obtain results for that algorithm when it is applied to continuous action spaces. Finally,
although we have not explored them here, consistency results for FQI can be obtained from our
results using standard methods, like the methods of sieves. We believe that the methods developed
here will eventually lead to algorithms where the function approximation methods are chosen based
on the data (similar to adaptive regression methods) so as to optimize performance, which in our
opinion is one of the biggest open questions in RL. Currently we are exploring this possibility.

Acknowledgments
Andr´as Antos would like to acknowledge support for this project from the Hungarian Academy of Sciences
(Bolyai Fellowship). Csaba Szepesv´ari greatly acknowledges the support received from the Alberta Ingenuity
Fund, NSERC, the Computer and Automation Research Institute of the Hungarian Academy of Sciences.
References
[1] A. Antos, Cs. Szepesv´ari, and R. Munos. Learning near-optimal policies with Bellman-residual mini-

mization based ﬁtted policy iteration and a single sample path. In COLT-19, pages 574–588, 2006.

[2] A. Antos, Cs. Szepesv´ari, and R. Munos. Learning near-optimal policies with Bellman-residual mini-

mization based ﬁtted policy iteration and a single sample path. Machine Learning, 2007. (accepted).

[3] A. Antos, Cs. Szepesv´ari, and R. Munos. Value-iteration based ﬁtted policy iteration: learning with a

single trajectory. In IEEE ADPRL, pages 330–337, 2007.

[4] D. P. Bertsekas and S.E. Shreve. Stochastic Optimal Control (The Discrete Time Case). Academic Press,

New York, 1978.

[5] D. Ernst, P. Geurts, and L. Wehenkel. Tree-based batch mode reinforcement learning. Journal of Machine

Learning Research, 6:503–556, 2005.

[6] R.S. Sutton and A.G. Barto. Reinforcement Learning: An Introduction. Bradford Book. MIT Press, 1998.
[7] N. Cristianini and J. Shawe-Taylor. An introduction to support vector machines (and other kernel-based

learning methods). Cambridge University Press, 2000.

[8] J.A. Boyan and A.W. Moore. Generalization in reinforcement learning: Safely approximating the value

function. In NIPS-7, pages 369–376, 1995.

[9] P.L. Bartlett, P.M. Long, and R.C. Williamson. Fat-shattering and the learnability of real-valued functions.

Journal of Computer and System Sciences, 52:434–452, 1996.

[10] A.N. Kolmogorov and V.M. Tihomirov. -entropy and -capacity of sets in functional space. American

Mathematical Society Translations, 17(2):277–364, 1961.

[11] R. Munos and Cs. Szepesv´ari. Finite time bounds for sampling based ﬁtted value iteration. Technical
report, Computer and Automation Research Institute of the Hungarian Academy of Sciences, Kende u.
13-17, Budapest 1111, Hungary, 2006.

[12] A.Y. Ng and M. Jordan. PEGASUS: A policy search method for large MDPs and POMDPs. In Proceed-

ings of the 16th Conference in Uncertainty in Artiﬁcial Intelligence, pages 406–415, 2000.

[13] P.L. Bartlett and A. Tewari. Sample complexity of policy search with known dynamics. In NIPS-19. MIT

Press, 2007.

[14] M. Anthony and P. L. Bartlett. Neural Network Learning: Theoretical Foundations. Cambridge University

Press, 1999.

[15] M. Riedmiller. Neural ﬁtted Q iteration – ﬁrst experiences with a data efﬁcient neural reinforcement

learning method. In 16th European Conference on Machine Learning, pages 317–328, 2005.

[16] S. Kalyanakrishnan and P. Stone. Batch reinforcement learning in a complex domain. In AAMAS-07,

2007.

8

"
276,2007,Predicting Brain States from fMRI Data: Incremental Functional Principal Component Regression,"We propose a method for reconstruction of human brain states directly from functional neuroimaging data. The method extends the traditional multivariate regression analysis of discretized fMRI data to the domain of stochastic functional measurements, facilitating evaluation of brain responses to naturalistic stimuli and boosting the power of functional imaging. The method searches for sets of voxel timecourses that optimize a multivariate functional linear model in terms of Rsquare-statistic. Population based incremental learning is used to search for spatially distributed voxel clusters, taking into account the variation in Haemodynamic lag across brain areas and among subjects by voxel-wise non-linear registration of stimuli to fMRI data. The method captures spatially distributed brain responses to naturalistic stimuli without attempting to localize function. Application of the method for prediction of naturalistic stimuli from new and unknown fMRI data shows that the approach is capable of identifying distributed clusters of brain locations that are highly predictive of a specific stimuli.","Predicting Brain States from fMRI Data:

Incremental Functional Principal Component

Regression

S. Ghebreab

ISLA/HCS lab, Informatics Institute

A.W.M. Smeulders

ISLA lab, Informatics Institute

University of Amsterdam, The Netherlands

University of Amsterdam, The Netherlands

ghebreab@science.uva.nl

smeulders@science.uva.nl

P. Adriaans

HCS lab, Informatics Institute

University of Amsterdam, The Netherlands

pietera@science.uva.nl

Abstract

We propose a method for reconstruction of human brain states directly from func-
tional neuroimaging data. The method extends the traditional multivariate re-
gression analysis of discretized fMRI data to the domain of stochastic functional
measurements, facilitating evaluation of brain responses to complex stimuli and
boosting the power of functional imaging. The method searches for sets of voxel
time courses that optimize a multivariate functional linear model in terms of R2-
statistic. Population based incremental learning is used to identify spatially dis-
tributed brain responses to complex stimuli without attempting to localize func-
tion ﬁrst. Variation in hemodynamic lag across brain areas and among subjects
is taken into account by voxel-wise non-linear registration of stimulus pattern to
fMRI data. Application of the method on an international test benchmark for
prediction of naturalistic stimuli from new and unknown fMRI data shows that
the method successfully uncovers spatially distributed parts of the brain that are
highly predictive of a given stimulus.

1 Introduction

To arrive at a better understanding of human brain function, functional neuroimaging traditionally
studies the brain’s responses to controlled stimuli. Controlled stimuli have the beneﬁt of leading to
clear and often localized response signals in fMRI as they are speciﬁcally designed to aﬀect only
certain brain functions. The drawback of controlled stimuli is that they are a reduction of reality: one
cannot be certain whether the response is due to the reduction or due to the stimulus. Naturalistic
stimuli open the possibility to avoid the question whether the response is due to the reduction or
the signal. Naturalistic stimuli, however, carry a high information content in their spatio-temporal
structure that is likely to instigate complex brain states. The immediate consequence hereof is that
one faces the task of isolating relevant responses amids complex patterns.

To reveal brain responses to naturalistic stimuli, advanced signal processing methods are required
that go beyond conventional mass univariate data analysis. Univariate techniques generally lack
suﬃcient power to capture the spatially distributed response of the brain to naturalistic stimuli. Mul-
tivariate pattern techniques, on the other hand, have the capacity to identify patterns of information
when they are present across the full spatial extent of the brain without attempting to localize func-

tion. Here, we propose a multivariate pattern analysis approach for predicting naturalistic stimuli
on the basis of fMRI data. Inverting the task from correlating stimuli with fMRI data to predicting
stimuli from fMRI data makes it easier to evaluate brain responses to naturalistic stimuli and may
extend the power of functional imaging substantially [1].

Various multivariate approaches for reconstruction of brain states directly from fMRI measurements
have recently been proposed. In most of these approaches, a classiﬁer is trained directly on the fMRI
data to discriminate between known diﬀerent brain states. This classiﬁer is then used to predict brain
states on the basis of new and unknown fMRI data alone. Such approaches have been used to predict
what percept is dominant in a binocular rivalry protocol [2], what the orientation is of structures sub-
jects are viewing [3] and what the semantic category is of objects [4] and words [5] subjects see on
a screen. In one competition [6], participants trained pattern analyzers on fMRI of subjects viewing
two short movies as well as on the subject’s movie feature ratings. Then participants employed the
analyzers to predict the experience of subjects watching a third movie based purely on fMRI data.
Very accurate predictions were reported for identifying the presence of speciﬁc time varying movie
features (e.g. faces, motion) and the observers who coded the movies [7].

We propose an incremental multivariate linear modeling approach for functional covariates, i.e.
where both the fMRI data and external stimuli are continuous. This approach diﬀers fundamentally
from existing multivariate linear approaches (e.g. [8]) that instantly ﬁt a given model to the data
within the linear framework under the assumption that both the data and the model are discrete.
Contemporary neuroimaging studies increasingly use high-resolution fMRI to accurately capture
continuous brain processes, frequently instigated by continuous stimulations. Hence, we propose
the use of functional data analysis [9], which treats data, or the processes giving rise to them, as
functions. This not only allows to overcome limitations in neuroimaing studies due to the large
number of data points compared to the number of samples, but also allows to exploit the fact that
functions deﬁned on a speciﬁc domain form an inner product vector space, and in most circum-
stances can be treated algebraically like vectors [10].

We extend classical multivariate regression analysis of fMRI data [11] to stochastic functional mea-
surements. We show that, cast into an incremental pattern searching framework, functional multi-
variate regression provides a powerful technique for fMRI-based prediction of naturalistic stimuli.

2 Method

In the remainder, we consider stimuli data and data produced by fMRI scanners as continuous func-
tions of time, sampled at the scan interval and subject to observational noise. We treat the data
within a functional linear model where both the predictant and predictor are functional, but where
the design matrix that takes care of the linear mapping between the two is vectorial.

2.1 The Predictor

The predictor data are derived directly from the four-dimensional fMRI data I(x, t), where x ∈ ℜ3
denotes the spatial position of a voxel and t denotes its temporal position. We represent each of
the S voxel time courses in functional form by fs(t), with t denoting the continuous path parameter
and s = 1, ..., S . Rather than directly using voxel time courses for prediction, we use their principal
components to eliminate collinearity in the predictor set. Following [10], we use functional principal
component analysis. Viviani et al. [10] showed that functional principal components analysis is
more eﬀective than is its ordinary counterpart in recovering the signal of interest in fMRI data, even
if limited or no prior knowledge of the hemodynamic function or experimental design is speciﬁed.
In contrast to [10], however, our approach incrementally zooms in on stimuli-related voxel time
courses for dimension reduction (see section 2.5).
Given the set of S voxel time courses represented by the vector of functionals f(t) = [ f1(t), ..., fS (t)]T ,
functional principal components analysis extracts main modes of variation in f(t). The number
of modes to retain is determined from the proportion of the variance that needs to be explained.
Assuming this is Q, the central concept is that of taking the linear combination

fsq = Zt

fs(t)αq(t)dt

(1)

where fsq is the principal component score value of voxel time course fs(t) in dimension q. Principal
components αq(t), q = 1, .., Q are sought for one-by-one by optimizing

αq(t) = max
q(t)
α∗

1
S

S

Xs=1

f 2
sq

where αq(t) is subject to the following orthonormal constraints

αq(t)2dt = 1

Zt

Zt

αk(t)αq(t)dt = 0, k ≤ q.

(2)

(3)

The mapping of fs(t) onto the subspace spanned by the ﬁrst Q principal component curves results in
the vector of scalars fs = [ fs1, ..., fsQ]. We deﬁne the S × Q matrix F = [f1, ..., fS ]T of principal com-
ponents scores as our predictor data in linear regression. That is, we perform principal component
regression with F as model, allowing to naturally deal with temporal correlations, multicollinearity
and systematic signal variation.

2.2 The Predictand

We represent the stimulus pattern by the functional (t), t being the continuous time parameter. We
register (t) to each voxel time course fs(t) in order to be able to compare equivalent time points
on stimulus and brain activity data. Alignment reduces to ﬁnding the warping function ωs(t) that
produces the warped stimulus function

gs(t) = (ωs(t)).

(4)

The time warping function ωs(t) is strictly monotonic, diﬀerentiable up to a certain order and takes
care of a small shift and nonlinear transformation. A global alignment criteria and least squares
estimation is used:

s Zt
ωs(t) = min
ω∗

((ω∗

s(t)) − fs(t))2dt.

(5)

Registration of (t) to all voxel time courses S results in predictand data g(t) = [g1(t), ..., gS (t)]T ,
where g(t) is (t) registered onto voxel times-course f (t). Our motivation for using voxel-wise
registration over standard convolution of stimulus (t) with the hemodynamic reponse function, is
the large variability in hemodynamic delays across brain regions and subjects. A non -linear warp
of (t) does not guarantee an outcome that is associated with brain physiology, however it allows
to capture unknown subtle localized variations in hemodynamic delays across brain regions and
subjects.

2.3 The Model

We employ the predictor data to explain the predictand data within a linear modeling approach, i.e.
our multivariate linear model is deﬁned as

(6)
with β(t) = [β1(t), ..., βQ(t)]T being the Q×1 vector of regression functions. The regression functions
are estimated by least squares minimization such that

g(t) = Fβ(t) + ǫ(t)

(t)Zt
ˆβ(t) = min

β∗

(g(t) − Fβ∗(t))2dt,

(7)

under the assumption that the residual functions ǫ(t) = [ǫ1(t), ...., ǫS (t)]T are independent and nor-
mally distributed with zero mean. The estimated regression functions provide the best estimate of
g(t) in least squares sense:

ˆg(t) = F ˆβ(t).

(8)

Given a new (sub)set of voxel time courses, prediction of a stimulus pattern now reduces to comput-
ing the matrix of principal component scores from this new set and weighting these scores by the
estimated regression functions ˆβ(t).

2.4 The Objective

The overall ﬁt of the model to the data is expressed in terms of adjusted R2 statistic. The functional
counterpart of the traditional R2 is computed on the basis of g(t), its mean ¯g(t) and its estimation
ˆg(t). For the voxel set S ,

S

˙gS (t) =

¨gS (t) =

(gs(t) − ¯g(t))2

Xs=1
(gs(t) − ˆgs(t))2
Xs=1

S

(9)

(10)

are derived, where the ﬁrst term is the variation of the response about its mean and the second the
error sum of squares function. The adjusted R-square function is then deﬁned as

RS (t) = 1 −

¨gS (t)/S − Q − 1

˙gS (t)/S − 1

(11)

where degrees of freedom S − Q − 1 and S − 1 adjust the R-square. Our objective is to ﬁnd the set
of voxel time courses S deﬁned as

S = max

S ∗⊂S Zt

RS ∗(t)dt

(12)

where S ∗ denotes a subset of the entire collection of voxels time courses S extracted from a single
fMRI scan. That is, we aim at ﬁnding spatially distributed voxel responses S that best explain the
naturalistic stimuli, without making any prior assumptions about location and size of voxel subsets.

2.5 The Search

In order to eﬃciently ﬁnd the subset of voxels that maximizes Equation (12), we use Population-
Based Incremental Learning (PBIL) [12], which combines Genetic Algorithms with Competitive
Learning. The PBIL algorithm uses a probability vector to explore the space of solutions. It in-
crementally generates solutions by sampling from that probability vector, evaluates these solutions
and selects promising ones to update the probability vector. Here, at increment i, the probability
vector pi = [pi
N], where
each member is an S-vector of binary values: mi
nS ]. A value of 1 for mns means
that for solution n the corresponding voxel time course fs(t) is included in the predictor set, while
n is evaluated in terms of its adjusted R2 value, and
a value 0 indicates exclusion. Each member mi
the members with highest values form the joint probability vector p∗. A new probability vector is
subsequently constructed for the next generation via competitive learning:

S ] is used to generate a population of N solutions Mi = [mi

n1, ..., mi

n = [mi

1, ..., mi

1, ..., pi

pi+1 = γpi + (1 − γ)p∗.

(13)
The learning parameter γ controls the search: a low value enables to focus entirely on the most
recent voxel subset while a low value ensures that previously selected voxel subsets are exploited.
In order to ensure spatial coherence and limit computation load, we employ the PBIl algorithm not
on single time courses, but on averages of spatial clusters of voxel time courses. That is, we ﬁrst
spatially cluster voxel locations as shown in Figure 1, then compute average time course for each
cluster and then explore the averages via PBIL for model building.

2.6 The Prediction

The subset of voxel time courses that results from population based incremental learning deﬁnes
the most predictive voxel locations and associated regression functions. Given new and spatially
normalized fMRI data, represented by ˜f(t) = [ ˜f1(t), ..., ˜fS (t)]T , prediction of a stimulus then reduces
to computing

(14)
In here, ˜g(t) is the vector of predicted stimuli of which the mean is considered to be the sought stim-
ulus. The matrix ˜F is the principal component scores matrix obtained from performing functional
principal components analysis on subset ˜fS(t), with S referring to the set of most predictive voxels
as determined by training.

˜g(t) = ˜F ˆβ(t).

Figure 1: Examples of K-means clustering of voxel locations using Euclidean distance. Left: 1024-
means clustering output. Right: 512-means clustering output. Diﬀerent gray values indicate diﬀer-
ent clusters in a spatially normalized brain atlas.

3 Experiments and Results

3.1 Experiment

Evaluation of our method is done on a data subset from the 2006 Pittsburgh brain activity interpre-
tation competition (PBAIC) [6, 7], involving fMRI scans of three diﬀerent subjects and two movie
sessions. In each session, a subject viewed a new Home Improvement sitcom movie for approxi-
mately 20 minutes. The 20-minute movie contained 5 interruptions where no video was present, only
a white ﬁxation cross on a black background. All three subjects watched the same two movies. The
scans produced volumes with approximately 35,000 brain voxels, each approximately 3.28mm by
3.28mm by 3.5mm, with one volume produced every 1.75 seconds. These scans were preprocessed
(motion correction, slice time correction, linear trend removal) and spatially normalized (non-linear
registration to the Montreal Neurological Institute brain atlas).

After fMRI scanning, the three subjects watched the movie again to rate 30 movie features at time
intervals corresponding to the fMRI scan rate. In our experiments, we focus on the 13 core movie
features: amusement, attention, arousal, body parts, environmental sounds, faces, food, language,
laughter, motion, music, sadness and tools. The real-valued ratings were convolved with a hemo-
dynamic response function (HRF) modeled by two gamma functions, then subjected to voxel-wise
non-linear registration as described in 2.2.

For training and testing our model, we removed parts corresponding with video presentations of a
white ﬁxation cross on a black background. Taking into account the hemodynamic lag, we divided
each fMRI scan and each subject rating into 6 parts corresponding with the movie on parts. On
average each movie part contained 105 discrete measurements. We then functionalized these parts
by ﬁtting a 30 coeﬃcient B-spline to each voxel’s discrete time course. This resulted in 18 data
sets for training (3 subjects × 6 movie parts) and another 18 for testing. We used movie 1 data for
training and movie 2 data for prediction, and vice versa. We performed data analysis at two levels.
For each feature, ﬁrst the individual brain scans were analyzed with our method, resulting in a ﬁrst
sifting of voxels. First-level analysis results for a given feature were then subjected to second level
analysis to identify across subject predictive voxels. Pearson product-moment correlation coeﬃcient
between manual feature rating functions and the automatically predicted feature functions was used
as an evaluation measure.

3.2 Results

All results were obtained with Q = 4 principal component dimensions, learning parameter value
γ = 0.6 and K-means clustering with 1024 clusters for all movie features. These values for Q
and γ produced overall highest average cross correlation value in a small parameter optimization
experiment (data not shown here). Little performance diﬀerences were seen for various numbers of
dimensions, indicating that the essential information can be captured with as little as 4 dimension.
Signiﬁcant performance diﬀerences across features, however, were observed for diﬀerent learning
parameter values, indicating considerable variation in brain response to distinct stimuli.

Manual versus Predicted Feature Ratings

      Manual
     Prediction

0.2

0.4

0.6

0.8

1

arguments

s
n
o

i
t
c
n
u

f

0.6

0.5

0.4

0.3

0.2

0.1

0

−0.1

0

Figure 2: Left: normalized cross correlation values from cross-validation for 13 core movie features.
Right: functionalized subject3 (solid red) and predicted (dotted blue) rating for the language feature
of part 5 of movie 1.

Figure 2 (left) shows the average of 2 × 18 cross correlation coeﬃcients from cross validation for all
13 movie features. For features faces, language and motion cross correlation values above 0.5 were
obtained, meaning that there is a signiﬁcant degree of match between the subject ratings and the
predicted ratings. Reasonable predictions were also obtained for features arousal and body parts.
Our results are consistent with top 3 rank entries of 2006 PBAIC in that features faces and language
are reliably predicted. These entries used recurrent neural networks, ridge regression and a dynamic
Gaussian Markov Random Field modeling on the entire test data benchmark, yielding across feature
average cross correlations of: 0.49, 0.49 and 0.47 respectively. Here, the feature average cross
correlation value based on the reduced training data set is 0.36. Note, that in the 2006 competition
our method ranked ﬁrst in the actor category [6]. We were able to accurately predict which actor the
subjects were seeing purely based on fMRI scans [7].

The best single result, with highest cross correlation value of 0.76, was obtained for feature language
of subject 3 watching part 5 of movie 1. For this feature, ﬁrst level analysis of each of the 18 training
data sets associated with movie 2 produced a total number of 1738 predictive voxels. In the second
level analysis, these voxels were analyzed again to arrive at a reduced data set of 680 voxels for
building the multivariate functional linear model and determining regression functions β(t). For
prediction of feature language, corresponding voxel time courses were extracted from the fMRI data
of subject 3 watching movie 1 part 5, and weighted by β(t). The manual rating of feature language
of movie 1 part 5 by subject 3 and the average of the automatically predicted feature functions are
shown in Figure 2 (right).

Figure 3: Glass view, gray level image with color overlay and surface rendering of 1738 voxels from
ﬁrst level analysis. Color denotes predictive power and cross hair shows most predictive location.

Figure 3 shows glass view, gray level image with color overlay and surface rendering of the 1738
voxels (approximately 40 clusters) from ﬁrst level analysis. The cross hair shows the voxel location
in Brodman area 47 that was found to be predictive across most subjects and movie parts: it was
selected in 6 out of 18 training items (see color bar). The predictive locations correspond with
the left and right inferior frontal gyrus, which are known to be involved in language processing.
The distributed nature of these clusters is consistent with earlier ﬁndings that processing involved
in language occurs in diﬀuse brain regions, including primary auditory and visual cortex, frontal
regions in the left and right hemisphere, in homologues regions [13].

As we are dealing with curves, the possibility exists to explore additional data characteristics such as
curvature. We performed an experiment with 1st order derivative functions, rather than the original
functions to exploit potentially available higher order structure. Figure 4 (left) shows the cross
correlation for 1st order derivative functions. The cross correlation values are similar to the ones
shown in Figure 2. The average cross correlation value is slightly better than for the original data:
0.38. This may indicate that higher order structures may contain more predictive power.

In order to get insight in the eﬀect of non-linear warping on prediction performance, we conducted
an experiment in which we used convolutions of the stimulus (t) with diﬀerent forms of a HRF
function modeled by two gamma functions. Various HRF functions were obtained by varing the
delay of response (relative to onset), delay of undershoot (relative to onset), dispersion of response,
dispersion of undershoot, ratio of response to undershoot. To determine gs(t), we convolved (t)
with 16 diﬀerent HRF functions, and selected the convolved one with highest cross correlation with
fs(t) to be gs(t). Hence, we parametrically modeled the HRF and learned its parameters from the
data.

Figure 4 (right) shows the results of the experiments with convolution of stimuli data with HRF
models learned from the data. As can be seen, the cross correlation values are much lower compared
to the values in Figure 2 (left). The average cross correlation value is 0.31. Hence, non-linear
warping of stimulus onto voxel time course signiﬁcantly enhances the predictive power of our model.
This suggests that non-linear warping is a potential alternative for determining the best possible HRF
estimate to overcome potential negative consequences of assuming HRF consistency across subjects
or brain regions [14].

Figure 4: Left: normalized cross correlation values from cross-validation for 13 core movie features,
using 1st order derivative data. Right: cross correlation values from cross-validation for 13 core
movie features, using HRF convoluted rather than warped stimuli data.

4 Conclusion

Functional data analysis provides the possibility to fully exploit structure in inherently continuous
data such as fMRI. The advantage of functional data analysis for principal component analysis of
fMRI data was recently demonstrated in [10]. Here, we proposed a functional linear model that
treats fMRI and stimuli as stochastic functional measurements. Cast into an incremental pattern
searching framework, the method provides the ability to identify important covariance structure

of spatially distributed brain responses and stimuli, i.e. it directly couples activation across brain
regions rather than ﬁrst localizing and then integrating function. The method is suited for unbiased
probing of functional characteristics of brain areas as well as for exposing meaningful relations
between complex stimuli and distributed brain responses. This ﬁnding is supported by the good
prediction performance of our method in the 2006 PBAIC international competition for brain activity
interpretation. We are currently extending the method with new objective functions, dimension
reduction techniques and multi-target search techniques to cope with multiple (interacting) stimuli.
Also, in this work we made use of spatial clusters at a single hierarchical level. Preliminary results
with hierarchical clustering to arrive at ”supervoxels” at diﬀerent spatial resolutions, seem to further
improve prediction power.

References

[1] J. Haynes and G. Rees. Decoding mental states from brain activity in humans. Nature Neuro-

science, 7(8):523–534, 2006.

[2] J. Haynes and G. Rees. Predicting the orientation of invisible stimuli from activity in human

primary visual cortex. Nature Neuroscience, 7(5):686–691, 2005.

[3] Y. Kamitani and F. Tong. Decoding the visual and subjective contents of the human brain.

Nature Neuroscience, 8(5):679–685, 2005.

[4] S.M. Polyn, V.S. Natu, J.D. Cohen, and K.A. Norman. Category-speciﬁc cortical activity

precedes retrieval during memory search. Science, 310(5756):1963–1966, 2005.

[5] T.M. Mitchell, R. Hutchinson, R.S. Niculescu, F. Pereira, X. Wang, M. Just, and S. Newman.

Learning to decode cognitive states from brain images. Machine Learning, 57(1-2), 2004.

[6] W. Schneider, A. Bartels, E. Formisano, J. Haxby, R. Goebel, T. Mitchell, T. Nichols, and
In Proceedings

G. Siegle. Competition: Inferring experience based cognition from fmri.
Organization of Human Brain Mapping Florence Italy June 15, 2006.
[7] Editorial. What’s on your mind. Nature Neuroscience, 6(8):981, 2006.
[8] K.J. Worsley, J.B. Poline, K.J. Friston, and A.C. Evans. Characterizing the response of pet and

fmri data using multivariate linear models. Neuroimage, 6, 1997.

[9] J. Ramsay and B. Silverman. Functional Data Analysis. Springer-Verlag, 1997.
[10] R. Viviani, G. Grohn, and M. Spitzer. Functional principal component analysis of fmri data.

Human Brain Mapping, 24:109–129, 2005.

[11] D.B. Rowe and R.G. Hoﬀmann. Multivariate statistical analysis in fmri. IEEE Engineering in

Medicine and Biology, 25:60–64, 2006.

[12] Shumeet Baluja. Population-based incremental learning: A method for integrating genetic
search based function optimization and competitive learning. Technical Report CMU-CS-94-
163, Computer Science Department, Carnegie Mellon University, Pittsburgh, PA, 1994.

[13] M.A. Gernsbacher and M.P. Kaschak. Neuroimaging studies of language production and com-

prehension. Annual Review of Psychology, 54:91–114, 2003.

[14] D.A. Handwerker, J.M. Ollinger, and M. D’Esposito. Variation of bold hemodynamic response
function across subjects and brain regions and their eﬀects on statistical analysis. NeuroImage,
8(21):1639–1651, 2004.

"
435,2007,Parallelizing Support Vector Machines on Distributed Computers,"Support Vector Machines (SVMs) suffer from a widely recognized scalability problem in both memory use and computational time. To improve scalability, we have developed a parallel SVM algorithm (PSVM), which reduces memory use through performing a row-based, approximate matrix factorization, and which loads only essential data to each machine to perform parallel computation. Let $n$ denote the number of training instances, $p$ the reduced matrix dimension after factorization ($p$ is significantly smaller than $n$), and $m$ the number of machines. PSVM reduces the memory requirement from $\MO$($n^2$) to $\MO$($np/m$), and improves computation time to $\MO$($np^2/m$). Empirical studies on up to $500$ computers shows PSVM to be effective.","PSVM: Parallelizing Support Vector Machines

on Distributed Computers

Edward Y. Chang∗, Kaihua Zhu, Hao Wang, Hongjie Bai,

Jian Li, Zhihuan Qiu, & Hang Cui
Google Research, Beijing, China

Abstract

Support Vector Machines (SVMs) suffer from a widely recognized scalability
problem in both memory use and computational time. To improve scalability,
we have developed a parallel SVM algorithm (PSVM), which reduces memory
use through performing a row-based, approximate matrix factorization, and which
loads only essential data to each machine to perform parallel computation. Let n
denote the number of training instances, p the reduced matrix dimension after
factorization (p is signiﬁcantly smaller than n), and m the number of machines.
PSVM reduces the memory requirement from O(n2) to O(np/m), and improves
computation time to O(np2/m). Empirical study shows PSVM to be effective.
PSVM Open Source is available for download at http://code.google.com/p/psvm/.

1 Introduction

Let us examine the resource bottlenecks of SVMs in a binary classiﬁcation setting to explain our
proposed solution. Given a set of training data X = {(xi, yi)|xi ∈ Rd}n
i=1, where xi is an obser-
vation vector, yi ∈ {−1, 1} is the class label of xi, and n is the size of X , we apply SVMs on X to
train a binary classiﬁer. SVMs aim to search a hyperplane in the Reproducing Kernel Hilbert Space
(RKHS) that maximizes the margin between the two classes of data in X with the smallest train-
ing error (Vapnik, 1995). This problem can be formulated as the following quadratic optimization
problem:

ξi

(1)

min P(w, b, ξ) =
2 + C
s.t. 1 − yi(wT φ(xi) + b) ≤ ξi,

(cid:107)w(cid:107)2

1
2

ξi > 0,

n(cid:88)

i=1

where w is a weighting vector, b is a threshold, C a regularization hyperparameter, and φ(·) a basis
function which maps xi to an RKHS space. The decision function of SVMs is f(x) = wT φ(x)+ b,
where the w and b are attained by solving P in (1). The optimization problem in (1) is the primal
formulation of SVMs. It is hard to solve P directly, partly because the explicit mapping via φ(·)
can make the problem intractable and partly because the mapping function φ(·) is often unknown.
The method of Lagrangian multipliers is thus introduced to transform the primal formulation into
the dual one

1
2 αT Qα − αT 1
min D(α) =
s.t. 0 ≤ α ≤ C, yT α = 0,
(cid:80)n

(2)

where [Q]ij = yiyjφT (xi)φ(xj), and α ∈ Rn is the Lagrangian multiplier variable (or dual
variable). The weighting vector w is related with α in w =

i=1 αiφ(xi).

∗This work was initiated in 2005 when the author was a professor at UCSB.

1

The dual formulation D(α) requires an inner product of φ(xi) and φ(xj). SVMs utilize the kernel
trick by specifying a kernel function to deﬁne the inner-product K(xi, xj) = φT (xi)φ(xj). We
thus can rewrite [Q]ij as yiyjK(xi, xj). When the given kernel function K is psd (positive semi-
deﬁnite), the dual problem D(α) is a convex Quadratic Programming (QP) problem with linear
constraints, which can be solved via the Interior-Point method (IPM) (Mehrotra, 1992). Both the
computational and memory bottlenecks of the SVM training are the IPM solver to the dual formu-
lation of SVMs in (2).
Currently, the most effective IPM algorithm is the primal-dual IPM (Mehrotra, 1992). The principal
idea of the primal-dual IPM is to remove inequality constraints using a barrier function and then
resort to the iterative Newton’s method to solve the KKT linear system related to the Hessian matrix
Q in D(α). The computational cost is O(n3) and the memory usage O(n2).
In this work, we propose a parallel SVM algorithm (PSVM) to reduce memory use and to parallelize
both data loading and computation. Given n training instances each with d dimensions, PSVM ﬁrst
loads the training data in a round-robin fashion onto m machines. The memory requirement per
machine is O(nd/m). Next, PSVM performs a parallel row-based Incomplete Cholesky Factor-
ization (ICF) on the loaded data. At the end of parallel ICF, each machine stores only a fraction
of the factorized matrix, which takes up space of O(np/m), where p is the column dimension of
the factorized matrix. (Typically, p can be set to be about
n without noticeably degrading train-
ing accuracy.) PSVM reduces memory use of IPM from O(n2) to O(np/m), where p/m is much
smaller than n. PSVM then performs parallel IPM to solve the quadratic optimization problem
in (2). The computation time is improved from about O(n2) of a decomposition-based algorithm
(e.g., SVMLight (Joachims, 1998), LIBSVM (Chang & Lin, 2001), SMO (Platt, 1998), and Sim-
pleSVM (Vishwanathan et al., 2003)) to O(np2/m). This work’s main contributions are: (1) PSVM
achieves memory reduction and computation speedup via a parallel ICF algorithm and parallel IPM.
(2) PSVM handles kernels (in contrast to other algorithmic approaches (Joachims, 2006; Chu et al.,
2006)). (3) We have implemented PSVM on our parallel computing infrastructures. PSVM effec-
tively speeds up training time for large-scale tasks while maintaining high training accuracy.
PSVM is a practical, parallel approximate implementation to speed up SVM training on today’s
distributed computing infrastructures for dealing with Web-scale problems. What we do not claim
are as follows: (1) We make no claim that PSVM is the sole solution to speed up SVMs. Algorithmic
approaches such as (Lee & Mangasarian, 2001; Tsang et al., 2005; Joachims, 2006; Chu et al.,
2006) can be more effective when memory is not a constraint or kernels are not used. (2) We do not
claim that the algorithmic approach is the only avenue to speed up SVM training. Data-processing
approaches such as (Graf et al., 2005) can divide a serial algorithm (e.g., LIBSVM) into subtasks
on subsets of training data to achieve good speedup. (Data-processing and algorithmic approaches
complement each other, and can be used together to handle large-scale training.)

√

2 PSVM Algorithm

The key step of PSVM is parallel ICF (PICF). Traditional column-based ICF (Fine & Scheinberg,
2001; Bach & Jordan, 2005) can reduce computational cost, but the initial memory requirement
is O(np), and hence not practical for very large data set. PSVM devises parallel row-based ICF
(PICF) as its initial step, which loads training instances onto parallel machines and performs factor-
ization simultaneously on these machines. Once PICF has loaded n training data distributedly on m
machines, and reduced the size of the kernel matrix through factorization, IPM can be solved on par-
allel machines simultaneously. We present PICF ﬁrst, and then describe how IPM takes advantage
of PICF.

2.1 Parallel ICF
ICF can approximate Q (Q ∈ Rn×n) by a smaller matrix H (H ∈ Rn×p, p (cid:191) n), i.e., Q ≈
HH T . ICF, together with SMW (the Sherman-Morrison-Woodbury formula), can greatly reduce
the computational complexity in solving an n × n linear system. The work of (Fine & Scheinberg,
2001) provides a theoretical analysis of how ICF inﬂuences the optimization problem in Eq.(2). The
authors proved that the error of the optimal objective value introduced by ICF is bounded by C 2l/2,
where C is the hyperparameter of SVM, l is the number of support vectors, and  is the bound of

2

Algorithm 1 Row-based PICF

Input: n training instances; p: rank of ICF matrix H; m: number of machines
Output: H distributed on m machines
Variables:
v: fraction of the diagonal vector of Q that resides in local machine
k: iteration number;
xi: the ith training instance
M: machine index set, M = {0, 1, . . . , m − 1}
Ic: row-index set on machine c (c ∈ M), Ic = {c, c + m, c + 2m, . . .}
1: for i = 0 to n − 1 do
2:
3: end for
4: k ← 0; H ← 0; v ← the fraction of the diagonal vector of Q that resides in local machine. (v(i)(i ∈ Im)

Load xi into machine imodulom.

can be obtained from xi)

5: Initialize master to be machine 0.
6: while k < p do
7:

Each machine c ∈ M selects its local pivot value, which is the largest element in v:

and records the local pivot index, the row index corresponds to lpvk,c:

lpvk,c = max
i∈Ic

v(i).

lpik,c = arg max
i∈Ic

v(i).

Gather lpvk,c’s and lpik,c’s (c ∈ M) to master.
The master selects the largest local pivot value as global pivot value gpvk and records in ik, row index
corresponding to the global pivot value.

gpvk = max
c∈M

lpvk,c.

The master broadcasts gpvk and ik.
Change master to machine ik%m.
Calculate H(ik, k) according to (3) on master.
The master broadcasts the pivot instance xik and the pivot row H(ik, :). (Only the ﬁrst k + 1 values of
the pivot row need to be broadcast, since the remainder are zeros.)
Each machine c ∈ M calculates its part of the kth column of H according to (4).
Each machine c ∈ M updates v according to (5).
k ← k + 1

8:
9:

10:
11:
12:
13:

14:
15:
16:
17: end while

√

n, the error can be negligible.

ICF approximation (i.e. tr(Q − HH T ) < ). Experimental results in Section 3 show that when p is
set to
Our row-based parallel ICF (PICF) works as follows: Let vector v be the diagonal of Q and suppose
the pivots (the largest diagonal values) are {i1, i2, . . . , ik}, the kth iteration of ICF computes three
equations:

(cid:112)

H(Jk, k) = (Q(Jk, k) − k−1(cid:88)

H(ik, k) =

v(ik)

j=1

H(Jk, j)H(ik, j))/H(ik, k)
v(Jk) = v(Jk) − H(Jk, k)2,

(3)

(4)

k (measured by trace(Q − HkH T

(5)
where Jk denotes the complement of {i1, i2, . . . , ik}. The algorithm iterates until the approximation
k )) is satisfactory, or the predeﬁned maximum
of Q by HkH T
iterations (or say, the desired rank of the ICF matrix) p is reached.
As suggested by G. Golub, a parallelized ICF algorithm can be obtained by constraining the par-
allelized Cholesky Factorization algorithm, iterating at most p times. However, in the proposed
algorithm (Golub & Loan, 1996), matrix H is distributed by columns in a round-robin way on m
machines (hence we call it column-based parallelized ICF). Such column-based approach is opti-
mal for the single-machine setting, but cannot gain full beneﬁt from parallelization for two major
reasons:

3

(cid:80)k−1

1. Large memory requirement. All training data are needed for each machine to calculate Q(Jk, k).
Therefore, each machine must be able to store a local copy of the training data.
2.
calculation
j=1 H(Jk, j)H(ik, j)) in (4) can be parallelized. The calculation of pivot selection, the
(
summation of local inner product result, column calculation in (4), and the vector update in (5)
must be performed on one single machine.

parallelizable

computation.

Limited

Only

the

inner

product

To remedy these shortcomings of the column-based approach, we propose a row-based approach to
parallelize ICF, which we summarize in Algorithm 1. Our row-based approach starts by initializing
variables and loading training data onto m machines in a round-robin fashion (Steps 1 to 5). The
algorithm then performs the ICF main loop until the termination criteria are satisﬁed (e.g., the rank
of matrix H reaches p). In the main loop, PICF performs ﬁve tasks in each iteration k:
• Distributedly ﬁnd a pivot, which is the largest value in the diagonal v of matrix Q (steps 7 to 10).
Notice that PICF computes only needed elements in Q from training data, and it does not store Q.
• Set the machine where the pivot resides as the master (step 11).
• On the master, PICF calculates H(ik, k) according to (3) (step 12).
• The master then broadcasts the pivot instance xik and the pivot row H(ik, :) (step 13).
• Distributedly compute (4) and (5) (steps 14 and 15).
At the end of the algorithm, H is stored distributedly on m machines, ready for parallel IPM (pre-
sented in the next section). PICF enjoys three advantages: parallel memory use (O(np/m)), parallel
computation (O(p2n/m)), and low communication overhead (O(p2 log(m))). Particularly on the
communication overhead, its fraction of the entire computation time shrinks as the problem size
grows. We will verify this in the experimental section. This pattern permits a larger problem to be
solved on more machines to take advantage of parallel memory use and computation.

2.2 Parallel IPM

As mentioned in Section 1, the most effective algorithm to solve a constrained QP problem is the
primal-dual IPM. For detailed description and notations of IPM, please consult (Boyd, 2004; Mehro-
tra, 1992). For the purpose of SVM training, IPM boils down to solving the following equations in
the Newton step iteratively.

+ diag( λi

C − αi

)(cid:52)x

(cid:182)

(cid:181)
(cid:181)

1

(cid:182)

(cid:52)λ = −λ + vec

(cid:52)ξ = −ξ + vec

t(C − αi)
1
tαi
yT Σ−1z + yT α

(cid:52)ν =

yT Σ−1y

− diag( ξi
αi

)(cid:52)x

(6)

(7)

(8)

(9)

(10)

(11)

(12)

D = diag( ξi
+ λi
αi
(cid:52)x = Σ−1(z − y(cid:52)ν),

C − αi

)

where Σ and z depend only on [α, λ, ξ, ν] from the last iteration as follows:

Σ = Q + diag( ξi
+ λi
αi
1
z = −Qα + 1n − νy +
t

C − αi
vec(

)

−

1
αi

1

C − αi

).

The computation bottleneck is on matrix inverse, which takes place on Σ for solving (cid:52)ν in (8)
and (cid:52)x in (10). Equation (11) shows that Σ depends on Q, and we have shown that Q can be
approximated through PICF by HH T . Therefore, the bottleneck of the Newton step can be sped up
from O(n3) to O(p2n), and be parallelized to O(p2n/m).
Distributed Data Loading
To minimize both storage and communication cost, PIPM stores data distributedly as follows:

4

• Distribute matrix data. H is distributedly stored at the end of PICF.
• Distribute n × 1 vector data. All n × 1 vectors are distributed in a round-robin fashion on m
machines. These vectors are z, α, ξ, λ, ∆z, ∆α, ∆ξ, and ∆λ.
• Replicate global scalar data. Every machine caches a copy of global data including ν, t, n, and
∆ν. Whenever a scalar is changed, a broadcast is required to maintain global consistency.
Parallel Computation of (cid:52)ν
Rather than walking through all equations, we describe how PIPM solves (8), where Σ−1 appears
twice. An interesting observation is that parallelizing Σ−1z (or Σ−1y) is simpler than parallelizing
Σ−1. Let us explain how parallelizing Σ−1z works, and parallelizing Σ−1y can follow suit.
According to SMW (the Sherman-Morrison-Woodbury formula), we can write Σ−1z as

Σ−1z = (D + Q)−1z ≈ (D + HH T )−1z

= D−1z − D−1H(I + H T D−1H)−1H T D−1z
= D−1z − D−1H(GGT )−1H T D−1z.

Σ−1z can be computed in four steps:
1. Compute D−1z. D can be derived from locally stored vectors, following (9). D−1z is a n × 1
vector, and can be computed locally on each of the m machines.
2. Compute t1 = H T D−1z. Every machine stores some rows of H and their corresponding part
of D−1z. This step can be computed locally on each machine. The results are sent to the master
(which can be a randomly picked machine for all PIPM iterations) to aggregate into t1 for the next
step.
3. Compute (GGT )−1t1. This step is completed on the master, since it has all the required
data. G can be obtained from H in a straightforward manner as shown in SMW. Computing
t2 = (GGT )−1t1 is equivalent to solving the linear equation system t1 = (GGT )t2. PIPM ﬁrst
solves t1 = Gy0, then y0 = GT t2. Once it has obtained y0, PIPM can solve GT t2 = y0 to obtain
t2. The master then broadcasts t2 to all machines.
4. Compute D−1Ht2 All machines have a copy of t2, and can compute D−1Ht2 locally to solve
for Σ−1z.
Similarly, Σ−1y can be computed at the same time. Once we have obtained both, we can solve ∆ν
according to (8).

2.3 Computing b and Writing Back

When the IPM iteration stops, we have the value of α and hence the classiﬁcation function

Ns(cid:88)

f(x) =

αiyik(si, x) + b

i=1

Here Ns is the number of support vectors and si are support vectors. In order to complete this
classiﬁcation function, b must be computed. According to the SVM model, given a support vector s,
if ys = −1.
we obtain one of the two results for f(s): f(s) = +1,
In practice, we can select M, say 1, 000, support vectors and compute the average of the bs in
parallel using MapReduce (Dean & Ghemawat, 2004).

if ys = +1, or f(s) = −1,

3 Experiments

We conducted experiments on PSVM to evaluate its 1) class-prediction accuracy, 2) scalability on
large datasets, and 3) overheads. The experiments were conducted on up to 500 machines in our
data center. Not all machines are identically conﬁgured; however, each machine is conﬁgured with
a CPU faster than 2GHz and memory larger than 4GBytes.

5

Table 1: Class-prediction Accuracy with Different p Settings.

dataset
svmguide1
mushrooms
news20
Image
CoverType
RCV

samples (train/test)
3, 089/4, 000
7, 500/624
18, 000/1, 996
199, 957/84, 507
522, 910/58, 102
781, 265/23, 149

LIBSVM p = n0.1
0.6563
0.9904
0.6949
0.7293
0.9764
0.8527

0.9608
1
0.7835
0.849
0.9769
0.9575

p = n0.2
0.9
0.9920
0.6949
0.7210
0.9762
0.8586

p = n0.3
0.917
1
0.6969
0.8041
0.9766
0.8616

p = n0.4
0.9495
1
0.7806
0.8121
0.9761
0.9065

p = n0.5
0.9593
1
0.7811
0.8258
0.9766
0.9264

3.1 Class-prediction Accuracy
PSVM employs PICF to approximate an n × n kernel matrix Q with an n × p matrix H. This
experiment evaluated how the choice of p affects class-prediction accuracy. We set p of PSVM to nt,
where t ranges from 0.1 to 0.5 incremented by 0.1, and compared its class-prediction accuracy with
that achieved by LIBSVM. The ﬁrst two columns of Table 1 enumerate the datasets and their sizes
with which we experimented. We use Gaussian kernel, and select the best C and σ for LIBSVM and
PSVM, respectively. For CoverType and RCV, we loosed the terminate condition (set -e 1, default
0.001) and used shrink heuristics (set -h 0) to make LIBSVM terminate within several days. The
table shows that when t is set to 0.5 (or p =
n), the class-prediction accuracy of PSVM approaches
that of LIBSVM.
We compared only with LIBSVM because it is arguably the best open-source SVM implementa-
tion in both accuracy and speed. Another possible candidate is CVM (Tsang et al., 2005). Our
experimental result on the CoverType dataset outperforms the result reported by CVM on the same
dataset in both accuracy and speed. Moreover, CVM’s training time has been shown unpredictable
by (Loosli & Canu, 2006), since the training time is sensitive to the selection of stop criteria and
hyper-parameters. For how we position PSVM with respect to other related work, please refer to
our disclaimer in the end of Section 1.

√

3.2 Scalability

For scalability experiments, we used three large datasets. Table 2 reports the speedup of PSVM
on up to m = 500 machines. Since when a dataset size is large, a single machine cannot store
the factorized matrix H in its local memory, we cannot obtain the running time of PSVM on one
machine. We thus used 10 machines as the baseline to measure the speedup of using more than
10 machines. To quantify speedup, we made an assumption that the speedup of using 10 machines
is 10, compared to using one machine. This assumption is reasonable for our experiments, since
PSVM does enjoy linear speedup when the number of machines is up to 30.

Table 2: Speedup (p is set to

√

n); LIBSVM training time is reported on the last row for reference.

Image (200k)

Speedup

CoverType (500k)
Time (s)

Speedup

RCV (800k)

Time (s)

Machines

10
30
50
100
150
200
250
500

Time (s)
(9)
(8)
(14)
(47)
(40)
(41)
(78)
(123)

1, 958
572
473
330
274
294
397
814

LIBSVM 4, 334 NA

10∗
34.2
41.4
59.4
71.4
66.7
49.4
24.1
NA

16, 818
(442)
(10)
5, 591
(60)
3, 598
(29)
2, 082
1, 865
(93)
1, 416
(24)
(115)
1, 405
(34)
1, 655
28, 149 NA

10∗
30.1
46.8
80.8
90.2
118.7
119.7
101.6
NA

45, 135
12, 289
7, 695
4, 992
3, 313
3, 163
2, 719
2, 671

(1373)
(98)
(92)
(34)
(59)
(69)
(203)
(193)

184, 199 NA

Speedup

10∗
36.7
58.7
90.4
136.3
142.7
166.0
169.0
NA

We trained PSVM three times for each dataset-m combination. The speedup reported in the table
is the average of three runs with standard deviation provided in brackets. The observed variance in
speedup was caused by the variance of machine loads, as all machines were shared with other tasks

6

running on our data centers. We can observe in Table 2 that the larger is the dataset, the better is
the speedup. Figures 1(a), (b) and (c) plot the speedup of Image, CoverType, and RCV, respectively.
All datasets enjoy a linear speedup when the number of machines is moderate. For instance, PSVM
achieves linear speedup on RCV when running on up to around 100 machines. PSVM scales well till
around 250 machines. After that, adding more machines receives diminishing returns. This result
led to our examination on the overheads of PSVM, presented next.

(a) Image (200k) speedup

(b) Covertype (500k) speedup

(c) RCV (800k) speedup

(d) Image (200k) overhead

(e) Covertype (500k) overhead

(f) RCV (800k) overhead

(g) Image (200k) fraction

(h) Covertype (500k) fraction

(i) RCV (800k) fraction

Figure 1: Speedup and Overheads of Three Datasets.

3.3 Overheads

PSVM cannot achieve linear speedup when the number of machines continues to increase beyond
a data-size-dependent threshold. This is expected due to communication and synchronization over-
heads. Communication time is incurred when message passing takes place between machines. Syn-
chronization overhead is incurred when the master machine waits for task completion on the slowest
machine. (The master could wait forever if a child machine fails. We have implemented a check-
point scheme to deal with this issue.)
The running time consists of three parts: computation (Comp), communication (Comm), and syn-
chronization (Sync). Figures 1(d), (e) and (f) show how Comm and Sync overheads inﬂuence the
speedup curves. In the ﬁgures, we draw on the top the computation only line (Comp), which ap-
proaches the linear speedup line. Computation speedup can become sublinear when adding ma-
chines beyond a threshold. This is because the computation bottleneck of the unparallelizable step
12 in Algorithm 1 (which computation time is O(p2)). When m is small, this bottleneck is insignif-
icant in the total computation time. According to the Amdahl’s law; however, even a small fraction
of unparallelizable computation can cap speedup. Fortunately, the larger the dataset is, the smaller
is this unparallelizable fraction, which is O(m/n). Therefore, more machines (larger m) can be
employed for larger datasets (larger n) to gain speedup.

7

When communication overhead or synchronization overhead is accounted for (the Comp + Comm
line and the Comp + Comm + Sync line), the speedup deteriorates. Between the two overheads, the
synchronization overhead does not impact speedup as much as the communication overhead does.
Figures 1(g), (h), and (i) present the percentage of Comp, Comm, and Sync in total running time.
The synchronization overhead maintains about the same percentage when m increases, whereas the
percentage of communication overhead grows with m. As mentioned in Section 2.1, the communi-
cation overhead is O(p2 log(m)), growing sub-linearly with m. But since the computation time per
node decreases as m increases, the fraction of the communication overhead grows with m. There-
fore, PSVM must select a proper m for a training task to maximize the beneﬁt of parallelization.

4 Conclusion

In this paper, we have shown how SVMs can be parallelized to achieve scalable performance. PSVM
distributedly loads training data on parallel machines, reducing memory requirement through ap-
proximate factorization on the kernel matrix. PSVM solves IPM in parallel by cleverly arranging
computation order. We have made PSVM open source at http://code.google.com/p/psvm/.
Acknowledgement

The ﬁrst author is partially supported by NSF under Grant Number IIS-0535085.
References
Bach, F. R., & Jordan, M. I. (2005). Predictive low-rank decomposition for kernel methods. Pro-

ceedings of the 22nd International Conference on Machine Learning.

Boyd, S. (2004). Convex optimization. Cambridge University Press.
Chang, C.-C., & Lin, C.-J. (2001). LIBSVM: a library for support vector machines. Software avail-

able at http://www.csie.ntu.edu.tw/ cjlin/libsvm.

Chu, C.-T., Kim, S. K., Lin, Y.-A., Yu, Y., Bradski, G., Ng, A. Y., & Olukotun, K. (2006). Map

reduce for machine learning on multicore. NIPS.

Dean, J., & Ghemawat, S. (2004). Mapreduce: Simpliﬁed data processing on large clusters.

OSDI’04: Symposium on Operating System Design and Implementation.

Fine, S., & Scheinberg, K. (2001). Efﬁcient svm training using low-rank kernel representations.

Journal of Machine Learning Research, 2, 243–264.

Ghemawat, S., Gobioff, H., & Leung, S.-T. (2003). The google ﬁle system. 19th ACM Symposium

on Operating Systems Principles.

Golub, G. H., & Loan, C. F. V. (1996). Matrix computations. Johns Hopkins University Press.
Graf, H. P., Cosatto, E., Bottou, L., Dourdanovic, I., & Vapnik, V. (2005). Parallel support vector
machines: The cascade svm. In Advances in neural information processing systems 17, 521–528.
Joachims, T. (1998). Making large-scale svm learning practical. Advances in Kernel Methods -

Support Vector Learning.

Joachims, T. (2006). Training linear svms in linear time. ACM KDD, 217–226.
Lee, Y.-J., & Mangasarian, O. L. (2001). Rsvm: Reduced support vector machines. First SIAM

International Conference on Data Mining. Chicago.

Loosli, G., & Canu, S. (2006). Comments on the core vector machines: Fast svm training on very

large data sets (Technical Report).

Mehrotra, S. (1992). On the implementation of a primal-dual interior point method. SIAM J. Opti-

mization, 2.

Platt, J. (1998). Sequential minimal optimization: A fast algorithm for training support vector

machines (Technical Report MSR-TR-98-14). Microsoft Research.

Tsang, I. W., Kwok, J. T., & Cheung, P.-M. (2005). Core vector machines: Fast svm training on

very large data sets. Journal of Machine Learning Research, 6, 363–392.

Vapnik, V. (1995). The nature of statistical learning theory. New York: Springer.
Vishwanathan, S., Smola, A. J., & Murty, M. N. (2003). Simplesvm. ICML.

8

"
695,2007,"Computational Equivalence of Fixed Points and No Regret Algorithms, and Convergence to Equilibria","We study the relation between notions of game-theoretic equilibria which are based on stability under a set of deviations, and empirical equilibria which are reached by rational players. Rational players are modelled by players using no regret algorithms, which guarantee that their payoff in the long run is almost as much as the most they could hope to achieve by consistently deviating from the algorithm's suggested action. We show that for a given set of deviations over the strategy set of a player, it is possible to efficiently approximate fixed points of a given deviation if and only if there exist efficient no regret algorithms resistant to the deviations. Further, we show that if all players use a no regret algorithm, then the empirical distribution of their plays converges to an equilibrium.","Computational Equivalence of Fixed Points and No
Regret Algorithms, and Convergence to Equilibria

IBM Almaden Research Center

Computer Science Department,

Elad Hazan

650 Harry Road

San Jose, CA 95120

hazan@us.ibm.com

Satyen Kale

Princeton University

35 Olden St.

Princeton, NJ 08540

satyen@cs.princeton.edu

Abstract

We study the relation between notions of game-theoretic equilibria which are
based on stability under a set of deviations, and empirical equilibria which are
reached by rational players. Rational players are modeled by players using no
regret algorithms, which guarantee that their payoff in the long run is close to
the maximum they could hope to achieve by consistently deviating from the algo-
rithm’s suggested action.
We show that for a given set of deviations over the strategy set of a player, it is
possible to efﬁciently approximate ﬁxed points of a given deviation if and only if
there exist efﬁcient no regret algorithms resistant to the deviations. Further, we
show that if all players use a no regret algorithm, then the empirical distribution
of their plays converges to an equilibrium.

1 Introduction

We consider a setting where a number of agents need to repeatedly make decisions in the face of
uncertainty. In each round, the agent obtains a payoff based on the decision she chose. Each agent
would like to be able to maximize her payoff. While this might seem like a natural objective, it
may be impossible to achieve without placing restrictions on the kind of payoffs that can arise. For
instance, if the payoffs were adversarially chosen, then the agent’s task would become essentially
hopeless.
In such a situation, one way for the agent to cope with the uncertainty is to aim for a relative
benchmark rather an absolute one. The notion of regret minimization captures this intuition. We
imagine that the agent has a choice of several well-deﬁned ways to change her decision, and now
the agent aims to maximize her payoff relative to what she could have obtained had she changed her
decisions in a consistent manner. As an example of what we mean by consistent changes, a possible
objective could be to maximize her payoff relative to the most she could have achieved by choosing
some ﬁxed decision in all the rounds. The difference between these payoffs is known as external
regret in the game theory literature. Another notion is that of internal regret, which arises when the
possible ways to change are the ones that switch from some decision i to another, j, whenever the
agent chose decision i, leaving all other decisions unchanged.
A learning algorithm for an agent is said to have no regret with respect to an associated set of decision
modiﬁers (also called deviations) Φ if the average payoff of an agent using the algorithm converges
to the largest average payoff she would have achieved had she changed her decisions using a ﬁxed
decision modiﬁer in all the rounds. Based on what set of decision modiﬁers are under consideration,
various no regret algorithms are known (for e.g. Hannan [10] gave algorithms to minimize external
regret, and Hart and Mas-Collel [11] give algorithms to minimize internal regret).

1

The reason no regret algorithms are so appealing, apart from the fact that they model rational behav-
ior of agents in the face of uncertainty, is that in various cases it can be shown that using no regret
algorithms guides the overall play towards a game theoretic equilibrium. For example, Freund and
Schapire [7] show that in a zero-sum game, if all agents use a no external regret algorithm, then
the empirical distribution of the play converges to the set of minimax equilibria. Similarly, Hart
and Mas-Collel [11] show that if all agents use a no internal regret algorithm, then the empirical
distribution of the play converges to the set of correlated equilibria.
In general, given a set of decision modiﬁers Φ, we can deﬁne a notion of game theoretic equilibrium
that is based on the property of being stable under deviations speciﬁed by Φ. This is a joint distri-
bution on the agents’ decisions that ensures that the expected payoff to any agent is no less than the
most she could achieve if she decided to unilaterally (and consistently) decided to deviate from her
suggested action using any decision modiﬁer in Φ. One can then show that if all agents use a Φ-no
regret algorithm, then the empirical distribution of the play converges to the set of Φ-equilibria.
This brings us to the question of whether it is possible to design no regret algorithms for various sets
of decision modiﬁers Φ. In this paper, we design algorithms which achieve no regret with respect to
Φ for a very general setting of arbitrary convex compact decision spaces, arbitrary concave payoff
functions, and arbitrary continuous decision modiﬁers. Our method works as long as it is possible
to compute approximate ﬁxed points for (convex combinations) of decision modiﬁers in Φ. Our
algorithms are based on a connection to the framework of Online Convex Optimization (see, e.g.
[18]) and we show how to apply known learning algorithms to obtain Φ-no regret algorithms. The
generality of our connection allows us to use various sophisticated Online Convex Optimization
algorithms which can exploit various structural properties of the utility functions and guarantee a
faster rate of convergence to the equilibrium.
Previous work by Greenwald and Jafari [9] gave algorithms for the case when the decision space is
the simplex of probability distributions over the agents’ decisions, the payoff functions are linear,
and the decision modiﬁers are also linear. Their algorithm, based on the work of Hart and Mas-
Collel [11], uses a version of Blackwell’s Approachability Theorem, and also needs to computes
ﬁxed points of the decision modiﬁers. Since these modiﬁers are linear, it is possible to compute
ﬁxed points for them by computing the stationary distribution of an appropriate stochastic matrix
(say, by computing its top eigenvector).
Computing Brouwer ﬁxed points of continuous functions is in general a very hard problem (it is
PPAD-complete, as shown by Papadimitriou [15]). Fixed points are ubiquitous in game theory.
Most common notions of equilibria in game theory are deﬁned as the set of ﬁxed points of a certain
mapping. For example, Nash Equilibria (NE) are the set of ﬁxed points of the best response mapping
(appropriately deﬁned to avoid ambiguity). The fact that Brouwer ﬁxed points are hard to compute in
general is no reason why computing speciﬁc ﬁxed points should be hard (for instance, as mentioned
earlier, computing ﬁxed points of linear functions is easy via eigenvector computations). More
speciﬁcally, could it be the case that the NE, being a ﬁxed point of some well-speciﬁed mapping,
is easy to compute? These hopes were dashed by the work of [6, 3] who showed that computing
NE is as computationally difﬁcult as ﬁnding ﬁxed points in a general mapping:
they show that
computing NE in a two-player game is PPAD-complete. Further work showed that even computing
an approximate NE is PPAD-complete [4].
Since our algorithms (and all previous ones as well) depend on computing (approximate) ﬁxed points
of various decision modiﬁers, the above discussion leads us to question whether this is necessary.
We show in this paper that indeed it is: a Φ-no-regret algorithm can be efﬁciently used to compute
approximate ﬁxed points of any convex combination of decision modiﬁers. This establishes an
equivalence theorem, which is the main contribution of this paper: there exist efﬁcient Φ-no-regret
algorithms if and only it is possible to efﬁciently compute ﬁxed points of convex combinations
of decision modiﬁers in Φ. This equivalence theorem allows us to translate complexity theoretic
lower bounds on computing ﬁxed points to designing no regret algorithms. For instance, a Nash
equilibrium can be obtained by applying Brouwer’s ﬁxed point theorem to an appropriately deﬁned
continuous mapping from the compact convex set of pairs of the players’ mixed strategies to itself.
Thus, if Φ contains this mapping, then it is PPAD-hard to design Φ-no-regret algorithms.
It was recently brought to our attention that Stolz and Lugosi [17], building on the work of Hart and
Schmeidler [12], have also considered Φ-no-regret algorithms. They also show how to design them

2

from ﬁxed-point oracles, and proved convergence to equilibria under even more general conditions
than we consider. Gordon, Greenwald, Marks, and Zinkevich [8] have also considered similar no-
tions of regret and showed convergence to equilibria, in the special case when the deviations in Φ
can be represented as the composition of a ﬁxed embedding into a higher dimensional space and
an adjustable linear transformation. The focus of our results is on the computational aspect of such
reductions, and the equivalence of ﬁxed-points computation and no-regret algorithms.
2 Preliminaries
2.1 Games and Equilibria

We consider the following kinds of games. First, the set of strategies for the players of the game is
a convex compact set. Second, the utility functions for the players are concave over their strategy
sets. To avoid cumbersome notation, we restrict ourselves to two player games, although all of our
results naturally extend to multi-player games.
Formally, for i = 1, 2, player i plays points from a convex compact set Ki ⊆ Rni. Her payoff
is given by function ui : K1 × K2 → R, i.e. if x1, x2 is the pair of strategies played by the two
players, then the payoff to player i is given by ui(x1, x2). We assume that u1 is a concave function
of x1 for any ﬁxed x2, and similarly u2 is a concave function of x2 for any ﬁxed x1.
We now deﬁne a notion of game theoretic equilibrium based on the property of being stable with
respect to consistent deviations. By this, we mean an online game-playing strategy for the players
that will guarantee that neither stands to gain if they decided to unilaterally, and consistently, deviate
from their suggested moves.
To model this, assume that each player i has a set of possible deviations Φi which is a ﬁnite1 set of
continuous mappings φi : Ki → Ki. Let Φ = (Φ1, Φ2). Let Ψ be a joint distribution on K1 × K2.
If it is the case that for any deviation φ1 ∈ Φ1, player 1’s expected payoff obtained by sampling x1
using Ψ is always larger than her expected payoff obtained by deviating to φ1(x1), then we call Ψ
stable under deviations in Φ1. The distribution Ψ is said to be a Φ-equilibrium if Ψ is stable under
deviations in Φ1 and Φ2. A similar deﬁnition appears in [12] and [17].
Deﬁnition 1 (Φ-equilibrium). A joint distribution Ψ over K1 × K2 is called a Φ-equilibrium if the
following holds, for any φ1 ∈ Φ1, and for any φ2 ∈ Φ2:

Z
Z

u1(x1, x2)Ψ(x1, x2) ≥
u2(x1, x2)Ψ(x1, x2) ≥

u1(φ1(x1), x2)Ψ(x1, x2)

u2(x1, φ2(x2))Ψ(x1, x2)

Z
Z

0

We say that Ψ is a ε-approximate Φ-equilibrium if the inequalities above are satisﬁed up to an
additive error of ε.

Intuitively, we imagine a repeated game between the two players, where at equilibrium, the players’
moves are correlated by a signal, which could be the past history of the play, and various external
factors. This signal samples a pair of moves from an equilibrium joint distribution over all pairs
of moves, and suggests to each player individually only the move she is supposed to play. If no
player stands to gain if she unilaterally, but consistently, used a deviation from her suggested move,
then the distribution of the correlating signal is stable under the set of deviations, and is hence an
equilibrium.
Example 1: Correlated Equilibria. A standard 2-player game is obtained when the Ki are the
simplices of distributions over some base sets of actions Ai and the utility functions ui are bilinear
in x1, x2. If the sets Φi consist of the maps φa,b : Ki → Ki for every pair a, b ∈ Ai deﬁned as

φa,b(x)[c] =

xa + xb
xc

if c = a
if c = b
otherwise

(1)

1It is highly plausible that the results in this paper extend to the case where Φ is inﬁnite – indeed, our results
hold for any set of mappings Φ which is obtained by taking all convex combinations of ﬁnitely many mappings
– but we restrict to ﬁnite Φ in this paper for simplicity.

3

then it can be shown that any Φ-equilibrium can be equivalently viewed as a correlated equilibrium
of the game, and vice-versa.

Example 2: The Stock Market game. Consider the following setting: there are two investors
(the generalization to many investors is straightforward), who invest their wealth in n stocks. In
each period, they choose portfolios x1 and x2 over the n stocks, and observe the stock returns. We
model the stock returns as a function r of the portfolios x1, x2 chosen by the investors, and it maps
the portfolios to the vector of stock returns. We make the assumption that each player has a small
inﬂuence on the market, and thus the function r is insensitive to the small perturbations in the input.
The wealth gain for each investor i is r(x1, x2) · xi. The standard way to measure performance of
an investment strategy is the logarithmic growth rate, viz. log(r(x1, x2) · xi). We can now deﬁne
the utility functions as ui(x1, x2) = log(r(x1, x2) · xi). Intuitively, this game models the setting in
which the market prices are affected by the investments of the players.
A natural goal for a good investment strategy would be to compare the wealth gain to that of the
best ﬁxed portfolio, i.e. Φi is the set of all constant maps. This was considered by Cover in his
Universal Portfolio Framework [5]. Another possible goal would be to compare the wealth gained
to that achievable by modifying the portfolios using the φa,b maps above, as considered by [16]. In
Section 3, we show that the stock market game admits algorithms that converge to an ε-equilibrium
in O( 1

ε ) rounds, whereas all previous algorithms need O( 1

ε2 ) rounds.

ε log 1

2.2 No regret algorithms

The online learning framework we consider is called online convex optimization [18], in which there
is a ﬁxed convex compact feasible set K ⊂ Rn and an arbitrary, unknown sequence of concave
payoff functions f (1), f (2), . . . : K → R. The decision maker must make a sequence of decisions,
where the tth decision is a selection of a point x(t) ∈ K and obtains a payoff of f (t)(x(t)) on period
t. The decision maker can only use the previous points x(1), . . . , x(t−1), and the previous payoff
functions f (1), . . . , f (t−1) to choose the point x(t).
The performance measure we use to evaluate online algorithms is regret, deﬁned as follows. The
decision maker has a ﬁnite set of N decision modiﬁers Φ which, as before, is a set of continuous
mappings from K → K. Then the regret for not using some deviation φ ∈ Φ is the excess payoff
the decision maker could have obtained if she had changed her points in each round by applying φ.
Deﬁnition 2 (Φ-Regret). Let Φ be a set of continuous functions from K → K. Given a set of T
concave utility functions f1, ..., fT , deﬁne the Φ-regret as

RegretΦ(T ) = max
φ∈Φ

f (t)(x(t)).

TX

f (t)(φ(x(t))) − TX

t=1

t=1

Two speciﬁc examples of Φ-regret deserve mention. The ﬁrst one is “external regret”, which is
deﬁned when Φ is the set of all constant mappings from K to itself. The second one is “internal
regret”, which is deﬁned when K is the simplex of distributions over some base set of actions A,
and Φ is the set of the φa,b functions (deﬁned in (1)) for all pairs a, b ∈ A.
A desirable property of an algorithm for Online Convex Optimization is Hannan consistency: the
regret, as a function of the number of rounds T , is sublinear. This implies that the average per
iteration payoff of the algorithm converges to the average payoff of a clairvoyant algorithm that uses
the best deviation in hindsight to change the point in every round. For the purpose of this paper, we
require a slightly stronger property for an algorithm, viz. that the regret is polynomially sublinear as
a function of T .
Deﬁnition 3 (No Φ-regret algorithm). A no Φ-regret algorithm is one which, given any sequence of
concave payoff functions f (1), f (2), . . ., generates a sequence of points x(1), x(2), . . . ∈ K such that
for all T = 1, 2, . . ., RegretΦ(T ) = O(T 1−c) for some constant c > 0. Such an algorithm will be
called efﬁcient if it computes x(t) in poly(n, N, t, L) time.

In the above deﬁnition, L is a description length parameter for K, deﬁned appropriately depending
on how the set K is represented. For instance, if K is the n-dimensional probability simplex, then

4

L = n. If K is speciﬁed by means of a separation oracle and inner and outer radii r and R, then
L = log(R/r), and we allow poly(n, N, t, L) calls to the separation oracle in each iteration.
The relatively new framework of Online Convex Optimization (OCO) has received much attention
recently in the machine learning community. Our no Φ-regret algorithms can use any of wide variety
of algorithms for OCO. In this paper, we will use Exponentiated Gradient (EG) algorithm ([14], [1]),
which has the following (external) regret bound:
Theorem 1. Let the domain K be the simplex of distributions over a base set of size n. Let
G∞ be an upper bound on the L∞ norm of the gradients of the payoff functions, i.e. G∞ ≥
supx∈K k∇f (t)(x)k∞. Then the EG algorithm generates points x(1), . . . , x(T ) such that

TX

f (t)(x) − TX

f (t)(x(t)) ≤ O(G∞plog(n)T )

max
x∈K

t=1

t=1

√

If the utility functions are strictly concave rather than linear, even stronger regret bounds, which
depend on log(T ) rather than
While most of the literature on online convex optimization focuses on external regret, it was ob-
served that any Online Convex Optimization algorithm for external regret can be converted to an
internal regret algorithm (for example, see [2], [16]).

T , are known [13].

2.3 Fixed Points

As mentioned in the introduction, our no regret algorithms depend on computing ﬁxed points of
the relevant mappings. For a given set of deviations Φ, denote by CH(Φ) the set of all convex
combinations of deviations in Φ, i.e.

nP
φ∈Φαφφ : αφ ≥ 0 andP

CH(Φ) =

o

φ∈Φαφ = 1

.

Since each map φ ∈ CH(Φ) is a continuous function from K → K, and K is a convex compact
there exists a point x ∈ K
domain, by Brouwer’s ﬁxed theorem, φ has a ﬁxed point in K, i.e.
such that φ(x) = x. We consider algorithms which approximate ﬁxed points for a given map in the
following sense.
Deﬁnition 4 (FPTAS for ﬁxed points of deviations). Let Φ be a set of N continuous functions
from K → K. A fully polynomial time approximation scheme (FPTAS) for ﬁxed points of Φ is an
algorithm, which, given any function φ ∈ CH(Φ) and an error parameter ε > 0, computes a point
x ∈ K such that kφ(x) − xk ≤ ε in poly(n, N, L, 1

ε ) time.

3 Convergence of no Φ-regret algorithms to Φ-equilibria

In this section we prove that if the players use no Φ-regret algorithms, then the empirical distribu-
tion of the moves converges to a Φ-equilibrium. [11] shows that if players use no internal regret
algorithms, then the empirical distribution of the moves converges to a correlated equilibrium. This
was generalized by [9] to any set of linear transformations Φ. The more general setting of this paper
also follows easily from the deﬁnitions. A similar theorem was also proved in [17].
The advantage of this general setting is that the connection to online convex optimization allows for
faster rates of convergence using recent online learning techniques. We give an example of a natural
game theoretic setting with faster convergence rate below.
Theorem 2. If each player i chooses moves using a no Φi-regret algorithms, then the empirical
game distribution of the players’ moves converges to a Φ-equilibrium. Further, an ε-approximate
Φ-equilibrium is reached after T iterations for the ﬁrst T which satisﬁes 1

T RegretΦ(T ) ≤ ε.

Proof. Consider the ﬁrst player. In each game iteration t, let (x1
(t)) be the pair of moves
played by the two players. From player 1’s point of view, the payoff function she obtains, f (t), is
the following:

(t), x2

∀x ∈ K1 :

f (t)(x) , u1(x, x2

(t)).

5

Note that this function is concave by assumption. Then we have, by deﬁnition 3,

RegretΦ1(T ) = max
φ∈Φ

f (t)(φ(x1

t

t

f (t)(x1

(t)).

(t))) −X

X

TX

t=1

Z

TX

t=1

1
T

Z

Rewriting this in terms of the original utility function, and scaling by the number of iterations we
get

u1(x1

(t), x2

(t)) ≥ 1
T

u1(φ(x1

(t)), x2

(t)) − 1
T

RegretΦ1(T ).

Denote by Ψ(T ) the empirical distribution of the played strategies till iteration T , i.e. the distribution
(t)) for t = 1, 2, . . . , T . Then, the above
which puts a probability mass of 1
inequality can be rewritten as

T on all pairs (x1

(t), x2

u1(x1, x2)Ψ(T )(x1, x2) ≥

u1(φ(x1), x2)Ψ(T )(x1, x2) − 1
T

RegretΦ1(T ).

A similar inequality holds for player 2 as well. Now assume that both players use no regret algo-
(T ) ≤ O(T 1−c) for some constant c > 0. Hence as T → ∞, we
rithms, which ensure that RegretΦi
(T ) → 0. Thus Ψ(T ) converges to a Φ-equilibrium. Also, Ψ(T ) is a ε-approximate
T RegretΦi
have 1
T RegretΦ2(T ) are less than ε,
equilibrium as soon as T is large enough so that 1
i.e. T ≥ Ω( 1

T RegretΦ1(T ) and 1

ε1/c ).

A corollary of Theorem 2 is that we can obtain faster rates of convergence using recent online
learning techniques, when the payoff functions are non-linear. This is natural in many situations,
since risk aversion is associated with the concavity of utility functions.
Corollary 3. For the stock market game as deﬁned in section 2.1, there exists no regret algorithms
which guarantee convergence to an ε-equilibrium in O( 1

ε log 1

ε ) iterations.

Proof sketch. The utility functions observed by the investor i in the stock market game are of the
form ui(x1, x2) = log(r(x1, x2) · xi). This logarithmic utility function is exp-concave, by the
assumption on the insensitivity of the function r to small perturbations in the input. Thus the online
algorithm of [5], or the more efﬁcient algorithms of [13] can be applied. In the full version of this
(T ) = O(log T ).
paper, we show that Lemma 6 can be modiﬁed to obtain algorithms with RegretΦi
By the Theorem 2 above, the investors reach ε-equilibrium in O( 1
ε ) iterations.

ε log 1

4 Computational Equivalence of Fixed Points and No Regret algorithms

In this section we prove our main result on the computational equivalence of computing ﬁxed points
and designing no regret algorithms. By the result of the previous section, players using no regret
algorithms converge to equilibria.
We assume that the payoff functions f (t) are scaled so that the (L2) norm of their gradients is
bounded by 1, i.e. k∇f (t)k ≤ 1. Our main theorem is the following:
Theorem 4. Let Φ be a given ﬁnite set of deviations. Then there is a FPTAS for ﬁxed points of Φ if
and only if there exists an efﬁcient no Φ-regret algorithm.

The ﬁrst direction of the theorem is proved by designing utility functions for which the no regret
property will imply convergence to an approximate ﬁxed point of the corresponding transformations.
The proof crucially depends on the fact that no regret algorithms have the stringent requirement that
their worst case regret, against arbitrary adversarially chosen payoff functions, is sublinear as a
function of the number of the rounds.
Lemma 5. If there exists a no Φ-regret algorithm then there exists an FPTAS for ﬁxed points of Φ.
Proof. Let φ0 ∈ CH(Φ) be a given mapping whose ﬁxed point we wish to compute. Let ε be a given
error parameter.

6

At iteration t, let x(t) be the point chosen by A. If kφ0(x(t)) − x(t)k ≤ ε, we can stop, because we
have found an approximate ﬁxed point. Else, supply A with the following payoff function:

f (t)(x) , (φ0(x(t)) − x(t))>

kφ0(x(t)) − x(t)k (x − x(t))

This is a linear function, with k∇f (t)(x)k = 1. Also, f (t)(x(t)) = 0, and f (t)(φ0(x(t))) =
kφ0(x(t)) − x(t)k ≥ ε. After T iterations, since φ0 is a convex combination of functions in Φ,
and since all the f (t) are linear functions, we have

max
φ∈Φ

Thus,

f (t)(φ(x(t))) ≥ TX

TX

t=1

X

t=1

f (t)(φ(x(t))) −X

f (t)(φ0(x(t))) ≥ εT.

RegretΦ(T ) = max
φ∈Φ

(2)
Since A is a no-regret algorithm, assume that A ensures that RegretΦ(T ) = O(T 1−c) for some
constant c > 0. Thus, when T = Ω( 1
ε1/c ) the lower bound (2) on the regret cannot hold unless we
have already found an ε-approximate ﬁxed point of φ0.

t

t

f (t)(x(t)) ≥ εT.

The second direction is on the lines of the algorithms of [2] and [16] which use ﬁxed point compu-
tations to obtain no internal regret algorithms.
√
Lemma 6. If there is an FPTAS for ﬁxed points of Φ, then there is an efﬁcient no Φ-regret algorithm.
In fact, the algorithm guarantees that RegretΦ(T ) = O(

T ). 2

Proof. We reduce the given OCO problem to an “inner” OCO problem. The “outer” OCO problem
is the original one. We use a no external regret algorithm for the inner OCO problem to generate
points in K for the outer one, and use the payoff functions obtained in the outer OCO problem to
generate appropriate payoff functions for the inner one.
Let Φ = {φ1, φ2, . . . , φN}. The domain for the inner OCO problem is the simplex of all distribu-
tions on Φ, denoted ∆N . For a distribution α ∈ ∆N , let αi be the probability measure assigned to
φi in the distribution α. There is a natural mapping from ∆N → CH(Φ): for any α ∈ ∆N , denote

by φα the functionPN

i=1 αiφi ∈ CH(Φ).

Let x(t) ∈ K be the point used in the outer OCO problem in the tth round, and let f (t) be the
obtained payoff function. Then the payoff functions for the inner OCO problem is the function
g(t) : ∆N → R deﬁned as follows:

∀α ∈ ∆N :

g(t)(α) , f (t)(φα(x(t))).

i αi(φi(x(t)) − x0)), becauseP

We can rewrite g(t) as g(t)(α) = f (t)(x0 +P

We now apply the Exponentiated Gradient (EG) algorithm (see Section 2.2) to the inner OCO prob-
lem. To analyze the algorithm, we bound k∇g(t)k∞ as follows. Let x0 be an arbitrary point in K.
i αi = 1. Then,
∇g(t) = X(t)∇f (t)(φα(x(t))), where X(t) is an N × n matrix whose ith row is (φi(x(t)) − x0)>.
Thus,
|(φi(x(t))−x0)>∇f (t)(φα(x(t)))| ≤ kφi(x(t))−x0kk∇f (t)(φα(x(t)))k ≤ 1.
k∇g(t)k∞ = max
The last inequality follows because we assumed that the diameter of K is bounded by 1, and the
norm of the gradient of f (t) is also bounded by 1.
Let α(t) be the distribution on Φ produced by the EG algorithm at time t. Now, the point x(t) is
computed by running the FPTAS for computing an 1√
-approximate ﬁxed point of the function φα(t),
i.e. we have kφα(t)(x(t)) − x(t)k ≤ 1√

.

t

i

t

2In the full version of the paper, we improve the regret bound to O(log T ) under some stronger concavity

assumptions on the payoff functions.

7

Now, using the deﬁnition of the g(t) functions, and by the regret bound for the EG algorithm, we
have that for any ﬁxed distribution α ∈ ∆N ,

TX

g(t)(α)− TX

g(t)(α(t)) ≤ O(plog(N)T ). (3)

TX

f (t)(φα(x(t)))− TX

f (t)(φα(t)(x(t))) =

t=1

t=1

Since k∇f (t)k ≤ 1,

t=1

t=1

f (t)(φα(t)(x(t))) − f (t)(x(t)) ≤ kφα(t)(x(t)) − x(t)k ≤ 1√

t

.

(4)

Summing (4) from t = 1 to T , and adding to (3), we get that for any distribution α over Φ,

TX
f (t)(φα(x(t))) −X
PT
t=1 f (t)(φi(x(t))) −PT

t=1

t

f (t)(x(t)) ≤ O(plog(N)T ) +
t=1 f (t)(x(t)) ≤ O(plog(N)T ), and thus we have a no Φ-regret al-

= O(plog(N)T ).

the above inequality implies that

TX

In particular, by concentrating α on any given φi,

1√
t

t=1

gorithm.

References
[1] S. Arora, E. Hazan, and S. Kale. The multiplicative weights update method: a meta algorithm and

applications. Manuscript, 2005.

[2] A. Blum and Y. Mansour. From external to internal regret. In COLT, pages 621–636, 2005.
[3] X. Chen and X. Deng. Settling the complexity of two-player nash equilibrium. In 47th FOCS, pages

261–272, 2006.

[4] X. Chen, X. Deng, and S-H. Teng. Computing nash equilibria: Approximation and smoothed complexity.

focs, 0:603–612, 2006.

[5] T. Cover. Universal portfolios. Math. Finance, 1:1–19, 1991.
[6] C. Daskalakis, P. W. Goldberg, and C. H. Papadimitriou. The complexity of computing a nash equilibrium.

In 38th STOC, pages 71–78, 2006.

[7] Y. Freund and R. E. Schapire. Adaptive game playing using multiplicative weights. Games and Economic

Behavior, 29:79–103, 1999.

[8] G. Gordon, A. Greenwald, C. Marks, and M. Zinkevich. No-regret learning in convex games. Brown

University Tech Report CS-07-10, 2007.

[9] A. Greenwald and A. Jafari. A general class of no-regret learning algorithms and game-theoretic equilib-

ria, 2003.

[10] J. Hannan. Approximation to bayes risk in repeated play. In M. Dresher, A. W. Tucker, and P. Wolfe,

editors, Contributions to the Theory of Games, volume III, pages 97–139, 1957.

[11] S. Hart and A. Mas-Colell. A simple adaptive procedure leading to correlated equilibrium. Econometrica,

68(5):1127–1150, 2000.

[12] S. Hart and D. Schmeidler. Existence of correlated equilibria. Mathematics of Operations Research,

14(1):18–25, 1989.

[13] E. Hazan, A. Kalai, S. Kale, and A. Agarwal. Logarithmic regret algorithms for online convex optimiza-

tion. In 19’th COLT, 2006.

[14] J. Kivinen and M. K. Warmuth. Exponentiated gradient versus gradient descent for linear predictors. Inf.

Comput., 132(1):1–63, 1997.

[15] C. H. Papadimitriou. On the complexity of the parity argument and other inefﬁcient proofs of existence.

J. Comput. Syst. Sci., 48(3):498–532, 1994.

[16] G. Stoltz and G. Lugosi. Internal regret in on-line portfolio selection. Machine Learning, 59:125–159,

2005.

[17] G. Stoltz and G. Lugosi. Learning correlated equilibria in games with compact sets of strategies. Games

and Economic Behavior, 59:187–208, 2007.

[18] M. Zinkevich. Online convex programming and generalized inﬁnitesimal gradient ascent. In 20th ICML,

pages 928–936, 2003.

8

"
700,2007,Catching Change-points with Lasso,"We propose a new approach for dealing with the estimation of the location of change-points in one-dimensional piecewise constant signals observed in white noise. Our approach consists in reframing this task in a variable selection context. We use a penalized least-squares criterion with a l1-type penalty for this purpose. We prove that, in an appropriate asymptotic framework, this method provides consistent estimators of the change-points. Then, we explain how to implement this method in practice by combining the LAR algorithm and a reduced version of the dynamic programming algorithm and we apply it to synthetic and real data.","Catching Change-points with Lasso

Zaid Harchaoui, C´eline L´evy-Leduc
LTCI, TELECOM ParisTech and CNRS
37/39 Rue Dareau, 75014 Paris, France
{zharchao,levyledu}@enst.fr

Abstract

We propose a new approach for dealing with the estimation of the location of
change-points in one-dimensional piecewise constant signals observed in white
noise. Our approach consists in reframing this task in a variable selection con-
text. We use a penalized least-squares criterion with a `1-type penalty for this
purpose. We prove some theoretical results on the estimated change-points and
on the underlying piecewise constant estimated function. Then, we explain how
to implement this method in practice by combining the LAR algorithm and a re-
duced version of the dynamic programming algorithm and we apply it to synthetic
and real data.

1 Introduction

Change-points detection tasks are pervasive in various ﬁelds, ranging from audio [10] to EEG seg-
mentation [5]. The goal is to partition a signal into several homogeneous segments of variable
durations, in which some quantity remains approximately constant over time. This issue was ad-
dressed in a large literature (see [20] [11]), where the problem was tackled both from an online
(sequential) [1] and an off-line (retrospective) [5] points of view. Most off-line approaches rely on a
Dynamic Programming algorithm (DP), allowing to retrieve K change-points within n observations
of a signal with a complexity of O(Kn2) in time [11]. Such a feature refrains practitioners from
applying these methods to large datasets. Moreover, one often observes a sub-optimal behavior of
the raw DP algorithm on real datasets.

We suggest here to slightly depart from this line of research, by focusing on a reformulation of
change-point estimation in a variable selection framework. Then, estimating change-point loca-
tions off-line turns into performing variable selection on dummy variables representing all possible
change-point locations. This allows us to take advantage of the latest theoretical [23], [3] and prac-
tical [7] advances in regression with Lasso penalty. Indeed, Lasso provides us with a very efﬁcient
method for selecting potential change-point locations. This selection is then reﬁned by using the DP
algorithm to estimate the change-point locations.

Let us outline the paper. In Section 2, we ﬁrst describe our theoretical reformulation of off-line
change-point estimation as regression with a Lasso penalty. Then, we show that the estimated mag-
nitude of jumps are close in mean, in a sense to be precized, to the true magnitude of jumps. We
also give a non asymptotic inequality to upper-bound the `2-loss of the true underlying piecewise
constant function and the estimated one. We describe our algorithm in Section 3. In Section 4, we
discuss related works. Finally, we provide experimental evidence of the relevance of our approach.

1

2 Theoretical approach

2.1 Framework

k ’s in the following model:
Yt = µ?

We describe, in this section, how off-line change-point estimation can be cast as a variable selection
problem. Off-line estimation of change-point locations within a signal (Yt) consists in estimating
the τ ?

k + εt,

(1)
where εt are i.i.d zero-mean random variables with ﬁnite variance. This problem can be reformulated
as follows. Let us consider:

k , 1 ≤ k ≤ K ? with τ ?

k−1 + 1 ≤ t ≤ τ ?

t = 1, . . . , n such that τ ?

0 = 0,

Yn = Xnβn + εn

(2)
where Yn is a n × 1 vector of observations, Xn is a n × n lower triangular matrix with nonzero
j ’s
n)0 is a zero-mean random vector such that the εn
elements equal to one and εn = (εn
are i.i.d with ﬁnite variance. As for βn, it is a n × 1 vector having all its components equal to
zero except those corresponding to the change-point instants. The above multiple change-point
estimation problem (1) can thus be tackled as a variable selection one:

1 , . . . , εn

Minimize

β

kYn − Xnβk2

n subject to kβk1 ≤ s ,

(3)

where kuk1 and kukn are deﬁned for a vector u = (u1, . . . , un) ∈ Rn by kuk1 = Pn
and kuk2
following counterpart objective in model (1):

j=1 |uj|
j respectively. Indeed, the above formulation amounts to minimize the

j=1 u2

n = n−1Pn

Minimize
µ1,...,µn

1
n

Xt=1

n

n−1

(Yt − µt)2

subject to

|µt+1 − µt| ≤ s,

(4)

Xt=1

which consists in imposing an `1-constraint on the magnitude of jumps. The underpinning insight
is the sparsity-enforcing property of the `1-constraint, which is expected to give a sparse vector,
whose non-zero components would match with those of βn and thus with change-point locations.
It is related to the popular Least Absolute Shrinkage eStimatOr (LASSO) in least-square regression
of [21], used for efﬁcient variable selection.

In the next section, we provide two results supporting the use of the formulation (3) for off-line
multiple change-point estimation. We show that estimates of jumps minimizing (3) are consistent
in mean, and we provide a non asymptotic upper bound for the `2 loss of the underlying estimated
piecewise constant function and the true underlying piecewise function. This inequality shows that,
at a precized rate, the estimated piecewise constant function tends to the true piecewise constant
function with a probability tending to one.

2.2 Main results

In this section, we shall study the properties of the solutions of the problem (3) deﬁned by

(5)

(6)

ˆβn(λ) = Arg min

β

nkYn − Xnβk2

n + λkβk1o .

Let us now introduce the notation sign. It maps positive entry to 1, negative entry to -1 and a null
entry to zero. Let

and let C n the covariance matrix be deﬁned by

A = {k, βn

k 6= 0} and A = {1, . . . , n}\A

In a general regression framework, [18] recall that, with probability tending to one, ˆβn(λ) and βn
have the same sign for a well-chosen λ, only if the following condition holds element-wise:

C n = n−1X 0

nXn .

(7)

(8)
IJ is a sub-matrix of C n obtained by keeping rows with index in the set I and columns with
k )k∈A. The condition (8) is not fulﬁlled in the

A)¯¯ < 1,

where C n
index in J. The vector βn

A is deﬁned by βn

AA)−1sign(βn

¯¯C n

A = (βn

AA(C n

2

change-point framework implying that we cannot have a perfect estimation of the change-points as
it is already known, see [13]. But, following [18] and [3], we can prove some consistency results,
see Propositions 1 and 2 below.

In the following, we shall assume that the number of break points is equal to K ?.
The following proposition ensures that for a large enough value of n the estimated change-point
locations are close to the true change-points.
Proposition 1. Assume that the observations (Yn) are given by (2) and that the εn
If λ = λn is such that λn√n → 0 as n tends to inﬁnity then

j ’s are centered.

kE( ˆβn(λn)) − βnkn → 0 .

Proof. We shall follow the proof of Theorem 1 in [18]. For this, we denote βn(λ) the estimator
ˆβn(λ) under the absence of noise and γn(λ) the bias associated to the Lasso estimator: γn(λ) =
βn(λ) − βn. For notational simplicity, we shall write γ instead of γn(λ). Note that γ satisﬁes the
following minimization: γ = Arg minζ∈Rn f (ζ) , where

f (ζ) = ζ 0C nζ + λXk∈A

k + ζk| + λXk∈ ¯A
|βn

|ζk| .

Since f (γ) ≤ f (0), we get

γ0C nγ + λXk∈A

k + γk| + λXk∈ ¯A
|βn

We thus obtain using the Cauchy-Schwarz inequality the following upper bound

|βn
k | .

|γk| ≤ λXk∈A
|γk|2!1/2

.

|γk| ≤ λ√K ?Ã n
Xk=1

γ0C nγ ≤ λXk∈A
k=1 |γk|2, we obtain: kγkn ≤ λ√nK ?.

Using that γ0C nγ ≥ n−1Pn

The following proposition ensures, thanks to a non asymptotic result, that the estimated underlying
piecewise function is close to the true piecewise constant function.
Proposition 2. Assume that the observations (Yn) are given by (2) and that the εn
Gaussian random variables with variance σ2 > 0. Assume also that (βn

j ’s are centered iid
k )k∈A belong to (βmin, βmax)
where βmin > 0. For all n ≥ 1 and A > √2 then, with a probability larger than 1 − n1−A2/2, if
λn = Aσplog n/n,

kXn( ˆβn(λn) − βn)k2

n ≤ 2AσβmaxK ?r log n

n

.

Proof. By deﬁnition of ˆβn(λ) in (5) as a minimizer of a criterion, we have

kYn − Xn ˆβn(λ)k2

n + λk ˆβn(λ)k1 ≤ kYn − Xnβnk2

n + λkβnk1 .

Using (2), we get

kXn(βn − ˆβn(λ))k2

n +

2
n

(βn − ˆβn(λ))0X 0

nεn + λ

n

Xj=1

| ˆβn
j (λ)| ≤ λ

n

Xj=1

|βn
j | .

Thus,

kXn(βn − ˆβn(λ))k2

n ≤

2
n

Observe that

( ˆβn(λ) − βn)0X 0

nεn + λXj∈A

(|βn

j | − | ˆβn

j (λ)|) − λXj∈ ¯A

| ˆβn
j (λ)| .

2
n

( ˆβn(λ) − βn)0X 0

nεn = 2

n

Xj=1

( ˆβn

j )
j (λ) − βn


1
n

n

Xi=j

εn

i
 .

3

n

n

n

εn

Xj=1

P( ¯E) ≤

i=j εn
zero-mean Gaussian random variables, we obtain

i¯¯¯ ≤ λo. Then, using the fact that the εn
j=1nn−1¯¯¯Pn
Let us deﬁne the event E = Tn
i¯¯¯¯¯¯
n−1¯¯¯¯¯¯
> λ
P
2σ2(n − j + 1)¶ .
Xj=1
Xi=j
 ≤
Thus, if λ = λn = Aσplog n/n,
With a probability larger than 1 − n1−A2/2, we get
| ˆβn
j (λ) − βn

j |) − λnXj∈ ¯A
We thus obtain with a probability larger than 1 − n1−A2/2 the following upper bound

kXn(βn − ˆβn(λ))k2

j | + λnXj∈A

P( ¯E) ≤ n1−A2/2 .

expµ−

j | − | ˆβn

n ≤ λn

Xj=1

(|βn

n2λ2

n

j | = 2Aσr log n
|βn

n Xj∈A

j | ≤ 2AσβmaxK ?r log n
|βn

n

.

i ’s are iid

| ˆβn
j | .

kXn(βn − ˆβn(λ))k2

n ≤ 2λnXj∈A

3 Practical approach

The previous results need to be efﬁciently implemented to cope with ﬁnite datasets. Our algorithm,
called Cachalot (CAtching CHAnge-points with LassO), can be split into the following three steps
described hereafter.

Estimation with a Lasso penalty We compute the ﬁrst Kmax non-null coefﬁcients ˆβτ1, . . . , ˆβτKmax
on the regularization path of the LASSO problem (3). The LAR/LASSO algorithm, as described in
[7], provides an efﬁcient algorithm to compute the entire regularization path for the LASSO problem.

SincePj |βj| ≤ s is a sparsity-enforcing constraint, the set {j, ˆβj 6= 0} = {τj} becomes larger as

we run through the regularization path. We shall denote by S the Kmax-selected variables:

(9)
The computational complexity of the Kmax-long regularization path of LASSO solutions is
maxn). Most of the time, we can see that the Lasso effectively catches the true change-
O(K 3
point but also irrelevant change-points at the vicinity of the true ones. Therefore, we propose to
reﬁne the set of change-points caught by the Lasso by performing a post-selection.

S = {τ1, . . . , τKmax} .

max + K 2

Reduced Dynamic Programming algorithm One can consider several strategies to remove ir-
relevant change-points from the ones retrieved by the Lasso. Among them, since usually in appli-
cations, one is only interested in change-point estimation up to a given accuracy, we could launch
the Lasso on a subsample of the signal. Here, we suggest to perform post-selection by using the
standard Dynamic Programming algorithm (DP) thoroughly described in [11] (Chapter 12, p. 450)
but on the reduced set S instead of {1, . . . , n}. This algorithm allows one to efﬁciently minimize
the following objective for each K in {1, . . . , Kmax}:
Xk=1

Xi=τk−1+1

(Yi − ˆµk)2,

s.t τ1,...,τK ∈S

J(K) =

τ1<···<τK

(10)

Min

S being deﬁned in (9) and outputs for each K,
the corresponding subset of change-points
(ˆτ1, . . . , ˆτK). The DP algorithm has a computational complexity of O(Kmax n2) if we look for
at most Kmax change-points within the signal. Here, our reduced DP calculations (rDP) scales
as O(Kmax K 2
max) where Kmax is the maximum number of change-points/variables selected by
LAR/LASSO algorithm. Since typically Kmax ¿ n, our method thus provides a reduction of the
computational burden associated with the classical change-points detection approach which consists
in running the DP algorithm over all the n observations.

τk

K

4

Selecting the number of change-points The point is now to select the adequate number of
change-points. As n → ∞, according to [15], the ratio ρk = J(k + 1)/J(k) should show different
qualitative behavior when k 6 K ? and when k > K ?, K ? being the true number of change-points.
In particular, ρk ≥ Cn for k > K ?, where Cn → 1 as n → ∞. Actually we found out that Cn was
close to 1, even in small-sample settings, for various experimental designs in terms of noise variance
and true number of change-points. Hence, conciliating theoretical guidance in large-sample setting
and experimental ﬁndings in ﬁxed-sample setting, we suggest the following rule of thumb for select-
ing the number of change-points ˆK : ˆK = Mink≥1 {ρk ≥ 1 − ν} , where ρk = J(k + 1)/J(k).
Cachalot Algorithm
Input

• Vector of observations Y ∈ Rn
• Upper bound Kmax on the number of change-points
• Model selection threshold ν

Processing

1. Compute the ﬁrst Kmax non-null coefﬁcients (βτ1 , . . . , βτKmax ) on the regularization path

with the LAR/LASSO algorithm.

2. Launch the rDP algorithm on the set of potential change-points (τ1, . . . , τKmax).
3. Select the smallest subset of the potential change-points (τ1, . . . , τKmax ) selected by the rDP

algorithm for which ρk ≥ 1 − ν.

Output Change-point locations estimates ˆτ1, . . . , ˆτ ˆK.

(Yn)

To illustrate our algorithm, we consider observations
(2) with
(β30, β50, β70, β90) = (5,−3, 4,−2), the other βj being equal to zero, n = 100 and εn a Gaus-
sian random vector with a covariance matrix equal to Id, Id being a n × n identity matrix. The
set of the ﬁrst nine active variables caught by the Lasso along the regularization path, i.e. the set
{k, ˆβk 6= 0} is given in this case by: S = {21, 23, 28, 29, 30, 50, 69, 70, 90}. The set S contains
the true change-points but also irrelevant ones close to the true change-points. Moreover the most
signiﬁcant variables do not necessarily appear at the beginning. This supports the use of the re-
duced version of the DP algorithm hereafter. Table 1 gathers the J(K), K = 1, . . . , Kmax and the
corresponding (ˆτ1, . . . , ˆτK).

satisfying model

Table 1: Toy example: The empirical risk J and the estimated change-points as a function of the
possible number of change-points K

K
0
1
2
3
4
5
6
7
8
9

J(K)
696.28
249.24
209.94
146.29
120.21
118.22
116.97
116.66
116.65
116.64

(ˆτ1, . . . , ˆτK )

∅
30

(30,70)

(30,50,69)

(30,50,70,90)

(30,50,69,70,90)

(21,30,50,69,70,90)

(21,29,30,50,69,70,90)

(21,23,29,30,50,69,70,90)

(21,23,28,29,30,50,69,70,90)

The different values of the ratio ρk for k = 0, . . . , 8 of the model selection procedure are given in
Table 2. Here we took ν = 0.05. We conclude, as expected, that ˆK = 4 and that the change-points
are (30, 50, 70, 90), thanks to the results obtained in Table 1.

4 Discussion

Off-line multiple change-point estimation has recently received much attention in theoretical works,
both in a non-asymptotic and in an asymptotic setting by [17] and [13] respectively. From a practi-
cal point of view, retrieving the set of change-point locations {τ ?
K} is challenging, since it is

1 , . . . , τ ?

5

Table 2: Toy example: The values of the ratio (ρk = J(k + 1)/J(k), k = 0, . . . , 8)

k
ρk

0
0.3580

1
0.8423

2
0.6968

3
0.8218

4
0.9834

5
0.9894

6
0.9974

7
0.9999

8
1.0000

plagued by the curse of dimensionality. Indeed, all of the n observation times have to be considered
as potential change-point instants. Yet, a dynamic programming algorithm (DP), proposed by [9]
and [2], allows to explore all the conﬁgurations with a complexity of O(n3) in time. Then selecting
the number of change-points is usually performed thanks to a Schwarz-like penalty λnK, where
λn has to be calibrated on data [13] [12], or a penalty K(a + b log(n/K)) as in [17] [14], where
a and b are data-driven as well. We should also mention that an abundant literature tackles both
change-point estimation and model selection issues from a Bayesian point of view (see [20] [8] and
references therein). All approaches cited above rely on DP, or variants in Bayesian settings, and
hence yield a computational complexity of O(n3), which makes them inappropriate for very large-
scale signal segmentation. Moreover, despite its theoretical optimality in a maximum likelihood
framework, raw DP may sometimes have poor performances when applied to very noisy obser-
vations. Our alternative framework for multiple change-point estimation was previously elusively
mentioned several times, e.g.
in [16] [4] [19]. However up to our knowledge neither successful
practical implementation nor theoretical grounding was given so far to support such an approach
for change-point estimation. Let us also mention [22], where the Fused Lasso is applied in a simi-
lar yet different way to perform hot-spot detection. However, this approach includes an additional
penalty, penalizing departures from the overall mean of the observations, and should thus rather be
considered as an outlier detection method.

5 Comparison with other methods

5.1 Synthetic data

We propose to compare our algorithm with a recent method based on a penalized least-squares crite-
rion studied by [12]. The main difﬁculty in such approaches is the choice of the constants appearing
in the penalty. In [12], a very efﬁcient approach to overcome this difﬁculty has been proposed: the
choice of the constants is completely data-driven and has been implemented in a toolbox available
online at http://www.math.u-psud.fr/˜lavielle/programs/index.html.
In the following, we benchmark our algorithm: A together with the latter method: B. We shall
use Recall and Precision as relevant performance measures to analyze the previous two algorithms.
More precisely, the Recall corresponds to the ratio of change-points retrieved by a method with
those really present in the data. As for the Precision, it corresponds to the number of change-points
retrieved divided by the number of suggested change-points. We shall also estimate the probability
of false alarm corresponding to the number of suggested change-points which are not present in the
signal divided by the number of true change-points.
To compute the precision and the recall of methods A and B, we ran Monte-Carlo experiments. More
precisely, we sampled 30 conﬁgurations of change-points for each real number of change-points K ?
equal to 5, 10, 15 and 20 within a signal containing 500 observations. Change-points were at least
distant of 10 observations. We sampled 30 conﬁgurations of levels from a Gaussian distribution.
We used the following setting for the noise: for each conﬁguration of change-points and levels,
we synthesized a Gaussian white noise such that the standard deviation is set to a multiple of the
minimum magnitude jump between two contiguous segments, i.e. σ = m Mink(µ∗
k), µ?
k
being the level of the kth segment. The number of noise replications was set to 10.
As shown in Tables 3, 4 and 5 below, our method A yields competitive results compared to method
B with 1 − ν = 0.99 and Kmax = 50. Performances in recall are comparable whereas method A
provides better results than method B in terms of precision and false alarm rate.
5.2 Real data

k+1 − µ∗

In this section, we propose to apply our method previously described to real data which have already
been analyzed by Bayesian methods: the well-log data which are described in [20] and [6] and

6

Table 3: Precision of methods A and B
K ? = 15

K ? = 10

K ? = 20

K ? = 5
B

A

A
A
0.95±0.05 0.86±0.13 0.97±0.03 0.91±0.09
0.81±0.15 0.71±0.29 0.89±0.08 0.8±0.22
0.8±0.16
0.95±0.05 0.86±0.13 0.97±0.03 0.92±0.09
0.73±0.29 0.89±0.08 0.8±0.21
0.78±0.17 0.71±0.27 0.88±0.09 0.78±0.21 0.93±0.06 0.85±0.13 0.96±0.04 0.9±0.09
0.73±0.19 0.66±0.28 0.84±0.1
0.93±0.06 0.84±0.13 0.95±0.04 0.9±0.1

0.79±0.2

B

B

A

B

Table 4: Recall of methods A and B

K ? = 5
B

K ? = 10

K ? = 15

K ? = 20

A

A
B
0.99±0.02 0.99±0.02 1±0
1±0
0.98±0.04 0.99±0.03 0.99±0.01 0.99±0.01 0.99±0.01 0.99±0.01 0.99±0.01 1±0
0.95±0.08 0.94±0.08 0.96±0.06 0.96±0.05 0.97±0.03 0.97±0.04 0.97±0.03 0.98±0.02
0.85±0.16 0.87±0.15 0.92±0.07 0.91±0.09 0.94±0.06 0.94±0.06 0.95±0.04 0.96±0.04

A
0.99±0

A
0.99±0

B
0.99±0

B
1±0

Table 5: False alarm rate of methods A and B

K ? = 5
B

K ? = 10

K ? = 15

K ? = 20

A
0.13±0.03 0.23±0.2
0.13±0.03 0.22±0.2
0.13±0.03 0.21±0.18 0.23±0.03 0.32±0.18 0.33±0.02 0.4±0.13
0.13±0.03 0.21±0.2
0.23±0.03 0.29±0.16 0.31±0.03 0.4±0.15

A
0.24±0.03 0.33±0.19 0.34±0.02 0.42±0.13 0.44±0.02 0.51±0.12
0.23±0.03 0.32±0.18 0.33±0.02 0.41±0.13 0.44±0.02 0.5±0.11
0.43±0.03 0.5±0.12
0.42±0.03 0.48±0.11

B

B

B

A

A

Method
m = 0.1
m = 0.5
m = 1.0
m = 1.5

Method
m = 0.1
m = 0.5
m = 1.0
m = 1.5

Method
m = 0.1
m = 0.5
m = 1.0
m = 1.5

displayed in Figure 1. They consist in nuclear magnetic response measurements expected to carry
information about rock structure and especially its stratiﬁcation.

One distinctive feature of these data is that they typically contain a non-negligible amount of outliers.
The multiple change-point estimation method should then, either be used after a data cleaning step
(median ﬁltering [6]), or explicitly make heavy-tailed noise distribution assumption. We restricted
ourselves to a median ﬁltering pre-processing. The results given by our method applied to the well-
log data processed with a median ﬁlter are displayed in Figure 1 for Kmax = 200 and 1 − ν = 0.99.
The vertical lines locate the change-points. We can note that they are close to those found out by [6]
(P. 206) who used Bayesian techniques to perform change-points detection.

x 105

1.5

1.4

1.3

1.2

1.1

1

0.9

0.8

0.7

0.6

0

500

1000

1500

2000

2500

3000

3500

4000

4500

x 105

1.4

1.35

1.3

1.25

1.2

1.15

1.1

1.05

1

0.95

0.9
0

500

1000

1500

2000

2500

3000

3500

4000

4500

Figure 1: Left: Raw well-log data, Right: Change-points locations obtained with our method in
well-log data processed with a median ﬁlter

7

6 Conclusion and prospects

We proposed here to cast the multiple change-point estimation as a variable selection problem. A
least-square criterion with a Lasso-penalty yields an efﬁcient primary estimation of change-point
locations. Yet these change-point location estimates can be further reﬁned thanks to a reduced
dynamic programming algorithm. We obtained competitive performances on both artiﬁcial and real
data, in terms of precision, recall and false alarm. Thus, Cachalot is a computationally efﬁcient
multiple change-point estimation method, paving the way for processing large datasets.

References

[1] M. Basseville and N. Nikiforov. The detection of abrupt changes. Information and System sciences series.

Prentice-Hall, 1993.

[2] R. Bellman. On the approximation of curves by line segments using dynamic programming. Communi-

cations of the ACM, 4(6), 1961.

[3] P. Bickel, Y. Ritov, and A. Tsybakov. Simultaneous analysis of Lasso and Dantzig selector. Preprint 2007.
[4] L. Boysen, A. Kempe, A. Munk, V. Liebscher, and O. Wittich. Consistencies and rates of convergence of

jump penalized least squares estimators. Annals of Statistics, In revision.

[5] B. Brodsky and B. Darkhovsky. Non-parametric statistical diagnosis: problems and methods. Kluwer

Academic Publishers, 2000.

[6] O. Capp´e, E. Moulines, and T. Ryden. Inference in Hidden Markov Models (Springer Series in Statistics).

Springer-Verlag New York, Inc., 2005.

[7] B. Efron, T. Hastie, and R. Tibshirani. Least angle regression. Annals of Statistics, 32:407–499, 2004.
[8] P. Fearnhead. Exact and efﬁcient bayesian inference for multiple changepoint problems. Statistics and

Computing, 16:203–213, 2006.

[9] W. D. Fisher. On grouping for maximum homogeneity. Journal of the American Statistical Society,

53:789–798, 1958.

[10] O. Gillet, S. Essid, and G. Richard. On the correlation of automatic audio and visual segmentation of

music videos. IEEE Transactions on Circuits and Systems for Video Technology, 2007.

[11] S. M. Kay. Fundamentals of statistical signal processing: detection theory. Prentice-Hall, Inc., 1993.
[12] M. Lavielle. Using penalized contrasts for the change-points problems. Signal Processing, 85(8):1501–

1510, 2005.

[13] M. Lavielle and E. Moulines. Least-squares estimation of an unknown number of shifts in a time series.

Journal of time series analysis, 21(1):33–59, 2000.

[14] E. Lebarbier. Detecting multiple change-points in the mean of a gaussian process by model selection.

Signal Processing, 85(4):717–736, 2005.

[15] C.-B. L. Lee. Estimating the number of change-points in a sequence of independent random variables.

Statistics and Probability Letters, 25:241–248, 1995.

[16] E. Mammen and S. Van De Geer. Locally adaptive regression splines. Annals of Statistics, 1997.
[17] P. Massart. A non asymptotic theory for model selection. pages 309–323. European Mathematical Society,

2005.

[18] N. Meinshausen and B. Yu. Lasso-type recovery of sparse representations for high-dimensional data.

Preprint 2006.

[19] S. Rosset and J. Zhu. Piecewise linear regularized solution paths. Annals of Statistics, 35, 2007.
[20] J. Ruanaidh and W. Fitzgerald. Numerical Bayesian Methods Applied to Signal Processing. Statistics and

Computing. Springer, 1996.

[21] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society,

Series B, 58(1):267–288, 1996.

[22] R. Tibshirani and P. Wang. Spatial smoothing and hot spot detection for cgh data using the fused lasso.

Biostatistics, 9(1):18–29, 2008.

[23] P. Zhao and B. Yu. On model selection consistency of lasso. Journal Of Machine Learning Research, 7,

2006.

8

"
102,2007,Spatial Latent Dirichlet Allocation,"In recent years, the language model Latent Dirichlet Allocation (LDA), which clusters co-occurring words into topics, has been widely appled in the computer vision field. However, many of these applications have difficulty with modeling the spatial and temporal structure among visual words, since LDA assumes that a document is a ``bag-of-words''. It is also critical to properly design ``words'' and “documents” when using a language model to solve vision problems. In this paper, we propose a topic model Spatial Latent Dirichlet Allocation (SLDA), which better encodes spatial structure among visual words that are essential for solving many vision problems. The spatial information is not encoded in the value of visual words but in the design of documents. Instead of knowing the partition of words into documents \textit{a priori}, the word-document assignment becomes a random hidden variable in SLDA. There is a generative procedure, where knowledge of spatial structure can be flexibly added as a prior, grouping visual words which are close in space into the same document. We use SLDA to discover objects from a collection of images, and show it achieves better performance than LDA.","Spatial Latent Dirichlet Allocation

Xiaogang Wang and Eric Grimson

Computer Science and Artiﬁcial Intelligence Lab

Massachusetts Institute of Technology, Cambridge, MA, 02139, USA

xgwang@csail.mit.edu, welg@csail.mit.edu

Abstract

In recent years, the language model Latent Dirichlet Allocation (LDA), which
clusters co-occurring words into topics, has been widely applied in the computer
vision ﬁeld. However, many of these applications have difﬁculty with modeling
the spatial and temporal structure among visual words, since LDA assumes that a
document is a “bag-of-words”. It is also critical to properly design “words” and
“documents” when using a language model to solve vision problems. In this pa-
per, we propose a topic model Spatial Latent Dirichlet Allocation (SLDA), which
better encodes spatial structures among visual words that are essential for solving
many vision problems. The spatial information is not encoded in the values of
visual words but in the design of documents. Instead of knowing the partition of
words into documents a priori, the word-document assignment becomes a random
hidden variable in SLDA. There is a generative procedure, where knowledge of
spatial structure can be ﬂexibly added as a prior, grouping visual words which are
close in space into the same document. We use SLDA to discover objects from a
collection of images, and show it achieves better performance than LDA.

1 Introduction

Latent Dirichlet Allocation (LDA) [1] is a language model which clusters co-occurring words into
topics. In recent years, LDA has been widely used to solve computer vision problems. For example,
LDA was used to discover objects from a collection of images [2, 3, 4] and to classify images into
different scene categories [5]. [6] employed LDA to classify human actions. In visual surveillance,
LDA was used to model atomic activities and interactions in a crowded scene [7]. In these ap-
plications, LDA clustered low-level visual words (which were image patches, spatial and temporal
interest points or moving pixels) into topics with semantic meanings (which corresponded to objects,
parts of objects, human actions or atomic activities) utilizing their co-occurrence information.
Even with these promising achievements, however, directly borrowing a language model to solve
vision problems has some difﬁculties. First, LDA assumes that a document is a bag of words,
such that spatial and temporal structures among visual words, which are meaningless in a language
model but important in many computer vision problems, are ignored. Second, users need to deﬁne
the meaning of “documents” in vision problems. The design of documents often implies some
assumptions on vision problems. For example, in order to cluster image patches, which are treated
as words, into classes of objects, researchers treated images as documents [2]. This assumes that
if two types of patches are from the same object class, they often appear in the same images. This
assumption is reasonable, but not strong enough. As an example shown in Figure 1, even though
sky is far from vehicles, if they often exist in the same images in some data set, they would be
clustered into the same topic by LDA. Furthermore, since in this image most of the patches are sky
and building, a patch on a vehicle is likely to be labeled as building or sky as well. These problems
could be solved if the document of a patch, such as the yellow patch in Figure 1, only includes other

1

Figure 1: There will be some problems (see text) if the whole image is treated as one document
when using LDA to discover classes of objects.

[9], and the correlated topic model

patches falling within its neighborhood, marked by the red dashed window in Figure 1, instead of
the whole image. So a better assumption is that if two types of image patches are from the same
object class, they are not only often in the same images but also close in space. We expect to utilize
spatial information in a ﬂexible way when designing documents for solving vision problems.
In this paper, we propose a Spatial Latent Dirichlet Allocation (SLDA) model which encodes the
spatial structure among visual words. It clusters visual words (e.g. an eye patch and a nose patch),
which often occur in the same images and are close in space, into one topic (e.g. face). This is
a more proper assumption for solving many vision problems when images often contain several
objects.
It is also easy for SLDA to model activities and human actions by encoding temporal
information. However the spatial or temporal information is not encoded in the values of visual
words, but in the design of documents. LDA and its extensions, such as the author-topic model [8],
the dynamic topic model
[10], all assume that the partition
of words into documents is known a priori. A key difference of SLDA is that the word-document
assignment becomes a hidden random variable. There is a generative procedure to assign words to
documents. When visual words are close in space or time, they have a high probability to be grouped
into the same document. Some approaches such as [11, 3, 12, 4] could also capture some spatial
structures among visual words.
[11] assumed that the spatial distribution of an object class could
be modeled as Gaussian and the number of objects in the image was known. Both [3] and [4] ﬁrst
roughly segmented images using graph cuts and added spatial constraint using these segments. [12]
modeled the spatial dependency among image patches as Markov random ﬁelds.
As an example application, we use the SLDA model to discover objects from a collection of images.
As shown in Figure 2, there are different classes of objects, such as cows, cars, faces, grasses,
sky, bicycles, etc., in the image set. And an image usually contains several objects of different
classes. The goal is to segment objects from images, and at the same time, to label these segments
as different object classes in an unsupervised way. It integrates object segmentation and recognition.
In our approach images are divided into local patches. A local descriptor is computed for each
image patch and quantized into a visual word. Using topic models, the visual words are clustered
into topics which correspond to object classes. Thus an image patch can be labeled as one of the
object classes. Our work is related to [2] which used LDA to cluster image patches. As shown in
Figure 2, SLDA achieves much better performance than LDA. We will compare more results of
LDA and SLDA in the experimental section.

2 Computation of Visual Words

To obtain the local descriptors, images are convolved with the ﬁlter bank proposed in [13], which is
a combination of 3 Gaussians, 4 Laplacian of Gaussians, and 4 ﬁrst order derivatives of Gaussians,
and was shown to have good performance for object categorization.
Instead of only computing
visual words at interest points as in [2], we divide an image into local patches on a grid and densely
sample a local descriptor for each patch. A codebook of size W is created by clustering all the
local descriptors in the image set using K-means. Each local patch is quantized into a visual word
according to the codebook. In the next step, these visual words (image patches) will be further
clustered into classes of objects. We will compare two clustering methods, LDA and SLDA.

2

Figure 2: Given a collection of images as shown in the ﬁrst row (which are selected from the MSRC
image dataset [13]), the goal is to segment images into objects and cluster these objects into different
classes. The second row uses manual segmentation and labeling as ground truth. The third row is
the LDA result and the fourth row is the SLDA result. Under the same labeling approach, image
patches marked in the same color are in one object cluster, but the meaning of colors changes across
different labeling methods.

3 LDA

When LDA is used to solve our problem, we treat local patches of images as words and the whole
image as a document. The graphical model of LDA is shown in Figure 3 (a). There are M docu-
ments (images) in the corpus. Each document j has Nj words (image patches). wji is the observed
value of word i in document j. All the words in the corpus will be clustered into K topics (classes
of objects). Each topic k is modeled as a multinomial distribution over the codebook. (cid:31) and φ are
Dirichlet prior hyperparameters. (cid:29)k, (cid:28) j, and zji are hidden variables to be inferred. The generative
process of LDA is:

1. For a topic k, a multinomial parameter (cid:29)k is sampled from Dirichlet prior (cid:29)k (cid:31) Dir(φ).
2. For a document j, a multinomial parameter (cid:28) j over the K topics is sampled from Dirichlet

3. For a word i in document j, a topic label zji is sampled from discrete distribution zji (cid:31)

prior (cid:28) j (cid:31) Dir((cid:31)).

Discrete((cid:28) j).

4. The value wji of word i in document j is sampled from the discrete distribution of topic

zji, wji (cid:31) Discrete((cid:29)zji).

zji can be sampled through a Gibbs sampling procedure which integrates out (cid:28) j and (cid:29)k [14].

p(zji = k(cid:124) z(cid:31) ji(cid:44) w(cid:44) (cid:31)(cid:44) φ) (cid:31)

(cid:30)

(cid:31)W

+ φwji

n(k)
(cid:31) ji(cid:44) wji
n(k)
(cid:31) ji(cid:44) w + φw

w=1

(cid:29) (cid:183)

(cid:31)K

(cid:30)

n(j)
(cid:31) ji(cid:44) k + (cid:31)k
n(j)
(cid:31) ji(cid:44) k(cid:31)+ (cid:31)k(cid:31)

k(cid:31)=1

(cid:29)

(1)

where n(k)
(cid:31) ji(cid:44) w is the number of words in the corpus with value w assigned to topic k excluding word
i in document j, and n(j)
(cid:31) ji(cid:44) k is the number of words in document j assigned to topic k excluding
word i in document j. Eq 1 is the product of two ratios: the probability of word wji under topic k
and the probability of topic k in document j. So LDA clusters the visual words often co-occurring
in the same images into one object class.
As shown by some examples in Figure 2 (see more results in the experimental section), there are
two problems in using LDA for object segmentation and recognition. The segmentation result is

3

Figure 3: Graphical model of LDA (a) and SLDA (b). See text for details.

noisy since spatial information is not considered. Although LDA assumes that one image contains
multiple topics, from experimental results we observe that the patches in the same image are likely
to have the same labels. Since the whole image is treated as one document, if one object class, e.g.
car in Figure 2, is dominant in the image, the second ratio in Eq 1 will lead to a large bias towards
the car class, and thus the patches of street are also likely to be labeled as car. This problem could
be solved if a local patch only considers its neighboring patches as being in the same document.

4 SLDA

We assume that if visual words are from the same class of objects, they not only often co-occur in the
same images but also are close in space. So we try to group image patches which are close in space
into the same documents. One straightforward way is to divide the image into regions as shown in
Figure 4 (a). Each region is treated as a document instead of the whole image. However, since these
regions are not overlapped, some patches, such as A (red patch) and B (cyan patch) in Figure 4 (a),
even though very close in space, are assigned to different documents. In Figure 4 (a), patch A on
the cow is likely to be labeled as grass, since most other patches in its document are grass. To solve
this problem, we may put many overlapped regions, each of which is a document, on the images as
shown in Figure 4 (b). If a patch is inside a region, it “could” belong to that document. Any two
patches whose distance is smaller than the region size “could” belong to the same document if the
regions are placed densely enough. We use the word “could” because each local patch is covered
by several regions, so we have to decide to which document it belongs. Different from the LDA
model, in which the word-document relationship is known a priori, we need a generative procedure
assigning words to documents. If two patches are closer in space, they have a higher probability
to be assigned to the same document since there are more regions covering both of them. Actually
we can go even further. As shown in Figure 4 (c), each document can be represented by a point
(marked by magenta circle) in the image, assuming its region covers the whole image. If an image
patch is close to a document, it has a high probability to be assigned to that document.
The graphical model is shown in Figure 3 (b). In SLDA, there are M documents and N words in the
corpus. A hidden variable di indicates which document word i is assigned to. For each document
j there is a hyperparameter cd
j is the index of the image where

j =(cid:0)gd
(cid:1) is the location of the document. For a word i, in addition to the

document j is placed and(cid:0)xd

(cid:1) known a priori. gd

j , xd

j , yd
j

j , yd
j

observed word value wi, its location (xi, yi) and image index gi are also observed and stored in
variable ci = (gi, xi, yi). The generative procedure of SLDA is:

1. For a topic k, a multinomial parameter φk is sampled from Dirichlet prior φk ∼ Dir(β).

4

Figure 4: There are several ways to add spatial information among image patches when designing
documents.
(a): Divide the image into regions without overlapping. Each region, marked by a
dashed window, corresponds to a document. Image patches inside the region are assigned to the
corresponding document. (b): densely put overlapped regions over images. One image patch is
covered by multiple regions. (c): Each document is associated with a point (marked in magenta
color). These points are densely placed over the image. If a image patch is close to a document, it
has a high probability to be assigned to that document.

2. For a document j, a multinomial parameter (cid:28) j over the K topics is sampled from Dirichlet
prior (cid:28) j (cid:31) Dir((cid:31)).
3. For a word (image patch) i, a random variable di is sampled from prior p(di(cid:124) σ) indicating
to which document word i is assigned. We choose p(di(cid:124) σ) as a uniform prior.
4. The image index and location of word i is sampled from distribution p(ci(cid:124) cd

choose this as a Gaussian kernel.

p((gi(cid:44) xi(cid:44) yi)(cid:124) (cid:28)gd

(cid:44) xd
di

(cid:44) yd
di

di

(cid:27)(cid:44) (cid:26)) (cid:31) πgd

(gi) exp

(cid:30)

(cid:26) 2
(cid:44) (cid:26)) = 0 if the word and the document are not in the same image.

di

(cid:26)

(cid:28)xd

di

di

(cid:27)2 +(cid:28)yd

di

(cid:27)2

(cid:44) (cid:26)). We may
(cid:30) yi

(cid:25)

(cid:30) xi

5. The topic label zi of word i is sampled from the discrete distribution of document di,

di

p(ci(cid:124) cd
zi (cid:31) Discrete((cid:28) di).

6. The value wi of word i is sampled from the discrete distribution of topic zi, wi

Discrete((cid:29)zi).

4.1 Gibbs Sampling

zi and di can be sampled through a Gibbs sampling procedure integrating out (cid:29)k and (cid:28) j. In SLDA
the conditional distribution of zi given di is the same as in LDA.

n(k)

(cid:30)

(cid:31)W

w=1

i(cid:44) wi
n(k)

+ φwi
i(cid:44) w + φw

(cid:29) (cid:183)

(cid:31)K

n(j)

(cid:30)

k(cid:31)=1

i(cid:44) k + (cid:31)k
n(j)

i(cid:44) k(cid:31)+ (cid:31)k(cid:31)

(cid:29) (2)

p(zi = k(cid:124) di = j(cid:44) d(cid:31)

i(cid:44) z(cid:31)

i(cid:44) w(cid:44) (cid:31)(cid:44) φ) (cid:31)

i(cid:44) w is the number of words in the corpus with value w assigned to topic k excluding word
i(cid:44) k is the number of words in document j assigned to topic k excluding word i. This is

where n(k)
i, and n(j)
easy to understand since if the word-document assignment is ﬁxed, SLDA is the same as LDA.
In addition, we also need to sample di from the conditional distribution given zi.

j(cid:31)(cid:125) (cid:44) (cid:31)(cid:44) φ(cid:44) σ(cid:44) (cid:26)(cid:27)

i(cid:124) di = j(cid:44) d(cid:31)

i(cid:44) (cid:31))

i(cid:44) d(cid:31)

i(cid:44) ci(cid:44)(cid:123) cd

i(cid:44) (cid:31)) is obtained by integrating out (cid:28) j(cid:31).

p(cid:28)di = j(cid:124) zi = k(cid:44) z(cid:31)
(cid:31) p (di = j(cid:124) σ) p(cid:28)ci(cid:124) cd
j (cid:44) (cid:26)(cid:27)p (zi = k(cid:44) z(cid:31)
(cid:23)
M(cid:24)
(cid:30)(cid:31)K
M(cid:24)
(cid:22) K

k(cid:31)=1 (cid:31)k(cid:31)
k(cid:31)=1 (cid:31)((cid:31)k(cid:31))

i(cid:44) (cid:31)) =

(cid:29)

j(cid:31)=1

j(cid:31)=1

(cid:31)

=

p((cid:28) j(cid:31)(cid:124) (cid:31))p(zj(cid:31)(cid:124) (cid:28) ji)d(cid:28) j(cid:31)

(cid:30)
(cid:22) K
(cid:30)(cid:31)K
k(cid:31) +(cid:31)K
k(cid:31)=1 (cid:31)
k(cid:31)=1 n(j(cid:31))

n(j(cid:31))
k(cid:31) + (cid:31)k(cid:31)

(cid:29)

k(cid:31)=1 (cid:31)k(cid:31)

(cid:29)(cid:46)

(cid:31)

p (zi = k(cid:44) z(cid:31)

i(cid:124) di = j(cid:44) d(cid:31)

p (zi = k(cid:44) z(cid:31)

i(cid:124) di = j(cid:44) d(cid:31)

5

(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:183)
tional distribution of di is

j , σ(cid:1) as a Gaussian kernel. Thus the condi-
We choose p (di = j|η) as a uniform prior and p(cid:0)ci|cd
p(cid:0)di = j|zi = k, z−i, d−i, ci,{cd
j(cid:48)}, α, β, η, σ(cid:1)
(cid:80)K

n(j)−i,k + αk

−(xd

j −xi)2

∝ δgd

j

(gi) · e

j −yi)2

+(yd
σ2

·

(3)

(cid:16)

(cid:17)

k(cid:48)=1

n(j)−i,k(cid:48) + αk(cid:48)

Word i is likely to be assigned to document j if they are in the same image, close in space and word
i has the same topic label as other words in document j. In real applications, we only care about the
distribution of zi while dj can be marginalized by simply ignoring its samples. From Eq 2 and 3,
we observed that a word tends to have the same topic label as other words in its document and words
closer in space are more likely to be assigned to the same documents. So essentially under SLDA a
word tends to be labeled as the same topic as other words close to it. This satisﬁes our assumption
that visual words from the same object class are closer in space.
Since we densely place many documents over one image, during Gibbs sampling some documents
are only assigned a few words and the distributions cannot be well estimated. To solve this problem
we replicate each image patch to get many particles. These particles have the same word value and
location but can be assigned to different documents and have different labels. Thus each document
will have enough samples of words to estimate the distributions.

4.2 Discussion

SLDA is a ﬂexible model intended to encode spatial structure among image patches and design
documents. If there is only one document placed over one image, SLDA simply reduces to LDA.
If p(ci|cd
j ) is an uniform distribution inside a local region, SLDA implements the scheme described
in Figure 4 (b). If these local regions are not overlapped, it is the case of Figure 4 (a). There are
also other possible ways to add spatial information by choosing different spatial priors p(ci|cd
j ). In
SLDA, the spatial information is used when designing documents. However the object class model
φk, simply a multinomial distribution over the codebook, has no spatial structure. So the objects of
a class could be in any shape and anywhere in the images, as long as they smoothly distribute in
j , it is easy for SLDA to encode temporal structure
space. By simply adding a time stamp to ci and cd
among visual words. So SLDA also can be applied to human action and activity analysis.

5 Experiments

We test LDA and SLDA on the MSRC image dataset [13] with 240 images. Our codebook size is
200 and the topic number is 15. In Figure 2, we show some examples of results using LDA and
SLDA. Colors are used indicate different topics. The results of LDA are noisy and within one image
most of the patches are labeled as one topic. SLDA achieves much better results than LDA. The
results are smoother and objects are well segmented. The detection rate and false alarm rate of four
classes, cows, cars, faces, and bicycles are shown in Table 1. They are counted in pixels. We use the
manual segmentation and labeling in [13] as ground truth.
The two models are also tested on a tiger video sequence with 252 frames. We treat all the frames
in the sequence as an image collection and ignore their temporal order. Figure 5 shows their results
on two sampled frames. Please see the result of the whole video sequence from our website [15].
Using LDA, usually there are one or two dominant topics distributed like noise in a frame. Topics
change as the video background changes. LDA cannot segment out any objects. SLDA clusters
image patches into tigers, rock, water, and grass. If we choose the topic of tiger, as shown in the last
row of Figure 5, all the tigers in the video can be segmented out.
6 Conclusion

We propose a novel Spatial Latent Dirichlet Allocation model which clusters co-occurring and spa-
tially neighboring visual words into the same topic. Instead of knowing word-document assignment
a priori, SLDA has a generative procedure partitioning visual words which are close in space into
the same documents. It is also easy to extend SLDA to including temporal information.

6

Figure 5: Discovering objects from a video sequence. The ﬁrst column shows two frames in the
video sequence. In the second column, we label the patches in the two frames as different topics
using LDA. The thrid column plots the topic labels using SLDA. The red color indicates the topic
of tigers. In the fourth column, we segment tigers out by choosing the topic marked in red.

Table 1: Detection(D) rate and False Alarm (FA) rate of LDA and SLDA on the MSRC data set

LDA(D)
SLDA(D)
LDA(FA)
SLDA(FA)

cows
0.3755
0.5662
0.5576
0.0334

cars
0.5552
0.6838
0.3963
0.2437

faces
0.7172
0.6973
0.5862
0.3714

bicycles
0.5563
0.5661
0.5285
0.4217

7 Acknowledgement

The authors wish to acknowledge DSO National Laboratory of Singapore for partially supporting
this research.

References
[1] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet allocation. Journal of Machine Learning Research,

3:993–1022, 2003.

[2] J. Sivic, B. C. Russell, A. A. Efros, A. Zisserman, and W. T. Freeman. Discovering object categories in

image collections. In Proc. ICCV, 2005.

[3] B. C. Russell, A. A. Efros, J. Sivic, W. T. Freeman, and A. Zisserman. Using multiple segmentations to

discover objects and their extent in image collections. In Proc. CVPR, 2006.

[4] L. Cao and L. Fei-Fei. Spatially coherent latent topic model for concurrent object segmentation and

classiﬁcation. In Proc. ICCV, 2007.

[5] L. Fei-Fei and P. Perona. A bayesian hierarchical model for learning natural scene categories. In Proc.

CVPR, 2005.

[6] J. C. Niebles, H. Wang, and L. Fei-Fei. Unsupervised learning of human action categories using spatial-

temporal words. In Proc. BMVC, 2006.

[7] X. Wang, X. Ma, and E. Grimson. Unsupervised activity perception by hierarchical bayesian models. In

Proc. CVPR, 2007.

[8] M. Rosen-Zvi, T. Grifﬁths, M. Steyvers, and P. Smyth. The author-topic model for authors and documents.

In Proc. of Uncertainty in Artiﬁcial Intelligence, 2004.

[9] D. Blei and J. Lafferty. Dynamic topic models. In Proc. ICML, 2006.
[10] D. Blei and J. Lafferty. Correlated topic models. In Proc. NIPS, 2006.
[11] E. B. Sudderth, A. Torralba, W. T. Freeman, and A. S. Willsky. Learning hierarchical models of scenes,

objects, and parts. In Proc. ICCV, 2005.

[12] J. Verbeek and B. Triggs. Region classiﬁcation with markov ﬁeld aspect models. In Proc. CVPR, 2007.

7

(a)

(b)

(c)

Figure 6: Examples of experimental results on the MSRC image data set. (a): original images; (b):
LDA results; (c) SLDA results.

[13] J. Winn, A. Criminisi, and T. Minka. Object categorization by learned universal visual dictionary. In

Proc. ICCV, 2005.

[14] T. Grifﬁths and M. Steyvers. Finding scientiﬁc topics. In Proc. of the National Academy of Sciences,

2004.

[15] http://people.csail.mit.edu/xgwang/slda.html.

8

"
472,2007,Progressive mixture rules are deviation suboptimal,"We consider the learning task consisting in predicting as well as the best function in a finite reference set G up to the smallest possible additive term. If R(g) denotes the generalization error of a prediction function g, under reasonable assumptions on the loss function (typically satisfied by the least square loss when the output is bounded), it is known that the progressive mixture rule g_n satisfies E R(g_n) < min_{g in G} R(g) + Cst (log|G|)/n where n denotes the size of the training set, E denotes the expectation wrt the training set distribution. This work shows that, surprisingly, for appropriate reference sets G, the deviation convergence rate of the progressive mixture rule is only no better than Cst / sqrt{n}, and not the expected Cst / n. It also provides an algorithm which does not suffer from this drawback.","Progressive mixture rules are deviation suboptimal

Jean-Yves Audibert

Willow Project - Certis Lab
ParisTech, Ecole des Ponts

77455 Marne-la-Vall´ee, France

audibert@certis.enpc.fr

Abstract

We consider the learning task consisting in predicting as well as the best function
in a ﬁnite reference set G up to the smallest possible additive term. If R(g) denotes
the generalization error of a prediction function g, under reasonable assumptions
on the loss function (typically satisﬁed by the least square loss when the output is
bounded), it is known that the progressive mixture rule ˆg satisﬁes

ER(ˆg) ≤ ming∈G R(g) + Cst log |G|n ,

(1)
where n denotes the size of the training set, and E denotes the expectation w.r.t.
the training set distribution.This work shows that, surprisingly, for appropriate
reference sets G, the deviation convergence rate of the progressive mixture rule is
no better than Cst /√n: it fails to achieve the expected Cst /n. We also provide
an algorithm which does not suffer from this drawback, and which is optimal in
both deviation and expectation convergence rates.

1 Introduction

Why are we concerned by deviations? The efﬁciency of an algorithm can be summarized by its
expected risk, but this does not precise the ﬂuctuations of its risk. In several application ﬁelds of
learning algorithms, these ﬂuctuations play a key role: in ﬁnance for instance, the bigger the losses
can be, the more money the bank needs to freeze in order to alleviate these possible losses. In this
case, a “good” algorithm is an algorithm having not only low expected risk but also small deviations.

Why are we interested in the learning task of doing as well as the best prediction function of a given
ﬁnite set? First, one way of doing model selection among a ﬁnite family of submodels is to cut the
training set into two parts, use the ﬁrst part to learn the best prediction function of each submodel
and use the second part to learn a prediction function which performs as well as the best of the
prediction functions learned on the ﬁrst part of the training set. This scheme is very powerful since
it leads to theoretical results, which, in most situations, would be very hard to prove without it. Our
work here is related to the second step of this scheme.

Secondly, assume we want to predict the value of a continuous variable, and that we have many
candidates for explaining it. An input point can then be seen as the vector containing the prediction
of each candidate. The problem is what to do when the dimensionality d of the input data (equiva-
lently the number of prediction functions) is much higher than the number of training points n. In
this setting, one cannot use linear regression and its variants in order to predict as well as the best
candidate up to a small additive term. Besides, (penalized) empirical risk minimization is doomed
to be suboptimal (see the second part of Theorem 2 and also [1]).

As far as the expected risk is concerned, the only known correct way of predicting as well as the
best prediction function is to use the progressive mixture rule or its variants. These algorithms are
introduced in Section 2 and their main good property is given in Theorem 1. In this work we prove
that they do not work well as far as risk deviations are concerned (see the second part of Theorem

1

3). We also provide a new algorithm for this ’predict as well as the best’ problem (see the end of
Section 4).

2 The progressive mixture rule and its variants

We assume that we observe n pairs of input-output denoted Z1 = (X1, Y1), . . . , Zn = (Xn, Yn)
and that each pair has been independently drawn from the same unknown distribution denoted P .
The input and output spaces are denoted respectively X and Y, so that P is a probability distribution
on the product space Z , X × Y. The quality of a (prediction) function g : X → Y is measured by
the risk (or generalization error):

R(g) = E(X,Y )∼P `[Y, g(X)],

where `[Y, g(X)] denotes the loss (possibly inﬁnite) incurred by predicting g(X) when the true
output is Y . We work under the following assumptions for the data space and the loss function
` : Y × Y → R ∪ {+∞}.
Main assumptions. The input space is assumed to be inﬁnite: |X| = +∞. The output space is
a non-trivial (i.e. inﬁnite) interval of R symmetrical w.r.t. some a ∈ R: for any y ∈ Y, we have
2a − y ∈ Y. The loss function is

• uniformly exp-concave: there exists λ > 0 such that for any y ∈ Y, the set (cid:8)y0 ∈ R :
7→ e−λ`(y,y0) is

`(y, y0) < +∞(cid:9) is an interval containing a on which the function y0
concave.

• symmetrical: for any y1, y2 ∈ Y, `(y1, y2) = `(2a − y1, 2a − y2),
• admissible: for any y, y0 ∈ Y∩]a; +∞[, `(y, 2a − y0) > `(y, y0),
• well behaved at center: for any y ∈ Y∩]a; +∞[, the function `y : y0 7→ `(y, y0) is twice
continuously differentiable on a neighborhood of a and `0y(a) < 0.

These assumptions imply that

for some ζ > 0.

• Y has necessarily one of the following form: ] − ∞; +∞[, [a − ζ; a + ζ] or ]a − ζ; a + ζ[
7→ `(y, y0) is
• for any y ∈ Y, from the exp-concavity assumption, the function `y : y0
convex on the interval on which it is ﬁnite1. As a consequence, the risk R is also a convex
function (on the convex set of prediction functions for which it is ﬁnite).

The assumptions were motivated by the fact that they are satisﬁed in the following settings:

we have a = (ymin + ymax)/2 and may take λ = 1/[2(ymax − ymin)2].

• least square loss with bounded outputs: Y = [ymin; ymax] and `(y1, y2) = (y1−y2)2. Then
• entropy loss: Y = [0; 1] and `(y1, y2) = y1 log(cid:0) y1
1−y2(cid:1). Note that
• exponential (or AdaBoost) loss: Y = [−ymax; ymax] and `(y1, y2) = e−y1y2. Then we
• logit loss: Y = [−ymax; ymax] and `(y1, y2) = log(1 + e−y1y2). Then we have a = 0 and

`(0, 1) = `(1, 0) = +∞. Then we have a = 1/2 and may take λ = 1.
have a = 0 and may take λ = e−y2

y2(cid:1) + (1 − y1) log (cid:0) 1−y1

max.

may take λ = e−y2

max.

Progressive indirect mixture rule. Let G be a ﬁnite reference set of prediction functions. Under the
previous assumptions, the only known algorithms satisfying (1) are the progressive indirect mixture
rules deﬁned below.
For any i ∈ {0, . . . , n}, the cumulative loss suffered by the prediction function g on the ﬁrst i pairs
of input-output is

1Indeed,

if ξ denotes the function e−λ`y , from Jensen’s inequality, for any probability distribution,

Σi(g) , Pi

j=1 `[Yj, g(Xj)],

E`y(Y ) = E(cid:0) − 1

λ log ξ(Y )(cid:1) ≥ − 1

λ log Eξ(Y ) ≥ − 1

λ log ξ(EY ) = `y(EY ).

2

where by convention we take Σ0 ≡ 0. Let π denote the uniform distribution on G. We deﬁne the
probability distribution ˆπi on G as

ˆπi ∝ e−λΣi · π
e−λΣi(g0)). This distribution concentrates
equivalently for any g ∈ G, ˆπi(g) = e−λΣi(g)/(Pg0∈G
on functions having low cumulative loss up to time i. For any i ∈ {0, . . . , n}, let ˆhi be a prediction
function such that

The progressive indirect mixture rule produces the prediction function

∀ (x, y) ∈ Z

`[y, ˆhi(x)] ≤ − 1

λ log Eg∼ˆπi e−λ`[y,g(x)].

(2)

ˆgpim = 1

n+1 Pn

i=0

ˆhi.

From the uniform exp-concavity assumption and Jensen’s inequality, ˆhi does exist since one may
take ˆhi = Eg∼ˆπi g. This particular choice leads to the progressive mixture rule, for which the
predicted output for any x ∈ X is

ˆgpm(x) = Pg∈G (cid:16) 1

n+1 Pn

i=0

e−λΣi (g)

P g0 ∈G e−λΣi (g0)(cid:17) g(x).

Consequently, any result that holds for any progressive indirect mixture rule in particular holds for
the progressive mixture rule.

The idea of a progressive mean of estimators has been introduced by Barron ([2]) in the context
of density estimation with Kullback-Leibler loss. The form ˆgpm is due to Catoni ([3]). It was also
independently proposed in [4]. The study of this procedure was made in density estimation and least
square regression in [5, 6, 7, 8]. Results for general losses can be found in [9, 10]. Finally, the
progressive indirect mixture rule is inspired by the work of Vovk, Haussler, Kivinen and Warmuth
[11, 12, 13] on sequential prediction and was studied in the “batch” setting in [10]. Finally, in the
upper bounds we state, e.g. Inequality (1), one should notice that there is no constant larger than 1
in front of ming∈G R(g), as opposed to some existing upper bounds (e.g. [14]). This work really
studies the behaviour of the excess risk, that is the random variable R(ˆg) − ming∈G R(g).
The largest integer smaller or equal to the logarithm in base 2 of x is denoted by blog2 xc .
3 Expectation convergence rate

The following theorem, whose proof is omitted, shows that the expectation convergence rate of any
progressive indirect mixture rule is (i) at least (log |G|)/n and (ii) cannot be uniformly improved,
even when we consider only probability distributions on Z for which the output has almost surely
two symmetrical values (e.g. {-1;+1} classication with exponential or logit losses).
Theorem 1 Any progressive indirect mixture rule satisﬁes

ER(ˆgpim) ≤ min
g∈G

R(g) + log |G|
λ(n+1) .

Let y1 ∈ Y −{a} and d be a positive integer. There exists a set G of d prediction functions such that:
for any learning algorithm, there exists a probability distribution generating the data for which

• the output marginal is supported by 2a − y1 and y1: P (Y ∈ {2a − y1; y1}) = 1,
• ER(ˆg) ≥ min
g∈G

R(g) + e−1κ(cid:0)1 ∧ blog2 |G|c

[`(y1, a) − `(y1, y)] > 0.

(cid:1), with κ , sup
y∈Y

n+1

The second part of Theorem 1 has the same (log |G|)/n rate as the lower bounds obtained in sequen-
tial prediction ([12]). From the link between sequential predictions and our “batch” setting with i.i.d.
data (see e.g. [10, Lemma 3]), upper bounds for sequential prediction lead to upper bounds for i.i.d.
data, and lower bounds for i.i.d. data leads to lower bounds for sequential prediction. The converse
of this last assertion is not true, so that the second part of Theorem 1 is not a consequence of the
lower bounds of [12].

3

The following theorem, whose proof is also omitted, shows that for appropriate set G: (i) the em-
pirical risk minimizer has a p(log |G|)/n expectation convergence rate, and (ii) any empirical risk
minimizer and any of its penalized variants are really poor algorithms in our learning task since their
expectation convergence rate cannot be faster than p(log |G|)/n (see [5, p.14] and [1] for results of
the same spirit). This last point explains the interest we have in progressive mixture rules.

Theorem 2 If B , supy,y0,y00∈Y [`(y, y0) − `(y, y00)] < +∞, then any empirical risk minimizer,
which produces a prediction function ˆgerm in argming∈G Σn, satisﬁes:
R(g) + Bq 2 log |G|n
.

ER(ˆgerm) ≤ min
g∈G

Let y1, ˜y1 ∈ Y∩]a; +∞[ and d be a positive integer. There exists a set G of d prediction functions
such that: for any learning algorithm producing a prediction function in G (e.g. ˆgerm) there exists a
probability distribution generating the data for which

• the output marginal is supported by 2a − y1 and y1: P (Y ∈ {2a − y1; y1}) = 1,
• ER(ˆg) ≥ min
g∈G

∧ 2(cid:17), with δ , `(y1, 2a − ˜y1) − `(y1, ˜y1) > 0.

8(cid:16)q blog2 |G|c

R(g) + δ

n

The lower bound of Theorem 2 also says that one should not use cross-validation. This holds for the
loss functions considered in this work, and not for, e.g., the classiﬁcation loss: `(y, y0) = 1y6=y0.

4 Deviation convergence rate

The following theorem shows that the deviation convergence rate of any progressive indirect mix-
ture rule is (i) at least 1/√n and (ii) cannot be uniformly improved, even when we consider only
probability distributions on Z for which the output has almost surely two symmetrical values (e.g.
{-1;+1} classication with exponential or logit losses).
Theorem 3 If B , supy,y0,y00∈Y [`(y, y0) − `(y, y00)] < +∞, then any progressive indirect mixture
rule satisﬁes: for any  > 0, with probability at least 1 −  w.r.t. the training set distribution, we
have

R(ˆgpim) ≤ min
g∈G

R(g) + Bq 2 log(2−1)

n+1 + log |G|

λ(n+1)

Let y1 and ˜y1 in Y∩]a; +∞[ such that `y1 is twice continuously differentiable on [a; ˜y1] and
`0y1 ( ˜y1) ≤ 0 and `00y1( ˜y1) > 0. Consider the prediction functions g1 ≡ ˜y1 and g2 ≡ 2a − ˜y1.
For any training set size n large enough, there exist  > 0 and a distribution generating the data
such that

• the output marginal is supported by y1 and 2a − y1
• with probability larger than , we have
R(ˆgpim) − min

g∈{g1,g2}

R(g) ≥ cq log(e−1)

n

where c is a positive constant depending only on the loss function, the symmetry parameter
a and the output values y1 and ˜y1.

Proof 1 See Section 5.

This result is quite surprising since it gives an example of an algorithm which is optimal in terms of
expectation convergence rate and for which the deviation convergence rate is (signiﬁcantly) worse
than the expectation convergence rate.

In fact, despite their popularity based on their unique expectation convergence rate, the progressive
mixture rules are not good algorithms since a long argument essentially based on convexity shows
that the following algorithm has both expectation and deviation convergence rate of order 1/n. Let

4

ˆgerm be the minimizer of the empirical risk among functions in G. Let ˜g be the minimizer of the
empirical risk in the star ˆG = ∪g∈G [g; ˆgerm]. The algorithm producing ˜g satisﬁes for some C > 0,
for any  > 0, with probability at least 1 −  w.r.t. the training set distribution, we have

R(˜g) ≤ min
g∈G

R(g) + C log(−1|G|)

.

n

This algorithm has also the beneﬁt of being parameter-free. On the contrary, in practice, one will
have recourse to cross-validation to tune the parameter λ of the progressive mixture rule.
To summarize, to predict as well as the best prediction function in a given set G, one should not
restrain the algorithm to produce its prediction function among the set G. The progressive mix-
ture rules satisfy this principle since they produce a prediction function in the convex hull of G.
This allows to achieve (log |G|)/n convergence rates in expectation. The proof of the lower bound
of Theorem 3 shows that the progressive mixtures overﬁt the data: the deviations of their excess
risk are not PAC bounded by C log(−1|G|)/n while an appropriate algorithm producing prediction
functions on the edges of the convex hull achieves the log(−1|G|)/n deviation convergence rate.
Future work might look at whether one can transpose this algorithm to the sequential prediction
setting, in which, up to now, the algorithms to predict as well as the best expert were dominated by
algorithms producing a mixture expert inside the convex hull of the set of experts.

5 Proof of Theorem 3

5.1 Proof of the upper bound

Let Zn+1 = (Xn+1, Yn+1) be an input-output pair independent from the training set Z1, . . . , Zn
and with the same distribution P . From the convexity of y0 7→ `(y, y0), we have

(3)
Now from [15, Theorem 1] (see also [16, Proposition 1]), for any  > 0, with probability at least
1 − , we have

R(ˆgpim) ≤ 1

n+1 Pn

i=0 R(ˆhi).

1

n+1 Pn

i=0 R(ˆhi) ≤ 1

n+1 Pn

i=0 `(cid:0)Yi+1, ˆh(Xi+1)(cid:1) + Bq log(−1)

2(n+1)

Using [12, Theorem 3.8] and the exp-concavity assumption, we have

Let ˜g ∈ argmin

i=0 `(cid:0)Yi+1, ˆh(Xi+1)(cid:1) ≤ min

Pn
G R. By Hoeffding’s inequality, with probability at least 1 − , we have

i=0 `(cid:0)Yi+1, g(Xi+1)(cid:1) + log |G|λ

g∈G Pn

1

n+1 Pn

i=0 `(cid:0)Yi+1, ˜g(Xi+1)(cid:1) ≤ R(˜g) + Bq log(−1)

2(n+1)

Merging (3), (4), (5) and (6), with probability at least 1 − 2, we get
i=0 `(cid:0)Yi+1, ˜g(Xi+1)(cid:1) + log |G|

R(ˆgpim) ≤

n+1 Pn

1

λ(n+1) + Bq log(−1)

2(n+1)

≤ R(˜g) + Bq 2 log(−1)

n+1 + log |G|
λ(n+1) .

(4)

(5)

(6)

5.2 Sketch of the proof of the lower bound

We cannot use standard tools like Assouad’s argument (see e.g. [17, Theorem 14.6]) because if it
were possible, it would mean that the lower bound would hold for any algorithm and in particular
for ˜g, and this is false. To prove that any progressive indirect mixture rule have no fast exponential
deviation inequalities, we will show that on some event with not too small probability, for most of
the i in {0, . . . , n}, π−λΣi concentrates on the wrong function.
The proof is organized as follows. First we deﬁne the probability distribution for which we will
prove that the progressive indirect mixture rules cannot have fast deviation convergence rates. Then
we deﬁne the event on which the progressive indirect mixture rules do not perform well. We lower
bound the probability of this excursion event. Finally we conclude by lower bounding R(ˆgpim) on
the excursion event.

Before starting the proof, note that from the “well behaved at center” and exp-concavity assump-
tions, for any y ∈ Y∩]a; +∞[, on a neighborhood of a, we have: `00y ≥ λ(`0y)2 and since `0y(a) < 0,
y1 and ˜y1 exist. Due to limited space, some technical computations have been removed.

5

5.2.1 Probability distribution generating the data and ﬁrst consequences.
Let γ ∈]0; 1] be a parameter to be tuned later. We consider a distribution generating the data such
that the output distribution satisﬁes for any x ∈ X

P (Y = y1|X = x) = (1 + γ)/2 = 1 − P (Y = y2|X = x),

where y2 = 2a− y1. Let ˜y2 = 2a− ˜y1. From the symmetry and admissibility assumptions, we have
`(y2, ˜y2) = `(y1, ˜y1) < `(y1, ˜y2) = `(y2, ˜y1). Introduce

We have

δ , `(y1, ˜y2) − `(y1, ˜y1) > 0.

(7)

R(g2) − R(g1) = 1+γ

2 [`(y1, ˜y2) − `(y1, ˜y1)] + 1−γ

(8)
Therefore g1 is the best prediction function in {g1, g2} for the distribution we have chosen. Introduce
Wj , 1Yj =y1 − 1Yj =y2 and Si , Pi
Σi(g2) − Σi(g1) = Pi

j=1 Wj. For any i ∈ {1, . . . , n}, we have
j=1[`(Yj, ˜y2) − `(Yj, ˜y1)] = Pi
The weight given by the Gibbs distribution π−λΣi to the function g1 is

2 [`(y2, ˜y2) − `(y2, ˜y1)] = γδ.

j=1 Wjδ = δ Si

π−λΣi(g1) =

e−λΣi(g1)

e−λΣi (g1)+e−λΣi(g2) =

1+eλ[Σi (g1)−Σi (g2)] =

1

1

1+e−λδSi .

(9)

5.2.2 An excursion event on which the progressive indirect mixture rules will not perform

well.

Equality (9) leads us to consider the event:

Eτ = (cid:8)∀i ∈ {τ, . . . , n}, Si ≤ −τ(cid:9),

with τ the smallest integer larger than (log n)/(λδ) such that n − τ is even (for convenience). We
have
(10)

log n

λδ ≤ τ ≤ log n

λδ + 2.

The event Eτ can be seen as an excursion event of the random walk deﬁned through the random
variables Wj = 1Yj =y1 − 1Yj =y2, j ∈ {1, . . . , n}, which are equal to +1 with probability (1 + γ)/2
and −1 with probability (1 − γ)/2.
From (9), on the event Eτ , for any i ∈ {τ, . . . , n}, we have
π−λΣi(g1) ≤ 1
n+1 .

(11)
This means that π−λΣi concentrates on the wrong function, i.e. the function g2 having larger risk
(see (8)).

5.2.3 Lower bound of the probability of the excursion event.

This requires to look at the probability that a slightly shifted random walk in the integer space has a
very long excursion above a certain threshold. To lower bound this probability, we will ﬁrst look at
the non-shifted random walk. Then we will see that for small enough shift parameter, probabilities
of shifted random walk events are close to the ones associated to the non-shifted random walk.
Let N be a positive integer. Let σ1, . . . , σN be N independent Rademacher variables: P(σi =
+1) = P(σi = −1) = 1/2. Let si , Pi
j=1 σi be the sum of the ﬁrst i Rademacher variables. We
start with the following lemma for sums of Rademacher variables (proof omitted).

Lemma 1 Let m and t be positive integers. We have

P(cid:0) max
1≤k≤N

sk ≥ t; sN 6= t;(cid:12)(cid:12)sN − t(cid:12)(cid:12) ≤ m(cid:1) = 2P(cid:0)t < sN ≤ t + m(cid:1)

(12)

Let σ01, . . . , σ0N be N independent shifted Rademacher variables to the extent that P(σ0i = +1) =
(1 + γ)/2 = 1 − P(σ0i = −1). These random variables satisfy the following key lemma (proof
omitted)

6

integer, we have

Lemma 2 For any set A ⊂ (cid:8)(1, . . . , N ) ∈ {−1, 1}n : (cid:12)(cid:12)PN
(cid:0)1 − γ2(cid:1)N/2

P(cid:8)(σ01, . . . , σ0N ) ∈ A(cid:9) ≥ (cid:16) 1−γ

1+γ(cid:17)M/2

i=1 i(cid:12)(cid:12) ≤ M(cid:9) where M is a positive
P(cid:8)(σ1, . . . , σN ) ∈ A(cid:9)

(13)

We may now lower bound the probability of the excursion event Eτ . Let M be an integer larger than
τ . We still use Wj , 1Yj =y1 − 1Yj =y2 for j ∈ {1, . . . , n}. By using Lemma 2 with N = n − 2τ ,
we obtain

P(Eτ ) ≥ P(cid:0)W1 = −1, . . . , W2τ = −1; ∀ 2τ < i ≤ n, Pi

j=2τ +1 Wj ≤ τ(cid:1)

= (cid:0) 1−γ
≥ (cid:0) 1−γ

2 (cid:1)2τ
2 (cid:1)2τ(cid:0) 1−γ

P(cid:0)∀ i ∈ {1, . . . , N} Pi
1+γ(cid:1)M/2(cid:0)1 − γ2(cid:1)

N

j=1 σ0j ≤ τ(cid:1)

2 P(cid:0)|sN| ≤ M ;∀ i ∈ {1, . . . , N}

si ≤ τ(cid:1)

By using Lemma 1, since τ ≤ M, the r.h.s. probability can be lower bounded, and after some
computations, we obtain

P(Eτ ) ≥ τ(cid:0) 1−γ

(14)
where we recall that τ have the order of log n, N = n − 2τ has the order of n and that γ > 0 and
M ≥ τ have to be appropriately chosen.
To control the probabilities of the r.h.s., we use Stirling’s formula

2 [P(sN = τ ) − P(sN = M )]

1+γ(cid:1)M/2(cid:0)1 − γ2(cid:1)

2 (cid:1)2τ(cid:0) 1−γ

N

nne−n√2πn e1/(12n+1) < n! < nne−n√2πn e1/(12n),

(15)

(16)

(17)

and get for any s ∈ [0; N ] such that N − s even,
πN (cid:16)1 − s2

P(sN = s) ≥ q 2

N 2(cid:17)− N

2 (cid:16) 1− s

N
1+ s

N (cid:17)

s
2

e− 1

6(N +s) − 1

6(N−s)

and similarly

P(sN = s) ≤ q 2

πN (cid:16)1 − s2

N 2(cid:17)− N

2 (cid:16) 1− s

N
1+ s

N (cid:17)

s
2

1

12N +1 .

e

These computations and (14) leads us to take M as the smallest integer larger than √n such that
n − M is even. Indeed, from (10), (16) and (17), we obtain limn→+∞ √n[P(sN = τ ) − P(sN =
M )] = c, where c = p2/π(cid:0)1 − e−1/2(cid:1) > 0. Therefore for n large enough we have

(18)
The last two terms of the r.h.s. of (18) leads us to take γ of order 1/√n up to possibly a logarithmic
term. We obtain the following lower bound on the excursion probability

P(Eτ ) ≥ cτ

1+γ(cid:1)M/2(cid:0)1 − γ2(cid:1)

2 (cid:1)2τ(cid:0) 1−γ

2√n(cid:0) 1−γ

N
2

Lemma 3 If γ = pC0(log n)/n with C0 a positive constant, then for any large enough n,

P(Eτ ) ≥ 1
nC0 .

5.2.4 Behavior of the progressive indirect mixture rule on the excursion event.
From now on, we work on the event Eτ . We have ˆgpim = (Pn
ˆhi)/(n + 1). We still use δ ,
`(y1, ˜y2)−`(y1, ˜y1) = `(y2, ˜y1)−`(y2, ˜y2). On the event Eτ , for any x ∈ X and any i ∈ {τ, . . . , n},
by deﬁnition of ˆhi, we have

i=0

`[y2, ˆhi(x)] − `(y2, ˜y2) ≤ − 1
= − 1
≤ − 1

e−λ{`[y2,g(x)]−`(y2, ˜y2)}
λ log Eg∼π−λΣi
λ log (cid:8)e−λδ + (1 − e−λδ)π−λΣi(g2)(cid:9)
λ log (cid:8)1 − (1 − e−λδ) 1

n+1(cid:9)

In particular, for any n large enough, we have `[y2, ˆhi(x)] − `(y2, ˜y2) ≤ Cn−1, with C > 0
independent from γ. From the convexity of the function y 7→ `(y2, y) and by Jensen’s inequality,
we obtain

`[y2, ˆgpim(x)] − `(y2, ˜y2) ≤ 1

n+1 Pn

i=0 `[y2, ˆhi(x)] − `(y2, ˜y2) ≤ τ δ

n+1 + Cn−1 < C1

log n

n

7

for some constant C1 > 0 independent from γ. Let us now prove that for n large enough, we have

˜y2 ≤ ˆgpim(x) ≤ ˜y2 + Cq log n

n ≤ ˜y1,

(19)

with C > 0 independent from γ.
From (19), we obtain
R(ˆgpim) − R(g1) = 1+γ
= 1+γ
= 1+γ
≥ γδ − (ˆgpim − ˜y2)|`0y1 ( ˜y2)|
≥ γδ − C2q log n
n ,

2 (cid:2)`(y1, ˆgpim) − `(y1, ˜y1)(cid:3) + 1−γ
2 (cid:2)`y1(ˆgpim) − `y1 ( ˜y1)(cid:3) + 1−γ
2 (cid:2)δ + `y1 (ˆgpim) − `y1( ˜y2)(cid:3) + 1−γ

2 (cid:2)`(y2, ˆgpim) − `(y2, ˜y1)(cid:3)
2 (cid:2)`y1(2a − ˆgpim) − `y1( ˜y2)(cid:3)
2 (cid:2) − δ + `y1 (2a − ˆgpim) − `y1 ( ˜y1)(cid:3)

(20)
with C2 independent from γ. We may take γ = 2C2
δ p(log n)/n and obtain: for n large enough,
on the event Eτ , we have R(ˆgpim) − R(g1) ≥ Cplog n/n. From Lemma 3, this inequality holds
with probability at least 1/nC4 for some C4 > 0. To conclude, for any n large enough, there exists
 > 0 s.t. with probability at least , R(ˆgpim) − R(g1) ≥ cq log(e−1)
. where c is a positive constant
depending only on the loss function, the symmetry parameter a and the output values y1 and ˜y1.

n

References

[1] G. Lecu´e. Suboptimality of penalized empirical risk minimization in classiﬁcation. In Proceedings of the

20th annual conference on Computational Learning Theory, 2007.

[2] A. Barron. Are bayes rules consistent in information? In T.M. Cover and B. Gopinath, editors, Open

Problems in Communication and Computation, pages 85–91. Springer, 1987.

[3] O. Catoni. A mixture approach to universal model selection. preprint LMENS 97-30, Available from

http://www.dma.ens.fr/edition/preprints/Index.97.html, 1997.

[4] A. Barron and Y. Yang. Information-theoretic determination of minimax rates of convergence. Ann. Stat.,

27(5):1564–1599, 1999.

[5] O. Catoni. Universal aggregation rules with exact bias bound. Preprint n.510, http://www.proba.

jussieu.fr/mathdoc/preprints/index.html\#1999, 1999.

[6] G. Blanchard. The progressive mixture estimator for regression trees. Ann. Inst. Henri Poincar´e, Probab.

Stat., 35(6):793–820, 1999.

[7] Y. Yang. Combining different procedures for adaptive regression. Journal of multivariate analysis,

74:135–161, 2000.

[8] F. Bunea and A. Nobel. Sequential procedures for aggregating arbitrary estimators of a conditional mean,

2005. Technical report.

[9] A. Juditsky, P. Rigollet, and A.B. Tsybakov. Learning by mirror averaging. Preprint n.1034, Laboratoire

de Probabilit´es et Mod`eles Al´eatoires, Universit´es Paris 6 and Paris 7, 2005.

[10] J.-Y. Audibert. A randomized online learning algorithm for better variance control. In Proceedings of the

19th annual conference on Computational Learning Theory, pages 392–407, 2006.

[11] V.G. Vovk. Aggregating strategies. In Proceedings of the 3rd annual workshop on Computational Learn-

ing Theory, pages 371–386, 1990.

[12] D. Haussler, J. Kivinen, and M. K. Warmuth. Sequential prediction of individual sequences under general

loss functions. IEEE Trans. on Information Theory, 44(5):1906–1925, 1998.

[13] V.G. Vovk. A game of prediction with expert advice. Journal of Computer and System Sciences, pages

153–173, 1998.

[14] M. Wegkamp. Model selection in nonparametric regression. Ann. Stat., 31(1):252–273, 2003.
[15] T. Zhang. Data dependent concentration bounds for sequential prediction algorithms. In Proceedings of

the 18th annual conference on Computational Learning Theory, pages 173–187, 2005.

[16] N. Cesa-Bianchi, A. Conconi, and C. Gentile. On the generalization ability of on-line learning algorithms.

IEEE Transactions on Information Theory, 50(9):2050–2057, 2004.

[17] L. Devroye, L. Gy¨orﬁ, and G. Lugosi. A Probabilistic Theory of Pattern Recognition. Springer-Verlag,

1996.

8

"
